Bayesian 
Reasoning 
and 
Machine 
Learning 


David 
Barber 
c

2007,2008,2009,2010 



Notation 
List 


V 


dom(x) 


x]= 
x 


p(x]= 
tr) 


p(x]= 
fa) 


p(x;]y) 


p(x]n 
y) 


p(x]. 
y) 


p(xjy) 


R

f(x)

x 


I[x]= 
y] 


pa 
(x) 


ch 
(x) 


ne 
(x) 


X 
.
??YjZ 


X>>YjZ 


dim 
x]

hf(x)ip(x) 


(a;]b) 


dim 
x 


](x]= 
s;]y]= 
t) 


D 


n]

N]

]x 


y 


S 


(x) 


erf(x) 


i]~ 
j]

Im 


II 


a 
calligraphic 
symbol 
typically 
denotes 
a 
set 
of 
random 
variables 
. 
. 
. 
. 
. 
. 
. 
. 
3 
Domain 
of 
a 
variable 
....................................................3 
The 
variable 
x]is 
in 
the 
state 
x 
..........................................3 
probability 
of 
event/variable 
x]being 
in 
the 
state 
true 
................... 
3 
probability 
of 
event/variable 
x]being 
in 
the 
state 
false 
...................3 
probability 
of 
x]and 
y]...................................................4 
probability 
of 
x]and 
y]...................................................4 
probability 
of 
x]or 
y].................................................... 
4 
The 
probability 
of 
x]conditioned 
on 
y]...................................4


R

For 
continuous 
variables 
this 
is 
shorthand 
forf(x)dx]and 
for 
discrete 
vari-

PR

ables 
means 
summation 
over 
the 
states 
of 
x, 
f(x) 
................... 
7 


x 


Indicator 
: 
has 
value 
1 
if 
x]= 
y, 
0 
otherwise 
............................11 
The 
parents 
of 
node 
x. 
................................................ 
19 
The 
children 
of 
node 
x. 
................................................19 
Neighbours 
of 
node 
x].................................................. 
20 
Variables 
X 
are 
independent 
of 
variables 
Y 
conditioned 
on 
variables 
Z. 
33 
Variables 
X 
are 
dependent 
on 
variables 
Y 
conditioned 
variables 
Z. 
.....33 
For 
a 
discrete 
variable 
x, 
this 
denotes 
the 
number 
of 
states 
x]can 
take 
..43 
The 
average 
of 
the 
function 
f(x) 
with 
respect 
to 
the 
distribution 
p(x). 
139 


Delta 
function. 
For 
discrete 
a, 
b, 
this 
is 
the 
Kronecker 
delta, 
a;b 
and 
for 
continuous 
a, 
b]the 
Dirac 
delta 
function 
(a]- 
b) 
......................142 


The 
dimension 
of 
the 
vector/matrix 
x. 
................................150The 
number 
of 
times 
variable 
x]is 
in 
state 
s]and 
y]in 
state 
t]simultaneously. 
172 


Dataset 
...............................................................251 
Data 
index 
........................................................... 
251 
Number 
of 
Dataset 
training 
points 
....................................251
The 
number 
of 
times 
variable 
x]is 
in 
state 
y].......................... 
265 
Sample 
Covariance 
matrix 
............................................283 
The 
logistic 
sigmoid 
1=(1 
+ 
exp(..x) 
..................................319 
The 
(Gaussian) 
error 
function 
........................................ 
319 
The 
set 
of 
unique 
neighbouring 
edges 
on 
a 
graph 
......................529 
The 
m]× 
m]identity 
matrix 
...........................................546 


DRAFT 
March 
9, 
2010 



Preface 


Machine 
Learning 


The 
last 
decade 
has 
seen 
considerable 
growth 
in 
interest 
in 
Articial 
Intelligence 
and 
Machine 
Learning. 
In 
the 
broadest 
sense, 
these 
elds 
aim 
to 
`learn 
something 
useful’ 
about 
the 
environment 
within 
which 
the 
organism 
operates. 
How 
gathered 
information 
is 
processed 
leads 
to 
the 
development 
of 
algorithms 
– 
how 
to 
process 
high 
dimensional 
data 
and 
deal 
with 
uncertainty. 
In 
the 
early 
stages 
of 
research 
in 
Machine 
Learning 
and 
related 
areas, 
similar 
techniques 
were 
discovered 
in 
relatively 
isolated 
research 
communities. 
Whilst 
not 
all 
techniques 
have 
a 
natural 
description 
in 
terms 
of 
probability 
theory, 
many 
do, 
and 
it 
is 
the 
framework 
of 
Graphical 
Models 
(a 
marriage 
between 
graph 
and 
probability 
theory) 
that 
has 
enabled 
the 
understanding 
and 
transference 
of 
ideas 
from 
statistical 
physics, 
statistics, 
machine 
learning 
and 
information 
theory. 
To 
this 
extent 
it 
is 
now 
reasonable 
to 
expect 
that 
machine 
learning 
researchers 
are 
familiar 
with 
the 
basics 
of 
statistical 
modelling 
techniques. 


This 
book 
concentrates 
on 
the 
probabilistic 
aspects 
of 
information 
processing 
and 
machine 
learning. 
Certainly 
no 
claim 
is 
made 
as 
to 
the 
correctness 
or 
that 
this 
is 
the 
only 
useful 
approach. 
Indeed, 
one 
might 
counter 
that 
this 
is 
unnecessary 
since 
\biological 
organisms 
don't 
use 
probability 
theory". 
Whether 
this 
is 
the 
case 
or 
not, 
it 
is 
undeniable 
that 
the 
framework 
of 
graphical 
models 
and 
probability 
has 
helped 
with 
the 
explosion 
of 
new 
algorithms 
and 
models 
in 
the 
machine 
learning 
community. 
One 
should 
also 
be 
clear 
that 
Bayesian 
viewpoint 
is 
not 
the 
only 
way 
to 
go 
about 
describing 
machine 
learning 
and 
information 
processing. 
Bayesian 
and 
probabilistic 
techniques 
really 
come 
into 
their 
own 
in 
domains 
where 
uncertainty 
is 
a 
necessary 
consideration. 


The 
structure 
of 
the 
book 


One 
aim 
of 
part 
I 
of 
the 
book 
is 
to 
encourage 
Computer 
Science 
students 
into 
this 
area. 
A 
particular 
diculty 
that 
many 
modern 
students 
face 
is 
a 
limited 
formal 
training 
in 
calculus 
and 
linear 
algebra, 
meaning 
that 
minutiae 
of 
continuous 
and 
high-dimensional 
distributions 
can 
turn 
them 
away. 
In 
beginning 
with 
probability 
as 
a 
form 
of 
reasoning 
system, 
we 
hope 
to 
show 
the 
reader 
how 
ideas 
from 
logical 
inference 
and 
dynamical 
programming 
that 
they 
may 
be 
more 
familiar 
with 
have 
natural 
parallels 
in 
a 
probabilistic 
context. 
In 
particular, 
Computer 
Science 
students 
are 
familiar 
with 
the 
concept 
of 
algorithms 
as 
core. 
However, 
it 
is 
more 
common 
in 
machine 
learning 
to 
view 
the 
model 
as 
core, 
and 
how 
this 
is 
implemented 
is 
secondary. 
From 
this 
perspective, 
understanding 
how 
to 
translate 
a 
mathematical 
model 
into 
a 
piece 
of 
computer 
code 
is 
central. 


Part 
II 
introduces 
the 
statistical 
background 
needed 
to 
understand 
continuous 
distributions 
and 
how 
learning 
can 
be 
viewed 
from 
a 
probabilistic 
framework. 
Part 
III 
discusses 
machine 
learning 
topics. 
Certainly 
some 
readers 
will 
raise 
an 
eyebrow 
to 
see 
their 
favourite 
statistical 
topic 
listed 
under 
machine 
learning. 
A 
dierence 
viewpoint 
between 
statistics 
and 
machine 
learning 
is 
what 
kinds 
of 
systems 
we 
would 
ultimately 


III 



like 
to 
construct 
(machines 
capable 
of 
`human/biological 
information 
processing 
tasks) 
rather 
than 
in 
some 
of 
the 
techniques. 
This 
section 
of 
the 
book 
is 
therefore 
what 
I 
feel 
would 
be 
useful 
for 
machine 
learners 
to 
know. 


Part 
IV 
discusses 
dynamical 
models 
in 
which 
time 
is 
explicitly 
considered. 
In 
particular 
the 
Kalman 
Filter 
is 
treated 
as 
a 
form 
of 
graphical 
model, 
which 
helps 
emphasise 
what 
the 
model 
is, 
rather 
than 
focusing 
on 
it 
as 
a 
`lter', 
as 
is 
more 
traditional 
in 
the 
engineering 
literature. 


Part 
V 
contains 
a 
brief 
introduction 
to 
approximate 
inference 
techniques, 
including 
both 
stochastic 
(Monte 
Carlo) 
and 
deterministic 
(variational) 
techniques. 


The 
references 
in 
the 
book 
are 
not 
generally 
intended 
as 
crediting 
authors 
with 
ideas, 
nor 
are 
they 
always 
to 
the 
most 
authoritative 
works. 
Rather, 
the 
references 
are 
largely 
to 
works 
which 
are 
at 
a 
level 
reasonably 
consistent 
with 
the 
book 
and 
which 
are 
readily 
available. 


Whom 
this 
book 
is 
for 


My 
primary 
aim 
was 
to 
write 
a 
book 
for 
nal 
year 
undergraduates 
and 
graduates 
without 
signicant 
experience 
in 
calculus 
and 
mathematics 
that 
gave 
an 
inroad 
into 
machine 
learning, 
much 
of 
which 
is 
currently 
phrased 
in 
terms 
of 
probabilities 
and 
multi-variate 
distributions. 
The 
aim 
was 
to 
encourage 
students 
that 
apparently 
unexciting 
statistical 
concepts 
are 
actually 
highly 
relevant 
for 
research 
in 
making 
intelligent 
systems 
that 
interact 
with 
humans 
in 
a 
natural 
manner. 
Such 
a 
research 
programme 
inevitably 
requires 
dealing 
with 
high-dimensional 
data, 
time-series, 
networks, 
logical 
reasoning, 
modelling 
and 
uncertainty. 


Other 
books 
in 
this 
area 


Whilst 
there 
are 
several 
excellent 
textbooks 
in 
this 
area, 
none 
currently 
meets 
the 
requirements 
that 
I 
personally 
need 
for 
teaching, 
namely 
one 
that 
contains 
demonstration 
code 
and 
gently 
introduces 
probability 
and 
statistics 
before 
leading 
on 
to 
more 
advanced 
topics 
in 
machine 
learning. 
This 
lead 
me 
to 
build 
on 
my 
lecture 
material 
from 
courses 
given 
at 
Aston, 
Edinburgh, 
EPFL 
and 
UCL 
and 
expand 
the 
demonstration 
software 
considerably. 
The 
book 
is 
due 
for 
publication 
by 
Cambridge 
University 
Press 
in 
2010. 


The 
literature 
on 
machine 
learning 
is 
vast, 
as 
is 
the 
overlap 
with 
the 
relevant 
areas 
of 
statistics, 
engineering 
and 
other 
physical 
sciences. 
In 
this 
respect, 
it 
is 
dicult 
to 
isolate 
particular 
areas, 
and 
this 
book 
is 
an 
attempt 
to 
integrate 
parts 
of 
the 
machine 
learning 
and 
statistics 
literature. 
The 
book 
is 
written 
in 
an 
informal 
style 
at 
the 
expense 
of 
rigour 
and 
detailed 
proofs. 
As 
an 
introductory 
textbook, 
topics 
are 
naturally 
covered 
to 
a 
somewhat 
shallow 
level 
and 
the 
reader 
is 
referred 
to 
more 
specialised 
books 
for 
deeper 
treatments. 
Amongst 
my 
favourites 
are: 


• 
Graphical 
models 
– 
Graphical 
models 
by 
S. 
Lauritzen, 
Oxford 
University 
Press, 
1996. 
– 
Bayesian 
Networks 
and 
Decision 
Graphs 
by 
F. 
Jensen 
and 
T. 
D. 
Nielsen, 
Springer 
Verlag, 
2007. 
– 
Probabilistic 
Networks 
and 
Expert 
Systems 
by 
R. 
G. 
Cowell, 
A. 
P. 
Dawid, 
S. 
L. 
Lauritzen 
and 
D. 
J. 
Spiegelhalter, 
Springer 
Verlag, 
1999. 
– 
Probabilistic 
Reasoning 
in 
Intelligent 
Systems 
by 
J. 
Pearl, 
Morgan 
Kaufmann, 
1988. 
– 
Graphical 
Models 
in 
Applied 
Multivariate 
Statistics 
by 
J. 
Whittaker, 
Wiley, 
1990. 
– 
Probabilistic 
Graphical 
Models: 
Principles 
and 
Techniques 
by 
D. 
Koller 
and 
N. 
Friedman, 
MIT 
Press, 
2009. 
• 
Machine 
Learning 
and 
Information 
Processing 
– 
Information 
Theory, 
Inference 
and 
Learning 
Algorithms 
by 
D. 
J. 
C. 
MacKay, 
Cambridge 
University 
Press, 
2003. 
DRAFT 
March 
9, 
2010 



– 
Pattern 
Recognition 
and 
Machine 
Learning 
by 
C. 
M. 
Bishop, 
Springer 
Verlag, 
2006. 
– 
An 
Introduction 
To 
Support 
Vector 
Machines, 
N. 
Cristianini 
and 
J. 
Shawe-Taylor, 
Cambridge 
University 
Press, 
2000. 
– 
Gaussian 
Processes 
for 
Machine 
Learning 
by 
C. 
E. 
Rasmussen 
and 
C. 
K. 
I. 
Williams, 
MIT 
press, 
2006. 
How 
to 
use 
this 
book 


Part 
I 
would 
be 
suitable 
for 
an 
introductory 
course 
on 
Graphical 
Models 
with 
a 
focus 
on 
inference. 
Part 
II 
contains 
enough 
material 
for 
a 
short 
lecture 
course 
on 
learning 
in 
probabilistic 
models. 
Part 
III 
is 
reasonably 
self-contained 
and 
would 
be 
suitable 
for 
a 
course 
on 
Machine 
Learning 
from 
a 
probabilistic 
perspective, 
particularly 
combined 
with 
the 
dynamical 
models 
material 
in 
part 
IV. 
Part 
V 
would 
be 
suitable 
for 
a 
short 
course 
on 
approximate 
inference. 


Accompanying 
code 


The 
MATLAB 
code 
is 
provided 
to 
help 
readers 
see 
how 
mathematical 
models 
translate 
into 
actual 
code. 
The 
code 
is 
not 
meant 
to 
be 
an 
industrial 
strength 
research 
tool, 
rather 
a 
reasonably 
lightweight 
toolbox 
that 
enables 
the 
reader 
to 
play 
with 
concepts 
in 
graph 
theory, 
probability 
theory 
and 
machine 
learning. 
In 
an 
attempt 
to 
retain 
readability, 
no 
extensive 
error 
and/or 
exception 
handling 
has 
been 
included. 
The 
code 
contains 
at 
the 
moment 
basic 
routines 
for 
manipulating 
discrete 
variable 
distributions, 
along 
with 
a 
set 
of 
routines 
that 
are 
more 
concerned 
with 
continuous 
variable 
machine 
learning. 
One 
could 
in 
principle 
extend 
the 
`graphical 
models’ 
part 
of 
the 
code 
considerably 
to 
support 
continuous 
variables. 
Limited 
support 
for 
continuous 
variables 
is 
currently 
provided 
so 
that, 
for 
example, 
inference 
in 
the 
linear 
dynamical 
system 
may 
be 
written 
in 
conducted 
of 
operations 
on 
Gaussian 
potentials. 
However, 
in 
general, 
potentials 
on 
continuous 
variables 
need 
to 
be 
manipulated 
with 
care 
and 
often 
specialised 
routines 
are 
required 
to 
ensure 
numerical 
stability. 


Acknowledgements 


Many 
people 
have 
helped 
this 
book 
along 
the 
way 
either 
in 
terms 
of 
reading, 
feedback, 
general 
insights, 
allowing 
me 
to 
present 
their 
work, 
or 
just 
plain 
motivation. 
Amongst 
these 
I 
would 
like 
to 
thank 
Massimiliano 
Pontil, 
Mark 
Herbster, 
John 
Shawe-Taylor, 
Vladimir 
Kolmogorov, 
Yuri 
Boykov, 
Tom 
Minka, 
Simon 
Prince, 
Silvia 
Chiappa, 
Bertrand 
Mesot, 
Robert 
Cowell, 
Ali 
Taylan 
Cemgil, 
David 
Blei, 
Jeff 
Bilmes, 
David 
Cohn, 
David 
Page, 
Peter 
Sollich, 
Chris 
Williams, 
Marc 
Toussaint, 
Amos 
Storkey, 
Zakria 
Hussain, 
Serafn 
Moral, 
Milan 
Studeny, 
Tristan 
Fletcher, 
Tom 
Furmston, 
Ed 
Challis 
and 
Chris 
Bracegirdle. 
I 
would 
also 
like 
to 
thank 
the 
many 
students 
that 
have 
helped 
improve 
the 
material 
during 
lectures 
over 
the 
years. 
I'm 
particularly 
grateful 
to 
Tom 
Minka 
for 
allowing 
parts 
of 
his 
Lightspeed 
toolbox 
to 
be 
bundled 
with 
the 
BRMLtoolbox 
and 
am 
similarly 
indebted 
to 
Taylan 
Cemgil 
for 
his 
GraphLayout 
package. 


A 
nal 
thankyou 
to 
my 
family 
and 
friends. 


Website 


The 
code 
along 
with 
an 
electronic 
version 
of 
the 
book 
is 
available 
from 


http://www.cs.ucl.ac.uk/staff/D.Barber/brml 


Instructors 
seeking 
solutions 
to 
the 
exercises 
can 
nd 
information 
at 
the 
website, 
along 
with 
additional 
teaching 
material. 
The 
website 
also 
contains 
a 
feedback 
form 
and 
errata 
list. 


DRAFT 
March 
9, 
2010 



DRAFT 
March 
9, 
2010 



Contents 


I 
Inference 
in 
Probabilistic 
Models 
1 


1 
Probabilistic 
Reasoning 
3 


1.1 
ProbabilityRefresher........................................ 
3 


1.1.1 
ProbabilityTables 
..................................... 
6 


1.1.2 
InterpretingConditionalProbability 
........................... 
7 


1.2 
ProbabilisticReasoning 
...................................... 
8 


1.3 
Prior,LikelihoodandPosterior 
.................................. 
10 


1.3.1 
Twodice:whatweretheindividualscores?....................... 
10 


1.4 
Furtherworkedexamples 
..................................... 
11 


1.5 
Code 
................................................. 
15 


1.5.1 
BasicProbabilitycode 
................................... 
15 


1.5.2 
Generalutilities 
...................................... 
16 


1.5.3 
Anexample 
......................................... 
17 


1.6 
Notes 
................................................ 
17 


1.7 
Exercises 
.............................................. 
17 


2 
Basic 
Graph 
Concepts 
19 


2.1 
Graphs................................................ 
19 


2.1.1 
Spanningtree........................................ 
21 


2.2 
NumericallyEncodingGraphs................................... 
21 


2.2.1 
Edgelist........................................... 
21 


2.2.2 
Adjacencymatrix 
..................................... 
21 


2.2.3 
Cliquematrix........................................ 
22 


2.3 
Code 
................................................. 
23 


2.3.1 
Utilityroutines 
....................................... 
23 


2.4 
Exercises 
.............................................. 
23 


3 
Belief 
Networks 
25 


3.1 
Probabilistic 
Inference 
in 
Structured 
Distributions 
. 
. 
. 
. 
. 
. 
. 
. 
. 
. 
. 
. 
. 
. 
. 
. 
. 
. 
. 
. 
. 
. 
. 
25 


3.2 
GraphicallyRepresentingDistributions.............................. 
26 


3.2.1 
ConstructingasimpleBeliefnetwork: 
wetgrass 
. 
. 
. 
. 
. 
. 
. 
. 
. 
. 
. 
. 
. 
. 
. 
. 
. 
. 
. 
. 
26 


3.2.2 
Uncertainevidence 
..................................... 
29 


3.3 
BeliefNetworks 
........................................... 
32 


3.3.1 
Conditionalindependence 
................................. 
33 


3.3.2 
Theimpactofcollisions 
.................................. 
34 


3.3.3 
d-Separation 
........................................ 
35 


3.3.4 
d-Connectionanddependence............................... 
36 


3.3.5 
Markovequivalenceinbeliefnetworks 
.......................... 
37 


3.3.6 
Beliefnetworkshavelimitedexpressibility 
. 
. 
. 
. 
. 
. 
. 
. 
. 
. 
. 
. 
. 
. 
. 
. 
. 
. 
. 
. 
. 
. 
. 
. 
39 


VII 



CONTENTS 
CONTENTS 


3.4 
Causality 
.............................................. 
39 


3.4.1 
Simpson'sparadox 
..................................... 
40 


3.4.2 
Inuencediagramsandthedo-calculus.......................... 
42 


3.4.3 
Learningthedirectionofarrows 
............................. 
43 


3.5 
ParameterisingBeliefNetworks 
.................................. 
43 


3.6 
FurtherReading 
.......................................... 
44 


3.7 
Code 
................................................. 
44 


3.7.1 
Naiveinferencedemo 
................................... 
44 


3.7.2 
Conditionalindependencedemo.............................. 
44 


3.7.3 
Utilityroutines 
....................................... 
44 


3.8 
Exercises 
.............................................. 
44 


4 
Graphical 
Models 
49 


4.1 
GraphicalModels.......................................... 
49 


4.2 
MarkovNetworks.......................................... 
50 


4.2.1 
Markovproperties 
..................................... 
51 


4.2.2 
Gibbsnetworks 
....................................... 
52 


4.2.3 
Markovrandomelds 
................................... 
53 


4.2.4 
Conditional 
independence 
using 
Markov 
networks 
. 
. 
. 
. 
. 
. 
. 
. 
. 
. 
. 
. 
. 
. 
. 
. 
. 
. 
. 
. 
53 


4.2.5 
LatticeModels 
....................................... 
54 


4.3 
ChainGraphicalModels 
...................................... 
55 


4.4 
ExpressivenessofGraphicalModels................................ 
56 


4.5 
FactorGraphs............................................ 
58 


4.5.1 
Conditionalindependenceinfactorgraphs. 
. 
. 
. 
. 
. 
. 
. 
. 
. 
. 
. 
. 
. 
. 
. 
. 
. 
. 
. 
. 
. 
. 
. 
59 


4.6 
Notes 
................................................ 
59 


4.7 
Code 
................................................. 
59 


4.8 
Exercises 
.............................................. 
59 


5 
Ecient 
Inference 
in 
Trees 
63 


5.1 
MarginalInference 
......................................... 
63 


5.1.1 
Variable 
elimination 
in 
a 
Markov 
chain 
and 
message 
passing 
. 
. 
. 
. 
. 
. 
. 
. 
. 
. 
. 
. 
. 
. 
63 


5.1.2 
Thesum-productalgorithmonfactorgraphs 
. 
. 
. 
. 
. 
. 
. 
. 
. 
. 
. 
. 
. 
. 
. 
. 
. 
. 
. 
. 
. 
. 
66 


5.1.3 
Computingthemarginallikelihood 
............................ 
69 


5.1.4 
Theproblemwithloops 
.................................. 
71 


5.2 
OtherFormsofInference 
..................................... 
71 


5.2.1 
Max-Product 
........................................ 
71 


5.2.2 
Finding 
the 
N 
mostprobablestates 
........................... 
73 


5.2.3 
Mostprobablepathandshortestpath 
.......................... 
75 


5.2.4 
Mixedinference....................................... 
77 


5.3 
InferenceinMultiply-ConnectedGraphs 
............................. 
78 


5.3.1 
Bucketelimination 
..................................... 
78 


5.3.2 
Loop-cutconditioning 
................................... 
79 


5.4 
MessagePassingforContinuousDistributions 
.......................... 
80 


5.5 
Notes 
................................................ 
80 


5.6 
Code 
................................................. 
81 


5.6.1 
Factorgraphexamples 
................................... 
81 


5.6.2 
Mostprobableandshortestpath 
............................. 
81 


5.6.3 
Bucketelimination 
..................................... 
81 


5.6.4 
MessagepassingonGaussians............................... 
82 


5.7 
Exercises 
.............................................. 
82 


DRAFT 
March 
9, 
2010 



CONTENTS 
CONTENTS 


6 
The 
Junction 
Tree 
Algorithm 
85 


6.1 
ClusteringVariables 
........................................ 
85 


6.1.1 
Reparameterisation..................................... 
85 


6.2 
CliqueGraphs 
........................................... 
86 


6.2.1 
Absorption 
......................................... 
87 


6.2.2 
Absorptionscheduleoncliquetrees............................ 
88 


6.3 
JunctionTrees 
........................................... 
88 


6.3.1 
Therunningintersectionproperty 
............................ 
89 


6.4 
Constructing 
a 
Junction 
Tree 
for 
Singly-Connected 
Distributions 
. 
. 
. 
. 
. 
. 
. 
. 
. 
. 
. 
. 
. 
. 
. 
92 


6.4.1 
Moralisation 
........................................ 
92 


6.4.2 
Formingthecliquegraph 
................................. 
92 


6.4.3 
Formingajunctiontreefromacliquegraph....................... 
92 


6.4.4 
Assigningpotentialstocliques 
.............................. 
92 


6.5 
Junction 
Trees 
for 
Multiply-Connected 
Distributions 
. 
. 
. 
. 
. 
. 
. 
. 
. 
. 
. 
. 
. 
. 
. 
. 
. 
. 
. 
. 
. 
. 
93 


6.5.1 
Triangulationalgorithms.................................. 
95 


6.6 
TheJunctionTreeAlgorithm 
................................... 
97 


6.6.1 
RemarksontheJTA.................................... 
98 


6.6.2 
Computing 
the 
normalisation 
constant 
of 
a 
distribution 
. 
. 
. 
. 
. 
. 
. 
. 
. 
. 
. 
. 
. 
. 
. 
. 
99 


6.6.3 
Themarginallikelihood 
.................................. 
99 


6.7 
FindingtheMostLikelyState...................................101 


6.8 
Reabsorption 
: 
Converting 
a 
Junction 
Tree 
to 
a 
Directed 
Network 
. 
. 
. 
. 
. 
. 
. 
. 
. 
. 
. 
. 
. 
. 
102 


6.9 
TheNeedForApproximations 
..................................103 


6.9.1 
Boundedwidthjunctiontrees 
...............................103 


6.10 
Code 
... 
. 
... 
... 
. 
... 
. 
... 
... 
. 
... 
... 
. 
... 
. 
... 
... 
. 
... 
. 
... 
..103 


6.10.1 
Utilityroutines 
.......................................103 


6.11Exercises 
..............................................104 


7 
Making 
Decisions 
107 


7.1 
ExpectedUtility 
..........................................107 


7.1.1 
Utilityofmoney 
......................................107 


7.2 
DecisionTrees............................................108 


7.3 
ExtendingBayesianNetworksforDecisions 
...........................111 


7.3.1 
Syntaxofinuencediagrams 
...............................111 


7.4 
SolvingInuenceDiagrams 
....................................115 


7.4.1 
Ecientinference 
.....................................115 


7.4.2 
Usingajunctiontree....................................116 


7.5 
MarkovDecisionProcesses.....................................120 


7.5.1 
Maximisingexpectedutilitybymessagepassing.....................120 


7.5.2 
Bellman'sequation 
.....................................121 


7.6 
TemporallyUnboundedMDPs 
..................................122 


7.6.1 
Valueiteration 
.......................................122 


7.6.2 
Policyiteration 
.......................................123 


7.6.3 
Acurseofdimensionality 
.................................124 


7.7 
ProbabilisticInferenceandPlanning 
...............................124 


7.7.1 
Non-stationaryMarkovDecisionProcess.........................124 


7.7.2 
Non-stationary 
probabilistic 
inference 
planner 
. 
. 
. 
. 
. 
. 
. 
. 
. 
. 
. 
. 
. 
. 
. 
. 
. 
. 
. 
. 
. 
. 
125 


7.7.3 
Stationaryplanner 
.....................................125 


7.7.4 
Utilitiesateachtimestep 
.................................127 


7.8 
FurtherTopics 
...........................................129 


7.8.1 
PartiallyobservableMDPs 
................................129 


7.8.2 
Restrictedutilityfunctions 
................................130 


7.8.3 
Reinforcementlearning 
..................................130 


7.9 
Code 
.................................................131 


7.9.1 
Sum/Maxunderapartialorder..............................131 


7.9.2 
Junctiontreesforinuencediagrams...........................131 


DRAFT 
March 
9, 
2010 



CONTENTS 
CONTENTS 


7.9.3 
Party-Friendexample 
...................................131 


7.9.4 
ChestClinicwithDecisions 
................................131 


7.9.5 
Markovdecisionprocesses 
.................................133 


7.10Exercises 
..............................................133 


II 
Learning 
in 
Probabilistic 
Models 
137 


8 
Statistics 
for 
Machine 
Learning: 
139 


8.1 
Distributions 
............................................139 


8.2 
Summarisingdistributions 
.....................................139 


8.2.1 
Estimatorbias 
.......................................142 


8.3 
DiscreteDistributions 
.......................................143 


8.4 
ContinuousDistributions 
.....................................144 


8.4.1 
Boundeddistributions 
...................................144 


8.4.2 
Unboundeddistributions..................................146 


8.5 
MultivariateDistributions 
.....................................147 


8.6 
MultivariateGaussian 
.......................................148 


8.6.1 
Conditioningassystemreversal..............................151 


8.6.2 
Completingthesquare 
...................................151 


8.6.3 
Gaussianpropagation 
...................................152 


8.6.4 
Whiteningandcentering..................................152 


8.6.5 
Maximumlikelihoodtraining 
...............................152 


8.6.6 
BayesianInferenceofthemeanandvariance 
......................153 


8.6.7 
Gauss-Gammadistribution 
................................155 


8.7 
ExponentialFamily.........................................155 


8.7.1 
Conjugatepriors 
......................................156 


8.8 
The 
Kullback-Leibler 
Divergence 
KL(qjp) 
............................157 


8.8.1 
Entropy 
...........................................157 


8.9 
Code 
.................................................158 


8.10Exercises 
..............................................158 


9 
Learning 
as 
Inference 
165 


9.1 
LearningasInference........................................165 


9.1.1 
Learningthebiasofacoin 
................................165 


9.1.2 
Makingdecisions 
......................................167 


9.1.3 
Acontinuumofparameters 
................................167 


9.1.4 
Decisionsbasedoncontinuousintervals 
.........................168 


9.2 
MaximumAPosterioriandMaximumLikelihood 
........................169 


9.2.1 
Summarisingtheposterior.................................169 


9.2.2 
Maximum 
likelihood 
and 
the 
empirical 
distribution 
. 
. 
. 
. 
. 
. 
. 
. 
. 
. 
. 
. 
. 
. 
. 
. 
. 
. 
. 
170 


9.2.3 
Maximumlikelihoodtrainingofbeliefnetworks 
.....................171 


9.3 
BayesianBeliefNetworkTraining 
.................................174 


9.3.1 
Globalandlocalparameterindependence 
........................174 


9.3.2 
LearningbinaryvariabletablesusingaBetaprior 
...................176 


9.3.3 
Learning 
multivariate 
discrete 
tables 
using 
a 
Dirichlet 
prior 
. 
. 
. 
. 
. 
. 
. 
. 
. 
. 
. 
. 
. 
. 
178 


9.3.4 
Parents 
...........................................179 


9.3.5 
Structurelearning 
.....................................180 


9.3.6 
Empiricalindependence 
..................................182 


9.3.7 
Networkscoring 
......................................184 


9.4 
MaximumLikelihoodforUndirectedmodels...........................185 


9.4.1 
Thelikelihoodgradient 
..................................186 


9.4.2 
DecomposableMarkovnetworks 
.............................187 


9.4.3 
Non-decomposableMarkovnetworks 
...........................188 


9.4.4 
ConstraineddecomposableMarkovnetworks 
......................189 


X 
DRAFT 
March 
9, 
2010 



CONTENTS 
CONTENTS 


9.4.5 
Iterativescaling.......................................192 


9.4.6 
Conditionalrandomelds 
.................................193 


9.4.7 
Pseudolikelihood......................................196 


9.4.8 
Learningthestructure 
...................................196 


9.5 
PropertiesofMaximumLikelihood 
................................196 


9.5.1 
Trainingassumingthecorrectmodelclass 
........................196 


9.5.2 
Trainingwhentheassumedmodelisincorrect......................197 


9.6 
Code 
.................................................197 


9.6.1 
PCalgorithmusinganoracle 
...............................197 


9.6.2 
Demoofempiricalconditionalindependence.......................197 


9.6.3 
BayesDirichletstructurelearning.............................198 


9.7 
Exercises 
..............................................198 


10 
Naive 
Bayes 
203 


10.1NaiveBayesandConditionalIndependence 
...........................203 


10.2 
EstimationusingMaximumLikelihood. 
. 
... 
. 
... 
... 
. 
... 
... 
. 
... 
. 
... 
...204 


10.2.1 
Binaryattributes 
......................................204 


10.2.2 
Multi-statevariables 
....................................207 


10.2.3 
Textclassication 
.....................................208 


10.3BayesianNaiveBayes 
.......................................208 


10.4TreeAugmentedNaiveBayes 
...................................210 


10.4.1 
Chow-LiuTrees.......................................210 


10.4.2 
LearningtreeaugmentedNaiveBayesnetworks 
.....................212 


10.5 
Code 
... 
. 
... 
... 
. 
... 
. 
... 
... 
. 
... 
... 
. 
... 
. 
... 
... 
. 
... 
. 
... 
..213 


10.6Exercises 
..............................................213 


11 
Learning 
with 
Hidden 
Variables 
217 


11.1 
HiddenVariablesandMissingData... 
. 
... 
. 
... 
... 
. 
... 
... 
. 
... 
. 
... 
...217 


11.1.1 
Why 
hidden/missing 
variables 
can 
complicate 
proceedings 
. 
. 
. 
. 
. 
. 
. 
. 
. 
. 
. 
. 
. 
. 
. 
217 


11.1.2 
Themissingatrandomassumption............................218 


11.1.3 
Maximumlikelihood 
....................................219 


11.1.4 
Identiabilityissues 
....................................219 


11.2 
ExpectationMaximisation 
. 
. 
... 
... 
. 
... 
. 
... 
... 
. 
... 
. 
... 
... 
. 
... 
...220 


11.2.1 
VariationalEM 
.......................................220 


11.2.2 
ClassicalEM 
........................................221 


11.2.3 
ApplicationtoBeliefnetworks 
..............................224 


11.2.4 
ApplicationtoMarkovnetworks 
.............................228 


11.2.5 
Convergence 
........................................229 


11.3 
ExtensionsofEM... 
... 
. 
... 
. 
... 
... 
. 
... 
. 
... 
... 
. 
... 
... 
. 
... 
. 
..229 


11.3.1 
PartialMstep 
.......................................229 


11.3.2 
PartialEstep........................................229 


11.4 
AFailureCaseforEM 
... 
. 
... 
... 
. 
... 
. 
... 
... 
. 
... 
... 
. 
... 
. 
... 
...230 


11.5 
VariationalBayes 
... 
. 
... 
... 
. 
... 
. 
... 
... 
. 
... 
... 
. 
... 
. 
... 
... 
. 
..231 


11.5.1 
EMisaspecialcaseofvariationalBayes.........................233 


11.5.2 
Factorisingtheparameterposterior............................233 


11.6BayesianMethodsandML-II 
...................................236 


11.7OptimisingtheLikelihoodbyGradientMethods 
........................236 


11.7.1 
Directedmodels 
......................................236 


11.7.2 
Undirectedmodels 
.....................................237 


11.8 
Code 
... 
. 
... 
... 
. 
... 
. 
... 
... 
. 
... 
... 
. 
... 
. 
... 
... 
. 
... 
. 
... 
..237 


11.9Exercises 
..............................................237 


DRAFT 
March 
9, 
2010 
XI 



CONTENTS 
CONTENTS 


12 
Bayesian 
Model 
Selection 
241 


12.1ComparingModelstheBayesianWay 
..............................241 


12.2 
Illustrations:cointossing 
. 
. 
... 
. 
... 
... 
. 
... 
... 
. 
... 
. 
... 
... 
. 
... 
. 
..242 


12.2.1 
Adiscreteparameterspace 
................................242 


12.2.2 
Acontinuousparameterspace...............................242 


12.3 
Occam's 
Razor 
and 
Bayesian 
Complexity 
Penalisation 
. 
. 
. 
. 
. 
. 
. 
. 
. 
. 
. 
. 
. 
. 
. 
. 
. 
. 
. 
. 
. 
244 


12.4Acontinuousexample:curvetting 
...............................245 


12.5 
ApproximatingtheModelLikelihood... 
... 
. 
... 
. 
.. 
. 
... 
. 
... 
... 
. 
... 
. 
..246 


12.5.1 
Laplace'smethod......................................246 


12.5.2 
Bayesinformationcriterion(BIC) 
............................247 


12.6Exercises 
..............................................247 


III 
Machine 
Learning 
249 


13 
Machine 
Learning 
Concepts 
251 


13.1StylesofLearning 
.........................................251 


13.1.1 
Supervisedlearning 
....................................251 


13.1.2 
Unsupervisedlearning 
...................................252 


13.1.3 
Anomalydetection 
.....................................253 


13.1.4 
Online(sequential)learning 
................................253 


13.1.5 
Interactingwiththeenvironment 
.............................253 


13.1.6 
Semi-supervisedlearning..................................254 


13.2SupervisedLearning 
........................................254 


13.2.1 
UtilityandLoss 
......................................254 


13.2.2 
What'sthecatch? 
.....................................255 


13.2.3 
Usingtheempiricaldistribution..............................255 


13.2.4 
Bayesiandecisionapproach 
................................258 


13.2.5 
Learning 
lower-dimensional 
representations 
in 
semi-supervised 
learning 
. 
. 
. 
. 
. 
. 
. 
. 
261 


13.2.6 
Featuresandpreprocessing 
................................262 


13.3BayesversusEmpiricalDecisions 
.................................262 


13.4RepresentingData 
.........................................263 


13.4.1 
Categorical 
.........................................263 


13.4.2 
Ordinal 
...........................................263 


13.4.3 
Numerical..........................................263 


13.5 
BayesianHypothesisTestingforOutcomeAnalysis 
. 
... 
. 
... 
... 
. 
... 
. 
... 
... 
.263 


13.5.1 
Outcomeanalysis......................................264 


13.5.2 
Hdiff 
:modellikelihood 
..................................265 


13.5.3 
Hsame 
:modellikelihood.. 
. 
... 
... 
. 
... 
. 
.. 
. 
... 
. 
... 
... 
. 
... 
. 
..265 


13.5.4 
Dependentoutcomeanalysis 
...............................266 


13.5.5 
Is 
classier 
A 
better 
than 
B? 
...............................268 


13.6 
Code 
... 
. 
... 
... 
. 
... 
. 
... 
... 
. 
... 
... 
. 
... 
. 
... 
... 
. 
... 
. 
... 
..269 


13.7Notes 
................................................270 


13.8Exercises 
..............................................270 


14 
Nearest 
Neighbour 
Classication 
273 


14.1 
DoAsYourNeighbourDoes. 
... 
... 
. 
... 
. 
... 
... 
. 
... 
. 
... 
... 
. 
... 
...273 


14.2 
K-NearestNeighbours 
.......................................274 


14.3 
A 
Probabilistic 
Interpretation 
of 
Nearest 
Neighbours 
. 
. 
. 
. 
. 
. 
. 
. 
. 
. 
. 
. 
. 
. 
. 
. 
. 
. 
. 
. 
. 
. 
275 


14.3.1 
Whenyournearestneighbourisfaraway 
........................277 


14.4 
Code 
... 
. 
... 
... 
. 
... 
. 
... 
... 
. 
... 
... 
. 
... 
. 
... 
... 
. 
... 
. 
... 
..277 


14.4.1 
UtilityRoutines 
......................................277 


14.4.2 
Demonstration 
.......................................277 


14.5Exercises 
..............................................277 


XII 
DRAFT 
March 
9, 
2010 



CONTENTS 
CONTENTS 


15 
Unsupervised 
Linear 
Dimension 
Reduction 
279 


15.1 
High-Dimensional 
Spaces 
– 
Low 
Dimensional 
Manifolds 
. 
. 
. 
. 
. 
. 
. 
. 
. 
. 
. 
. 
. 
. 
. 
. 
. 
. 
. 
. 
279 


15.2PrincipalComponentsAnalysis 
..................................279 


15.2.1 
Derivingtheoptimallinearreconstruction 
........................280 


15.2.2 
Maximumvariancecriterion................................282 


15.2.3 
PCAalgorithm 
.......................................282 


15.2.4 
PCAandnearestneighbours 
...............................284 


15.2.5 
CommentsonPCA.....................................285 


15.3HighDimensionalData 
......................................285 


15.3.1 
Eigen-decomposition 
for 
N<D 
. 
... 
... 
. 
... 
. 
... 
... 
. 
... 
... 
. 
...286 


15.3.2 
PCAviaSingularvaluedecomposition 
..........................286 


15.4LatentSemanticAnalysis 
.....................................287 


15.4.1 
LSAforinformationretrieval 
...............................288 


15.5 
PCAWithMissingData.. 
. 
... 
. 
... 
... 
. 
... 
. 
... 
... 
. 
... 
... 
. 
... 
. 
..289 


15.5.1 
Findingtheprincipaldirections..............................291 


15.5.2 
Collaborative 
ltering 
using 
PCA 
with 
missing 
data 
. 
. 
. 
. 
. 
. 
. 
. 
. 
. 
. 
. 
. 
. 
. 
. 
. 
. 
291 


15.6 
MatrixDecompositionMethods.. 
. 
... 
... 
. 
... 
. 
... 
... 
. 
... 
... 
. 
... 
. 
..292 


15.6.1 
Probabilisticlatentsemanticanalysis...........................292 


15.6.2 
Extensionsandvariations 
.................................295 


15.6.3 
ApplicationsofPLSA/NMF................................296 


15.7 
KernelPCA... 
... 
. 
... 
. 
... 
... 
. 
... 
... 
. 
... 
. 
... 
... 
. 
... 
. 
... 
..298 


15.8CanonicalCorrelationAnalysis 
..................................300 


15.8.1 
SVDformulation 
......................................301 


15.9Notes 
................................................301 
15.10Code 
... 
. 
... 
... 
. 
... 
. 
... 
... 
. 
... 
... 
. 
... 
. 
... 
... 
. 
... 
. 
... 
..301 
15.11Exercises 
..............................................301 


16 
Supervised 
Linear 
Dimension 
Reduction 
303 


16.1 
SupervisedLinearProjections 
.. 
. 
... 
. 
... 
... 
. 
... 
. 
... 
... 
. 
... 
... 
. 
...303 


16.2 
Fisher'sLinearDiscriminant. 
... 
... 
. 
... 
... 
. 
... 
. 
... 
... 
. 
... 
. 
... 
...303 


16.3CanonicalVariates 
.........................................305 


16.3.1 
Dealingwiththenullspace.................................307 


16.4Usingnon-GaussianDataDistributions 
.............................308 


16.5 
Code 
... 
. 
... 
... 
. 
... 
. 
... 
... 
. 
... 
... 
. 
... 
. 
... 
... 
. 
... 
. 
... 
..308 


16.6Exercises 
..............................................308 


17 
Linear 
Models 
311 


17.1Introduction:FittingAStraightLine 
..............................311 


17.2LinearParameterModelsforRegression 
.............................312 


17.2.1 
Vectoroutputs 
.......................................314 


17.2.2 
Regularisation 
.......................................314 


17.2.3 
Radialbasisfunctions 
...................................315 


17.3TheDualRepresentationandKernels 
..............................316 


17.3.1 
Regressioninthedual-space................................317 


17.3.2 
Positive 
denite 
kernels 
(covariance 
functions) 
. 
. 
. 
. 
. 
. 
. 
. 
. 
. 
. 
. 
. 
. 
. 
. 
. 
. 
. 
. 
. 
318 


17.4LinearParameterModelsforClassication 
...........................319 


17.4.1 
Logisticregression 
.....................................319 


17.4.2 
Maximumlikelihoodtraining 
...............................321 


17.4.3 
Beyondrstordergradientascent 
............................324 


17.4.4 
Avoidingovercondentclassication 
...........................324 


17.4.5 
Multipleclasses.......................................324 


17.5TheKernelTrickforClassication 
................................324 


17.6SupportVectorMachines 
.....................................325 


17.6.1 
Maximummarginlinearclassier 
.............................325 


17.6.2 
Usingkernels 
........................................328 


DRAFT 
March 
9, 
2010 
XIII 



CONTENTS 
CONTENTS 


17.6.3 
Performingtheoptimisation................................329 


17.6.4 
Probabilisticinterpretation 
................................329 


17.7 
SoftZero-OneLossforOutlierRobustness.. 
. 
... 
... 
. 
... 
. 
... 
... 
. 
... 
. 
...329 


17.8Notes 
................................................330 


17.9 
Code 
... 
. 
... 
... 
. 
... 
. 
... 
... 
. 
... 
... 
. 
... 
. 
... 
... 
. 
... 
. 
... 
..330 
17.10Exercises 
..............................................330 


18 
Bayesian 
Linear 
Models 
333 


18.1RegressionWithAdditiveGaussianNoise 
............................333 


18.1.1 
Bayesianlinearparametermodels 
............................334 


18.1.2 
Determininghyperparameters:ML-II 
..........................335 


18.1.3 
LearningthehyperparametersusingEM.........................336 


18.1.4 
Hyperparameter 
optimisation 
: 
using 
the 
gradient 
. 
. 
. 
. 
. 
. 
. 
. 
. 
. 
. 
. 
. 
. 
. 
. 
. 
. 
. 
337 


18.1.5 
Validationlikelihood 
....................................338 


18.1.6 
Prediction..........................................339 


18.1.7 
Therelevancevectormachine 
...............................339 


18.2Classication 
............................................340 


18.2.1 
Hyperparameteroptimisation 
...............................340 


18.2.2 
Laplaceapproximation...................................341 


18.2.3 
Makingpredictions.....................................342 


18.2.4 
Relevancevectormachineforclassication........................344 


18.2.5 
Multi-classcase.......................................345 


18.3 
Code 
... 
. 
... 
... 
. 
... 
. 
... 
... 
. 
... 
... 
. 
... 
. 
... 
... 
. 
... 
. 
... 
..345 


18.4Exercises 
..............................................345 


19 
Gaussian 
Processes 
347 


19.1Non-ParametricPrediction 
....................................347 


19.1.1 
Fromparametrictonon-parametric 
...........................347 


19.1.2 
From 
Bayesian 
linear 
models 
to 
Gaussian 
processes 
. 
. 
. 
. 
. 
. 
. 
. 
. 
. 
. 
. 
. 
. 
. 
. 
. 
. 
. 
348 


19.1.3 
Aprioronfunctions 
....................................349 


19.2 
GaussianProcessPrediction. 
... 
. 
... 
... 
. 
... 
. 
... 
... 
. 
... 
... 
. 
... 
. 
..350 


19.2.1 
Regressionwithnoisytrainingoutputs..........................350 


19.3 
CovarianceFunctions. 
... 
. 
... 
. 
... 
... 
. 
... 
. 
... 
... 
. 
... 
... 
. 
... 
. 
..351 


19.3.1 
Makingnewcovariancefunctionsfromold........................352 


19.3.2 
Stationarycovariancefunctions 
..............................353 


19.3.3 
Non-stationarycovariancefunctions 
...........................355 


19.4AnalysisofCovarianceFunctions 
.................................356 


19.4.1 
Smoothnessofthefunctions................................356 


19.4.2 
Mercerkernels 
.......................................356 


19.4.3 
Fourieranalysisforstationarykernels 
..........................358 


19.5GaussianProcessesforClassication 
...............................358 


19.5.1 
Binaryclassication 
....................................359 


19.5.2 
Laplace'sapproximation 
..................................359 


19.5.3 
Hyperparameteroptimisation 
...............................362 


19.5.4 
Multipleclasses.......................................362 


19.6FurtherReading 
..........................................362 


19.7 
Code 
... 
. 
... 
... 
. 
... 
. 
... 
... 
. 
... 
... 
. 
... 
. 
... 
... 
. 
... 
. 
... 
..362 


19.8Exercises 
..............................................363 


20 
Mixture 
Models 
365 


20.1 
DensityEstimationUsingMixtures... 
. 
... 
... 
. 
... 
. 
... 
... 
. 
... 
... 
. 
...365 


20.2ExpectationMaximisationforMixtureModels 
.........................366 


20.2.1 
Unconstraineddiscretetables 
...............................366 


20.2.2 
MixtureofproductofBernoullidistributions 
......................368 


20.3TheGaussianMixtureModel 
...................................370 


XIV 
DRAFT 
March 
9, 
2010 



CONTENTS 
CONTENTS 


20.3.1 
EMalgorithm........................................370 


20.3.2 
Practicalissues 
.......................................373 


20.3.3 
ClassicationusingGaussianmixturemodels 
......................373 


20.3.4 
TheParzenestimator 
...................................375 


20.3.5 
K-Means 
..........................................375 


20.3.6 
Bayesianmixturemodels 
.................................376 


20.3.7 
Semi-supervisedlearning..................................376 


20.4 
MixtureofExperts 
.. 
... 
. 
... 
. 
... 
... 
. 
... 
. 
... 
... 
. 
... 
. 
... 
... 
. 
..377 


20.5IndicatorModels 
..........................................378 


20.5.1 
Jointindicatorapproach:factorisedprior 
........................378 


20.5.2 
Jointindicatorapproach:Polyaprior 
..........................378 


20.6MixedMembershipModels 
....................................380 


20.6.1 
LatentDirichletallocation.................................380 


20.6.2 
Graphbasedrepresentationsofdata 
...........................381 


20.6.3 
Dyadicdata.........................................382 


20.6.4 
Monadicdata 
........................................383 


20.6.5 
Cliques 
and 
adjacency 
matrices 
for 
monadic 
binary 
data 
. 
. 
. 
. 
. 
. 
. 
. 
. 
. 
. 
. 
. 
. 
. 
. 
383 


20.7FurtherReading 
..........................................387 


20.8 
Code 
... 
. 
... 
... 
. 
... 
. 
... 
... 
. 
... 
... 
. 
... 
. 
... 
... 
. 
... 
. 
... 
..387 


20.9Exercises 
..............................................387 


21 
Latent 
Linear 
Models 
389 


21.1 
FactorAnalysis 
. 
... 
. 
... 
... 
. 
... 
. 
... 
... 
. 
... 
... 
. 
... 
. 
... 
... 
. 
..389 


21.1.1 
Findingtheoptimalbias..................................390 


21.2 
FactorAnalysis:MaximumLikelihood.. 
... 
. 
... 
. 
... 
... 
. 
... 
. 
... 
... 
. 
..391 


21.2.1 
Directlikelihoodoptimisation...............................391 


21.2.2 
Expectationmaximisation 
.................................394 


21.3 
Interlude:ModellingFaces. 
. 
... 
. 
... 
... 
. 
... 
... 
. 
... 
. 
... 
... 
. 
... 
. 
..395 


21.4ProbabilisticPrincipalComponentsAnalysis 
..........................397 


21.5CanonicalCorrelationAnalysisandFactorAnalysis 
......................398 


21.6IndependentComponentsAnalysis 
................................399 


21.7 
Code 
... 
. 
... 
... 
. 
... 
. 
... 
... 
. 
... 
... 
. 
... 
. 
... 
... 
. 
... 
. 
... 
..401 


21.8Exercises 
..............................................401 


22 
Latent 
Ability 
Models 
403 


22.1 
TheRaschModel... 
. 
... 
... 
. 
... 
. 
... 
... 
. 
... 
. 
... 
... 
. 
... 
... 
. 
..403 


22.1.1 
MaximumLikelihoodtraining...............................403 


22.1.2 
BayesianRaschmodels 
..................................404 


22.2CompetitionModels 
........................................404 


22.2.1 
Bradly-Terry-Lucemodel 
.................................404 


22.2.2 
Elorankingmodel 
.....................................406 


22.2.3 
GlickoandTrueSkill 
....................................406 


22.3 
Code 
... 
. 
... 
... 
. 
... 
. 
... 
... 
. 
... 
... 
. 
... 
. 
... 
... 
. 
... 
. 
... 
..407 


22.4Exercises 
..............................................407 


IV 
Dynamical 
Models 
409 


23 
Discrete-State 
Markov 
Models 
411 


23.1MarkovModels 
...........................................411 


23.1.1 
Equilibrium 
and 
stationary 
distribution 
of 
a 
Markov 
chain 
. 
. 
. 
. 
. 
. 
. 
. 
. 
. 
. 
. 
. 
. 
. 
412 


23.1.2 
FittingMarkovmodels...................................413 


23.1.3 
MixtureofMarkovmodels.................................414 


23.2HiddenMarkovModels 
......................................416 


23.2.1 
Theclassicalinferenceproblems 
.............................416 


DRAFT 
March 
9, 
2010 
XV 



CONTENTS 
CONTENTS 


23.2.2 
Filtering 
p(htjv1:t) 
.....................................417 


23.2.3 
Parallel 
smoothing 
p(htjv1:T 
) 
...............................418 


23.2.4 
Correctionsmoothing 
...................................418 


23.2.5 
Mostlikelyjointstate 
...................................420 


23.2.6 
Selflocalisationandkidnappedrobots 
..........................421 


23.2.7 
Naturallanguagemodels 
.................................422 


23.3LearningHMMs 
..........................................422 


23.3.1 
EMalgorithm........................................423 


23.3.2 
Mixtureemission 
......................................424 


23.3.3 
TheHMM-GMM 
......................................425 


23.3.4 
Discriminativetraining...................................425 


23.4 
RelatedModels 
. 
... 
. 
... 
... 
. 
... 
... 
. 
... 
. 
... 
... 
. 
... 
. 
... 
... 
. 
..426 


23.4.1 
Explicitdurationmodel 
..................................426 


23.4.2 
Input-OutputHMM 
....................................427 


23.4.3 
LinearchainCRFs 
.....................................428 


23.4.4 
DynamicBayesiannetworks................................430 


23.5 
Applications.. 
. 
... 
. 
... 
... 
. 
... 
. 
... 
... 
. 
... 
. 
... 
... 
. 
... 
... 
. 
..430 


23.5.1 
Objecttracking.......................................430 


23.5.2 
Automaticspeechrecognition 
...............................430 


23.5.3 
Bioinformatics 
.......................................431 


23.5.4 
Part-of-speechtagging 
...................................431 


23.6 
Code 
... 
. 
... 
... 
. 
... 
. 
... 
... 
. 
... 
... 
. 
... 
. 
... 
... 
. 
... 
. 
... 
..432 


23.7Exercises 
..............................................432 


24 
Continuous-state 
Markov 
Models 
437 


24.1ObservedLinearDynamicalSystems 
...............................437 


24.1.1 
Stationarydistributionwithnoise 
............................438 


24.2Auto-RegressiveModels 
......................................438 


24.2.1 
TraininganARmodel 
...................................439 


24.2.2 
ARmodelasanOLDS 
..................................440 


24.2.3 
Time-varyingARmodel 
..................................440 


24.3LatentLinearDynamicalSystems 
................................442 


24.4 
Inference. 
. 
... 
... 
. 
... 
... 
. 
... 
. 
... 
... 
. 
... 
. 
... 
... 
. 
... 
... 
. 
..443 


24.4.1 
Filtering...........................................444 


24.4.2 
Smoothing 
: 
Rauch-Tung-Striebel 
correction 
method 
. 
. 
. 
. 
. 
. 
. 
. 
. 
. 
. 
. 
. 
. 
. 
. 
. 
. 
446 


24.4.3 
Thelikelihood 
.......................................447 


24.4.4 
Mostlikelystate 
......................................448 


24.4.5 
TimeindependenceandRiccatiequations 
........................448 


24.5LearningLinearDynamicalSystems 
...............................449 


24.5.1 
Identiabilityissues 
....................................449 


24.5.2 
EMalgorithm........................................450 


24.5.3 
SubspaceMethods 
.....................................451 


24.5.4 
StructuredLDSs 
......................................452 


24.5.5 
BayesianLDSs 
.......................................452 


24.6SwitchingAuto-RegressiveModels 
................................452 


24.6.1 
Inference 
..........................................452 


24.6.2 
MaximumLikelihoodLearningusingEM 
........................453 


24.7 
Code 
... 
. 
... 
... 
. 
... 
. 
... 
... 
. 
... 
... 
. 
... 
. 
... 
... 
. 
... 
. 
... 
..454 


24.7.1 
Autoregressivemodels 
...................................455 


24.8Exercises 
..............................................455 


XVI 
DRAFT 
March 
9, 
2010 



CONTENTS 
CONTENTS 


25 
Switching 
Linear 
Dynamical 
Systems 
457 


25.1 
Introduction... 
... 
. 
... 
. 
... 
... 
. 
... 
... 
. 
... 
. 
... 
... 
. 
... 
. 
... 
..457 


25.2 
TheSwitchingLDS.. 
. 
... 
... 
. 
... 
... 
. 
... 
. 
... 
... 
. 
... 
. 
... 
... 
. 
..457 


25.2.1 
Exact 
inference 
is 
computationally 
intractable 
. 
. 
. 
. 
. 
. 
. 
. 
. 
. 
. 
. 
. 
. 
. 
. 
. 
. 
. 
. 
. 
. 
458 


25.3GaussianSumFiltering 
......................................458 


25.3.1 
Continuousltering 
....................................459 


25.3.2 
Discreteltering 
......................................461 


25.3.3 
The 
likelihood 
p(v1:T 
) 
...................................461 


25.3.4 
CollapsingGaussians....................................461 


25.3.5 
Relationtoothermethods.................................462 


25.4GaussianSumSmoothing 
.....................................462 


25.4.1 
Continuoussmoothing 
...................................464 


25.4.2 
Discretesmoothing.....................................464 


25.4.3 
Collapsingthemixture...................................464 


25.4.4 
Usingmixturesinsmoothing 
...............................465 


25.4.5 
Relationtoothermethods.................................466 


25.5ResetModels 
............................................468 


25.5.1 
APoissonresetmodel 
...................................470 


25.5.2 
HMM-reset 
.........................................471 


25.6 
Code 
... 
. 
... 
... 
. 
... 
. 
... 
... 
. 
... 
... 
. 
... 
. 
... 
... 
. 
... 
. 
... 
..472 


25.7Exercises 
..............................................472 


26 
Distributed 
Computation 
475 


26.1 
Introduction... 
... 
. 
... 
. 
... 
... 
. 
... 
. 
.. 
. 
... 
. 
... 
... 
. 
... 
. 
... 
..475 


26.2StochasticHopeldNetworks 
...................................475 


26.3 
LearningSequences.. 
. 
... 
... 
. 
... 
... 
. 
... 
. 
... 
... 
. 
... 
. 
... 
... 
. 
..476 


26.3.1 
Asinglesequence......................................476 


26.3.2 
Multiplesequences 
.....................................481 


26.3.3 
Booleannetworks......................................482 


26.3.4 
Sequencedisambiguation 
.................................482 


26.4 
TractableContinuousLatentVariableModels 
. 
... 
. 
... 
... 
. 
... 
. 
... 
... 
. 
...482 


26.4.1 
Deterministiclatentvariables 
...............................482 


26.4.2 
AnaugmentedHopeldnetwork 
.............................483 


26.5NeuralModels 
...........................................484 


26.5.1 
Stochasticallyspikingneurons...............................485 


26.5.2 
Hopeldmembranepotential 
...............................485 


26.5.3 
Dynamicsynapses 
.....................................486 


26.5.4 
Leakyintegrateandremodels..............................486 


26.6 
Code 
... 
. 
... 
... 
. 
... 
. 
... 
... 
. 
... 
... 
. 
... 
. 
... 
... 
. 
... 
. 
... 
..487 


26.7Exercises 
..............................................487 


V 
Approximate 
Inference 
489 


27 
Sampling 
491 


27.1 
Introduction... 
... 
. 
... 
. 
... 
... 
. 
... 
. 
.. 
. 
... 
. 
... 
... 
. 
... 
. 
... 
..491 


27.1.1 
Univariatesampling 
....................................492 


27.1.2 
Multi-variatesampling...................................493 


27.2 
AncestralSampling.. 
... 
. 
... 
... 
. 
... 
. 
... 
... 
. 
... 
. 
... 
... 
. 
... 
...494 


27.2.1 
Dealingwithevidence 
...................................494 


27.2.2 
PerfectsamplingforaMarkovnetwork 
.........................495 


27.3 
GibbsSampling. 
... 
. 
... 
... 
. 
... 
. 
... 
... 
. 
... 
... 
. 
... 
. 
... 
... 
. 
..495 


27.3.1 
GibbssamplingasaMarkovchain 
............................496 


27.3.2 
StructuredGibbssampling 
................................497 


27.3.3 
Remarks...........................................498 


DRAFT 
March 
9, 
2010 
XVII 



CONTENTS 
CONTENTS 


27.4MarkovChainMonteCarlo(MCMC) 
..............................499 


27.4.1 
Markovchains 
.......................................499 


27.4.2 
Metropolis-Hastingssampling 
...............................499 


27.5AuxiliaryVariableMethods 
....................................501 


27.5.1 
HybridMonteCarlo 
....................................502 


27.5.2 
Swendson-Wang 
......................................504 


27.5.3 
Slicesampling 
.......................................505 


27.6 
ImportanceSampling. 
... 
. 
... 
. 
... 
... 
. 
... 
. 
... 
... 
. 
... 
. 
... 
... 
. 
..506 


27.6.1 
Sequentialimportancesampling..............................508 


27.6.2 
Particle 
ltering 
as 
an 
approximate 
forward 
pass 
. 
. 
. 
. 
. 
. 
. 
. 
. 
. 
. 
. 
. 
. 
. 
. 
. 
. 
. 
. 
509 


27.7 
Code 
... 
. 
... 
... 
. 
... 
. 
... 
... 
. 
... 
... 
. 
... 
. 
... 
... 
. 
... 
. 
... 
..512 


27.8Exercises 
..............................................512 


28 
Deterministic 
Approximate 
Inference 
515 


28.1 
Introduction... 
... 
. 
... 
. 
... 
... 
. 
... 
. 
.. 
. 
... 
. 
... 
... 
. 
... 
. 
... 
..515 


28.2TheLaplaceapproximation 
....................................515 


28.3 
Properties 
of 
Kullback-Leibler 
Variational 
Inference 
. 
. 
. 
. 
. 
. 
. 
. 
. 
. 
. 
. 
. 
. 
. 
. 
. 
. 
. 
. 
. 
. 
516 


28.3.1 
Boundingthenormalisationconstant...........................516 


28.3.2 
Boundingthemarginallikelihood.............................517 


28.3.3 
GaussianapproximationsusingKLdivergence 
.....................517 


28.3.4 
Moment 
matching 
properties 
of 
minimising 
KL(pjq) 
..................518 


28.4 
Variational 
Bounding 
Using 
KL(qjp) 
...............................519 


28.4.1 
PairwiseMarkovrandomeld...............................519 


28.4.2 
Generalmeaneldequations 
...............................522 


28.4.3 
Asynchronous 
updating 
guarantees 
approximation 
improvement 
. 
. 
. 
. 
. 
. 
. 
. 
. 
. 
. 
522 


28.4.4 
Intractableenergy 
.....................................523 


28.4.5 
Structuredvariationalapproximation...........................524 


28.5 
Mutual 
Information 
Maximisation 
: 
A 
KL 
Variational 
Approach 
. 
. 
. 
. 
. 
. 
. 
. 
. 
. 
. 
. 
. 
. 
. 
524 


28.5.1 
Theinformationmaximisationalgorithm 
........................525 


28.5.2 
LinearGaussiandecoder..................................526 


28.6LoopyBeliefPropagation 
.....................................526 


28.6.1 
ClassicalBPonanundirectedgraph 
...........................527 


28.6.2 
LoopyBPasavariationalprocedure 
...........................527 


28.7 
ExpectationPropagation.. 
. 
... 
... 
. 
... 
. 
... 
... 
. 
... 
. 
... 
... 
. 
... 
...530 


28.8 
MAPforMRFs 
. 
... 
... 
. 
... 
. 
... 
... 
. 
... 
. 
... 
... 
. 
... 
... 
. 
... 
. 
..533 


28.8.1 
MAPassignment 
......................................533 


28.8.2 
AttractivebinaryMRFs 
..................................534 


28.8.3 
Pottsmodel.........................................536 


28.9FurtherReading 
..........................................538 
28.10Code 
... 
. 
... 
... 
. 
... 
. 
... 
... 
. 
... 
... 
. 
... 
. 
... 
... 
. 
... 
. 
... 
..538 
28.11Exercises 
..............................................538 


A 
Background 
Mathematics 
543 


A.1 
LinearAlgebra 
...........................................543 


A.1.1 
Vectoralgebra 
.......................................543 


A.1.2 
Thescalarproductasaprojection 
............................544 


A.1.3 
Linesinspace........................................544 


A.1.4 
Planesandhyperplanes 
..................................545 


A.1.5 
Matrices...........................................545 


A.1.6 
Lineartransformations...................................546 


A.1.7 
Determinants 
........................................547 


A.1.8 
Matrixinversion 
......................................548 


A.1.9 
Computingthematrixinverse...............................548 


A.1.10Eigenvaluesandeigenvectors 
...............................548 


A.1.11Matrixdecompositions... 
... 
. 
... 
. 
... 
... 
. 
... 
... 
. 
... 
. 
... 
...550 


XVIII 
DRAFT 
March 
9, 
2010 



CONTENTS 
CONTENTS 


A.2 
MatrixIdentities 
..........................................551 


A.3 
MultivariateCalculus 
.......................................551 


A.3.1 
Interpretingthegradientvector..............................552 


A.3.2 
Higherderivatives 
.....................................552 


A.3.3 
Chainrule..........................................553 


A.3.4 
Matrixcalculus 
.......................................553 


A.4 
Inequalities 
.............................................554 


A.4.1 
Convexity 
..........................................554 


A.4.2 
Jensen'sinequality 
.....................................554 


A.5 
Optimisation 
............................................555 


A.5.1 
Criticalpoints 
.......................................555 


A.6 
GradientDescent 
..........................................555 


A.6.1 
Gradientdescentwithxedstepsize 
...........................556 


A.6.2 
Gradientdescentwithmomentum 
............................556 


A.6.3 
Gradientdescentwithlinesearches............................557 


A.6.4 
Exactlinesearchcondition 
................................557 


A.7 
MultivariateMinimization:Quadraticfunctions.........................557 


A.7.1 
Minimising 
quadratic 
functions 
using 
line 
search 
. 
. 
. 
. 
. 
. 
. 
. 
. 
. 
. 
. 
. 
. 
. 
. 
. 
. 
. 
. 
557 


A.7.2 
Gram-Schmidt 
construction 
of 
conjugate 
vectors 
. 
. 
. 
. 
. 
. 
. 
. 
. 
. 
. 
. 
. 
. 
. 
. 
. 
. 
. 
. 
558 


A.7.3 
Theconjugatevectorsalgorithm 
.............................559 


A.7.4 
Theconjugategradientsalgorithm 
............................560 


A.7.5 
Newton'smethod......................................561 


A.7.6 
Quasi-Newtonmethods 
..................................561 


A.7 
Constrained 
Optimisation 
using 
Lagrange 
Multipliers 
. 
. 
. 
. 
. 
. 
. 
. 
. 
. 
. 
. 
. 
. 
. 
. 
. 
. 
. 
. 
. 
. 
562 


DRAFT 
March 
9, 
2010 



CONTENTS 
CONTENTS 


DRAFT 
March 
9, 
2010 



Part 
I 
Inference 
in 
Probabilistic 
Models 


1 



CHAPTER 
1 


Probabilistic 
Reasoning 


1.1 
Probability 
Refresher 
Variables, 
States 
and 
Notational 
Shortcuts 


Variables 
will 
be 
denoted 
using 
either 
upper 
case 
X 
or 
lower 
case 
x 
and 
a 
set 
of 
variables 
will 
typically 
be 
denoted 
by 
a 
calligraphic 
symbol, 
for 
example 
V 
= 
fa, 
B, 
c} 
. 


The 
domain 
of 
a 
variable 
x 
is 
written 
dom(x), 
and 
denotes 
the 
states 
x 
can 
take. 
States 
will 
typically 
be 
represented 
using 
sans-serif 
font. 
For 
example, 
for 
a 
coin 
c, 
we 
might 
have 
dom(c)= 
fheads, 
tails} 
and 
p(c 
= 
heads) 
represents 
the 
probability 
that 
variable 
c 
is 
in 
state 
heads. 


The 
meaning 
of 
p(state) 
will 
often 
be 
clear, 
without 
specic 
reference 
to 
a 
variable. 
For 
example, 
if 
we 
are 
discussing 
an 
experiment 
about 
a 
coin 
c, 
the 
meaning 
of 
p(heads) 
is 
clear 
from 
the 
context, 
being 
short-

PX

hand 
for 
p(c 
= 
heads). 
When 
summing 
(or 
performing 
some 
other 
operation) 
over 
a 
variable 
f(x), 
the 


x

PPX

interpretation 
is 
that 
all 
states 
of 
x 
are 
included, 
i.e. 
f(x) 
= 
f(x 
= 
s).

x 
s2dom(x) 


For 
our 
purposes, 
events 
are 
expressions 
about 
random 
variables, 
such 
as 
Two 
heads 
in 
6 
coin 
tosses. 
Two 
events 
are 
mutually 
exclusive 
if 
they 
cannot 
both 
simultaneously 
occur. 
For 
example 
the 
events 
The 
coin 
is 
heads 
and 
The 
coin 
is 
tails 
are 
mutually 
exclusive. 
One 
can 
think 
of 
dening 
a 
new 
variable 
named 
by 
the 
event 
so, 
for 
example, 
p(The 
coin 
is 
tails) 
can 
be 
interpreted 
as 
p(The 
coin 
is 
tails 
= 
true). 
We 
use 
p(x 
= 
tr) 
for 
the 
probability 
of 
event/variable 
x 
being 
in 
the 
state 
true 
and 
p(x 
= 
fa) 
for 
the 
probability 
of 
event/variable 
x 
being 
in 
the 
state 
false. 


The 
Rules 
of 
Probability 


Denition 
1 
(Rules 
of 
Probability 
(Discrete 
Variables)). 


The 
probability 
of 
an 
event 
x 
occurring 
is 
represented 
by 
a 
value 
between 
0 
and 
1. 


p(x) 
= 
1 
means 
that 
we 
are 
certain 
that 
the 
event 
does 
occur. 


Conversely, 
p(x) 
= 
0 
means 
that 
we 
are 
certain 
that 
the 
event 
does 
not 
occur. 


The 
summation 
of 
the 
probability 
over 
all 
the 
states 
is 
1:

X

p(x 
= 
x) 
= 
1 
(1.1.1) 


3 



Probability 
Refresher 


P

Such 
probabilities 
are 
normalised. 
We 
will 
usually 
more 
conveniently 
writep(x) 
= 
1. 


x 


Two 
events 
x 
and 
y 
can 
interact 
through 


p(x 
or 
y)= 
p(x)+ 
p(y) 
- 
p(x 
and 
y) 
(1.1.2) 


We 
will 
use 
the 
shorthand 
p(x, 
y) 
for 
p(x 
and 
y). 
Note 
that 
p(y, 
x)= 
p(x, 
y) 
and 
p(x 
or 
y)= 
p(y 
or 
x). 


Denition 
2 
(Set 
notation). 
An 
alternative 
notation 
in 
terms 
of 
set 
theory 
is 
to 
write 


p(x 
or 
y) 
= 
p(x 
. 
y);p(x, 
y) 
= 
p(x 
n 
y) 
(1.1.3) 


Denition 
3 
(Marginals). 
Given 
a 
joint 
distribution 
p(x, 
y) 
the 
distribution 
of 
a 
single 
variable 
is 
given 
by 


X

p(x)=p(x, 
y) 
(1.1.4) 


y 


Here 
p(x) 
is 
termed 
a 
marginal 
of 
the 
joint 
probability 
distribution 
p(x, 
y). 
The 
process 
of 
computing 
a 
marginal 
from 
a 
joint 
distribution 
is 
called 
marginalisation. 
More 
generally, 
one 
has 


X

p(x1;:::;xi..1;xi+1;:::;xn)=p(x1;:::;xn) 
(1.1.5) 


xi 


An 
important 
denition 
that 
will 
play 
a 
central 
role 
in 
this 
book 
is 
conditional 
probability. 


Denition 
4 
(Conditional 
Probability 
/ 
Bayes’ 
Rule). 
The 
probability 
of 
event 
x 
conditioned 
on 
knowing 
event 
y 
(or 
more 
shortly, 
the 
probability 
of 
x 
given 
y) 
is 
dened 
as 


p(x, 
y) 


p(xjy) 
= 
(1.1.6) 


p(y) 


If 
p(y) 
= 
0 
then 
p(xjy) 
is 
not 
dened. 


Probability 
Density 
Functions 


Denition 
5 
(Probability 
Density 
Functions). 
For 
a 
single 
continuous 
variable 
x, 
the 
probability 
density 
p(x) 
is 
dened 
such 
that 


p(x) 
= 
0 
(1.1.7)

Z8 


p(x)dx 
= 
1 
(1.1.8) 


..8 


DRAFT 
March 
9, 
2010 



Probability 
Refresher 


Zb 
p(a 
< 
x 
< 
b) 
=p(x)dx 
(1.1.9) 
a 
As 
shorthand 
we 
will 
sometimes 
writeRb 
p(x), 
particularly 
when 
we 
want 
an 
expression 
to 
be 
valid 
for 


x=a 


either 
continuous 
or 
discrete 
variables. 
The 
multivariate 
case 
is 
analogous 
with 
integration 
over 
all 
real 
space, 
and 
the 
probability 
that 
x 
belongs 
to 
a 
region 
of 
the 
space 
dened 
accordingly. 


For 
continuous 
variables, 
formally 
speaking, 
events 
are 
dened 
for 
the 
variable 
occurring 
within 
a 
dened 
region, 
for 
example 


Z1:7 
p(x 
26[..1, 
1:7]) 
=f(x)dx 
(1.1.10) 


..1 


where 
here 
f(x) 
is 
the 
probability 
density 
function 
(pdf) 
of 
the 
continuous 
random 
variable 
x. 
Unlike 
probabilities, 
probability 
densities 
can 
take 
positive 
values 
greater 
than 
1. 


Formally 
speaking, 
for 
a 
continuous 
variable, 
one 
should 
not 
speak 
of 
the 
probability 
that 
x 
=0:2 
since 
the 
probability 
of 
a 
single 
value 
is 
always 
zero. 
However, 
we 
shall 
often 
write 
p(x) 
for 
continuous 
variables, 
thus 
not 
distinguishing 
between 
probabilities 
and 
probability 
density 
function 
values. 
Whilst 
this 
may 


R

appear 
strange, 
the 
nervous 
reader 
may 
simply 
replace 
our 
p(X 
= 
x) 
notation 
forf(x)dx, 
where 
. 


x2. 


is 
a 
small 
region 
centred 
on 
x. 
This 
is 
well 
dened 
in 
a 
probabilistic 
sense 
and, 
in 
the 
limit 
. 
being 
very 
small, 
this 
would 
give 
approximately 
f(x). 
If 
we 
consistently 
use 
the 
same 
. 
for 
all 
occurrences 
of 
pdfs, 
then 
we 
will 
simply 
have 
a 
common 
prefactor 
. 
in 
all 
expressions. 
Our 
strategy 
is 
to 
simply 
ignore 
these 
values 
(since 
in 
the 
end 
only 
relative 
probabilities 
will 
be 
relevant) 
and 
write 
p(x). 
In 
this 
way, 
all 
the 
standard 
rules 
of 
probability 
carry 
over, 
including 
Bayes’ 
Rule. 


Interpreting 
Conditional 
Probability 


Imagine 
a 
circular 
dart 
board, 
split 
into 
20 
equal 
sections, 
labelled 
from 
1 
to 
20 
and 
Randy, 
a 
dart 
thrower 
who 
hits 
any 
one 
of 
the 
20 
sections 
uniformly 
at 
random. 
Hence 
the 
probability 
that 
a 
dart 
thrown 
by 
Randy 
occurs 
in 
any 
one 
of 
the 
20 
regions 
is 
p(region 
i)=1=20. 
A 
friend 
of 
Randy 
tells 
him 
that 
he 
hasn't 
hit 
the 
20 
region. 
What 
is 
the 
probability 
that 
Randy 
has 
hit 
the 
5 
region? 
Conditioned 
on 
this 
information, 
only 
regions 
1 
to 
19 
remain 
possible 
and, 
since 
there 
is 
no 
preference 
for 
Randy 
to 
hit 
any 
of 
these 
regions, 
the 
probability 
is 
1/19. 
The 
conditioning 
means 
that 
certain 
states 
are 
now 
inaccessible, 
and 
the 
original 
probability 
is 
subsequently 
distributed 
over 
the 
remaining 
accessible 
states. 
From 
the 
rules 
of 
probability 
: 


p(region 
5, 
not 
region 
20) 
p(region 
5) 
1=20 
1 


p(region 
5jnot 
region 
20)= 
= 
== 


p(not 
region 
20) 
p(not 
region 
20) 
19=20 
19 


giving 
the 
intuitive 
result. 
In 
the 
above 
p(region 
5, 
not 
region 
20) 
= 
p(region 
f5 
\61 
\62\;:::, 
\19g)= 
p(region 
5). 


An 
important 
point 
to 
clarify 
is 
that 
p(A 
= 
ajB 
= 
b) 
should 
not 
be 
interpreted 
as 
`Given 
the 
event 
B 
= 
b 
has 
occurred, 
p(A 
= 
ajB 
= 
b) 
is 
the 
probability 
of 
the 
event 
A 
= 
a 
occurring'. 
In 
most 
contexts, 
no 
such 
explicit 
temporal 
causality 
is 
implied1 
and 
the 
correct 
interpretation 
should 
be 
‘ 
p(A 
= 
ajB 
= 
b) 
is 
the 
probability 
of 
A 
being 
in 
state 
a 
under 
the 
constraint 
that 
B 
is 
in 
state 
b'. 


The 
relation 
between 
the 
conditional 
p(A 
= 
ajB 
= 
b) 
and 
the 
joint 
p(A 
= 
a;B 
= 
b) 
is 
just 
a 
normalisation 


PR

constant 
since 
p(A 
= 
a;B 
= 
b) 
is 
not 
a 
distribution 
in 
A 
– 
in 
other 
words, 
p(A 
= 
a;B 
= 
b)=1. 
To 
6

a

PR

make 
it 
a 
distribution 
we 
need 
to 
divide 
: 
p(A 
= 
a;B 
= 
b)=p(A 
= 
a;B 
= 
b) 
which, 
when 
summed 


a 


over 
a 
does 
sum 
to 
1. 
Indeed, 
this 
is 
just 
the 
denition 
of 
p(A 
= 
ajB 
= 
b). 


1We 
will 
discuss 
issues 
related 
to 
causality 
further 
in 
section(3.4). 


DRAFT 
March 
9, 
2010 



Probability 
Refresher 


Denition 
6 
(Independence). 


Events 
x 
and 
y 
are 
independent 
if 
knowing 
one 
event 
gives 
no 
extra 
information 
about 
the 
other 
event. 
Mathematically, 
this 
is 
expressed 
by 


p(x, 
y)= 
p(x)p(y) 
(1.1.11) 


Provided 
that 
p(x)66

= 
0 
and 
p(y)= 
0 
independence 
of 
x 
and 
y 
is 
equivalent 
to 


p(xjy)= 
p(x) 
,6p(yjx)= 
p(y) 
(1.1.12) 


If 
p(xjy)= 
p(x) 
for 
all 
states 
of 
x 
and 
y, 
then 
the 
variables 
x 
and 
y 
are 
said 
to 
be 
independent. 
If 


p(x, 
y)= 
kf(x)g(y) 
(1.1.13) 


for 
some 
constant 
k, 
and 
positive 
functions 
f() 
and 
g() 
then 
x 
and 
y 
are 
independent. 


Deterministic 
Dependencies 


Sometimes 
the 
concept 
of 
independence 
is 
perhaps 
a 
little 
strange. 
Consider 
the 
following 
: 
variables 
x 
and 
y 
are 
both 
binary 
(their 
domains 
consist 
of 
two 
states). 
We 
dene 
the 
distribution 
such 
that 
x 
and 
y 
are 
always 
both 
in 
a 
certain 
joint 
state: 


p(x 
= 
a;y 
= 
1)=1 


p(x 
= 
a;y 
= 
2)=0 


p(x 
= 
b;y 
= 
2)=0 


p(x 
= 
b;y 
= 
1)=0 
Are 
x 
and 
y 
dependent? 
The 
reader 
may 
show 
that 
p(x 
= 
a) 
= 
1, 
p(x 
= 
b) 
= 
0 
and 
p(y 
= 
1) 
= 
1, 
p(y 
= 
2) 
= 
0. 
Hence 
p(x)p(y)= 
p(x, 
y) 
for 
all 
states 
of 
x 
and 
y, 
and 
x 
and 
y 
are 
therefore 
independent. 
This 
may 
seem 
strange 
– 
we 
know 
for 
sure 
the 
relation 
between 
x 
and 
y, 
namely 
that 
they 
are 
always 
in 
the 
same 
joint 
state, 
yet 
they 
are 
independent. 
Since 
the 
distribution 
is 
trivially 
concentrated 
in 
a 
single 
joint 
state, 
knowing 
the 
state 
of 
x 
tells 
you 
nothing 
that 
you 
didn't 
anyway 
know 
about 
the 
state 
of 
y, 
and 
vice 
versa. 
This 
potential 
confusion 
comes 
from 
using 
the 
term 
`independent’ 
which, 
in 
English, 
suggests 
that 
there 
is 
no 
inuence 
or 
relation 
between 
objects 
discussed. 
The 
best 
way 
to 
think 
about 
statistical 
independence 
is 
to 
ask 
whether 
or 
not 
knowing 
the 
state 
of 
variable 
y 
tells 
you 
something 
more 
than 
you 
knew 
before 
about 
variable 
x, 
where 
`knew 
before’ 
means 
working 
with 
the 
joint 
distribution 
of 
p(x, 
y) 
to 
gure 
out 
what 
we 
can 
know 
about 
x, 
namely 
p(x). 


1.1.1 
Probability 
Tables 
Based 
on 
the 
populations 
60776238, 
5116900 
and 
2980700 
of 
England 
(E), 
Scotland 
(S) 
and 
Wales 
(W), 
the 
a 
priori 
probability 
that 
a 
randomly 
selected 
person 
from 
these 
three 
countries 
would 
live 
in 
England, 
Scotland 
or 
Wales, 
would 
be 
approximately 
0:88, 
0:08 
and 
0:04 
respectively. 
We 
can 
write 
this 
as 
a 
vector 
(or 
probability 
table) 
: 


. 
10. 


p(Cnt 
= 
E)0:88 
. 
p(Cnt 
= 
S) 
. 
= 
. 
0:08 
. 
(1.1.14) 


p(Cnt 
= 
W)0:04 
whose 
component 
values 
sum 
to 
1. 
The 
ordering 
of 
the 
components 
in 
this 
vector 
is 
arbitrary, 
as 
long 
as 
it 
is 
consistently 
applied. 


DRAFT 
March 
9, 
2010 



Probability 
Refresher 


For 
the 
sake 
of 
simplicity, 
let's 
assume 
that 
only 
three 
Mother 
Tongue 
languages 
exist 
: 
English 
(Eng), 
Scottish 
(Scot) 
and 
Welsh 
(Wel), 
with 
conditional 
probabilities 
given 
the 
country 
of 
residence, 
England 
(E), 
Scotland 
(S) 
and 
Wales 
(W). 
We 
write 
a 
(ctitious) 
conditional 
probability 
table 


p(MT 
= 
EngjCnt 
= 
E)=0:95 
p(MT 
= 
ScotjCnt 
= 
E)=0:04 
p(MT 
= 
WeljCnt 
= 
E)=0:01 


p(MT 
= 
EngjCnt 
= 
S)=0:7 
p(MT 
= 
ScotjCnt 
= 
S)=0:3 
p(MT 
= 
WeljCnt 
= 
S)=0:0 


p(MT 
= 
EngjCnt 
= 
W)=0:6 
p(MT 
= 
ScotjCnt 
= 
W)=0:0 
p(MT 
= 
WeljCnt 
= 
W)=0:4 


(1.1.15) 
From 
this 
we 
can 
form 
a 
joint 
distribution 
p(Cnt, 
MT 
)= 
p(MT 
jCnt)p(Cnt). 
This 
could 
be 
written 
as 
a 
3 
63 
matrix 
with 
(say) 
rows 
indexed 
by 
country 
and 
columns 
indexed 
by 
Mother 
Tongue: 


0R0:95 
60:88 
0:7 
60:08 
0:6 
60:04 
1R0R0:836 
0:056 
0:024 
1R@R0:04 
60:88 
0:3 
60:08 
0:0 
60:04 
AR= 
@R0:0352 
0:024 
0 
AR(1.1.16) 
0:01 
60:88 
0:0 
60:08 
0:4 
60:04 
0:0088 
0 
0:016 


The 
joint 
distribution 
contains 
all 
the 
information 
about 
the 
model 
of 
this 
environment. 
By 
summing 
a 
column 
of 
this 
table, 
we 
have 
the 
marginal 
p(Cnt). 
Summing 
the 
row 
gives 
the 
marginal 
p(MT 
). 
Similarly, 
one 
could 
easily 
infer 
p(CntjMT 
) 
/6p(CntjMT 
)p(MT 
) 
from 
this 
joint 
distribution. 


For 
joint 
distributions 
over 
a 
larger 
number 
of 
variables, 
xi;i 
=1;:::;D, 
with 
each 
variable 
xi 
taking

QD

Ki 
states, 
the 
table 
describing 
the 
joint 
distribution 
is 
an 
array 
with 
Ki 
entries. 
Explicitly 
storing 


i=1 


tables 
therefore 
requires 
space 
exponential 
in 
the 
number 
of 
variables, 
which 
rapidly 
becomes 
impractical 
for 
a 
large 
number 
of 
variables. 


A 
probability 
distribution 
assigns 
a 
value 
to 
each 
of 
the 
joint 
states 
of 
the 
variables. 
For 
this 
reason, 
p(T, 
J, 
R, 
S) 
is 
considered 
equivalent 
to 
p(J, 
S, 
R, 
T 
) 
(or 
any 
such 
reordering 
of 
the 
variables), 
since 
in 
each 
case 
the 
joint 
setting 
of 
the 
variables 
is 
simply 
a 
dierent 
index 
to 
the 
same 
probability. 
This 
situation 
is 
more 
clear 
in 
the 
set 
theoretic 
notation 
p(J 
\6S 
\6T 
\6R). 
We 
abbreviate 
this 
set 
theoretic 
notation 
by 
using 
the 
commas 
– 
however, 
one 
should 
be 
careful 
not 
to 
confuse 
the 
use 
of 
this 
indexing 
type 
notation 
with 
functions 
f(x, 
y) 
which 
are 
in 
general 
dependent 
on 
the 
variable 
order. 
Whilst 
the 
variables 
to 
the 
left 
of 
the 
conditioning 
bar 
may 
be 
written 
in 
any 
order, 
and 
equally 
those 
to 
the 
right 
of 
the 
conditioning 
bar 
may 
be 
written 
in 
any 
order, 
moving 
variables 
across 
the 
bar 
is 
not 
generally 
equivalent, 
so 
that 


p(x1jx2)6

= 
p(x2jx1). 


1.1.2 
Interpreting 
Conditional 
Probability 
Together 
with 
the 
rules 
of 
probability, 
conditional 
probability 
enables 
one 
to 
reason 
in 
a 
rational, 
logical 
and 
consistent 
way. 
One 
could 
argue 
that 
much 
of 
science 
deals 
with 
problems 
of 
the 
form 
: 
tell 
me 
something 
about 
the 
parameters 
. 
given 
that 
I 
have 
observed 
data 
D6and 
have 
some 
knowledge 
of 
the 
underlying 
data 
generating 
mechanism. 
From 
a 
modelling 
perspective, 
this 
requires 


p(Dj)p() 
p(Dj)p()

p(jD)= 
= 
R(1.1.17) 


p(D) 
p(Dj)p()

. 


This 
shows 
how 
from 
a 
forward 
or 
generative 
model 
p(Dj) 
of 
the 
dataset, 
and 
coupled 
with 
a 
prior 
belief 
p() 
about 
which 
parameter 
values 
are 
appropriate, 
we 
can 
infer 
the 
posterior 
distribution 
p(jD) 
of 
parameters 
in 
light 
of 
the 
observed 
data. 


This 
use 
of 
a 
generative 
model 
sits 
well 
with 
physical 
models 
of 
the 
world 
which 
typically 
postulate 
how 
to 
generate 
observed 
phenomena, 
assuming 
we 
know 
the 
correct 
parameters 
of 
the 
model. 
For 
example, 
one 
might 
postulate 
how 
to 
generate 
a 
time-series 
of 
displacements 
for 
a 
swinging 
pendulum 
but 
with 
unknown 
mass, 
length 
and 
damping 
constant. 
Using 
this 
generative 
model, 
and 
given 
only 
the 
displacements, 
we 
could 
infer 
the 
unknown 
physical 
properties 
of 
the 
pendulum, 
such 
as 
its 
mass, 
length 
and 
friction 
damping 
constant. 


DRAFT 
March 
9, 
2010 
7 



Probabilistic 
Reasoning 


Subjective 
Probability 


Probability 
is 
a 
contentious 
topic 
and 
we 
do 
not 
wish 
to 
get 
bogged 
down 
by 
the 
debate 
here, 
apart 
from 
pointing 
out 
that 
it 
is 
not 
necessarily 
the 
axioms 
of 
probability 
that 
are 
contentious 
rather 
what 
interpretation 
we 
should 
place 
on 
them. 
In 
some 
cases 
potential 
repetitions 
of 
an 
experiment 
can 
be 
envisaged 
so 
that 
the 
`long 
run’ 
(or 
frequentist) 
denition 
of 
probability 
in 
which 
probabilities 
are 
dened 
with 
respect 
to 
a 
potentially 
innite 
repetition 
of 
`experiments’ 
makes 
sense. 
For 
example, 
in 
coin 
tossing, 
the 
probability 
of 
heads 
might 
be 
interpreted 
as 
`If 
I 
were 
to 
repeat 
the 
experiment 
of 
ipping 
a 
coin 
(at 
`random'), 
the 
limit 
of 
the 
number 
of 
heads 
that 
occurred 
over 
the 
number 
of 
tosses 
is 
dened 
as 
the 
probability 
of 
a 
head 
occurring. 


Here's 
another 
problem 
that 
is 
typical 
of 
the 
kind 
of 
scenario 
one 
might 
face 
in 
a 
machine 
learning 
situation. 
A 
lm 
enthusiast 
joins 
a 
new 
online 
lm 
service. 
Based 
on 
expressing 
a 
few 
lms 
a 
user 
likes 
and 
dislikes, 
the 
online 
company 
tries 
to 
estimate 
the 
probability 
that 
the 
user 
will 
like 
each 
of 
the 
10000 
lms 
in 
their 
database. 
If 
we 
were 
to 
dene 
probability 
as 
a 
limiting 
case 
of 
innite 
repetitions 
of 
the 
same 
experiment, 
this 
wouldn't 
make 
much 
sense 
in 
this 
case 
since 
we 
can't 
repeat 
the 
experiment. 
However, 
if 
we 
assume 
that 
the 
user 
behaves 
in 
a 
manner 
consistent 
with 
other 
users, 
we 
should 
be 
able 
to 
exploit 
the 
large 
amount 
of 
data 
from 
other 
users’ 
ratings 
to 
make 
a 
reasonable 
`guess’ 
as 
to 
what 
this 
consumer 
likes. 
This 
degree 
of 
belief 
or 
Bayesian 
subjective 
interpretation 
of 
probability 
sidesteps 
non-repeatability 
issues 
– 
it's 
just 
a 
consistent 
framework 
for 
manipulating 
real 
values 
consistent 
with 
our 
intuition 
about 
probability[145]. 


1.2 
Probabilistic 
Reasoning 
The 
axioms 
of 
probability, 
combined 
with 
Bayes’ 
rule 
make 
for 
a 
complete 
reasoning 
system, 
one 
which 
includes 
traditional 
deductive 
logic 
as 
a 
special 
case[145]. 


Remark 
1. 
The 
central 
paradigm 
of 
probabilistic 
reasoning 
is 
to 
identify 
all 
relevant 
variables 
x1;:::;xN 
in 
the 
environment, 
and 
make 
a 
probabilistic 
model 
p(x1;:::;xN 
) 
of 
their 
interaction. 
Reasoning 
(inference) 
is 
then 
performed 
by 
introducing 
evidence2 
that 
sets 
variables 
in 
known 
states, 
and 
subsequently 
computing 
probabilities 
of 
interest, 
conditioned 
on 
this 
evidence. 


Example 
1 
(Hamburgers). 
Consider 
the 
following 
ctitious 
scientic 
information: 
Doctors 
nd 
that 
people 
with 
Kreuzfeld-Jacob 
disease 
(KJ) 
almost 
invariably 
ate 
hamburgers, 
thus 
p(Hamburger 
EaterjKJ 
)= 


0:9. 
The 
probability 
of 
an 
individual 
having 
KJ 
is 
currently 
rather 
low, 
about 
one 
in 
100,000. 
1. 
Assuming 
eating 
lots 
of 
hamburgers 
is 
rather 
widespread, 
say 
p(Hamburger 
Eater)=0:5, 
what 
is 
the 
probability 
that 
a 
hamburger 
eater 
will 
have 
Kreuzfeld-Jacob 
disease? 
This 
may 
be 
computed 
as 


p(Hamburger 
Eater, 
KJ 
) 
p(Hamburger 
EaterjKJ 
)p(KJ 
) 


p(KJ 
jHamburger 
Eater)= 
= 


p(Hamburger 
Eater) 
p(Hamburger 
Eater) 


(1.2.1) 
91



10 
100000

= 
=1:8 
× 
10..5 
(1.2.2)

1 
2 


2. 
If 
the 
fraction 
of 
people 
eating 
hamburgers 
was 
rather 
small, 
p(Hamburger 
Eater)=0:001, 
what 
is 
the 
probability 
that 
a 
regular 
hamburger 
eater 
will 
have 
Kreuzfeld-Jacob 
disease? 
Repeating 
the 
above 
calculation, 
this 
is 
given 
by 
91



10 
100000 


˜ 
1=100 
(1.2.3)

1 
1000 


DRAFT 
March 
9, 
2010 



Probabilistic 
Reasoning 


Intuitively, 
this 
is 
much 
higher 
than 
in 
scenario 
(1) 
since 
here 
we 
can 
be 
more 
sure 
that 
eating 
hamburgers 
is 
related 
to 
the 
illness. 
In 
this 
case 
only 
a 
small 
number 
of 
people 
in 
the 
population 
eat 
hamburgers, 
and 
most 
of 
them 
get 
ill. 


Example 
2 
(Inspector 
Clouseau). 
Inspector 
Clouseau 
arrives 
at 
the 
scene 
of 
a 
crime. 
The 
victim 
lies 
dead 
in 
the 
room 
and 
the 
inspector 
quickly 
nds 
the 
murder 
weapon, 
a 
Knife 
(K). 
The 
Butler 
(B) 
and 
Maid 
(M) 
are 
his 
main 
suspects. 
The 
inspector 
has 
a 
prior 
belief 
of 
0.8 
that 
the 
Butler 
is 
the 
murderer, 
and 
a 
prior 
belief 
of 
0.2 
that 
the 
Maid 
is 
the 
murderer. 
These 
probabilities 
are 
independent 
in 
the 
sense 
that 
p(B, 
M)= 
p(B)p(M). 
(It 
is 
possible 
that 
both 
the 
Butler 
and 
the 
Maid 
murdered 
the 
victim 
or 
neither). 
The 
inspector's 
prior 
criminal 
knowledge 
can 
be 
formulated 
mathematically 
as 
follows: 


dom(B) 
= 
dom(M)= 
fmurderer, 
not 
murderer} 
, 
dom(K)= 
fknife 
used, 
knife 
not 
used} 
(1.2.4) 


p(B 
= 
murderer)=0:8;p(M 
= 
murderer)=0:2 
(1.2.5) 


p(knife 
usedjB 
= 
not 
murderer;M 
= 
not 
murderer) 
=0:3 
p(knife 
usedjB 
= 
not 
murderer;M 
= 
murderer) 
=0:2 


(1.2.6)
p(knife 
usedjB 
= 
murderer;M 
= 
not 
murderer) 
=0:6 
p(knife 
usedjB 
= 
murderer;M 
= 
murderer) 
=0:1 


What 
is 
the 
probability 
that 
the 
Butler 
is 
the 
murderer? 
(Remember 
that 
it 
might 
be 
that 
neither 
is 
the 
murderer). 
Using 
b 
for 
the 
two 
states 
of 
B 
and 
m 
for 
the 
two 
states 
of 
M, 


P

XX

p(B, 
m, 
K) 
p(B) 
p(KjB, 
m)p(m)

m

p(BjK)=p(B, 
mjK)== 
PPX(1.2.7) 


p(K) 
b 
p(b) 
p(Kjb, 
m)p(m)

m

mm 


Plugging 
in 
the 
values 
we 
have 


..

821 
86

× 
+ 
× 
200

1010 
10 
10 
10

p(B 
= 
murdererjknife 
used)= 
....= 
˜ 
0:877 
(1.2.8) 


82186 
22283

× 
+ 
× 
+ 
× 
+ 
× 
228

1010 
10 
10 
101010 
10 
10 
10

The 
role 
of 
p(knife 
used) 
in 
the 
Inspector 
Clouseau 
example 
can 
cause 
some 
confusion. 
In 
the 
above, 


XX

p(knife 
used)=p(b)p(knife 
usedjb, 
m)p(m) 
(1.2.9) 


bm 


is 
computed 
to 
be 
0:456. 
But 
surely, 
p(knife 
used) 
= 
1, 
since 
this 
is 
given 
in 
the 
question! 
Note 
that 
the 
quantity 
p(knife 
used) 
relates 
to 
the 
prior 
probability 
the 
model 
assigns 
to 
the 
knife 
being 
used 
(in 
the 
absence 
of 
any 
other 
information). 
If 
we 
know 
that 
the 
knife 
is 
used, 
then 
the 
posterior 


p(knife 
used, 
knife 
used) 
p(knife 
used) 


p(knife 
usedjknife 
used) 
= 
= 
= 
1 
(1.2.10) 


p(knife 
used) 
p(knife 
used) 


which, 
naturally, 
must 
be 
the 
case. 


Another 
potential 
confusion 
is 
the 
choice 


p(B 
= 
murderer)=0:8;p(M 
= 
murderer)=0:2 
(1.2.11) 


which 
means 
that 
p(B 
= 
not 
murderer)=0:2, 
p(M 
= 
not 
murderer)=0:8. 
These 
events 
are 
not 
exclusive 
and 
it's 
just 
`coincidence’ 
that 
the 
numerical 
values 
are 
chosen 
this 
way. 
For 
example, 
we 
could 
have 
also 
chosen 


p(B 
= 
murderer)=0:6;p(M 
= 
murderer)=0:9 
(1.2.12) 


which 
means 
that 
p(B 
= 
not 
murderer)=0:4, 
p(M 
= 
not 
murderer)=0:1 


DRAFT 
March 
9, 
2010 



Prior, 
Likelihood 
and 
Posterior 


1.3 
Prior, 
Likelihood 
and 
Posterior 
The 
prior, 
likelihood 
and 
posterior 
are 
all 
probabilities. 
They 
are 
assigned 
these 
names 
due 
to 
their 
role 
in 
Bayes’ 
rule, 
described 
below. 


Denition 
7. 
Prior 
Likelihood 
and 
Posterior 


For 
data 
D 
and 
variable 
, 
Bayes’ 
rule 
tells 
us 
how 
to 
update 
our 
prior 
beliefs 
about 
the 
variable 
. 
in 
light 
of 
the 
data 
to 
a 
posterior 
belief: 


p(Dj) 
p()

p 
{a 
}|{z" 


likelihood 
prior

(jD) 
= 
(1.3.1)

p 
p|
{a 
" 
(D)

p
p|
{z}

posterior 


evidence 


The 
evidence 
is 
also 
called 
the 
marginal 
likelihood. 


The 
term 
likelihood 
is 
used 
for 
the 
probability 
that 
a 
model 
generates 
observed 
data. 
More 
fully, 
if 
we 
condition 
on 
the 
model 
M, 
we 
have 


p(Dj, 
M)p(jM) 


p(jD;M)= 


p(DjM) 


where 
we 
see 
the 
role 
of 
the 
likelihood 
p(Dj, 
M) 
and 
marginal 
likelihood 
p(DjM). 
The 
marginal 
likelihood 
is 
also 
called 
the 
model 
likelihood. 


The 
most 
probable 
a 
posteriori 
(MAP) 
setting 
is 
that 
which 
maximises 
the 
posterior, 
* 
= 
argmax 
p(jD;M). 


. 


Bayes’ 
rule 
tells 
us 
how 
to 
update 
our 
prior 
knowledge 
with 
the 
data 
generating 
mechanism. 
The 
prior 
distribution 
p() 
describes 
the 
information 
we 
have 
about 
the 
variable 
before 
seeing 
any 
data. 
After 
data 
D 
arrives, 
we 
update 
the 
prior 
distribution 
to 
the 
posterior 
p(jD) 
. 
p(Dj)p(). 


1.3.1 
Two 
dice 
: 
what 
were 
the 
individual 
scores? 
Two 
fair 
dice 
are 
rolled. 
Someone 
tells 
you 
that 
the 
sum 
of 
the 
two 
scores 
is 
9. 
What 
is 
the 
probability 
distribution 
of 
the 
two 
dice 
scores3? 


The 
score 
of 
die 
a 
is 
denoted 
sa 
with 
dom(sa)= 
f1, 
2, 
3, 
4, 
5, 
6} 
and 
similarly 
for 
sb. 
The 
three 
variables 
involved 
are 
then 
sa, 
sb 
and 
the 
total 
score, 
t 
= 
sa 
+ 
sb. 
A 
model 
of 
these 
three 
variables 
naturally 
takes 
the 
form 


p(t, 
sa;sb)= 
p(tjsa;sb) 
p(sa;sb) 
(1.3.2)

p 
{a 
}|{a 
" 


likelihood 
prior 


The 
prior 
p(sa;sb) 
is 
the 
joint 
probability 
of 
score 
sa 


p(sa)p(sb):

and 
score 
sb 
without 
knowing 
anything 
else. 
Assuming 
no 
dependency 
in 
the 
rolling 
mechanism, 


p(sa;sb)= 
p(sa)p(sb) 
(1.3.3) 


Since 
the 
dice 
are 
fair 
both 
p(sa) 
and 
p(sb) 
are 
uniform 
distributions, 
p(sa 
= 
s)=1=6. 


sa 
= 
1 
sa 
= 
2 
sa 
= 
3 
sa 
= 
4 
sa 
= 
5 
sa 
= 
6 
sb 
= 
1 
1/36 
1/36 
1/36 
1/36 
1/36 
1/36 
sb 
= 
2 
1/36 
1/36 
1/36 
1/36 
1/36 
1/36 
sb 
= 
3 
1/36 
1/36 
1/36 
1/36 
1/36 
1/36 
sb 
= 
4 
1/36 
1/36 
1/36 
1/36 
1/36 
1/36 
sb 
= 
5 
1/36 
1/36 
1/36 
1/36 
1/36 
1/36 
sb 
= 
6 
1/36 
1/36 
1/36 
1/36 
1/36 
1/36 


3This 
example 
is 
due 
to 
Taylan 
Cemgil. 


DRAFT 
March 
9, 
2010 



Further 
worked 
examples 


p(t 
=9jsa;sb):

Here 
the 
likelihood 
term 
is 


p(tjsa;sb)= 
I[t 
= 
sa 
+ 
sb] 
(1.3.4) 


which 
states 
that 
the 
total 
score 
is 
given 
by 
sa 
+ 
sb. 
Here 
I[x 
= 
y] 
is 
the 
indicator 
function 
dened 
as 
I[x 
= 
y]=1 
if 
x 
= 
y 
and 
0 
otherwise. 


sa 
= 
1 
sa 
= 
2 
sa 
= 
3 
sa 
= 
4 
sa 
= 
5 
sa 
= 
6 
sb 
= 
1 
0 
0 
0 
0 
0 
0 
sb 
= 
2 
0 
0 
0 
0 
0 
0 
sb 
= 
3 
0 
0 
0 
0 
0 
1 
sb 
= 
4 
0 
0 
0 
0 
1 
0 
sb 
= 
5 
0 
0 
0 
1 
0 
0 
sb 
= 
6 
0 
0 
1 
0 
0 
0 


p(t 
=9jsa;sb)p(sa)p(sb):

Hence, 
our 
complete 
model 
is 


p(t, 
sa;sb)= 
p(tjsa;sb)p(sa)p(sb) 
(1.3.5) 
where 
the 
terms 
on 
the 
right 
are 
explicitly 
dened. 


sa 
= 
1 
sa 
= 
2 
sa 
= 
3 
sa 
= 
4 
sa 
= 
5 
sa 
= 
6 
sb 
= 
1 
0 
0 
0 
0 
0 
0 
sb 
= 
2 
0 
0 
0 
0 
0 
0 
sb 
= 
3 
0 
0 
0 
0 
0 
1/36 
sb 
= 
4 
0 
0 
0 
0 
1/36 
0 
sb 
= 
5 
0 
0 
0 
1/36 
0 
0 
sb 
= 
6 
0 
0 
1/36 
0 
0 
0 


Our 
interest 
is 
then 
obtainable 
using 
Bayes’ 
rule, 


sa 
=1 
sa 
=2 
sa 
=3 
sa 
=4 
sa 
=5 
sa 
=6 
sb 
=1 
0 
0 
0 
0 
0 
0 
sb 
=2 
0 
0 
0 
0 
0 
0 
sb 
=3 
0 
0 
0 
0 
01/4 
sb 
=4 
0 
0 
0 
01/40 
sb 
=5 
0 
0 
01/40 
0 
sb 
=6 
0 
01/40 
0 
0 


p(sa;sbjt 
= 
9): 


p(t 
=9jsa;sb)p(sa)p(sb) 


p(sa;sbjt 
= 
9) 
= 
(1.3.6) 


p(t 
= 
9) 
where 


X

p(t 
= 
9) 
=p(t 
=9jsa;sb)p(sa)p(sb) 
(1.3.7) 
sa;sb 


P

The 
term 
p(t 
= 
9) 
=p(t 
=9jsa;sb)p(sa)p(sb)=4 
× 
1=36 
= 
1=9. 
Hence 
the 
posterior 
is 
given 
by 


sa;sb 


equal 
mass 
in 
only 
4 
non-zero 
elements, 
as 
shown. 


1.4 
Further 
worked 
examples 
Example 
3 
(Who's 
in 
the 
bathroom?). 
Consider 
a 
household 
of 
three 
people, 
Alice, 
Bob 
and 
Cecil. 
Cecil 
wants 
to 
go 
to 
the 
bathroom 
but 
nds 
it 
occupied. 
He 
then 
goes 
to 
Alice's 
room 
and 
sees 
she 
is 
there. 
Since 
Cecil 
knows 
that 
only 
either 
Alice 
or 
Bob 
can 
be 
in 
the 
bathroom, 
from 
this 
he 
infers 
that 
Bob 
must 
be 
in 
the 
bathroom. 


To 
arrive 
at 
the 
same 
conclusion 
in 
a 
mathematical 
framework, 
let's 
dene 
the 
following 
events 
A 
= 
Alice 
is 
in 
her 
bedroom;B 
= 
Bob 
is 
in 
his 
bedroom;O 
= 
Bathroom 
occupied 
(1.4.1) 
We 
can 
encode 
the 
information 
that 
if 
either 
Alice 
or 
Bob 
are 
not 
in 
their 
bedrooms, 
then 
they 
must 
be 
in 
the 
bathroom 
(they 
might 
both 
be 
in 
the 
bathroom) 
as 
p(O 
= 
trjA 
= 
fa;B)=1;p(O 
= 
trjA, 
B 
= 
fa) 
= 
1 
(1.4.2) 
The 
rst 
term 
expresses 
that 
the 
bathroom 
is 
occupied 
if 
Alice 
is 
not 
in 
her 
bedroom, 
wherever 
Bob 
is. 
Similarly, 
the 
second 
term 
expresses 
bathroom 
occupancy 
as 
long 
as 
Bob 
is 
not 
in 
his 
bedroom. 
Then 
p(B 
= 
fa;O 
= 
tr;A 
= 
tr) 
p(O 
= 
trjA 
= 
tr;B 
= 
fa)p(A 
= 
tr;B 
= 
fa) 


p(B 
= 
fajO 
= 
tr;A 
= 
tr)= 
= 


p(O 
= 
tr;A 
= 
tr) 
p(O 
= 
tr;A 
= 
tr) 


(1.4.3) 
where 
p(O 
= 
tr;A 
= 
tr)= 
p(O 
= 
trjA 
= 
tr;B 
= 
fa)p(A 
= 
tr;B 
= 
fa) 


+ 
p(O 
= 
trjA 
= 
tr;B 
= 
tr)p(A 
= 
tr;B 
= 
tr) 
(1.4.4) 
DRAFT 
March 
9, 
2010 
11 



Further 
worked 
examples 


Using 
the 
fact 
p(O 
= 
trjA 
= 
tr;B 
= 
fa)=1 
and 
p(O 
= 
trjA 
= 
tr;B 
= 
tr) 
= 
0, 
which 
encodes 
that 
if 
if 
Alice 
is 
in 
her 
room 
and 
Bob 
is 
not, 
the 
bathroom 
must 
be 
occupied, 
and 
similarly, 
if 
both 
Alice 
and 
Bob 
are 
in 
their 
rooms, 
the 
bathroom 
cannot 
be 
occupied, 


p(A 
= 
tr;B 
= 
fa) 


p(B 
= 
fajO 
= 
tr;A 
= 
tr) 
= 
= 
1 
(1.4.5) 


p(A 
= 
tr;B 
= 
fa) 


This 
example 
is 
interesting 
since 
we 
are 
not 
required 
to 
make 
a 
full 
probabilistic 
model 
in 
this 
case 
thanks 
to 
the 
limiting 
nature 
of 
the 
probabilities 
(we 
don't 
need 
to 
specify 
p(A, 
B)). 
The 
situation 
is 
common 
in 
limiting 
situations 
of 
probabilities 
being 
either 
0 
or 
1, 
corresponding 
to 
traditional 
logic 
systems. 


Example 
4 
(Aristotle 
: 
Resolution). 
We 
can 
represent 
the 
statement 
`All 
apples 
are 
fruit’ 
by 
p(F 
= 
trjA 
= 
tr) 
= 
1. 
Similarly, 
`All 
fruits 
grow 
on 
trees’ 
may 
be 
represented 
by 
p(T 
= 
trjF 
= 
tr) 
= 
1. 
Additionally 
we 
assume 
that 
whether 
or 
not 
something 
grows 
on 
a 
tree 
depends 
only 
on 
whether 
or 
not 
it 
is 
a 
fruit, 
p(T 
jA, 
F 
)= 
P 
(T 
jF 
). 
From 
these, 
we 
can 
compute 


XX

p(T 
= 
trjA 
= 
tr)=p(T 
= 
trjF, 
A 
= 
tr)p(F 
jA 
= 
tr)=p(T 
= 
trjF 
)p(F 
jA 
= 
tr) 
(1.4.6) 


FF 


= 
p(T 
= 
trjF 
= 
fa)(F 
= 
fajA 
= 
tr)+ 
(T 
= 
trjF 
= 
tr)(F 
= 
trjA 
= 
tr) 
= 
1 
(1.4.7)

p
p|
{z}p
p|
{z}p
p|
{z}X

=0 
=1=1 


In 
other 
words 
we 
have 
deduced 
that 
`All 
apples 
grow 
on 
trees’ 
is 
a 
true 
statement, 
based 
on 
the 
information 
presented. 
(This 
kind 
of 
reasoning 
is 
called 
resolution 
and 
is 
a 
form 
of 
transitivity 
: 
from 
the 
statements 
A 
. 
F 
and 
F 
. 
T 
we 
can 
infer 
A 
. 
T 
). 


Example 
5 
(Aristotle 
: 
Inverse 
Modus 
Ponens). 
According 
to 
Logic, 
from 
the 
statement 
: 
`If 
A 
is 
true 
then 
B 
is 
true', 
one 
may 
deduce 
that 
`if 
B 
is 
false 
then 
A 
is 
false'. 
Let's 
see 
how 
this 
ts 
in 
with 
a 
probabilistic 
reasoning 
system. 
We 
can 
express 
the 
statement 
: 
`If 
A 
is 
true 
then 
B 
is 
true’ 
as 
p(B 
= 
trjA 
= 
tr) 
= 
1. 
Then 
we 
may 
infer 


p(A 
= 
fajB 
= 
fa)=1 
- 
p(A 
= 
trjB 
= 
fa) 
p(B 
= 
fajA 
= 
tr)p(A 
= 
tr)


=1 
- 
= 
1 
(1.4.8) 


p(B 
= 
fajA 
= 
tr)p(A 
= 
tr)+ 
p(B 
= 
fajA 
= 
fa)p(A 
= 
fa) 


This 
follows 
since 
p(B 
= 
fajA 
= 
tr)=1 
- 
p(B 
= 
trjA 
= 
tr)=1 
- 
1 
= 
0, 
annihilating 
the 
second 
term. 


Both 
the 
above 
examples 
are 
intuitive 
expressions 
of 
deductive 
logic. 
The 
standard 
rules 
of 
Aristotelian 
logic 
are 
therefore 
seen 
to 
be 
limiting 
cases 
of 
probabilistic 
reasoning. 


Example 
6 
(Soft 
XOR 
Gate). 
A 
standard 
XOR 
logic 
gate 
is 
given 
by 
the 
table 
on 
the 
right. 
If 
we 
observe 
that 
the 
output 
of 
the 
XOR 
gate 
is 
0, 
what 
can 
we 
say 
about 
A 
and 
B? 
In 
this 
case, 
either 
A 
and 
B 
were 
both 
0, 
or 
A 
and 
B 
were 
both 
1. 
This 
means 
we 
don't 
know 
which 
state 
A 
was 
in 
– 
it 
could 
equally 
likely 
have 
been 
1 
or 
0. 


A 
B 
A 
xor 
B 
0 
0 
0 
0 
1 
1 
1 
0 
1 
1 
1 
0 


DRAFT 
March 
9, 
2010 



Further 
worked 
examples 


A 
B 
p(C 
= 
1jA, 
B) 
Consider 
a 
`soft’ 
version 
of 
the 
XOR 
gate 
given 
on 
the 
right, 
0 
0 
0.1 
with 
additionally 
p(A 
= 
1) 
= 
0:65, 
p(B 
= 
1) 
= 
0:77. 
What 
is 
0 
1 
0.99 
p(A 
= 
1jC 
= 
0)? 
1 
0 
0.8 
1 
1 
0.25 
XX

p(A 
=1;C 
= 
0) 
=p(A 
=1, 
B, 
C 
= 
0) 
=p(C 
=0jA 
=1;B)p(A 
= 
1)p(B) 


BB 


= 
p(A 
= 
1)(p(C 
=0jA 
=1;B 
= 
0)p(B 
= 
0)+ 
p(C 
=0jA 
=1;B 
= 
1)p(B 
= 
1)) 
=0:65 
× 
(0:2 
× 
0:23 
+ 
0:75 
× 
0:77) 
= 
0:4053 
(1.4.9) 


XX

p(A 
=0;C 
= 
0) 
=p(A 
=0, 
B, 
C 
= 
0) 
=p(C 
=0jA 
=0;B)p(A 
= 
0)p(B) 


BB 


= 
p(A 
= 
0)(p(C 
=0jA 
=0;B 
= 
0)p(B 
= 
0)+ 
p(C 
=0jA 
=0;B 
= 
1)p(B 
= 
1)) 
=0:35 
× 
(0:9 
× 
0:23 
+ 
0:01 
× 
0:77) 
= 
0:0751 


Then 
p(A 
=1;C 
=0) 
0:4053 


p(A 
=1jC 
=0)= 
= 
=0:8437 
(1.4.10) 


p(A 
=1;C 
= 
0)+ 
p(A 
=0;C 
=0) 
0:4053 
+ 
0:0751 


Example 
7 
(Larry). 
Larry 
is 
typically 
late 
for 
school. 
If 
Larry 
is 
late, 
we 
denote 
this 
with 
L 
= 
late, 
otherwise, 
L 
= 
not 
late. 
When 
his 
mother 
asks 
whether 
or 
not 
he 
was 
late 
for 
school 
he 
never 
admits 
to 
being 
late. 
The 
response 
Larry 
gives 
RL 
is 
represented 
as 
follows 


p(RL 
= 
not 
latejL 
= 
not 
late)=1;p(RL 
= 
latejL 
= 
late) 
= 
0 
(1.4.11) 
The 
remaining 
two 
values 
are 
determined 
by 
normalisation 
and 
are 
p(RL 
= 
latejL 
= 
not 
late)=0;p(RL 
= 
not 
latejL 
= 
late) 
= 
1 
(1.4.12) 
Given 
that 
RL 
= 
not 
late, 
what 
is 
the 
probability 
that 
Larry 
was 
late, 
i.e. 
p(L 
= 
latejRL 
= 
not 
late)? 


Using 
Bayes’ 
we 
have 
p(L 
= 
late;RL 
= 
not 
late) 


p(L 
= 
latejRL 
= 
not 
late)= 


p(RL 
= 
not 
late) 
p(L 
= 
late;RL 
= 
not 
late)

= 
(1.4.13) 


p(L 
= 
late;RL 
= 
not 
late)+ 
p(L 
= 
not 
late;RL 
= 
not 
late) 
In 
the 
above 
p(L 
= 
late;RL 
= 
not 
late)= 
p(RL 
= 
not 
latejL 
= 
late) 
p(L 
= 
late) 
(1.4.14)

|{z}X

=1 


and 
p(L 
= 
not 
late;RL 
= 
not 
late)= 
(RL 
= 
not 
latejL 
= 
not 
late) 
p(L 
= 
not 
late) 
(1.4.15)

p
p|
{z}X

=1 


Hence 
p(L 
= 
latejRL 
= 
not 
late) 
= 
p(L 
= 
late) 
p(L 
= 
late) 
+ 
p(L 
= 
not 
late) 
= 
p(L 
= 
late) 
(1.4.16) 
DRAFT 
March 
9, 
2010 
13 



Further 
worked 
examples 


Where 
we 
used 
normalisation 
in 
the 
last 
step, 
p(L 
= 
late)+ 
p(L 
= 
not 
late) 
= 
1. 
This 
result 
is 
intuitive 
– 
Larry's 
mother 
knows 
that 
he 
never 
admits 
to 
being 
late, 
so 
her 
belief 
about 
whether 
or 
not 
he 
really 
was 
late 
is 
unchanged, 
regardless 
of 
what 
Larry 
actually 
says. 


Example 
8 
(Larry 
and 
Sue). 
Continuing 
the 
example 
above, 
Larry's 
sister 
Sue 
always 
tells 
the 
truth 
to 
her 
mother 
as 
to 
whether 
or 
not 
Larry 
was 
late 
for 
School. 


p(RS 
= 
not 
latejL 
= 
not 
late)=1;p(RS 
= 
latejL 
= 
late) 
= 
1 
(1.4.17) 


The 
remaining 
two 
values 
are 
determined 
by 
normalisation 
and 
are 


p(RS 
= 
latejL 
= 
not 
late)=0;p(RS 
= 
not 
latejL 
= 
late) 
= 
0 
(1.4.18) 


We 
also 
assume 
p(RS;RLjL)= 
p(RSjL)p(RLjL). 
We 
can 
then 
write 


p(RL;RS;L)= 
p(RLjL)p(RSjL)p(L) 
(1.4.19) 


Given 
that 
RS 
= 
late, 
what 
is 
the 
probability 
that 
Larry 
was 
late? 


Using 
Bayes’ 
rule, 
we 
have 


1 


p(L 
= 
latejRL 
= 
not 
late;RS 
= 
late)= 
p(RS 
= 
latejL 
= 
late)p(RL 
= 
not 
latejL 
= 
late)p(L 
= 
late)

Z 
where 
the 
normalisation 
Z 
is 
given 
by 


p(RS 
= 
latejL 
= 
late)p(RL 
= 
not 
latejL 
= 
late)p(L 
= 
late) 


+ 
p(RS 
= 
latejL 
= 
not 
late)p(RL 
= 
not 
latejL 
= 
not 
late)p(L 
= 
not 
late) 
Hence 


1 
61 
6p(L 
= 
late) 


p(L 
= 
latejRL 
= 
not 
late;RS 
= 
late) 
= 
= 
1 
(1.4.20)

1 
61 
6p(L 
= 
late)+0 
61 
6p(L 
= 
not 
late) 


This 
result 
is 
also 
intuitive 
– 
Since 
Larry's 
mother 
knows 
that 
Sue 
always 
tells 
the 
truth, 
no 
matter 
what 
Larry 
says, 
she 
knows 
he 
was 
late. 


Example 
9 
(Luke). 
Luke 
has 
been 
told 
he's 
lucky 
and 
has 
won 
a 
prize 
in 
the 
lottery. 
There 
are 
5 
prizes 
available 
of 
value 
$10, 
$100, 
$1000, 
$10000, 
$1000000. 
The 
prior 
probabilities 
of 
winning 
these 
5 
prizes 
are 
p1;p2;p3;p4;p5, 
with 
p0 
being 
the 
prior 
probability 
of 
winning 
no 
prize. 
Luke 
asks 
eagerly 
`Did 
I 
win 
$1000000?!'. 
`I'm 
afraid 
not 
sir', 
is 
the 
response 
of 
the 
lottery 
phone 
operator. 
`Did 
I 
win 
$10000?!’ 
asks 
Luke. 
`Again, 
I'm 
afraid 
not 
sir'. 
What 
is 
the 
probability 
that 
Luke 
has 
won 
$1000? 


Note 
rst 
that 
p0 
+p1 
+p2 
+p3 
+p4 
+p5 
= 
1. 
We 
denote 
W 
= 
1 
for 
the 
rst 
prize 
of 
$10, 
and 
W 
=2;:::, 
5 
for 
the 
remaining 
prizes 
and 
W 
= 
0 
for 
no 
prize. 
We 
need 
to 
compute 


p(W 
=3;W666

=5;W=4;W= 
0) 


p(W 
=3jW666

=5;W=4;W= 
0) 
= 


p(W666

=5;W=4;W= 
0) 


p(W 
= 
3) 
p3 


= 
= 
(1.4.21) 


p(W 
=1 
or 
W 
=2 
or 
W 
= 
3) 
p1 
+ 
p2 
+ 
p3 


DRAFT 
March 
9, 
2010 



Code 


where 
the 
term 
in 
the 
denominator 
is 
computed 
using 
the 
fact 
that 
the 
events 
W 
are 
mutually 
exclusive 
(one 
can 
only 
win 
one 
prize). 
This 
result 
makes 
intuitive 
sense 
: 
once 
we 
have 
removed 
the 
impossible 
states 
of 
W 
, 
the 
probability 
that 
Luke 
wins 
the 
prize 
is 
proportional 
to 
the 
prior 
probability 
of 
that 
prize, 
with 
the 
normalisation 
being 
simply 
the 
total 
set 
of 
possible 
probability 
remaining. 


1.5 
Code 
The 
BRMLtoolbox 
code 
accompanying 
this 
book 
is 
intended 
to 
give 
the 
reader 
some 
insight 
into 
representing 
discrete 
probability 
tables 
and 
performing 
simple 
inference. 
The 
MATLAB 
code 
is 
written 
with 
only 
minimal 
error 
trapping 
to 
keep 
the 
code 
as 
short 
and 
hopefully 
reasonably 
readable4 
. 


1.5.1 
Basic 
Probability 
code 
At 
the 
simplest 
level, 
we 
only 
need 
two 
basic 
routines. 
One 
for 
multiplying 
probability 
tables 
together 
(called 
potentials 
in 
the 
code), 
and 
one 
for 
summing 
a 
probability 
table. 
Potentials 
are 
represented 
using 
a 
structure. 
For 
example, 
in 
the 
code 
corresponding 
to 
the 
Inspector 
Clouseau 
example 
demoClouseau.m, 
we 
dene 
a 
probability 
table 
as 


>> 
pot(1) 
ans 
= 
variables: 
[1 
3 
2] 
table: 
[2x2x2 
double] 


This 
says 
that 
the 
potential 
depends 
on 
the 
variables 
1, 
3, 
2 
and 
the 
entries 
are 
stored 
in 
the 
array 
given 
by 
the 
table 
eld. 
The 
size 
of 
the 
array 
informs 
how 
many 
states 
each 
variable 
takes 
in 
the 
order 
given 
by 
variables. 
The 
order 
in 
which 
the 
variables 
are 
dened 
in 
a 
potential 
is 
irrelevant 
provided 
that 
one 
indexes 
the 
array 
consistently. 
A 
routine 
that 
can 
help 
with 
setting 
table 
entries 
is 
setstate.m. 
For 
example, 


>> 
pot(1) 
= 
setstate(pot(1),[2 
1 
3],[2 
1 
1],0.3) 


means 
that 
for 
potential 
1, 
the 
table 
entry 
for 
variable 
2 
being 
in 
state 
2, 
variable 
1 
being 
in 
state 
1 
and 
variable 
3 
being 
in 
state 
1 
should 
be 
set 
to 
value 
0.3. 


The 
philosophy 
of 
the 
code 
is 
to 
keep 
the 
information 
required 
to 
perform 
computations 
to 
a 
minimum. 
Additional 
information 
about 
the 
labels 
of 
variables 
and 
their 
domains 
can 
be 
useful 
to 
check 
results, 
but 
is 
not 
actually 
required 
to 
carry 
out 
computations. 
One 
may 
also 
specify 
the 
name 
and 
domain 
of 
each 
variable, 
for 
example 


>>variable(3) 
ans 
= 
domain: 
{'murderer’ 
'not 
murderer'} 
name: 
'butler’ 


The 
variable 
name 
and 
domain 
information 
in 
the 
Clouseau 
example 
is 
stored 
in 
the 
structure 
variable, 
which 
can 
be 
helpful 
to 
display 
the 
potential 
table: 


>> 
disptable(pot(1),variable); 
knife 
= 
used 
maid 
= 
murderer 
butler 
= 
murderer 
0.100000 
knife 
= 
not 
used 
maid 
= 
murderer 
butler 
= 
murderer 
0.900000 
knife 
= 
used 
maid 
= 
not 
murderer 
butler 
= 
murderer 
0.600000 
knife 
= 
not 
used 
maid 
= 
not 
murderer 
butler 
= 
murderer 
0.400000 
knife 
= 
used 
maid 
= 
murderer 
butler 
= 
not 
murderer 
0.200000 
knife 
= 
not 
used 
maid 
= 
murderer 
butler 
= 
not 
murderer 
0.800000 
knife 
= 
used 
maid 
= 
not 
murderer 
butler 
= 
not 
murderer 
0.300000 
knife 
= 
not 
used 
maid 
= 
not 
murderer 
butler 
= 
not 
murderer 
0.700000 


4At 
the 
time 
of 
writing, 
some 
versions 
of 
MATLAB 
suer 
from 
serious 
memory 
indexing 
bugs, 
some 
of 
which 
may 
appear 
in 
the 
array 
structures 
used 
in 
the 
code 
provided. 
To 
deal 
with 
this, 
turn 
off 
the 
JIT 
accelerator 
by 
typing 
feature 
accel 
off. 


DRAFT 
March 
9, 
2010 
15 



Code 


Multiplying 
Potentials 


In 
order 
to 
multiply 
potentials 
(as 
for 
arrays) 
the 
tables 
of 
each 
potential 
must 
be 
dimensionally 
consistent 


– 
that 
is 
the 
number 
of 
states 
of 
variable 
i 
in 
potential 
1 
must 
match 
the 
number 
of 
states 
of 
variable 
i 
in 
any 
other 
potential. 
This 
can 
be 
checked 
using 
potvariables.m. 
This 
consistency 
is 
also 
required 
for 
other 
basic 
operations 
such 
as 
summing 
potentials. 
multpots.m: 
Multiplying 
two 
or 
more 
potentials 
divpots.m: 
Dividing 
a 
potential 
by 
another 
Summing 
a 
Potential 


sumpot.m: 
Sum 
(marginalise) 
a 
potential 
over 
a 
set 
of 
variables 
sumpots.m: 
Sum 
a 
set 
of 
potentials 
together 


Making 
a 
conditional 
Potential 


condpot.m: 
Make 
a 
potential 
conditioned 
on 
variables 


Setting 
a 
Potential 


setpot.m: 
Set 
variables 
in 
a 
potential 
to 
given 
states 
setevpot.m: 
Set 
variables 
in 
a 
potential 
to 
given 
states 
and 
return 
also 
an 
identity 
potential 
on 
the 
given 
states 


The 
philosophy 
of 
BRMLtoolbox 
is 
that 
all 
information 
about 
variables 
is 
local 
and 
is 
read 
off 
from 
a 
potential. 
Using 
setevpot.m 
enables 
one 
to 
set 
variables 
in 
a 
state 
whilst 
maintaining 
information 
about 
the 
number 
of 
states 
of 
a 
variable. 


Maximising 
a 
Potential 


maxpot.m: 
Maximise 
a 
potential 
over 
a 
set 
of 
variables 


See 
also 
maxNarray.m 
and 
maxNpot.m 
which 
return 
the 
N-highest 
values 
and 
associated 
states. 


Other 
potential 
utilities 


setstate.m: 
Set 
the 
a 
potential 
state 
to 
a 
given 
value 
table.m: 
Return 
a 
table 
from 
a 
potential 
whichpot.m: 
Return 
potentials 
which 
contain 
a 
set 
of 
variables 
potvariables.m: 
Variables 
and 
their 
number 
of 
states 
in 
a 
set 
of 
potentials 
orderpotfields.m: 
Order 
the 
elds 
of 
a 
potential 
structure 
uniquepots.m: 
Merge 
redundant 
potentials 
and 
return 
only 
unique 
ones 
numstates.m: 
Number 
of 
states 
of 
a 
variable 
in 
a 
domain 
squeezepots.m: 
Remove 
redundant 
potentials 
by 
merging 
normpot.m: 
Normalise 
a 
potential 
to 
form 
a 
distribution 


1.5.2 
General 
utilities 
condp.m: 
Return 
a 
table 
p(xjy) 
from 
p(x, 
y) 
condexp.m: 
Form 
a 
conditional 
distribution 
from 
a 
log 
value 
logsumexp.m: 
Compute 
the 
log 
of 
a 
sum 
of 
exponentials 
in 
a 
numerically 
precise 
way 
normp.m: 
Return 
a 
normalised 
table 
from 
an 
unnormalised 
table 


16 
DRAFT 
March 
9, 
2010 



Exercises 


assign.m: 
Assign 
values 
to 
multiple 
variables 
maxarray.m: 
Maximize 
a 
multi-dimensional 
array 
over 
a 
subset 


1.5.3 
An 
example 
The 
following 
code 
highlights 
the 
use 
of 
the 
above 
routines 
in 
solving 
the 
Inspector 
Clouseau, 
example(2). 
demoClouseau.m: 
Solving 
the 
Inspector 
Clouseau 
example 


1.6 
Notes 
The 
interpretation 
of 
probability 
is 
contentious 
and 
we 
refer 
the 
reader 
to 
[145, 
183, 
179] 
for 
detailed 
discussions. 
A 
useful 
website 
that 
relates 
to 
understanding 
probability 
and 
Bayesian 
reasoning 
is 
understandinguncer

1.7 
Exercises 
Exercise 
1. 
Prove 


p(x, 
yjz)= 
p(xjz)p(yjx, 
z) 
(1.7.1) 


and 
also 


p(yjx, 
z)p(xjz) 


p(xjy, 
z) 
= 
(1.7.2) 


p(yjz) 


Exercise 
2. 
Prove 
the 
Bonferroni 
inequality 


p(a, 
b) 
= 
p(a)+ 
p(b) 
- 
1 
(1.7.3) 


Exercise 
3 
(Adapted 
from 
[167]). 
There 
are 
two 
boxes. 
Box 
1 
contains 
three 
red 
and 
ve 
white 
balls 
and 
box 
2 
contains 
two 
red 
and 
ve 
white 
balls. 
A 
box 
is 
chosen 
at 
random 
p(box 
= 
1) 
= 
p(box 
= 
2) 
= 
0:5 
and 
a 
ball 
chosen 
at 
random 
from 
this 
box 
turns 
out 
to 
be 
red. 
What 
is 
the 
posterior 
probability 
that 
the 
red 
ball 
came 
from 
box 
1? 


Exercise 
4 
(Adapted 
from 
[167]). 
Two 
balls 
are 
placed 
in 
a 
box 
as 
follows: 
A 
fair 
coin 
is 
tossed 
and 
a 
white 
ball 
is 
placed 
in 
the 
box 
if 
a 
head 
occurs, 
otherwise 
a 
red 
ball 
is 
placed 
in 
the 
box. 
The 
coin 
is 
tossed 
again 
and 
a 
red 
ball 
is 
placed 
in 
the 
box 
if 
a 
tail 
occurs, 
otherwise 
a 
white 
ball 
is 
placed 
in 
the 
box. 
Balls 
are 
drawn 
from 
the 
box 
three 
times 
in 
succession 
(always 
with 
replacing 
the 
drawn 
ball 
back 
in 
the 
box). 
It 
is 
found 
that 
on 
all 
three 
occasions 
a 
red 
ball 
is 
drawn. 
What 
is 
the 
probability 
that 
both 
balls 
in 
the 
box 
are 
red? 


Exercise 
5 
(From 
David 
Spiegelhalter 
understandinguncertainty.org). 
A 
secret 
government 
agency 
has 
developed 
a 
scanner 
which 
determines 
whether 
a 
person 
is 
a 
terrorist. 
The 
scanner 
is 
fairly 
reliable; 
95% 
of 
all 
scanned 
terrorists 
are 
identied 
as 
terrorists, 
and 
95% 
of 
all 
upstanding 
citizens 
are 
identied 
as 
such. 
An 
informant 
tells 
the 
agency 
that 
exactly 
one 
passenger 
of 
100 
aboard 
an 
aeroplane 
in 
which 
you 
are 
seated 
is 
a 
terrorist. 
The 
agency 
decide 
to 
scan 
each 
passenger 
and 
the 
shifty 
looking 
man 
sitting 
next 
to 
you 
is 
the 
rst 
to 
test 
positive. 
What 
are 
the 
chances 
that 
this 
man 
is 
a 
terrorist? 


Exercise 
6 
(The 
Monty 
Hall 
problem). 
On 
a 
gameshow 
there 
are 
three 
doors. 
Behind 
one 
door 
is 
a 
prize. 
The 
gameshow 
host 
asks 
you 
to 
pick 
a 
door. 
He 
then 
opens 
a 
dierent 
door 
to 
the 
one 
you 
chose 
and 
shows 
that 
there 
is 
no 
prize 
behind 
it. 
Is 
is 
better 
to 
stick 
with 
your 
original 
guess 
as 
to 
where 
the 
prize 
is, 
or 
better 
to 
change 
your 
mind? 


Exercise 
7. 
Consider 
three 
variable 
distributions 
which 
admit 
the 
factorisation 


p(a, 
b, 
c)= 
p(ajb)p(bjc)p(c) 
(1.7.4) 


where 
all 
variables 
are 
binary. 
How 
many 
parameters 
are 
needed 
to 
specify 
distributions 
of 
this 
form? 


DRAFT 
March 
9, 
2010 
17 



Exercises 


Exercise 
8. 
Repeat 
the 
Inspector 
Clouseau 
scenario, 
example(2), 
but 
with 
the 
restriction 
that 
either 
the 
Maid 
or 
the 
Butler 
is 
the 
murderer, 
but 
not 
both. 
Explicitly, 
the 
probability 
of 
the 
Maid 
being 
the 
Murder 
and 
not 
the 
Butler 
is 
0.04, 
the 
probability 
of 
the 
Butler 
being 
the 
Murder 
and 
not 
the 
Maid 
is 
0.64. 
Modify 
demoClouseau.m 
to 
implement 
this. 


Exercise 
9. 
Prove 


p(a, 
(b 
or 
c)) 
= 
p(a, 
b)+ 
p(a, 
c) 
- 
p(a, 
b, 
c) 
(1.7.5) 


Exercise 
10. 
Prove 


XX

p(xjz)=p(xjy, 
z)p(yjz)=p(xjw, 
y, 
z)p(wjy, 
z)p(yjz) 
(1.7.6) 


y 
y;w 


Exercise 
11. 
As 
a 
young 
man 
Mr 
Gott 
visits 
Berlin 
in 
1969. 
He's 
surprised 
that 
he 
cannot 
cross 
into 
East 
Berlin 
since 
there 
is 
a 
wall 
separating 
the 
two 
halves 
of 
the 
city. 
He's 
told 
that 
the 
wall 
was 
erected 
8 
years 
previously. 
He 
reasons 
that 
: 
The 
wall 
will 
have 
a 
nite 
lifespan; 
his 
ignorance 
means 
that 
he 
arrives 
uniformly 
at 
random 
at 
some 
time 
in 
the 
lifespan 
of 
the 
wall. 
Since 
only 
5% 
of 
the 
time 
one 
would 
arrive 
in 
the 
rst 
or 
last 
2.5% 
of 
the 
lifespan 
of 
the 
wall 
he 
asserts 
that 
with 
95% 
condence 
the 
wall 
will 
survive 
between 
8=0:975 
˜ 
8:2 
and 
8=0:025 
= 
320 
years. 
In 
1989 
the 
now 
Professor 
Gott 
is 
pleased 
to 
nd 
that 
his 
prediction 
was 
correct 
and 
promotes 
his 
prediction 
method 
in 
elite 
journals. 
This 
`delta-t’ 
method 
is 
widely 
adopted 
and 
used 
to 
form 
predictions 
in 
a 
range 
of 
scenarios 
about 
which 
researchers 
are 
`totally 
ignorant'. 
Would 
you 
`buy’ 
a 
prediction 
from 
Prof. 
Gott? 
Explain 
carefully 
your 
reasoning. 


Exercise 
12. 
Implement 
the 
soft 
XOR 
gate, 
example(6) 
using 
BRMLtoolbox. 
You 
may 
nd 
condpot.m 
of 
use. 


Exercise 
13. 
Implement 
the 
hamburgers, 
example(1) 
(both 
scenarios) 
using 
BRMLtoolbox. 
To 
do 
so 
you 
will 
need 
to 
dene 
the 
joint 
distribution 
p(hamburgers, 
KJ) 
in 
which 
dom(hamburgers)= 
dom(KJ)= 
ftr, 
fag. 


Exercise 
14. 
Implement 
the 
two-dice 
example, 
section(1.3.1) 
using 
BRMLtoolbox. 


Exercise 
15. 
A 
redistribution 
lottery 
involves 
picking 
the 
correct 
four 
numbers 
from 
1 
to 
9 
(without 
replacement, 
so 
3,4,4,1 
for 
example 
is 
not 
possible). 
The 
order 
of 
the 
picked 
numbers 
is 
irrelevant. 
Every 
week 
a 
million 
people 
play 
this 
game, 
each 
paying 
$1 
to 
enter, 
with 
the 
numbers 
3,5,7,9 
being 
the 
most 
popular 
(1 
in 
every 
100 
people 
chooses 
these 
numbers). 
Given 
that 
the 
million 
pounds 
prize 
money 
is 
split 
equally 
between 
winners, 
and 
that 
any 
four 
(dierent) 
numbers 
come 
up 
at 
random, 
what 
is 
the 
expected 
amount 
of 
money 
each 
of 
the 
players 
choosing 
3,5,7,9 
will 
win 
each 
week? 
The 
least 
popular 
set 
of 
numbers 
is 
1,2,3,4 
with 
only 
1 
in 
10,000 
people 
choosing 
this. 
How 
much 
do 
they 
prot 
each 
week, 
on 
average? 
Do 
you 
think 
there 
is 
any 
`skill’ 
involved 
in 
playing 
this 
lottery? 


Exercise 
16. 
In 
a 
test 
of 
`psychometry’ 
the 
car 
keys 
and 
wrist 
watches 
of 
5 
people 
are 
given 
to 
a 
medium. 
The 
medium 
then 
attempts 
to 
match 
the 
wrist 
watch 
with 
the 
car 
key 
of 
each 
person. 
What 
is 
the 
expected 
number 
of 
correct 
matches 
that 
the 
medium 
will 
make 
(by 
chance)? 
What 
is 
the 
probability 
that 
the 
medium 
will 
obtain 
at 
least 
1 
correct 
match? 


DRAFT 
March 
9, 
2010 



CHAPTER 
2 


Basic 
Graph 
Concepts 


2.1 
Graphs 
Denition 
8 
(Graph). 
A 
graph 
G 
consists 
of 
vertices 
(nodes) 
and 
edges 
(links) 
between 
the 
vertices. 
Edges 
may 
be 
directed 
(they 
have 
an 
arrow 
in 
a 
single 
direction) 
or 
undirected. 
A 
graph 
with 
all 
edges 
directed 
is 
called 
a 
directed 
graph, 
and 
one 
with 
all 
edges 
undirected 
is 
called 
an 
undirected 
graph. 


Denition 
9 
(Path, 
Ancestors, 
Descendants). 
A 
path 
A7!7B 
from 
node 
A 
to 
node 
B 
is 
a 
sequence 
of 
vertices 
A0 
= 
A, 
A1;:::;An..1;An 
= 
B, 
with 
(An;An+1) 
an 
edge 
in 
the 
graph, 
thereby 
connecting 
A 
to 


B. 
For 
a 
directed 
graph 
this 
means 
that 
a 
path 
is 
a 
sequence 
of 
nodes 
which 
when 
we 
follow 
the 
direction 
of 
the 
arrows 
leads 
us 
from 
A 
to 
B. 
The 
vertices 
A 
such 
that 
A7!7B 
and 
B67!7A 
are 
the 
ancestors 
of 
B. 
The 
vertices 
B 
such 
that 
A7!7B 
and 
B67!7A 
are 
the 
descendants 
of 
A[168]. 


Denition 
10 
(Directed 
Acyclic 
Graph 
(DAG)). 
A 
DAG 
is 
a 
graph 
G 
with 
directed 
edges 
(arrows 
on 
each 
link) 
between 
the 
vertices 
(nodes) 
such 
that 
by 
following 
a 
path 
of 
vertices 
from 
one 
node 
to 
another 
along 
the 
direction 
of 
each 
edge 
no 
path 
will 
revisit 
a 
vertex. 
In 
a 
DAG 
the 
ancestors 
of 
B 
are 
those 
nodes 
who 
have 
a 
directed 
path 
ending 
at 
B. 
Conversely, 
the 
descendants 
of 
A 
are 
those 
nodes 
who 
have 
a 
directed 
path 
starting 
at 
A. 


Denition 
11 
(Relationships 
in 
a 
DAG). 


The 
parents 
of 
x4 
are 
pa(x4)= 
fx1;x2;x3g. 
The 
children 
of 
x4 
are 
ch(x4)= 
fx5;x6g. 
The 
family 
of 
a 
node 
is 
itself 
and 
its 
parents. 
The 
Markov 
blanket 
of 
a 
node 
is 
itself, 
its 
parents, 
children 
and 
the 
parents 
of 
its 
children. 
In 
this 
case, 
the 
Markov 
blanket 
of 
x4 
is 
x1;x2;:::;x7. 


x4
x1x2x3
x5x6
x7x8
19 



Graphs 


Figure 
2.1: 
(a) 
Singly-Connected 
graph. 


(b) 
Multiply-Connected 
graph. 
d
a
c
b
e
fg
d
a
c
b
e
fg
(a) 
(b) 
Denition 
12 
(Undirected 
graph). 


AB
CD
E
An 
undirected 
graph 
G 
consists 
of 
undirected 
edges 
between 
nodes. 


Denition 
13 
(Neighbour). 
For 
an 
undirected 
graph 
G 
the 
neighbours 
of 
x, 
ne(x) 
are 
those 
nodes 
directly 
connected 
to 
x. 


Denition 
14 
(Connected 
graph). 
An 
undirected 
graph 
is 
connected 
if 
there 
is 
a 
path 
between 
every 
set 
of 
vertices 
(i.e. 
there 
are 
no 
isolated 
islands). 
For 
a 
graph 
which 
is 
not 
connected, 
the 
connected 
components 
are 
those 
subgraphs 
which 
are 
connected. 


Denition 
15 
(Clique). 


Given 
an 
undirected 
graph, 
a 
clique 
is 
a 
maximally 
connected 
subset 
of 
vertices. 
All 
the 
members 
of 
the 
clique 
are 
connected 
to 
each 
other; 
furthermore 
there 
is 
no 
larger 
clique 
that 
can 
be 
made 
from 
a 
clique. 
For 
example 
this 
graph 
has 
two 
cliques, 
C1 
= 
fA, 
B, 
C, 
D} 
and 
C2 
= 
fB, 
C, 
Eg. 
Whilst 
A, 
B, 
C 
are 
fully 
connected, 
this 
is 
a 
non-maximal 
clique 
since 
there 
is 
a 
larger 
fully 
connected 
set, 
A, 
B, 
C, 
D 
that 
contains 
this. 
A 
non-maximal 
clique 
is 
sometimes 
called 
a 
cliquo. 


AB
CD
E
Denition 
16 
(Singly-Connected 
Graph). 
A 
graph 
is 
singly-connected 
if 
there 
is 
only 
one 
path 
from 
a 
vertex 
a 
to 
another 
vertex 
b. 
Otherwise 
the 
graph 
is 
multiply-connected. 
This 
denition 
applies 
regardless 
of 
whether 
or 
not 
the 
edges 
in 
the 
graph 
are 
directed. 
An 
alternative 
name 
for 
a 
singly-connected 
graph 
is 
a 
tree. 
A 
multiply-connected 
graph 
is 
also 
called 
loopy. 


DRAFT 
March 
9, 
2010 



Numerically 
Encoding 
Graphs 


2.1.1 
Spanning 
tree 
Denition 
17 
(Spanning 
Tree). 


A 
spanning 
tree 
of 
an 
undirected 
graph 
G 
is 
a 
singly-connected 
subset 
of 
the 
existing 
edges 
such 
that 
the 
resulting 
singly-
connected 
graph 
covers 
all 
vertices 
of 
G. 
On 
the 
right 
is 
a 
graph 
and 
an 
associated 
spanning 
tree. 
A 
maximum 
weight 
spanning 
tree 
is 
a 
spanning 
tree 
such 
that 
the 
sum 
of 
all 
weights 
on 
the 
edges 
of 
the 
tree 
is 
larger 
than 
for 
any 
other 
spanning 
tree 
of 
G. 


Finding 
the 
maximal 
weight 
spanning 
tree 


A 
simple 
algorithm 
to 
nd 
a 
spanning 
tree 
with 
maximal 
weight 
is 
as 
follows: 
Start 
by 
picking 
the 
edge 
with 
the 
largest 
weight 
and 
add 
this 
to 
the 
edge 
set. 
Then 
pick 
the 
next 
candidate 
edge 
which 
has 
the 
largest 
weight 
and 
add 
this 
to 
the 
edge 
set 
– 
if 
this 
results 
in 
an 
edge 
set 
with 
cycles, 
then 
reject 
the 
candidate 
edge 
and 
nd 
the 
next 
largest 
edge 
weight. 
Note 
that 
there 
may 
be 
more 
than 
one 
maximal 
weight 
spanning 
tree. 


2.2 
Numerically 
Encoding 
Graphs 
To 
express 
the 
structure 
of 
GMs 
we 
need 
to 
numerically 
encode 
the 
links 
on 
the 
graphs. 
For 
a 
graph 
of 
N 
vertices, 
we 
can 
describe 
the 
graph 
structure 
in 
various 
equivalent 
ways. 


2.2.1 
Edge 
list 
As 
the 
name 
suggests, 
an 
edge 
list 
simply 
lists 
which 
vertex-vertex 
pairs 
are 
in 
the 
graph. 
For 
g(2.2a), 
an 
edge 
list 
is 
L 
= 
f(1, 
2), 
(2, 
1), 
(1, 
3), 
(3, 
1), 
(2, 
3), 
(3, 
2), 
(2, 
4), 
(4, 
2), 
(3, 
4), 
(4, 
3)} 
where 
an 
undirected 
edge 
is 
represented 
by 
a 
bidirectional 
edge. 


2.2.2 
Adjacency 
matrix 
An 
alternative 
is 
to 
use 
an 
adjacency 
matrix 



.


.


0 
1 
1 
0 
1 
0 
1 
1 
1 
1 
0 
1 
0 
1 
1 
0 


BB
. 


CC
.


A 
= 


(2.2.1) 
where 
Aij 
= 
1 
if 
there 
is 
an 
edge 
from 
variable 
i 
to 
j 
in 
the 
graph, 
and 
0 
otherwise. 
Some 
authors 
include 
self-connections 
in 
this 
denition. 
An 
undirected 
graph 
has 
a 
symmetric 
adjacency 
matrix. 


Provided 
that 
the 
vertices 
are 
labelled 
in 
ancestral 
order 
(parents 
always 
come 
before 
children) 
a 
directed 
graph 
g(2.2b) 
can 
be 
represented 
as 
a 
triangular 
adjacency 
matrix: 


1

. 


BB
. 


0 
1 
1 
0 
0 
0 
1 
1 
0 
0 
0 
1 
0 
0 
0 
0 


CC
.


T 
= 


(2.2.2) 
DRAFT 
March 
9, 
2010 



Numerically 
Encoding 
Graphs 


12341234
Figure 
2.2: 
(a): 
An 
undirected 
graph 
can 
be 
represented 


as 
a 
symmetric 
adjacency 
matrix. 
(b): 
A 
directed 
graph 
with 
vertices 
labelled 
in 
ancestral 
order 
corresponds 
to 
a 


triangular 
adjacency 
matrix. 


(a) 
(b) 
Adjacency 
matrix 
powers 


 


For 
an 
N 
× 
N 
adjacency 
matrix 
A, 
powers 
of 
the 
adjacency 
matrixAkspecify 
how 
many 
paths 
there 


ij

are 
from 
node 
i 
to 
node 
j 
in 
k 
edge 
hops. 


 






is 
non-zero 
when 
there 
is 
a 
path 
connecting 
j 
to 
i 
in 
the 
graph. 
If 
A 
corresponds 
to 
a 
DAG 
the 
non-zero 
entries 
of 
the 
jth 
row 
ofANcorrespond 
to 
the 
descendants 
of 
node 
j. 


2.2.3 
Clique 
matrix 
For 
an 
undirected 
graph 
with 
N 
vertices 
and 
maximal 
cliques 
C1;:::;CK 
a 
clique 
matrix 
is 
an 
n 
× 
K 
matrix 
in 
which 
each 
column 
ck 
has 
zeros 
expect 
for 
ones 
on 
entries 
describing 
the 
clique. 
A 
cliquo 
matrix 
relaxes 
the 
constraint 
that 
cliques 
are 
required 
to 
be 
maximal1 
. 
For 
example 


If 
we 
include 
1's 
on 
the 
diagonal 
of 
A 
thenAN

ij 


.
. 


BB
. 


10 
11 


11 
01 


CC
.


C 
= 


(2.2.3) 
is 
a 
clique 
matrix 
for 
g(2.2a). 
A 
cliquo 
matrix 
containing 
only 
two-dimensional 
maximal 
cliques 
is 
called 
an 
incidence 
matrix. 
For 
example 


1

0

Cinc 
= 


BB
. 


1 
1 
0 
0 
0 
1 
0 
1 
1 
0 
0 
1 
1 
0 
1 
0 
0 
0 
1 
1 


CC
.


(2.2.4) 
is 
an 
incidence 
matrix 
for 
g(2.2b). 


It 
is 
straightforward 
to 
show 
that 
CincCT 
is 
equal 
to 
the 
adjacency 
matrix 
except 
that 
the 
diagonals 
now 


inc 


contain 
the 
degree 
of 
each 
vertex 
(the 
number 
of 
edges 
it 
touches). 
Similarly, 
for 
any 
cliquo 
matrix 
the 
diagonal 
entry 
of 
[CCT]ii 
expresses 
the 
number 
of 
cliquos 
(columns) 
that 
vertex 
i 
occurs 
in. 
Off 
diagonal 
elements 
[CCT]ij 
contain 
the 
number 
of 
cliquos 
that 
vertices 
i 
and 
j 
jointly 
inhabit. 


Remark 
2 
(Graph 
Confusions). 
Graphs 
are 
widely 
used, 
but 
dier 
markedly 
in 
what 
they 
represent. 
Two 
potential 
pitfalls 
are 
described 
below. 


State 
Transition 
Diagrams 
Such 
graphical 
representations 
are 
common 
in 
Markov 
Chains 
and 
Finite 
State 
Automata. 
A 
set 
of 
states 
is 
written 
as 
set 
of 
nodes(vertices) 
of 
a 
graph, 
and 
a 
directed 
edge 
between 
node 
i 
and 
node 
j 
(with 
an 
associated 
weight 
pij) 
represents 
that 
a 
transition 
from 
state 
i 
to 
state 
j 
can 
occur 
with 
probability 
pij. 
From 
the 
graphical 
models 
perspective 
we 
would 
simply 
write 
down 
a 
directed 
graph 
x(t) 
. 
x(t 
+ 
1) 
to 
represent 
this 
Markov 
Chain. 
The 
state-transition 
diagram 
simply 
provides 
a 
graphical 
description 
of 
the 
conditional 
probability 
table 
p(x(t 
+ 
1)jx(t)). 


1The 
term 
`cliquo’ 
for 
a 
non-maximal 
clique 
is 
attributed 
to 
Julian 
Besag. 


DRAFT 
March 
9, 
2010 



Exercises 


Neural 
Networks 
Neural 
networks 
also 
have 
vertices 
and 
edges. 
In 
general, 
however, 
neural 
networks 
are 
graphical 
representations 
of 
functions, 
whereas 
as 
graphical 
models 
are 
representations 
of 
distributions 
(a 
richer 
formalism). 
Neural 
networks 
(or 
any 
other 
parametric 
description) 
may 
be 
used 
to 
represent 
the 
conditional 
probability 
tables, 
as 
in 
sigmoid 
belief 
networks[204]. 


2.3 
Code 
2.3.1 
Utility 
routines 
ancestors.m: 
Find 
the 
ancestors 
of 
a 
node 
in 
a 
DAG 
edges.m: 
Edge 
list 
from 
an 
adjacency 
matrix 
ancestralorder.m: 
Ancestral 
order 
from 
a 
DAG 
connectedComponents.m: 
Connected 
Components 


parents.m: 
Parents 
of 
a 
node 
given 
an 
adjacency 
matrix 
children.m: 
Children 
of 
a 
node 
given 
an 
adjacency 
matrix 
neigh.m: 
Neighbours 
of 
a 
node 
given 
an 
adjacency 
matrix 


A 
connected 
graph 
is 
a 
tree 
if 
the 
number 
of 
edges 
plus 
1 
is 
equal 
to 
the 
number 
of 
nodes. 
However, 
for 
a 
possibly 
disconnected 
graph 
this 
is 
not 
the 
case. 
The 
code 
below 
deals 
with 
the 
possibly 
disconnected 
case. 
The 
routine 
is 
based 
on 
the 
observation 
that 
any 
singly-connected 
graph 
must 
always 
possess 
a 
simplical 
node 
(a 
leaf 
node) 
which 
can 
be 
eliminated 
to 
reveal 
a 
smaller 
singly-connected 
graph. 
istree.m: 
If 
graph 
is 
singly 
connected 
return 
1 
and 
elimination 
sequence 
spantree.m: 
Return 
a 
spanning 
tree 
from 
an 
ordered 
edge 
list 
singleparenttree.m: 
Find 
a 
directed 
tree 
with 
at 
most 
one 
parent 
from 
an 
undirected 
tree 


Additional 
routines 
for 
basic 
manipulations 
in 
graphs 
are 
given 
at 
the 
end 
of 
chapter(6). 


2.4 
Exercises 
Exercise 
17. 
Consider 
an 
adjacency 
matrix 
A 
with 
elements 
[A]ij 
=1 
if 
one 
can 
reach 
state 
i 
from 
state 




j 
in 
one 
timestep, 
and 
0 
otherwise. 
Show 
that 
the 
matrixAk 
ij 
represents 
the 
number 
of 
paths 
that 
lead 
from 
state 
j 
to 
i 
in 
k 
timesteps. 
Hence 
derive 
an 
algorithm 
that 
will 
nd 
the 
minimum 
number 
of 
steps 
to 
get 
from 
state 
j 
to 
state 
i. 


Exercise 
18. 
For 
an 
N 
× 
N 
symmetric 
adjacency 
matrix 
A, 
describe 
an 
algorithm 
to 
nd 
the 
connected 
components. 
You 
may 
wish 
to 
examine 
connectedComponents.m. 


Exercise 
19. 
Show 
that 
for 
a 
connected 
graph 
that 
is 
singly-connected, 
the 
number 
of 
edges 
E 
must 
be 
equal 
to 
the 
number 
of 
vertices 
minus 
1, 
E 
= 
V 
- 
1. 
Give 
an 
example 
graph 
with 
E 
= 
V 
- 
1 
that 
is 
not 
singly-connected. 


DRAFT 
March 
9, 
2010 



Exercises 


DRAFT 
March 
9, 
2010 



CHAPTER 
3 


Belief 
Networks 


3.1 
Probabilistic 
Inference 
in 
Structured 
Distributions 
Consider 
an 
environment 
composed 
of 
N 
variables, 
with 
a 
corresponding 
distribution 
p(x1;:::;xN 
). 
Writing 
E 
as 
the 
set 
of 
evidential 
variables 
and 
using 
evidence 
= 
fxe 
= 
xe;e 
. 
E} 
to 
denote 
all 
available 
evidence, 
then 
inference 
and 
reasoning 
can 
be 
carried 
out 
automatically 
by 
the 
`brute 
force’ 
method1 


P

p(fxe 
= 
xe;e 
. 
E} 
;xi 
= 
xi;xfnE;nig)

xfnE;nig

p(xi 
= 
xijevidence)=P(3.1.1) 


p(fxe 
= 
xe;e 
. 
E} 
;xnE 
)

xnE 


If 
all 
variables 
are 
binary 
(take 
two 
states), 
these 
summations 
require 
O(2N..jEj) 
operations. 
Such 
exponential 
computation 
is 
impractical 
and 
techniques 
that 
reduce 
this 
burden 
by 
exploiting 
any 
structure 
in 
the 
joint 
probability 
table 
are 
the 
topic 
of 
our 
discussions 
on 
ecient 
inference. 


Naively 
specifying 
all 
the 
entries 
of 
a 
table 
p(x1;:::;xN 
) 
over 
binary 
variables 
xi 
takes 
O(2N 
) 
space. 
We 
will 
need 
to 
deal 
with 
large 
numbers 
of 
variables 
in 
machine 
learning 
and 
related 
application 
areas, 
with 
distributions 
on 
potentially 
hundreds 
if 
not 
millions 
of 
variables. 
The 
only 
way 
to 
deal 
with 
such 
large 
distributions 
is 
to 
constrain 
the 
nature 
of 
the 
variable 
interactions 
in 
some 
manner, 
both 
to 
render 
specication 
and 
ultimately 
inference 
in 
such 
systems 
tractable. 
The 
key 
idea 
is 
to 
specify 
which 
variables 
are 
independent 
of 
others, 
leading 
to 
a 
structured 
factorisation 
of 
the 
joint 
probability 
distribution. 
Belief 
Networks 
are 
a 
convenient 
framework 
for 
representing 
such 
factorisations 
into 
local 
conditional 
distributions. 
We 
will 
discuss 
Belief 
Networks 
more 
formally 
in 
section(3.3), 
rst 
discussing 
their 
natural 
graphical 
representations 
of 
distributions. 


Denition 
18 
(Belief 
Network). 
A 
Belief 
Network 
is 
a 
distribution 
of 
the 
form 


D

YP

p(x1;:::;xD)= 
p(xijpa 
(xi)) 
(3.1.2) 
i=1 


where 
pa 
(xi) 
represent 
the 
parental 
variables 
of 
variable 
xi. 
Written 
as 
a 
Directed 
Graph, 
with 
an 
arrow 
pointing 
from 
a 
parent 
variable 
to 
child 
variable, 
a 
Belief 
Network 
is 
a 
Directed 
Acyclic 
Graph 
(DAG), 
with 
the 
ith 
vertex 
in 
the 
graph 
corresponding 
to 
the 
factor 
p(xijpa 
(xi)). 


1The 
extension 
to 
continuous 
variables 
is 
straightforward, 
replacing 
summation 
with 
integration 
over 
pdfs; 
we 
defer 
treatment 
of 
this 
to 
later 
chapters, 
since 
our 
aim 
is 
to 
here 
outline 
more 
the 
intuitions 
without 
needing 
to 
deal 
with 
integration 
of 
high 
dimensional 
distributions. 


25 



Graphically 
Representing 
Distributions 


3.2 
Graphically 
Representing 
Distributions 
Belief 
Networks 
(also 
called 
Bayes’ 
Networks 
or 
Bayesian 
Belief 
Networks) 
are 
a 
way 
to 
depict 
the 
independence 
assumptions 
made 
in 
a 
distribution 
[148, 
168]. 
Their 
application 
domain 
is 
widespread, 
ranging 
from 
troubleshooting[50] 
and 
expert 
reasoning 
under 
uncertainty 
to 
machine 
learning. 
Before 
we 
more 
formally 
dene 
a 
BN, 
an 
example 
will 
help 
motivate 
the 
development2 
. 


3.2.1 
Constructing 
a 
simple 
Belief 
network 
: 
wet 
grass 
One 
morning 
Tracey 
leaves 
her 
house 
and 
realises 
that 
her 
grass 
is 
wet. 
Is 
it 
due 
to 
overnight 
rain 
or 
did 
she 
forget 
to 
turn 
off 
the 
sprinkler 
last 
night? 
Next 
she 
notices 
that 
the 
grass 
of 
her 
neighbour, 
Jack, 
is 
also 
wet. 
This 
explains 
away 
to 
some 
extent 
the 
possibility 
that 
her 
sprinkler 
was 
left 
on, 
and 
she 
concludes 
therefore 
that 
it 
has 
probably 
been 
raining. 


Making 
a 
model 


We 
can 
model 
the 
above 
situation 
using 
probability 
by 
following 
a 
general 
modelling 
approach. 
First 
we 
dene 
the 
variables 
we 
wish 
to 
include 
in 
our 
model. 
In 
the 
above 
situation, 
the 
natural 
variables 
are 


R 
2f0, 
1} 
(R 
= 
1 
means 
that 
it 
has 
been 
raining, 
and 
0 
otherwise). 


S 
2f0, 
1} 
(S 
= 
1 
means 
that 
Tracey 
has 
forgotten 
to 
turn 
off 
the 
sprinkler, 
and 
0 
otherwise). 


J 
2f0, 
1} 
(J 
= 
1 
means 
that 
Jack's 
grass 
is 
wet, 
and 
0 
otherwise). 


T 
2f0, 
1} 
(T 
= 
1 
means 
that 
Tracey's 
Grass 
is 
wet, 
and 
0 
otherwise). 


A 
model 
of 
Tracey's 
world 
then 
corresponds 
to 
a 
probability 
distribution 
on 
the 
joint 
set 
of 
the 
variables 
of 
interest 
p(T, 
J, 
R, 
S) 
(the 
order 
of 
the 
variables 
is 
irrelevant). 


Since 
each 
of 
the 
variables 
in 
this 
example 
can 
take 
one 
of 
two 
states, 
it 
would 
appear 
that 
we 
naively 
have 
to 
specify 
the 
values 
for 
each 
of 
the 
24 
= 
16 
states, 
e.g. 
p(T 
= 
1;J 
= 
0;R 
= 
1;S 
= 
1)=0:7 
etc. 
However, 
since 
there 
are 
normalisation 
conditions 
for 
probabilities, 
we 
do 
not 
need 
to 
specify 
all 
the 
state 
probabilities. 
To 
see 
how 
many 
states 
need 
to 
be 
specied, 
consider 
the 
following 
decomposition. 
Without 
loss 
of 
generality 
(wlog) 
and 
repeatedly 
using 
Bayes’ 
rule, 
we 
may 
write 


p(T, 
J, 
R, 
S)= 
p(T 
jJ, 
R, 
S)p(J, 
R, 
S) 
(3.2.1) 
= 
p(T 
jJ, 
R, 
S)p(JjR, 
S)p(R, 
S) 
(3.2.2) 
= 
p(T 
jJ, 
R, 
S)p(JjR, 
S)p(RjS)p(S) 
(3.2.3) 


That 
is, 
we 
may 
write 
the 
joint 
distribution 
as 
a 
product 
of 
conditional 
distributions. 
The 
rst 
term 
p(T 
jJ, 
R, 
S) 
requires 
us 
to 
specify 
23 
= 
8 
values 
– 
we 
need 
p(T 
= 
1jJ, 
R, 
S) 
for 
the 
8 
joint 
states 
of 
J, 
R, 
S. 
The 
other 
value 
p(T 
= 
0jJ, 
R, 
S) 
is 
given 
by 
normalisation 
: 
p(T 
= 
0jJ, 
R, 
S)=1 
- 
p(T 
= 
1jJ, 
R, 
S). 
Similarly, 
we 
need 
4 
+ 
2 
+ 
1 
values 
for 
the 
other 
factors, 
making 
a 
total 
of 
15 
values 
in 
all. 
In 
general, 
for 
a 
distribution 
on 
n 
binary 
variables, 
we 
need 
to 
specify 
2n 
- 
1 
values 
in 
the 
range 
[0, 
1]. 
The 
important 
point 
here 
is 
that 
the 
number 
of 
values 
that 
need 
to 
be 
specied 
in 
general 
scales 
exponentially 
with 
the 
number 
of 
variables 
in 
the 
model 
– 
this 
is 
impractical 
in 
general 
and 
motivates 
simplications. 


Conditional 
independence 


The 
modeler 
often 
knows 
constraints 
on 
the 
system. 
For 
example, 
in 
the 
scenario 
above, 
we 
may 
assume 
that 
Tracey's 
grass 
is 
wet 
depends 
only 
directly 
on 
whether 
or 
not 
is 
has 
been 
raining 
and 
whether 
or 
not 
her 
sprinkler 
was 
on. 
That 
is, 
we 
make 
a 
conditional 
independence 
assumption 


p(T 
jJ, 
R, 
S)= 
p(T 
jR, 
S) 
(3.2.4) 


2The 
scenario 
is 
adapted 
from 
[219]. 


26 
DRAFT 
March 
9, 
2010 



Graphically 
Representing 
Distributions 


J
R
T
SB
A
E
R
Figure 
3.1: 
(a): 
Belief 
network 
structure 
for 
the 
`wet 
grass’ 
example. 
Each 
node 
in 
the 
graph 
represents 
a 
variable 
in 
the 
joint 
distribution, 
and 
the 
variables 
which 
feed 
in 
(the 
parents) 
to 
another 
variable 
represent 
which 
variables 
are 


(a) 
(b) 
to 
the 
right 
of 
the 
conditioning 
bar. 
(b): 
BN 
for 
the 
Burglar 
model. 


Similarly, 
since 
whether 
or 
not 
Jack's 
grass 
is 
wet 
is 
inuenced 
only 
directly 
by 
whether 
or 
not 
it 
has 
been 
raining, 
we 
write 


p(JjR, 
S)= 
p(JjR) 
(3.2.5) 


and 
since 
the 
rain 
is 
not 
directly 
inuenced 
by 
the 
sprinkler, 


p(RjS)= 
p(R) 
(3.2.6) 


which 
means 
that 
our 
model 
now 
becomes 
: 


p(T, 
J, 
R, 
S)= 
p(T 
jR, 
S)p(JjR)p(R)p(S) 
(3.2.7) 


We 
can 
represent 
these 
conditional 
independencies 
graphically, 
as 
in 
g(3.1a). 
This 
reduces 
the 
number 
of 
values 
that 
we 
need 
to 
specify 
to 
4 
+ 
2 
+ 
1 
+ 
1 
= 
8, 
a 
saving 
over 
the 
previous 
15 
values 
in 
the 
case 
where 
no 
conditional 
independencies 
had 
been 
assumed. 


To 
complete 
the 
model, 
we 
need 
to 
numerically 
specify 
the 
values 
of 
each 
conditional 
probability 
table 
(CPT). 
Let 
the 
prior 
probabilities 
for 
R 
and 
S 
be 
p(R 
= 
1)=0:2 
and 
p(S 
= 
1)=0:1. 
We 
set 
the 
remaining 
probabilities 
to 
p(J 
= 
1jR 
= 
1) 
= 
1, 
p(J 
= 
1jR 
= 
0)=0:2 
(sometimes 
Jack's 
grass 
is 
wet 
due 
to 
unknown 
eects, 
other 
than 
rain), 
p(T 
= 
1jR 
= 
1;S) 
= 
1, 
p(T 
= 
1jR 
= 
0;S 
= 
1)=0:9 
(there's 
a 
small 
chance 
that 
even 
though 
the 
sprinkler 
was 
left 
on, 
it 
didn't 
wet 
the 
grass 
noticeably), 
p(T 
= 
1jR 
= 
0;S 
= 
0) 
= 
0. 


Inference 


Now 
that 
we've 
made 
a 
model 
of 
an 
environment, 
we 
can 
perform 
inference. 
Let's 
calculate 
the 
probability 
that 
the 
sprinkler 
was 
on 
overnight, 
given 
that 
Tracey's 
grass 
is 
wet: 
p(S 
= 
1jT 
= 
1). 
To 
do 
this, 
we 
use 
Bayes’ 
rule: 


P

p(S 
= 
1;T 
= 
1)= 
1, 
J, 
R, 
S 
= 
1)

J;R 
p(T 


p(S 
= 
1jT 
= 
1)= 
=P(3.2.8) 


p(T 
= 
1)= 
1, 
J, 
R, 
S)

J;R;S 
p(T 


P

J;R 
p(JjR)p(T 
= 
1jR, 
S 
= 
1)p(R)p(S 
= 
1)
=P(3.2.9) 


J;R;S 
p(JjR)p(T 
= 
1jR, 
S)p(R)p(S)

P

R 
p(T 
= 
1jR, 
S 
= 
1)p(R)p(S 
= 
1)

=P(3.2.10) 


R;S 
p(T 
= 
1jR, 
S)p(R)p(S) 


0:9 
× 
0:8 
× 
0:1+1 
× 
0:2 
× 
0:1 
= 
=0:3382 
(3.2.11)

0:9 
× 
0:8 
× 
0:1+1 
× 
0:2 
× 
0:1+0 
× 
0:8 
× 
0:9+1 
× 
0:2 
× 
0:9 
so 
the 
belief 
that 
the 
sprinkler 
is 
on 
increases 
above 
the 
prior 
probability 
0.1, 
due 
to 
the 
fact 
that 
the 
grass 
is 
wet. 


Let 
us 
now 
calculate 
the 
probability 
that 
Tracey's 
sprinkler 
was 
on 
overnight, 
given 
that 
her 
grass 
is 
wet 
and 
that 
Jack's 
grass 
is 
also 
wet, 
p(S 
= 
1jT 
= 
1;J 
= 
1). 
We 
use 
Bayes’ 
rule 
again: 


DRAFT 
March 
9, 
2010 



Graphically 
Representing 
Distributions 


p(S 
= 
1;T 
= 
1;J 
= 
1) 


p(S 
= 
1jT 
= 
1;J 
= 
1) 
= 
(3.2.12) 


p(T 
= 
1;J 
= 
1)

P

R 
p(T 
= 
1;J 
= 
1, 
R, 
S 
= 
1)

=P(3.2.13)

= 
1;J 
= 
1, 
R, 
S)

R;S 
p(T 


P

R 
p(J 
= 
1jR)p(T 
= 
1jR, 
S 
= 
1)p(R)p(S 
= 
1)

=P(3.2.14) 


R;S 
p(J 
= 
1jR)p(T 
= 
1jR, 
S)p(R)p(S) 
0:0344 


= 
=0:1604 
(3.2.15)

0:2144 


The 
probability 
that 
the 
sprinkler 
is 
on, 
given 
the 
extra 
evidence 
that 
Jack's 
grass 
is 
wet, 
is 
lower 
than 
the 
probability 
that 
the 
grass 
is 
wet 
given 
only 
that 
Tracey's 
grass 
is 
wet. 
That 
is, 
that 
the 
grass 
is 
wet 
due 
to 
the 
sprinkler 
is 
(partly) 
explained 
away 
by 
the 
fact 
that 
Jack's 
grass 
is 
also 
wet 
– 
this 
increases 
the 
chance 
that 
the 
rain 
has 
played 
a 
role 
in 
making 
Tracey's 
grass 
wet. 


Naturally, 
we 
don't 
wish 
to 
carry 
out 
such 
inference 
calculations 
by 
hand 
all 
the 
time. 
General 
purpose 
algorithms 
exist 
for 
this, 
such 
as 
the 
Junction 
Tree 
Algorithm, 
and 
we 
shall 
introduce 
these 
in 
later 
chapters. 


Example 
10 
(Was 
it 
the 
Burglar?). 
Here's 
another 
example 
using 
binary 
variables, 
adapted 
from 
[219]. 
Sally 
comes 
home 
to 
nd 
that 
the 
burglar 
alarm 
is 
sounding 
(A 
= 
1). 
Has 
she 
been 
burgled 
(B 
= 
1), 
or 
was 
the 
alarm 
triggered 
by 
an 
earthquake 
(E 
= 
1)? 
She 
turns 
the 
car 
radio 
on 
for 
news 
of 
earthquakes 
and 
nds 
that 
the 
radio 
broadcasts 
an 
earthquake 
alert 
(R 
= 
1). 


Using 
Bayes’ 
rule, 
we 
can 
write, 
without 
loss 
of 
generality, 


p(B, 
E, 
A, 
R)= 
p(AjB, 
E, 
R)p(B, 
E, 
R) 
(3.2.16) 


We 
can 
repeat 
this 
for 
p(B, 
E, 
R), 
and 
continue 


p(B, 
E, 
A, 
R)= 
p(AjB, 
E, 
R)p(RjB, 
E)p(EjB)p(B) 
(3.2.17) 


However, 
the 
alarm 
is 
surely 
not 
directly 
inuenced 
by 
any 
report 
on 
the 
Radio 
– 
that 
is, 
p(AjB, 
E, 
R)= 
p(AjB, 
E). 
Similarly, 
we 
can 
make 
other 
conditional 
independence 
assumptions 
such 
that 


p(B, 
E, 
A, 
R)= 
p(AjB, 
E)p(RjE)p(E)p(B) 
(3.2.18) 


Specifying 
conditional 
probability 
tables 


Alarm 
= 
1 
Burglar 
Earthquake 
0.9999 
1 
1 
0.99 
1 
0 
0.99 
0 
1 
0.0001 
0 
0 


Radio 
= 
1 
Earthquake 
1 
1 
0 
0 


The 
remaining 
tables 
are 
p(B 
= 
1) 
= 
0:01 
and 
p(E 
= 
1) 
= 
0:000001. 
The 
tables 
and 
graphical 
structure 
fully 
specify 
the 
distribution. 
Now 
consider 
what 
happens 
as 
we 
observe 
evidence. 


Initial 
Evidence: 
The 
Alarm 
is 
sounding 


P

E;R 
p(B 
=1, 
E, 
A 
=1;R)
p(B 
=1jA 
= 
1) 
=P(3.2.19) 


B;E;R 
p(B, 
E, 
A 
=1;R)

P

E;R 
p(A 
=1jB 
=1;E)p(B 
= 
1)p(E)p(RjE)
=P˜ 
0:99 
(3.2.20) 
B;E;R 
p(A 
=1jB, 
E)p(B)p(E)p(RjE) 


DRAFT 
March 
9, 
2010 



Graphically 
Representing 
Distributions 


Additional 
Evidence: 
The 
Radio 
broadcasts 
an 
Earthquake 
warning: 
A 
similar 
calculation 
gives 
p(B 
=1jA 
=1;R 
= 
1) 
˜ 
0:01. 
Thus, 
initially, 
because 
the 
Alarm 
sounds, 
Sally 
thinks 
that 
she's 
been 
burgled. 
However, 
this 
probability 
drops 
dramatically 
when 
she 
hears 
that 
there 
has 
been 
an 
Earthquake. 
That 
is, 
the 
Earthquake 
`explains 
away’ 
to 
an 
extent 
the 
fact 
that 
the 
Alarm 
is 
ringing. 
See 
demoBurglar.m. 


3.2.2 
Uncertain 
evidence 
In 
soft 
or 
uncertain 
evidence, 
the 
variable 
is 
in 
more 
than 
one 
state, 
with 
the 
strength 
of 
our 
belief 
about 
each 
state 
being 
given 
by 
probabilities. 
For 
example, 
if 
x 
has 
the 
states 
dom(x)= 
fred, 
blue, 
green} 
the 
vector 
(0:6, 
0:1, 
0:3) 
represents 
the 
probabilities 
of 
the 
respective 
states. 
In 
contrast, 
for 
hard 
evidence 
we 
are 
certain 
that 
a 
variable 
is 
in 
a 
particular 
state. 
In 
this 
case, 
all 
the 
probability 
mass 
is 
in 
one 
of 
the 
vector 
components, 
for 
example 
(0, 
0, 
1). 


Performing 
inference 
with 
soft-evidence 
is 
straightforward 
and 
can 
be 
achieved 
using 
Bayes’ 
rule. 
Writing 
the 
soft 
evidence 
as 
~y, 
we 
have 


X

p(xjy~) 
=p(xjy)p(yjy~) 
(3.2.21) 


y 


where 
p(y 
= 
ijy~) 
represents 
the 
probability 
that 
y 
is 
in 
state 
i 
under 
the 
soft-evidence. 
This 
is 
a 
generalisation 
of 
hard-evidence 
in 
which 
the 
vector 
p(yjy~) 
has 
all 
zero 
component 
values, 
except 
for 
all 
but 
a 
single 
component. 


Note 
that 
the 
soft 
evidence 
p(y 
= 
ijy~) 
does 
not 
correspond 
to 
the 
marginal 
p(y 
= 
i) 
in 
the 
original 
joint 
distribution 
p(x, 
y). 
A 
procedure 
to 
form 
a 
joint 
distribution, 
known 
as 
Jerey's 
rule 
is 
to 
begin 
with 
an 
original 
distribution 
p1(x, 
y), 
from 
which 
we 
can 
dene 


p1(x, 
y)

p1(xjy)= 
P(3.2.22) 


p1(x, 
y)

x 


Using 
the 
soft 
evidence 
p(yjy~) 
we 
then 
dene 
a 
new 
joint 
distribution 


p2(x, 
yjy~) 
= 
p1(xjy)p(yjy~) 
(3.2.23) 


In 
the 
BN 
we 
use 
a 
dashed 
circle 
to 
represent 
that 
a 
variable 
is 
in 
a 
soft-evidence 
state. 


Example 
11 
(soft-evidence). 
Revisiting 
the 
earthquake 
scenario, 
example(10), 
imagine 
that 
we 
think 
we 
hear 
the 
burglar 
alarm 
sounding, 
but 
are 
not 
sure, 
specically 
we 
are 
only 
70% 
sure 
we 
heard 
the 
alarm. 
For 
this 
binary 
variable 
case 
we 
represent 
this 
soft-evidence 
for 
the 
states 
(1, 
0) 
as 
A~= 
(0:7, 
0:3). 
What 
is 
the 
probability 
of 
a 
burglary 
under 
this 
soft-evidence? 


X

p(B 
= 
1jA~) 
=p(B 
= 
1jA)p(AjA~) 
= 
p(B 
= 
1jA 
= 
1) 
× 
0:7+ 
p(B 
= 
1jA 
= 
0) 
× 
0:3 
(3.2.24) 


A 


The 
probabilities 
p(B 
= 
1jA 
= 
1) 
˜ 
0:99 
and 
p(B 
= 
1jA 
= 
0) 
˜ 
0:0001 
are 
calculated 
using 
Bayes’ 
rule 
as 
before 
to 
give 


p(B 
= 
1jA~) 
˜ 
0:6930 
(3.2.25) 


Uncertain 
evidence 
versus 
unreliable 
modelling 


An 
entertaining 
example 
of 
uncertain 
evidence 
is 
given 
by 
Pearl[219]: 


DRAFT 
March 
9, 
2010 
29 



Graphically 
Representing 
Distributions 


BA
G
W
BA
H
W
BA
G
W
J
N
BA
G
W
(a) 
(b) 
(c) 
(d) 
Figure 
3.2: 
(a): 
Mr 
Holmes’ 
burglary 
worries 
as 
given 
in 
[219]: 
(B)urglar, 
(A)larm, 
(W)atson, 
Mrs 
(Gibbon). 
(b): 
Virtual 
Evidence 
can 
be 
represented 
by 
a 
dashed 
line. 
(c): 
Modied 
problem. 
Mrs 


Gibbon 
is 
not 
drinking 
but 
somewhat 
deaf; 
we 
represent 
such 
uncertain 
(soft-evidence) 
by 
a 
circle. 
(d): 
Holmes 
gets 
additional 
information 
from 
his 
neighbour 
Mrs 
(N)osy 
and 
informant 
Dodgy 
(J)oe. 


Mr 
Holmes 
receives 
a 
telephone 
call 
from 
his 
neighbour 
Dr 
Watson, 
who 
states 
that 
he 
hears 
the 
sound 
of 
a 
burglar 
alarm 
from 
the 
direction 
of 
Mr 
Holmes’ 
house. 
While 
preparing 
to 
rush 
home, 
Mr 
Holmes 
recalls 
that 
Dr 
Watson 
is 
known 
to 
be 
a 
tasteless 
practical 
joker, 
and 
he 
decides 
to 
rst 
call 
another 
neighbour, 
Mrs 
Gibbon 
who, 
despite 
occasional 
drinking 
problems, 
is 
far 
more 
reliable. 


When 
Mr 
Holmes 
calls 
Mrs 
Gibbon, 
he 
soon 
realises 
that 
she 
is 
somewhat 
tipsy. 
Instead 
of 
answering 
his 
question 
directly, 
she 
goes 
on 
and 
on 
about 
her 
latest 
back 
operation 
and 
about 
how 
terribly 
noisy 
and 
crime-ridden 
the 
neighbourhood 
has 
become. 
When 
he 
nally 
hangs 
up, 
all 
Mr 
Holmes 
can 
glean 
from 
the 
conversation 
is 
that 
there 
is 
probably 
an 
80% 
chance 
that 
Mrs 
Gibbon 
did 
hear 
an 
alarm 
sound 
from 
her 
window. 


A 
BN 
for 
this 
scenario 
is 
depicted 
in 
g(3.2a) 
which 
deals 
with 
four 
binary 
variables: 
House 
is 
(B)urgled, 
(A)larm 
has 
sounded, 
(W)atson 
hears 
alarm, 
and 
Mrs 
(G)ibbon 
hears 
alarm3: 


p(B, 
A, 
G, 
W 
)= 
p(AjB)p(B)p(W 
jA)p(GjA) 
(3.2.26) 


Holmes 
is 
interested 
in 
the 
likelihood 
that 
his 
house 
has 
been 
burgled. 
Naively, 
Holmes’ 
might 
calculate4 


p(B 
= 
trjW 
= 
tr;G 
= 
tr) 
(3.2.27) 


However, 
after 
nding 
out 
about 
Mrs 
Gibbon's 
state, 
Mr 
Holmes 
no 
longer 
nds 
the 
above 
model 
reliable. 
He 
wants 
to 
ignore 
the 
eect 
that 
Mrs 
Gibbon's 
evidence 
has 
on 
the 
inference, 
and 
replace 
it 
with 
his 
own 
belief 
as 
to 
what 
Mrs 
Gibbon 
observed. 
Mr 
Holmes 
can 
achieve 
this 
by 
replacing 
the 
term 
p(G 
= 
trjA) 
by 
a 
so-called 
virtual 
evidence 
term 




0:8 
A 
= 
tr 
p(G 
= 
trjA) 
. 
p(HjA), 
where 
p(HjA) 
=(3.2.28)

0:2 
A 
= 
fa 
Here 
the 
state 
H 
is 
arbitrary 
and 
xed. 
This 
is 
used 
to 
modify 
the 
joint 
distribution 
to 


p(B, 
A, 
H;W 
)= 
p(AjB)p(B)p(W 
jA)p(HjA), 
(3.2.29) 


see 
g(3.2b). 
When 
we 
then 
compute 
p(B 
= 
trjW 
= 
tr, 
H) 
the 
eect 
of 
Mr 
Holmes’ 
judgement 
will 
count 
for 
a 
factor 
of 
4 
times 
more 
in 
favour 
of 
the 
Alarm 
sounding 
than 
not. 
The 
values 
of 
the 
table 
entries 
are 
irrelevant 
up 
to 
normalisation 
since 
any 
constants 
can 
be 
absorbed 
into 
the 
proportionality 
constant. 
Note 
also 
that 
p(HjA) 
is 
not 
a 
distribution 
in 
A, 
and 
hence 
no 
normalisation 
is 
required. 
This 
form 
of 
evidence 
is 
also 
called 
likelihood 
evidence. 


A 
twist 
on 
Pearl's 
scenario 
is 
that 
Mrs 
Gibbon 
has 
not 
been 
drinking. 
However, 
she 
is 
a 
little 
deaf 
and 
cannot 
be 
sure 
herself 
that 
she 
heard 
the 
alarm. 
She 
is 
80% 
sure 
she 
heard 
it. 
In 
this 
case, 
Holmes 
would 


3One 
might 
be 
tempted 
to 
include 
an 
additional 
(T)ipsy 
variable 
as 
a 
parent 
of 
G. 
This 
would 
then 
require 
us 
to 
specify 
the 
joint 
distribution 
p(GjT;A) 
for 
the 
4 
parental 
joint 
states 
of 
T 
and 
A. 
Here 
we 
assume 
that 
we 
do 
not 
have 
access 
to 
such 
information. 


4The 
notation 
tr 
is 
equivalent 
to 
1 
and 
fa 
to 
0 
from 
example(10). 


DRAFT 
March 
9, 
2010 



Graphically 
Representing 
Distributions 


trust 
the 
model 
– 
however, 
the 
observation 
itself 
is 
now 
uncertain, 
g(3.2c). 
This 
can 
be 
dealt 
with 
using 
the 
soft 
evidence 
technique. 
From 
Jerey's 
rule, 
one 
uses 
the 
original 
model 
equation 
(3.2.26) 
to 
compute 


P

p(B 
= 
tr;W 
= 
tr;G) 
A 
p(GjA)p(W 
= 
trjA)p(AjB 
= 
tr) 


p(B 
= 
trjW 
= 
tr;G)= 
= 
P(3.2.30) 


p(W 
= 
tr;G) 
B;A 
p(GjA)p(W 
= 
trjA)p(AjB) 


and 
then 
uses 
the 
soft-evidence 




0:8 
G 
= 
tr 
p(GjG~
=(3.2.31)

0:2 
G 
= 
fa 
to 
compute 


~

p(B 
= 
trjW 
= 
tr;G)= 
p(B 
= 
trjW 
= 
tr;G 
= 
tr)p(G 
= 
trjG~
p(B 
= 
trjW 
= 
tr;G 
= 
fa)p(G 
= 
fajG~


(3.2.32) 
The 
reader 
may 
show 
that 
an 
alternative 
way 
to 
represent 
an 
uncertain 
observation 
such 
as 
Mrs 
Gibbon 
being 
non-tipsy 
but 
hard-of-hearing 
above 
is 
to 
use 
a 
virtual 
evidence 
child 
from 
G. 


Uncertain 
evidence 
within 
an 
unreliable 
model 


To 
highlight 
uncertain 
evidence 
in 
an 
unreliable 
model 
we 
introduce 
two 
additional 
characters. 
Mrs 
Nosy 
lives 
next 
door 
to 
Mr 
Holmes 
and 
is 
completely 
deaf, 
but 
nevertheless 
an 
incorrigible 
curtain-peeker 
who 
seems 
to 
notice 
most 
things. 
Unfortunately, 
she's 
also 
rather 
prone 
to 
imagining 
things. 
Based 
on 
his 
conversation 
with 
her, 
Mr 
Holmes 
counts 
her 
story 
as 
3 
times 
in 
favour 
of 
there 
not 
being 
a 
burglary 
to 
there 
being 
a 
burglary, 
and 
therefore 
uses 
a 
virtual 
evidence 
term 




1 
B 
= 
tr 


p(NosyjB) 
=(3.2.33)

3 
B 
= 
fa 


Mr 
Holmes 
also 
telephones 
Dodgy 
Joe, 
his 
contact 
in 
the 
criminal 
underworld 
to 
see 
if 
he's 
heard 
of 
any 
planned 
burglary 
on 
Mr 
Holmes’ 
home. 
He 
summarises 
this 
information 
using 
a 
virtual 
evidence 
term 




1 
B 
= 
tr 


p(JoejB) 
=(3.2.34)

5 
B 
= 
fa 


When 
all 
this 
information 
is 
combined 
: 
Mrs 
Gibbon 
is 
not 
tipsy 
but 
somewhat 
hard 
of 
hearing, 
Mrs 
Nosy, 
and 
Dodgy 
Joe, 
we 
rst 
deal 
with 
the 
unreliable 
model 


p(B, 
A, 
W 
= 
tr, 
G, 
Nosy, 
Joe) 
. 
p(B)p(NosyjB)p(JoejB)p(AjB)p(W 
= 
trjA)p(GjA) 
(3.2.35) 


from 
which 
we 
can 
compute 


p(B 
= 
trjW 
= 
tr, 
G, 
Nosy, 
Joe) 
(3.2.36) 


Finally 
we 
perform 
inference 
with 
the 
soft-evidence 


X

~

p(B 
= 
trjW 
= 
tr, 
G, 
Nosy, 
Joe)=p(B 
= 
trjW 
= 
tr, 
G, 
Nosy, 
Joe)p(GjG~
) 
(3.2.37) 


G 


An 
important 
consideration 
above 
is 
that 
the 
virtual 
evidence 
does 
not 
replace 
the 
prior 
p(B) 
with 
another 
prior 
distribution 
– 
rather 
the 
virtual 
evidence 
terms 
modify 
the 
prior 
through 
the 
inclusion 
of 
extra 
factors. 
The 
usual 
assumption 
is 
that 
each 
virtual 
evidence 
acts 
independently, 
although 
one 
can 
consider 
dependent 
scenarios 
if 
required. 


DRAFT 
March 
9, 
2010 



Belief 
Networks 


x1x2x3x4x3x4x1x2
(a) 
(b) 
Figure 
3.3: 
Two 
BNs 
for 
a 
4 
variable 
distribution. 
Both 
graphs 
(a) 
and 
(b) 
represent 
the 
same 
distribution 
p(x1;x2;x3;x4). 
Strictly 
speaking 
they 
represent 
the 
same 
(lack 
of) 
independence 
assumptions 
– 
the 
graphs 
say 
nothing 
about 
the 
content 
of 
the 
CPTs. 
The 
extension 
of 
this 
`cascade’ 
to 
many 
variables 
is 
clear 
and 
always 
results 
in 
a 
Directed 
Acyclic 
Graph. 


3.3 
Belief 
Networks 
In 
the 
Wet 
Grass 
and 
Burglar 
examples, 
we 
had 
a 
choice 
as 
to 
how 
we 
recursively 
used 
Bayes’ 
rule. 
In 
a 


general 
4 
variable 
case 
we 
could 
choose 
the 
factorisation, 
p(x1, 
x2, 
x3, 
x4) 
= 
p(x1jx2, 
x3, 
x4)p(x2jx3, 
x4)p(x3jx4)p(x4) 
An 
equally 
valid 
choice 
is 
(see 
g(3.3)) 
(3.3.1) 
p(x1, 
x2, 
x3, 
x4) 
= 
p(x3jx4, 
x1, 
x2)p(x4jx1, 
x2)p(x1jx2)p(x2). 
(3.3.2) 


In 
general, 
two 
dierent 
graphs 
may 
represent 
the 
same 
independence 
assumptions, 
as 
we 
will 
discuss 
further 
in 
section(3.3.1). 
If 
one 
wishes 
to 
make 
independence 
assumptions, 
then 
the 
choice 
of 
factorisation 
becomes 
signicant. 


The 
observation 
that 
any 
distribution 
may 
be 
written 
in 
the 
cascade 
form, 
g(3.3), 
gives 
an 
algorithm 
for 
constructing 
a 
BN 
on 
variables 
x1;:::;xn 
: 
write 
down 
the 
n..variable 
cascade 
graph; 
assign 
any 
ordering 
of 
the 
variables 
to 
the 
nodes; 
you 
may 
then 
delete 
any 
of 
the 
directed 
connections. 
More 
formally, 
this 
corresponds 
to 
an 
ordering 
of 
the 
variables 
which, 
without 
loss 
of 
generality, 
we 
may 
write 
as 
x1;:::;xn. 
Then, 
from 
Bayes’ 
rule, 
we 
have 


p(x1;:::;xn)= 
p(x1jx2;:::;xn)p(x2;:::;xn) 
(3.3.3) 


= 
p(x1jx2;:::;xn)p(x2jx3;:::;xn)p(x3;:::;xn) 
(3.3.4) 


n..1

n 


= 
p(xn) 
p(xijxi+1;:::;xn) 
(3.3.5) 
i=1 


The 
representation 
of 
any 
BN 
is 
therefore 
a 
Directed 
Acyclic 
Graph 
(DAG). 


Every 
probability 
distribution 
can 
be 
written 
as 
a 
BN, 
even 
though 
it 
may 
correspond 
to 
a 
fully 
connected 
`cascade’ 
DAG. 
The 
particular 
role 
of 
a 
BN 
is 
that 
the 
structure 
of 
the 
DAG 
corresponds 
to 
a 
set 
of 
conditional 
independence 
assumptions, 
namely 
which 
parental 
variables 
are 
sucient 
to 
specify 
each 
conditional 
probability 
table. 
Note 
that 
this 
does 
not 
mean 
that 
non-parental 
variables 
have 
no 
inuence. 
For 
example, 
for 
distribution 
p(x1jx2)p(x2jx3)p(x3) 
with 
DAG 
x1 
. 
x2 
. 
x3, 
this 
does 
not 
imply 
p(x2jx1;x3)= 
p(x2jx3). 
The 
DAG 
species 
conditional 
independence 
statements 
of 
variables 
on 
their 
ancestors 
– 
namely 
which 
ancestors 
are 
`causes’ 
for 
the 
variable. 


The 
DAG 
corresponds 
to 
a 
statement 
of 
conditional 
independencies 
in 
the 
model. 
To 
complete 
the 
speci
cation 
of 
the 
BN 
we 
need 
to 
dene 
all 
elements 
of 
the 
conditional 
probability 
tables 
p(xijpa 
(xi)). 
Once 
the 
graphical 
structure 
is 
dened, 
the 
entries 
of 
the 
conditional 
probability 
tables 
(CPTs) 
p(xijpa 
(xi)) 
can 
be 
expressed. 
For 
every 
possible 
state 
of 
the 
parental 
variables 
pa 
(xi), 
a 
value 
for 
each 
of 
the 
states 
of 
xi 
needs 
to 
be 
specied 
(except 
one, 
since 
this 
is 
determined 
by 
normalisation). 
For 
a 
large 
number 
of 
parents, 
writing 
out 
a 
table 
of 
values 
is 
intractable, 
and 
the 
tables 
are 
usually 
parameterised 
in 
a 
low 
dimensional 
manner. 
This 
will 
be 
a 
central 
topic 
of 
our 
discussion 
on 
the 
application 
of 
BNs 
in 
machine 
learning. 


DRAFT 
March 
9, 
2010 



Belief 
Networks 


3.3.1 
Conditional 
independence 
Whilst 
a 
BN 
corresponds 
to 
a 
set 
of 
conditional 
independence 
assumptions, 
it 
is 
not 
always 
immediately 
clear 
from 
the 
DAG 
whether 
a 
set 
of 
variables 
is 
conditionally 
independent 
of 
a 
set 
of 
other 
variables. 
For 
example, 
in 
g(3.4) 
are 
x1 
and 
x2 
independent, 
given 
the 
state 
of 
x4? 
The 
answer 
is 
yes, 
since 
we 
have 


XX

11 


p(x1;x2jx4)= 
p(x1;x2;x3;x4)= 
p(x1jx4)p(x2jx3;x4)p(x3)p(x4) 
(3.3.6) 


p(x4)p(x4)

x3 
x3

X

= 
p(x1jx4)
x3 
p(x2jx3, 
x4)p(x3) 
(3.3.7) 
Now 
p(x2jx4) 
= 
1 
p(x4)
X
Xx1;x3 
p(x1, 
x2, 
x3, 
x4) 
= 
1 
p(x4)Xx1;x3 
p(x1jx4)p(x2jx3, 
x4)p(x3)p(x4) 
(3.3.8) 
=
x3 
p(x2jx3, 
x4)p(x3) 
(3.3.9) 


Combining 
the 
two 
results 
above 
we 
have 


p(x1;x2jx4)= 
p(x1jx4)p(x2jx4) 
(3.3.10) 
so 
that 
x1 
and 
x2 
are 
indeed 
independent 
conditioned 
on 
x4. 


Denition 
19 
(Conditional 
Independence). 


X 
.
??YjZ 
(3.3.11) 
denotes 
that 
the 
two 
sets 
of 
variables 
X 
and 
Y 
are 
independent 
of 
each 
other 
provided 
we 
know 
the 
state 
of 
the 
set 
of 
variables 
Z. 
For 
full 
conditional 
independence, 
X 
and 
Y 
must 
be 
independent 
given 
all 
states 
of 
Z. 
Formally, 
this 
means 
that 


p(X 
, 
YjZ)= 
p(X 
jZ)p(YjZ) 
(3.3.12) 
for 
all 
states 
of 
X 
, 
Y, 
Z. 
In 
case 
the 
conditioning 
set 
is 
empty 
we 
may 
also 
write 
X 
.
??Y 
for 
X 
.
??Yj;, 
in 
which 
case 
X 
is 
(unconditionally) 
independent 
of 
Y. 
If 
X 
and 
Y 
are 
not 
conditionally 
independent, 
they 
are 
conditionally 
dependent. 
This 
is 
written 


X 
>>YjZ 
(3.3.13) 


To 
develop 
intuition 
about 
conditional 
independence 
consider 
the 
three 
variable 
distribution 
p(x1;x2;x3). 
We 
may 
write 
this 
in 
any 
of 
the 
6 
ways 


p(x1;x2;x3)= 
p(xi1 
jxi2 
;xi3 
)p(xi2 
jxi3 
)p(xi3 
) 
(3.3.14) 


where 
(i1;i2;i3) 
is 
any 
of 
the 
6 
permutations 
of 
(1, 
2, 
3). 
Whilst 
all 
dierent 
DAGs, 
they 
represent 
the 
same 
distribution, 
namely 
that 
which 
makes 
no 
conditional 
independence 
statements. 


To 
make 
an 
independence 
statement, 
we 
need 
to 
drop 
one 
of 
the 
links. 
This 
gives 
rise 
to 
the 
4 
DAGs 
in 
g(3.5). 
Are 
any 
of 
these 
graphs 
equivalent, 
in 
the 
sense 
that 
they 
represent 
the 
same 
distribution? 


Figure 
3.4: 
p(x1;x2;x3;x4)= 
p(x1jx4)p(x2jx3;x4)p(x3)p(x4). 


x1 
x2 
x3 
x4 


DRAFT 
March 
9, 
2010 



Belief 
Networks 


x1
x3
x2x1
x3
x2x1
x3
x2x1
x3
x2
(a) 
(b) 
(c) 
(d) 
Figure 
3.5: 
By 
dropping 
say 
the 
connection 
between 
variables 
x1 
and 
x2, 
we 
reduce 
the 
6 
possible 
BN 
graphs 
amongst 
three 
variables 
to 
4. 
(The 
6 
fully 
connected 
`cascade’ 
graphs 
correspond 
to 
(a) 
with 
x1 
. 
x2, 
(a) 
with 
x2 
. 
x1, 
(b) 
with 
x1 
. 
x2, 
(b) 
with 
x2 
. 
x1, 
(c) 
with 
x1 
. 
x3 
and 
(d) 
with 
x2 
. 
x1. 
Any 
other 
graphs 
would 
be 
cyclic 
and 
therefore 
not 
distributions). 


x
z
yx
z
yx
z
y
xy
w
z
(a) 
(b) 
(c) 
(d) 
Figure 
3.6: 
In 
graphs 
(a) 
and 
(b), 
variable 
z 
is 
not 
a 
collider. 
(c): 
Variable 
z 
is 
a 
collider. 
Graphs 


(a) 
and 
(b) 
represent 
conditional 
independence 
x 
. 
??y| 
z. 
In 
graphs 
(c) 
and 
(d), 
x 
and 
y 
are 
'graphically’ 
conditionally 
dependent 
given 
variable 
z. 
Applying 
Bayes’ 
rule 
gives 
: 


(x2jx3)p(x3jx1)p(x1)p
p|
{z}
= 
p(x2, 
x3)p(x3, 
x1)=p(x3) 
= 
p(x1jx3)p(x2, 
x3) 
(3.3.15) 
graph(c) 
= 
(x1jx3)p(x3jx2)p(x2)p
p|
{z}
= 
(x1jx3)p(x2jx3)p(x3)p
p|
{z}
(3.3.16) 
graph(d) 
graph(b) 


so 
that 
DAGs 
(b),(c) 
and 
(d) 
represent 
the 
same 
CI 
assumptions 
namely 
that, 
given 
the 
state 
of 
variable 
x3, 
variables 
x1 
and 
x2 
are 
independent, 
x1 
.
??x2jx3. 


However, 
graph 
(a) 
represents 
something 
fundamentally 
dierent, 
namely: 
p(x1;x2)= 
p(x1)p(x2). 
There 
is 
no 
way 
to 
transform 
the 
distribution 
p(x3jx1;x2)p(x1)p(x2) 
into 
any 
of 
the 
others. 


Remark 
3 
(Graphical 
Dependence). 
Belief 
networks 
are 
good 
for 
encoding 
conditional 
independence, 
but 
are 
not 
well 
suited 
to 
describing 
dependence. 
For 
example, 
consider 
the 
trivial 
network 
p(a, 
b)= 
p(bja)p(a) 
which 
has 
the 
DAG 
representation 
a 
. 
b. 
This 
may 
appear 
to 
encode 
that 
a 
and 
b 
are 
dependent. 
However, 
there 
are 
certainly 
instances 
when 
this 
is 
not 
the 
case. 
For 
example, 
it 
may 
be 
that 
the 
conditional 
is 
such 
that 
p(bja)= 
p(b), 
so 
that 
p(a, 
b)= 
p(a)p(b). 
In 
this 
case, 
although 
generally 
there 
may 
appear 
to 
be 
a 
`graphical’ 
dependence 
from 
the 
DAG, 
there 
can 
be 
instances 
of 
the 
distributions 
for 
which 
dependence 
does 
not 
follow. 
The 
same 
holds 
for 
Markov 
networks, 
section(4.2), 
in 
which 
p(a, 
b)= 
(a, 
b). 
Whilst 
this 
suggests 
`graphical’ 
dependence 
between 
a 
and 
b, 
for 
(a, 
b)= 
(a)(b), 
the 
variables 
are 
independent. 


3.3.2 
The 
impact 
of 
collisions 
In 
a 
general 
BN, 
how 
could 
we 
check 
if 
x.
??yjz? 
In 
g(3.6)(a,b), 
x 
and 
y 
are 
independent 
when 
conditioned 
on 
z. 
In 
g(3.6)(c) 
they 
are 
dependent; 
in 
this 
situation, 
variable 
z 
is 
called 
a 
collider 
– 
the 
arrows 
of 
its 


34 
DRAFT 
March 
9, 
2010 



Belief 
Networks 


a 


d
bc
e
Figure 
3.7: 
The 
variable 
d 
is 
a 
collider 
along 
the 
path 
a..b..d..c, 
but 
not 
along 
the 
path 
a 
..6b 
..6d 
..6e. 
Is 
a 
. 
??6ej6b? 
a 
and 
b 
are 
not 
d-connected 
since 
there 
are 
no 
colliders 
on 
the 
only 
path 
between 
a 
and 
e, 
and 
since 
there 
is 
a 
non-collider 
b 
which 
is 
in 
the 
conditioning 
set. 
Hence 
a 
and 
b 
are 
d-separated, 
i.e. 
a.
??ejb. 


neighbours 
are 
pointing 
towards 
it. 
What 
about 
g(3.6)(d)? 
In 
(d), 
when 
we 
condition 
on 
z, 
x 
and 
y 
will 
be 
`graphically’ 
dependent, 
since 


X

p(x, 
y, 
z)1 


p(x, 
yjz)= 
= 
p(zjw)p(wjx, 
y)p(x)p(y)6(3.3.17)

= 
p(xjz)p(yjz) 


p(z) 
p(z)

w 


– 
intuitively, 
variable 
w 
becomes 
dependent 
on 
the 
value 
of 
z, 
and 
since 
x 
and 
y 
are 
conditionally 
dependent 
on 
w, 
they 
are 
also 
conditionally 
dependent 
on 
z. 
Roughly 
speaking, 
if 
there 
is 
a 
non-collider 
z 
which 
is 
conditioned 
on 
along 
the 
path 
between 
x 
and 
y 
(as 
in 
g(3.6)(a,b)), 
then 
this 
path 
does 
not 
make 
x 
and 
y 
dependent. 
Similarly, 
if 
there 
is 
a 
path 
between 
x 
and 
y 
which 
contains 
a 
collider, 
provided 
that 
this 
collider 
is 
not 
in 
the 
conditioning 
set 
(and 
neither 
are 
any 
of 
its 
descendants) 
then 
this 
path 
does 
not 
make 
x 
and 
y 
dependent. 
If 
there 
is 
a 
path 
between 
x 
and 
y 
which 
contains 
no 
colliders 
and 
no 
conditioning 
variables, 
then 
this 
path 
`d-connects’ 
x 
and 
y. 


Note 
that 
a 
collider 
is 
dened 
relative 
to 
a 
path. 
In 
g(3.7), 
the 
variable 
d 
is 
a 
collider 
along 
the 
path 
a 
..6b 
..6d 
..6c, 
but 
not 
along 
the 
path 
a 
..6b 
..6d 
..6e 
(since, 
relative 
to 
this 
path, 
the 
two 
arrows 
do 
not 
point 
inwards 
to 
d). 


Consider 
the 
BN: 
A 
!6B 
 6C. 
Here 
A 
and 
C 
are 
(unconditionally) 
independent. 
However, 
conditioning 
of 
B 
makes 
them 
`graphically’ 
dependent. 
Intuitively, 
whilst 
we 
believe 
the 
root 
causes 
are 
independent, 
given 
the 
value 
of 
the 
observation, 
this 
tells 
us 
something 
about 
the 
state 
of 
both 
the 
causes, 
coupling 
them 
and 
making 
them 
(generally) 
dependent. 


3.3.3 
d-Separation 
The 
DAG 
concepts 
of 
d-separation 
and 
d-connection 
are 
central 
to 
determining 
conditional 
independence 
in 
any 
BN 
with 
structure 
given 
by 
the 
DAG[284]. 


Denition 
20 
(d-connection, 
d-separation). 
If 
G 
is 
a 
directed 
graph 
in 
which 
X6, 
Y6and 
Z6are 
disjoint 
sets 
of 
vertices, 
then 
X6and 
Y6are 
d-connected 
by 
Z6in 
G 
if 
and 
only 
if 
there 
exists 
an 
undirected 
path 
U 
between 
some 
vertex 
in 
X6and 
some 
vertex 
in 
Y6such 
that 
for 
every 
collider 
C 
on 
U, 
either 
C 
or 
a 
descendent 
of 
C 
is 
in 
Z, 
and 
no 
non-collider 
on 
U 
is 
in 
Z. 


X6and 
Y6are 
d-separated 
by 
Z6in 
G 
if 
and 
only 
if 
they 
are 
not 
d-connected 
by 
Z6in 
G. 


One 
may 
also 
phrase 
this 
as 
follows. 
For 
every 
variable 
x 
2X6and 
y 
2Y, 
check 
every 
path 
U 
between 
x 
and 
y. 
A 
path 
U 
is 
said 
to 
be 
blocked 
if 
there 
is 
a 
node 
w 
on 
U 
such 
that 
either 


1. 
w 
is 
a 
collider 
and 
neither 
w 
nor 
any 
of 
its 
descendants 
is 
in 
Z. 
2. 
w 
is 
not 
a 
collider 
on 
U 
and 
w 
is 
in 
Z. 
DRAFT 
March 
9, 
2010 
35 



Belief 
Networks 


If 
all 
such 
paths 
are 
blocked 
then 
X 
and 
Y 
are 
d-separated 
by 
Z. 


If 
the 
variable 
sets 
X 
and 
Y 
are 
d-separated 
by 
Z, 
they 
are 
independent 
conditional 
on 
Z 
in 
all 
probability 
distributions 
such 
a 
graph 
can 
represent. 


The 
Bayes 
Ball 
algorithm[241] 
provides 
a 
linear 
time 
complexity 
algorithm 
which 
given 
a 
set 
of 
nodes 
X 
and 
Z 
determines 
the 
set 
of 
nodes 
Y 
such 
that 
X 
.
??YjZ. 
Y 
is 
called 
the 
set 
of 
irrelevant 
nodes 
for 
X 
given 
Z. 


3.3.4 
d-Connection 
and 
dependence 
Given 
a 
DAG 
we 
can 
imply 
with 
certainty 
that 
two 
variables 
are 
(conditionally) 
independent, 
provided 
they 
are 
d-separated. 
Can 
we 
infer 
that 
they 
are 
dependent, 
provided 
they 
are 
d-connected? 
Consider 
the 
following 
situation 


p(a, 
b, 
c)= 
p(cja, 
b)p(a)p(b) 
(3.3.18) 


for 
which 
we 
note 
that 
a 
and 
b 
are 
d-connected 
by 
c. 
For 
concreteness, 
we 
assume 
c 
is 
binary 
with 
states 
1,2. 
The 
question 
is 
whether 
a 
and 
b 
are 
dependent, 
conditioned 
on 
c, 
a.
??bjc. 
To 
answer 
this, 
consider 


p(c 
=1ja, 
b)p(a)p(b)

p(a, 
bjc 
= 
1) 
= 
P(3.3.19) 


a;b 
p(c 
=1ja, 
b)p(a)p(b) 


In 
general, 
the 
rst 
term 
p(c 
=1ja, 
b) 
does 
not 
need 
to 
be 
a 
factored 
function 
of 
a 
and 
b 
and 
therefore 
a 
and 
b 
are 
conditionally 
`graphically’ 
dependent. 
However, 
we 
can 
construct 
cases 
where 
this 
is 
not 
so. 
For 
example, 
let 


p(c 
=1ja, 
b)= 
(a) (b), 
and 
p(c 
=2ja, 
b)=1 
- 
p(c 
=1ja, 
b) 
(3.3.20) 


where 
(a) 
and 
 (b) 
are 
arbitrary 
potentials 
between 
0 
and 
1. 
Then 


XX

1 


p(a, 
bjc 
= 
1) 
= 
(a)p(a) (b)p(b);Z 
=(a)p(a) (b)p(b) 
(3.3.21)

Z 


ab 


which 
shows 
that 
p(a, 
bjc 
= 
1) 
is 
a 
product 
of 
a 
function 
in 
a 
and 
function 
in 
b, 
so 
that 
a 
and 
b 
are 
independent, 
conditioned 
on 
c 
= 
1. 


A 
second 
example 
is 
given 
by 
the 
distribution 


p(a, 
b, 
c)= 
p(cjb)p(bja)p(a) 
(3.3.22) 


in 
which 
a 
and 
c 
are 
d-connected 
by 
b. 
The 
question 
is, 
are 
a 
and 
c 
dependent, 
a 
. 
??cj;? 
For 
simplicity 
we 
assume 
b 
takes 
the 
two 
states 
1,2. 
Then 


X

p(a, 
c)= 
p(a)p(cjb)p(bja)= 
p(a)(p(cjb 
= 
1)p(b 
=1ja)+ 
p(cjb 
= 
2)p(b 
=2ja)) 
(3.3.23) 


b 




1 


= 
p(a)p(b 
=1ja)p(cjb 
= 
1)+ 
p(cjb 
= 
2)- 
1(3.3.24) 


p(b 
=1ja) 


For 
the 
setting 
p(b 
=1ja)= 
, 
for 
some 
constant 
. 
for 
all 
states 
of 
a, 
then 




1 


p(a, 
c)= 
p(a)p(cjb 
= 
1)+ 
p(cjb 
= 
2)- 
1(3.3.25)

. 


which 
is 
a 
product 
of 
a 
function 
of 
a 
and 
a 
function 
of 
c. 
Hence 
a 
and 
c 
are 
independent. 


36 
DRAFT 
March 
9, 
2010 



Belief 
Networks 


a
b
c
d
ea
b
c
d
e
Figure 
3.8: 
Examples 
for 
d-separation. 
(a): 
b 
d-separates 
a 
from 
e. 
The 
joint 
variables 
fb, 
d} 
d-connect 
a 
and 
e. 
(b): 
c 
and 
e 
are 
(unconditionally) 
d-connected. 
b 
d


(a) 
(b) 
connects 
a 
and 
e. 


bgf
st
bgf
stu
Figure 
3.9: 
(a): 
t 
and 
f 
are 
d-connected 
by 
g. 
(b): 
b 
and 
f 
are 
d-separated 
by 
u. 


(a) 
(b) 
The 
moral 
of 
the 
story 
is 
that 
d-separation 
necessarily 
implies 
independence. 
However, 
d-connection 
does 
not 
necessarily 
imply 
dependence. 
It 
might 
be 
that 
there 
are 
numerical 
settings 
for 
which 
variables 
are 
independent, 
even 
though 
they 
are 
d-connected. 
For 
this 
reason 
we 
use 
the 
term 
`graphical’ 
dependence 
when 
the 
graph 
would 
suggest 
that 
variables 
are 
dependent, 
even 
though 
there 
may 
be 
numerical 
instantiations 
were 
dependence 
does 
not 
hold, 
see 
denition(21). 


Example 
12. 
Consider 
g(3.8a). 
Is 
a 
. 
??e 
| 
b? 
If 
we 
sum 
out 
variable 
d, 
then 
we 
see 
that 
a 
and 
e 
are 
independent 
given 
b, 
since 
the 
variable 
e 
will 
appear 
as 
an 
isolated 
factor 
independent 
of 
all 
other 
variables, 
hence 
indeed 
a 
.
??ejb. 
Whilst 
b 
is 
a 
collider 
which 
is 
in 
the 
conditioning 
set, 
we 
need 
all 
colliders 
on 
the 
path 
to 
be 
in 
the 
conditioning 
set 
(or 
their 
descendants) 
for 
d-connectedness. 


In 
g(3.8b), 
if 
we 
sum 
out 
variable 
d, 
then 
c 
and 
e 
become 
intrinsically 
linked 
and 
p(a, 
b, 
c, 
e) 
will 
not 
factorise 
into 
a 
function 
of 
a 
multiplied 
by 
a 
function 
of 
e 
– 
hence 
they 
are 
dependent. 


Example 
13. 
Consider 
the 
graph 
in 
g(3.9a). 


1. 
Are 
the 
variables 
t 
and 
f 
unconditionally 
independent, 
i.e. 
t 
. 
??fj;? 
Here 
there 
are 
two 
colliders, 
namely 
g 
and 
s 
– 
however, 
these 
are 
not 
in 
the 
conditioning 
set 
(which 
is 
empty), 
and 
hence 
they 
are 
d-separated 
and 
therefore 
unconditionally 
independent. 
2. 
What 
about 
t 
. 
??f| 
g? 
There 
is 
a 
collider 
on 
the 
path 
between 
t 
and 
f 
which 
is 
in 
the 
conditioning 
set. 
Hence 
t 
and 
f 
are 
d-connected 
by 
g, 
and 
therefore 
t 
and 
f 
are 
not 
independent 
conditioned 
on 
g. 
3. 
What 
about 
b 
. 
??f| 
s? 
Since 
there 
is 
a 
collider 
s 
in 
the 
conditioning 
set 
on 
the 
path 
between 
t 
and 
f, 
then 
b 
and 
f 
are 
`graphically’ 
conditionally 
dependent 
given 
s. 
Example 
14. 
Is 
fb, 
fg. 
??uj;? 
in 
g(3.9b). 
Since 
the 
conditioning 
set 
is 
empty 
and 
every 
path 
from 
either 
b 
or 
f 
to 
u 
contains 
a 
collider, 
b, 
f 
are 
unconditionally 
independent 
of 
u. 


3.3.5 
Markov 
equivalence 
in 
belief 
networks 
DRAFT 
March 
9, 
2010 
37 



Belief 
Networks 


Denition 
21 
(Some 
properties 
of 
Belief 
Networks). 


AB
C
p(A, 
B, 
C)= 
p(CjA, 
B)p(A)p(B) 
(3.3.26) 
A 
and 
B 
are 
(unconditionally) 
independent 
: 
p(A, 
B)= 
p(A)p(B). 
A 
and 
B 
are 
conditionally 
dependent 
on 
C 
: 
p(A, 
BjC) 


= 
p(AjC)p(BjC). 


AB
C
!6AB
Marginalising 
over 
C 
makes 
A 
and 
B 
independent. 


AB
C
!6AB
Conditioning 
on 
C 
makes 
A 
and 
B 
(graphically) 
dependent. 


AB
C
p(A, 
B, 
C)= 
p(Ajc)p(BjC)p(C) 
(3.3.27) 
A 
and 
B 
are 
(unconditionally) 
dependent 
: 
p(A, 
B)6

= 
p(A)p(B). 
A 
and 
B 
are 
conditionally 
independent 
on 
C 
: 
p(A, 
BjC)= 
p(AjC)p(BjC). 


AB
C
!6AB
Marginalising 
over 
C 
makes 
A 
and 
B 
(graphically) 
dependent. 


AB
C
!6AB
Conditioning 
on 
C 
makes 
A 
and 
B 
independent. 


AB
C
=AB
C
=AB
C
=
Denition 
22 
(Markov 
Equivalence). 
Two 
graphs 
are 
Markov 
equivalent 
if 
they 
both 
represent 
the 
same 
set 
of 
conditional 
independence 
statements. 


Dene 
the 
skeleton 
of 
a 
graph 
by 
removing 
the 
directions 
on 
the 
arrows. 
Dene 
an 
immorality 
in 
a 
DAG 
as 
a 
conguration 
of 
three 
nodes, 
A,B,C 
such 
that 
C 
is 
a 
child 
of 
both 
A 
and 
B, 
with 
A 
and 
B 
not 
directly 
connected. 
Two 
DAGs 
represent 
the 
same 
set 
of 
independence 
assumptions 
(they 
are 
Markov 
equivalent) 
if 
and 
only 
if 
they 
have 
the 
same 
skeleton 
and 
the 
same 
set 
of 
immoralities 
[74]. 


Using 
this 
rule 
we 
see 
that 
in 
g(3.5), 
BNs 
(b,c,d) 
have 
the 
same 
skeleton 
with 
no 
immoralities 
and 
are 
therefore 
equivalent. 
However 
BN 
(a) 
has 
an 
immorality 
and 
is 
therefore 
not 
equivalent 
to 
DAGS 
(b,c,d). 


DRAFT 
March 
9, 
2010 



Causality 


3.3.6 
Belief 
networks 
have 
limited 
expressibility 
h
t1
y1
t2
y2
t1
y1
t2
y2
Figure 
3.10: 
(a): 
Two 
treatments 
t1;t2 
and 
corresponding 
outcomes 
y1;y2. 
The 
health 
of 
a 
patient 
is 
represented 
by 
h. 
This 
DAG 
embodies 
the 
conditional 
independence 
statements 
t1 
. 
??t2;y2 
j;, 
t2 
. 
??t1;y1 
j;, 
namely 
that 
the 
treatments 
have 
no 
eect 
on 
each 
other. 
(b): 
One 
could 
represent 
the 
marginalised 
latent 
variable 
using 
a 
bi-directional 
edge. 


(a) 
(b) 
Consider 
the 
DAG 
in 
g(3.10a), 
(from 
[232]). 
This 
DAG 
could 
be 
used 
to 
represent 
two 
successive 
experiments 
where 
t1 
and 
t2 
are 
two 
treatments 
and 
y1 
and 
y2 
represent 
two 
outcomes 
of 
interest; 
h 
is 
the 
underlying 
health 
status 
of 
the 
patient; 
the 
rst 
treatment 
has 
no 
eect 
on 
the 
second 
outcome 
hence 
there 
is 
no 
edge 
from 
y1 
to 
y2. 
Now 
consider 
the 
implied 
independencies 
in 
the 
marginal 
distribution 
p(t1;t2;y1;y2), 
obtained 
by 
marginalising 
the 
full 
distribution 
over 
h. 
There 
is 
no 
DAG 
containing 
only 
the 
vertices 
t1;y1;t2;y2 
which 
represents 
the 
independence 
relations 
and 
does 
not 
also 
imply 
some 
other 
independence 
relation 
that 
is 
not 
implied 
by 
g(3.10a). 
Consequently, 
any 
DAG 
on 
vertices 
t1;y1;t2;y2 
alone 
will 
either 
fail 
to 
represent 
an 
independence 
relation 
of 
p(t1;t2;y1;y2), 
or 
will 
impose 
some 
additional 
independence 
restriction 
that 
is 
not 
implied 
by 
the 
DAG. 
In 
the 
above 
example 


X

p(t1;t2;y1;y2)= 
p(t1)p(t2)p(y1jt1;h)p(y2jt2;h)p(h) 
(3.3.28) 


h 


cannot 
in 
general 
be 
expressed 
as 
a 
product 
of 
functions 
dened 
on 
a 
limited 
set 
of 
the 
variables. 
However, 
it 
is 
the 
case 
that 
the 
conditional 
independence 
conditions 
t1 
.
??t2;y2j;, 
t2 
.
??t1;y1jØ 
hold 
in 
p(t1;t2;y1;y2) 


– 
they 
are 
there, 
encoded 
in 
the 
form 
of 
the 
conditional 
probability 
tables. 
It 
is 
just 
that 
we 
cannot 
`see’ 
this 
independence 
since 
it 
is 
not 
present 
in 
the 
structure 
of 
the 
marginalised 
graph 
(though 
one 
can 
naturally 
infer 
this 
in 
the 
larger 
graph 
p(t1;t2;y1;y2;h)). 
This 
example 
demonstrates 
that 
BNs 
cannot 
express 
all 
the 
conditional 
independence 
statements 
that 
could 
be 
made 
on 
that 
set 
of 
variables 
(the 
set 
of 
conditional 
independence 
statements 
can 
be 
increased 
by 
considering 
extra 
latent 
variables 
however). 
This 
situation 
is 
rather 
general 
in 
the 
sense 
that 
any 
graphical 
model 
has 
limited 
expressibility 
in 
terms 
of 
independence 
statements[265]. 
It 
is 
worth 
bearing 
in 
mind 
that 
Belief 
Networks 
may 
not 
always 
be 
the 
most 
appropriate 
framework 
to 
express 
one's 
independence 
assumptions 
and 
intuitions. 


A 
natural 
consideration 
is 
to 
use 
a 
bi-directional 
arrow 
when 
a 
latent 
variable 
is 
marginalised. 
For 
g(3.10a), 
one 
could 
depict 
the 
marginal 
distribution 
using 
a 
bi-directional 
edge, 
g(3.10b). 
Similarly 
a 
BN 
with 
a 
latent 
conditioned 
variable 
can 
be 
represented 
using 
an 
undirected 
edge. 
For 
a 
discussion 
of 
these 
and 
related 
issues, 
see 
[232]. 


3.4 
Causality 
Causality 
is 
a 
contentious 
topic 
and 
the 
purpose 
of 
this 
section 
is 
make 
the 
reader 
aware 
of 
some 
pitfalls 
that 
can 
occur 
and 
which 
may 
give 
rise 
to 
erroneous 
inferences. 
The 
reader 
is 
referred 
to 
[220] 
and 
[74] 
for 
further 
details. 


The 
word 
`causal’ 
is 
contentious 
particularly 
in 
cases 
where 
the 
model 
of 
the 
data 
contains 
no 
explicit 
temporal 
information, 
so 
that 
formally 
only 
correlations 
or 
dependencies 
can 
be 
inferred. 
For 
a 
distribution 
p(a, 
b), 
we 
could 
write 
this 
as 
either 
(i) 
p(ajb)p(b) 
or 
(ii) 
p(bja)p(a). 
In 
(i) 
we 
might 
think 
that 
b 
`causes’ 
a, 
and 
in 
(ii) 
a 
`causes’ 
b. 
Clearly, 
this 
is 
not 
very 
meaningful 
since 
they 
both 
represent 
exactly 
the 
same 
distribution. 
Formally 
Belief 
Networks 
only 
make 
independence 
statements, 
not 
causal 
ones. 
Nevertheless, 
in 
constructing 
BNs, 
it 
can 
be 
helpful 
to 
think 
about 
dependencies 
in 
terms 
of 
causation 


DRAFT 
March 
9, 
2010 
39 



Causality 


ABABRWRW
(a) 
(b) 
(c) 
(d) 
Figure 
3.11: 
Both 
(a) 
and 
(b) 
represent 
the 
same 
distribution 
p(a, 
b)= 
p(ajb)p(b)= 
p(bja)p(a). 
(c): 
The 
graph 
represents 
p(rain, 
grasswet)= 
p(grasswetjrain)p(rain). 
(d): 
We 
could 
equally 
have 
written 
p(rainjgrasswet)p(grasswet), 
although 
this 
appears 
to 
be 
causally 
non-sensical. 


G
DRFD
G
DR
Figure 
3.12: 
(a): 
A 
DAG 
for 
the 
relation 
between 
Gender 
(G), 
Drug 
(D) 
and 
Recovery 
(R), 
see 
table(3.1). 
(b): 
Inuence 
diagram. 
No 
decision 
variable 
is 
required 
for 
G 
since 
G 
has 
no 
parents. 


(a) 
(b) 
since 
our 
intuitive 
understanding 
is 
usually 
framed 
in 
how 
one 
variable 
`inuences’ 
another. 
First 
we 
discuss 
a 
classic 
conundrum 
that 
highlights 
potential 
pitfalls 
that 
can 
arise. 


3.4.1 
Simpson's 
paradox 
Simpson's 
`paradox’ 
is 
a 
cautionary 
tale 
in 
causal 
reasoning 
in 
BNs. 
Consider 
a 
medical 
trial 
in 
which 
patient 
treatment 
and 
outcome 
are 
recovered. 
Two 
trials 
were 
conducted, 
one 
with 
40 
females 
and 
one 
with 
40 
males. 
The 
data 
is 
summarised 
in 
table(3.1). 
The 
question 
is 
: 
Does 
the 
drug 
cause 
increased 
recovery? 
According 
to 
the 
table 
for 
males, 
the 
answer 
is 
no, 
since 
more 
males 
recovered 
when 
they 
were 
not 
given 
the 
drug 
than 
when 
they 
were. 
Similarly, 
more 
females 
recovered 
when 
not 
given 
the 
drug 
than 
recovered 
when 
given 
the 
drug. 
The 
conclusion 
appears 
that 
the 
drug 
cannot 
be 
benecial 
since 
it 
aids 
neither 
subpopulation. 


However, 
ignoring 
the 
gender 
information, 
and 
collating 
both 
the 
male 
and 
female 
data 
into 
one 
combined 
table, 
we 
nd 
that 
more 
people 
recovered 
when 
given 
the 
drug 
than 
when 
not. 
Hence, 
even 
though 
the 
drug 
doesn't 
seem 
to 
work 
for 
either 
males 
or 
females, 
it 
does 
seem 
to 
work 
overall! 
Should 
we 
therefore 
recommend 
the 
drug 
or 
not? 


Resolution 
of 
the 
paradox 


The 
`paradox’ 
occurs 
since 
we 
are 
asking 
a 
causal 
(or 
interventional) 
question. 
The 
question 
we 
are 
intuitively 
asking 
is, 
if 
we 
give 
someone 
the 
drug, 
what 
happens? 
However, 
the 
calculation 
we 
performed 
above 
was 
only 
an 
observational 
calculation. 
The 
calculation 
we 
really 
want 
is 
to 
rst 
intervene, 
setting 


Males 
Recovered 
Not 
Recovered 
Rec. 
Rate 
Given 
Drug 
18 
12 
60% 
Not 
Given 
Drug 
7 
3 
70% 


Females 
Recovered 
Not 
Recovered 
Rec. 
Rate 
Given 
Drug 
2 
8 
20% 
Not 
Given 
Drug 
9 
21 
30% 


Combined 
Recovered 
Not 
Recovered 
Rec. 
Rate 
Given 
Drug 
20 
20 
50% 
Not 
Given 
Drug 
16 
24 
40% 


Table 
3.1: 
Table 
for 
Simpson's 
Paradox 
(from 
[220]) 


DRAFT 
March 
9, 
2010 



Causality 


the 
drug 
state, 
and 
then 
observe 
what 
eect 
this 
has 
on 
recovery. 
Pearl[220] 
describes 
this 
as 
the 
dierence 
between 
`given 
that 
we 
see’ 
(observational 
evidence), 
versus 
`given 
that 
we 
do’ 
(interventional 
evidence). 


A 
model 
of 
the 
Gender, 
Drug 
and 
Recovery 
data 
(which 
makes 
no 
conditional 
independence 
assumptions) 
is 


p(G, 
D, 
R)= 
p(RjG, 
D)p(DjG)p(G) 
(3.4.1) 


An 
observational 
calculation 
concerns 
computing 
p(RjG, 
D) 
and 
p(RjD). 
Ina 
causal 
interpretation, 
however, 
if 
we 
intervene 
and 
give 
the 
drug, 
then 
the 
term 
p(DjG) 
in 
equation 
(3.4.1) 
should 
play 
no 
role 
in 
the 
experiment 
(otherwise 
the 
distribution 
models 
that 
given 
the 
gender 
we 
select 
a 
drug 
with 
probability 
p(DjG), 
which 
is 
not 
the 
case 
– 
we 
decide 
to 
give 
the 
drug 
or 
not, 
independent 
of 
gender). 
In 
the 
causal 
case 
we 
are 
modelling 
the 
causal 
experiment; 
in 
this 
case 
the 
term 
p(DjG) 
needs 
to 
be 
replaced 
by 
a 
term 
that 
reects 
the 
setup 
of 
the 
experiment. 
In 
an 
atomic 
intervention 
a 
single 
variable 
is 
set 
in 
a 
particular 
state5 
. 
In 
our 
atomic 
causal 
intervention 
in 
setting 
D, 
we 
are 
dealing 
with 
the 
modied 
distribution 


p~(G, 
RjD)= 
p(RjG, 
D)p(G) 
(3.4.2) 


where 
the 
terms 
on 
the 
right 
hand 
side 
of 
this 
equation 
are 
taken 
from 
the 
original 
BN 
of 
the 
data. 
To 
denote 
an 
intervention 
we 
use 
jj: 


p(RjG, 
D)p(G)

p(RjjG, 
D) 
= 
p~(RjG, 
D)= 
P= 
p(RjG, 
D) 
(3.4.3) 


R 
p(RjG, 
D)p(G) 


(One 
can 
also 
consider 
here 
G 
as 
being 
interventional 
– 
in 
this 
case 
it 
doesn't 
matter 
since 
the 
fact 
that 
the 
variable 
G 
has 
no 
parents 
means 
that 
for 
any 
distribution 
conditional 
on 
G, 
the 
prior 
factor 
p(G) 
will 
not 
be 
present). 
Using 
equation 
(3.4.3), 
for 
the 
males 
given 
the 
drug 
60% 
recover, 
versus 
70% 
recovery 
when 
not 
given 
the 
drug. 
For 
the 
females 
given 
the 
drug 
20% 
recover, 
versus 
30% 
recovery 
when 
not 
given 
the 
drug. 


Similarly, 


P

X

G 
p(RjG, 
D)p(G)

p(RjjD) 
= 
p~(RjD)=P=p(RjG, 
D)p(G) 
(3.4.4) 


R;G 
p(RjG, 
D)p(G) 


G 


Using 
the 
above 
post 
intervention 
distribution 
we 
have 


p(recoveryjdrug) 
= 
0:6 
× 
0:5+0:2 
× 
0:5=0:4 
(3.4.5) 


and 


p(recoveryjno 
drug)=0:7 
× 
0:5+0:3 
× 
0:5=0:5 
(3.4.6) 


Hence 
we 
correctly 
infer 
that 
the 
drug 
is 
overall 
not 
helpful, 
as 
we 
intuitively 
expect, 
and 
is 
consistent 
with 
the 
results 
from 
both 
subpopulations. 


Here 
p(RjjD) 
means 
that 
we 
rst 
choose 
either 
a 
Male 
or 
Female 
patient 
at 
random, 
and 
then 
give 
them 
the 
drug, 
or 
not 
depending 
on 
the 
state 
of 
D. 
The 
point 
is 
that 
we 
do 
not 
randomly 
decide 
whether 
or 
not 
to 
give 
the 
drug, 
hence 
the 
absence 
of 
the 
term 
p(DjG) 
from 
the 
joint 
distribution. 
One 
way 
to 
think 
about 
such 
models 
is 
to 
consider 
how 
to 
draw 
a 
sample 
from 
the 
joint 
distribution 
of 
the 
random 
variables 


– 
in 
most 
cases 
this 
should 
clarify 
the 
role 
of 
causality 
in 
the 
experiment. 
In 
contrast 
to 
the 
interventional 
calculation, 
the 
observational 
calculation 
makes 
no 
conditional 
independence 
assumptions. 
This 
means 
that, 
for 
example, 
the 
term 
p(DjG) 
plays 
a 
role 
in 
the 
calculation 
(the 
reader 
might 
wish 
to 
verify 
that 
the 
result 
given 
in 
the 
combined 
data 
in 
table(3.1) 
is 
equivalent 
to 
inferring 
with 
the 
full 
distribution 
equation 
(3.4.1)). 


5More 
general 
experimental 
conditions 
can 
be 
modelled 
by 
replacing 
p(DjG) 
by 
an 
intervention 
distribution 
(DjG) 


DRAFT 
March 
9, 
2010 



Causality 


Denition 
23 
(Pearl's 
Do 
Operator). 


In 
a 
causal 
inference, 
in 
which 
the 
eect 
of 
setting 
variables 
Xc1 
;:::;XcK 
, 
ck 
2C, 
in 
states 
xc1 
;:::, 
xcK 
, 
is 
to 
be 
inferred, 
this 
is 
equivalent 
to 
standard 
evidential 
inference 
in 
the 
post 
intervention 
distribution: 


Y

p(X1;:::;Xnjxc1 
;:::, 
xcK 
)

p(Xjdo(Xc1 
= 
xc1 
), 
. 
. 
. 
, 
do(XcK 
= 
xcK 
)) 
= 
QK 
=p 
(Xjjpa 
(Xj)) 
(3.4.7) 
i=1 
p(Xci 
jpa 
(Xci 
)) 
j62C6

where 
any 
parental 
states 
of 
pa 
(Xj) 
of 
Xj 
are 
set 
in 
their 
evidential 
states. 
An 
alternative 
notation 
is 
p(Xjjxc1 
;:::, 
xcK 
). 


In 
words, 
for 
those 
variables 
for 
which 
we 
causally 
intervene 
and 
set 
in 
a 
particular 
state, 
the 
corresponding 
terms 
p(Xci 
jpa 
(Xci 
)) 
are 
removed 
from 
the 
original 
Belief 
Network. 
For 
variables 
which 
are 
evidential 
but 
non-causal, 
the 
corresponding 
factors 
are 
not 
removed 
from 
the 
distribution. 
The 
interpretation 
is 
that 
the 
post 
intervention 
distribution 
corresponds 
to 
an 
experiment 
in 
which 
the 
causal 
variables 
are 
rst 
set 
and 
non-causal 
variables 
are 
subsequently 
observed. 


3.4.2 
Inuence 
diagrams 
and 
the 
do-calculus 
In 
making 
causal 
inferences 
we 
must 
adjust 
the 
model 
to 
reect 
any 
causal 
experimental 
conditions. 
In 
setting 
any 
variable 
into 
a 
particular 
state 
we 
need 
to 
surgically 
remove 
all 
parental 
links 
of 
that 
variable. 
Pearl 
calls 
this 
the 
do 
operator, 
and 
contrasts 
an 
observational 
(`see') 
inference 
p(xjy) 
with 
a 
causal 
(`make’ 
or 
`do') 
inference 
p(xjdo(y)). 


A 
useful 
alternative 
representation 
is 
to 
append 
variables 
X 
upon 
which 
an 
intervention 
can 
possibly 
be 
made 
with 
a 
parental 
decision 
variable 
FX 
[74]. 
For 
example6 


p~(D, 
G, 
R, 
FD)= 
p(DjFD;G)p(G)p(RjG, 
D)p(FD) 
(3.4.8) 


where 


p(DjFD 
= 
;;G) 
= 
p(Djpa 
(D)) 


p(DjFD 
= 
d;G)=1 
for 
D 
= 
d 
and 
0 
otherwise 


Hence, 
if 
the 
decision 
variable 
FD 
is 
set 
to 
the 
empty 
state, 
the 
variable 
D 
is 
determined 
by 
the 
standard 
observational 
term 
p(Djpa 
(D)). 
If 
the 
decision 
variable 
is 
set 
to 
a 
state 
of 
D, 
then 
the 
variable 
puts 
all 
its 
probability 
in 
that 
single 
state 
of 
D 
= 
d. 
This 
has 
the 
eect 
of 
replacing 
the 
conditional 
probability 
term 
a 
unit 
factor 
and 
any 
instances 
of 
D 
set 
to 
the 
variable 
in 
its 
interventional 
state7 
. 


A 
potential 
advantage 
of 
the 
inuence 
diagram 
approach 
over 
the 
do-calculus 
is 
that 
deriving 
conditional 
independence 
statements 
can 
be 
made 
based 
on 
standard 
techniques 
for 
the 
augmented 
BN. 
Additionally, 
for 
parameter 
learning, 
standard 
techniques 
apply 
in 
which 
the 
decision 
variables 
are 
set 
to 
the 
condition 
under 
which 
each 
data 
sample 
was 
collected 
(a 
causal 
or 
non-causal 
sample). 


Example 
15 
(Drivers 
and 
Accidents: 
A 
causal 
Belief 
Network). 


6Here 
the 
Inuence 
Diagram 
is 
a 
distribution 
over 
variables 
in 
including 
decision 
variables, 
in 
contrast 
to 
the 
application 
of 
IDs 
in 
chapter(7). 


7More 
general 
cases 
can 
be 
considered 
in 
which 
the 
variables 
are 
placed 
in 
a 
distribution 
of 
states 
[74]. 


42 
DRAFT 
March 
9, 
2010 



Parameterising 
Belief 
Networks 


x1x2x3x4x5
y
x1x2x3x4x5
z1z2
y
x1x2x3x4x5
z1z2z3z4z5
y
(a) 
(b) 
(c) 
Figure 
3.13: 
(a): 
If 
all 
variables 
are 
binary 
25 
= 
32 
states 
are 
required 
to 
specify 
p(yjx1;:::;x5). 
(b): 
Here 
only 
16 
states 
are 
required. 
(c): 
Noisy 
logic 
gates. 


D
A
FD
FA
Consider 
the 
following 
CPT 
entries 
p(D 
= 
bad)=0:3, 
p(A 
= 
trjD 
= 
bad)=0:9. 
If 
we 
intervene 
and 
use 
a 
bad 
driver, 
what 
is 
the 
probability 
of 
an 
accident? 


p(A 
= 
trjD 
= 
bad;FD 
= 
bad;FA 
= 
;)= 
p(A 
= 
trjD 
= 
bad)=0:9 
(3.4.9) 


On 
the 
other 
hand, 
if 
we 
intervene 
and 
make 
an 
accident, 
what 
is 
the 
probability 
the 
driver 
involved 
is 
bad? 
This 
is 


p(D 
= 
badjjA 
= 
tr;FD 
= 
;;FA 
= 
tr)= 
p(D 
= 
bad)=0:3 


3.4.3 
Learning 
the 
direction 
of 
arrows 
In 
the 
absence 
of 
data 
from 
causal 
experiments, 
one 
should 
be 
justiably 
sceptical 
about 
learning 
`causal’ 
networks. 
Nevertheless, 
one 
might 
prefer 
a 
certain 
direction 
of 
a 
link 
based 
on 
assumptions 
of 
the 
`simplicity’ 
of 
the 
CPTs. 
This 
preference 
may 
come 
from 
a 
`physical 
intuition’ 
that 
whilst 
root 
`causes’ 
may 
be 
uncertain, 
the 
relationship 
from 
cause 
to 
eect 
is 
clear. 
In 
this 
sense 
a 
measure 
of 
the 
complexity 
of 
a 
CPT 
is 
required, 
such 
as 
entropy. 
Such 
heuristics 
can 
be 
numerically 
encoded 
and 
the 
`directions’ 
learned 
in 
an 
otherwise 
Markov 
equivalent 
graph. 


3.5 
Parameterising 
Belief 
Networks 
Consider 
a 
variable 
y 
with 
many 
parental 
variables 
x1;:::;xn, 
g(3.13a). 
Formally, 
the 
structure 
of 
the 
graph 
implies 
nothing 
about 
the 
form 
of 
the 
parameterisation 
of 
the 
table 
p(yjx1;:::;xn). 
If 
each 
parent 
xi 
has 
dim 
(xi) 
states, 
and 
there 
is 
no 
constraint 
on 
the 
table, 
then 
the 
table 
p(yjx1;:::;xn) 
contains 


QX

(dim 
(y) 
- 
1) 
dim(xi) 
entries. 
If 
stored 
explicitly 
for 
each 
state, 
this 
would 
require 
potentially 
huge 


i 


storage. 
An 
alternative 
is 
to 
constrain 
the 
table 
to 
have 
a 
simpler 
parametric 
form. 
For 
example, 
one 
might 
write 
a 
decomposition 
in 
which 
only 
a 
limited 
number 
of 
parental 
interactions 
are 
required 
(this 
is 
called 
divorcing 
parents 
in 
[148]). 
For 
example, 
in 
g(3.13b), 
assuming 
all 
variables 
are 
binary, 
the 
number 
of 
states 
requiring 
specication 
is 
23 
+22 
+22 
= 
16, 
compared 
to 
the 
25 
= 
32 
states 
in 
the 
unconstrained 
case. 
The 
distribution 


X

p(yjx1;:::;x5)=p(yjz1;z2)p(z1jx1;x2;x3)p(z2jx4;x5) 
(3.5.1) 


z1;z2 


can 
be 
stored 
using 
only 
16 
independent 
parameters. 


DRAFT 
March 
9, 
2010 
43 



Exercises 


Logic 
gates 


Another 
technique 
to 
constrain 
CPTs 
uses 
simple 
classes 
of 
conditional 
tables. 
For 
example, 
in 
g(3.13c), 
one 
could 
use 
a 
logical 
OR 
gate 
on 
binary 
zi, 
say 




1 
if 
at 
least 
one 
of 
the 
zi 
is 
in 
state 
1 


p(yjz1;:::;z5) 
=(3.5.2)

0 
otherwise 


We 
can 
then 
make 
a 
CPT 
p(yjx1;:::;x5) 
by 
including 
the 
additional 
terms 
p(zi 
=1jxi). 
When 
each 
xi 
is 
binary 
there 
are 
in 
total 
only 
2 
+ 
2 
+ 
2 
+ 
2 
+ 
2 
= 
10 
quantities 
required 
for 
specifying 
p(yjx). 
In 
this 
case, 
g(3.13c) 
can 
be 
used 
to 
represent 
any 
noisy 
logic 
gate, 
such 
as 
the 
noisy 
OR 
or 
noisy 
AND, 
where 
the 
number 
of 
parameters 
required 
to 
specify 
the 
noisy 
gate 
is 
linear 
in 
the 
number 
of 
parents 
x. 


The 
noisy-OR 
is 
particularly 
common 
in 
disease-symptom 
networks 
in 
which 
many 
diseases 
x 
can 
give 
rise 
to 
the 
same 
symptom 
y– 
provided 
that 
at 
least 
one 
of 
the 
diseases 
is 
present, 
the 
probability 
that 
the 
symptom 
will 
be 
present 
is 
high. 


3.6 
Further 
Reading 
An 
introduction 
to 
Bayesian 
Networks 
and 
graphical 
models 
in 
expert 
systems 
is 
to 
be 
found 
in 
[258], 
which 
also 
discusses 
general 
inference 
techniques 
which 
will 
be 
discussed 
during 
later 
chapters. 


3.7 
Code 
3.7.1 
Naive 
inference 
demo 
demoBurglar.m: 
Was 
it 
the 
Burglar 
demo 
demoChestClinic.m: 
Naive 
Inference 
on 
Chest 
Clinic 


3.7.2 
Conditional 
independence 
demo 
The 
following 
demo 
determines 
whether 
X 
.
??YjZ 
for 
the 
Chest 
Clinic 
network, 
and 
checks 
the 
result 
numerically8 
. 
The 
independence 
test 
is 
based 
on 
the 
Markov 
method 
of 
section(4.2.4). 
This 
is 
preferred 
over 
the 
d-separation 
method 
since 
it 
is 
arguably 
simpler 
to 
code 
and 
also 
more 
general 
in 
that 
it 
deals 
also 
with 
conditional 
independence 
in 
Markov 
Networks 
as 
well 
as 
Belief 
Networks. 


Running 
the 
demo 
code 
below, 
it 
may 
happen 
that 
the 
numerical 
dependence 
is 
very 
low 
– 
that 
is 


p(X 
, 
YjZ) 
˜ 
p(X 
jZ)p(YjZ) 
(3.7.1) 


even 
though 
X 
>>YjZ. 
This 
highlights 
the 
dierence 
between 
`structural’ 
and 
`numerical’ 
independence. 
condindepPot.m: 
Numerical 
measure 
of 
conditional 
independence 
demoCondindep.m: 
Demo 
of 
conditional 
independence 
(using 
Markov 
method) 


3.7.3 
Utility 
routines 
dag.m: 
Find 
the 
DAG 
structure 
for 
a 
Belief 
Network 


3.8 
Exercises 
Exercise 
20 
(Party 
Animal). 
The 
party 
animal 
problem 
corresponds 
to 
the 
network 
in 
g(3.14). 
The 
boss 
is 
angry 
and 
the 
worker 
has 
a 
headache 
– 
what 
is 
the 
probability 
the 
worker 
has 
been 
to 
a 
party? 
To 


8The 
code 
for 
(structural) 
conditional 
independence 
is 
given 
in 
chapter(4). 


44 
DRAFT 
March 
9, 
2010 



Exercises 


U
PD
H
A
Figure 
3.14: 
Party 
animal. 
Here 
all 
variables 
are 
binary. 
When 
set 
to 
1 
the 
statements 
are 
true: 
P 
= 
Been 
to 
Party, 
H 
= 
Got 
a 
Headache, 
D 
= 
Demotivated 
at 
work, 
U 
= 
Underperform 
at 
work, 
A 
=Boss 
Angry. 
Shaded 
variables 
are 
observed 
in 
the 
true 
state. 


xd
e
tlb
a
s
x 
= 
Positive 
X-ray 
d 
= 
Dyspnea 
(Shortness 
of 
breath) 
e 
= 
Either 
Tuberculosis 
or 
Lung 
Cancer 
t 
= 
Tuberculosis 
l 
= 
Lung 
Cancer 
b 
= 
Bronchitis 
a 
= 
Visited 
Asia 
s 
= 
Smoker 


Figure 
3.15: 
Belief 
network 
structure 
for 
the 
Chest 
Clinic 
example. 


complete 
the 
specications, 
the 
probabilities 
are 
given 
as 
follows: 


p(U 
= 
trjP 
= 
tr;D 
= 
tr)=0:999 
p(U 
= 
trjP 
= 
fa;D 
= 
tr)=0:9 
p(U 
= 
trjP 
= 
tr;D 
= 
fa)=0:9 
p(U 
= 
trjP 
= 
fa;D 
= 
fa)=0:01 


Exercise 
21. 
Consider 
the 
distribution 
p(a, 
b, 
c)= 
p(cja, 
b)p(a)p(b). 
(i) 
Is 
a.
??bj;?. 
(ii) 
Is 
a.
??bjc? 


Exercise 
22. 
The 
Chest 
Clinic 
network 
[170] 
concerns 
the 
diagnosis 
of 
lung 
disease 
(tuberculosis, 
lung 
cancer, 
or 
both, 
or 
neither). 
In 
this 
model 
a 
visit 
to 
Asia 
is 
assumed 
to 
increase 
the 
probability 
of 
tuberc
ulosis. 
State 
if 
the 
following 
conditional 
independence 
relationships 
are 
true 
or 
false 


1. 
tuberculosis.
??smokingjshortness 
of 
breath, 
2. 
lung 
cancer 
.
??bronchitisjsmoking, 
3. 
visit 
to 
Asia.
??smokingjlung 
cancer 
4. 
visit 
to 
Asia.
??smokingjlung 
cancer, 
shortness 
of 
breath. 
Exercise 
23 
([128]). 
Consider 
the 
network 
in 
g(3.16), 
which 
concerns 
the 
probability 
of 
a 
car 
starting. 


p(b 
= 
bad)=0:02 
p(f 
= 
empty)=0:05 
p(g 
= 
emptyjb 
= 
good;f 
= 
not 
empty)=0:04 
p(g 
= 
emptyjb 
= 
good;f 
= 
empty)=0:97 
p(g 
= 
emptyjb 
= 
bad;f 
= 
not 
empty)=0:1 
p(g 
= 
emptyjb 
= 
bad;f 
= 
empty)=0:99 
p(t 
= 
fajb 
= 
good)=0:03 
p(t 
= 
fajb 
= 
bad)=0:98 
p(s 
= 
fajt 
= 
tr;f 
= 
not 
empty)=0:01 
p(s 
= 
fajt 
= 
tr;f 
= 
empty)=0:92 
p(s 
= 
fajt 
= 
fa;f 
= 
not 
empty)=1:0 
p(s 
= 
fajt 
= 
fa;f 
= 
empty)=0:99 


Calculate 
P 
(f 
= 
emptyjs 
= 
no), 
the 
probability 
of 
the 
fuel 
tank 
being 
empty 
conditioned 
on 
the 
observation 
that 
the 
car 
does 
not 
start. 


Exercise 
24. 
Consider 
the 
Chest 
Clinic 
Bayesian 
Network 
in 
g(3.15) 
[170]. 
Calculate 
by 
hand 
the 
values 
for 
p(D), 
p(DjS 
= 
tr), 
p(DjS 
= 
fa). 
The 
table 
values 
are: 


p(a 
= 
tr) 
=0:01 
p(s 
= 
tr) 
=0:5 
p(t 
= 
trja 
= 
tr) 
=0:05 
p(t 
= 
trja 
= 
fa) 
=0:01 
p(l 
= 
trjs 
= 
tr) 
=0:1 
p(l 
= 
trjs 
= 
fa) 
=0:01 
p(b 
= 
trjs 
= 
tr) 
=0:6 
p(b 
= 
trjs 
= 
fa) 
=0:3 
p(x 
= 
trje 
= 
tr) 
=0:98 
p(x 
= 
trje 
= 
fa) 
=0:05 
p(d 
= 
trje 
= 
tr;b 
= 
tr) 
=0:9 
p(d 
= 
trje 
= 
tr;b 
= 
fa) 
=0:7 
p(d 
= 
trje 
= 
fa;b 
= 
tr) 
=0:8 
p(d 
= 
trje 
= 
fa;b 
= 
fa) 
=0:1 


p(e 
= 
trjt, 
l)=0 
only 
if 
both 
t 
and 
l 
are 
fa, 
1 
otherwise. 


DRAFT 
March 
9, 
2010 



Exercises 


GaugeBatteryFuel
Figure 
3.16: 
Belief 
Network 
of 
car 
not 
starting[128], 
see 
exercise(23). 


TurnOverStart
Exercise 
25. 
If 
we 
interpret 
the 
Chest 
Clinic 
network 
exercise(24) 
causally, 
how 
can 
we 
help 
a 
doctor 
answer 
the 
question 
`If 
I 
could 
cure 
my 
patients 
of 
Bronchitis, 
how 
would 
this 
aect 
my 
patients's 
chance 
of 
being 
short 
of 
breath?'. 
How 
does 
this 
compare 
with 
p(d 
= 
trjb 
= 
fa) 
in 
a 
non-causal 
interpretation, 
and 
what 
does 
this 
mean? 


Exercise 
26. 
There 
is 
a 
synergistic 
relationship 
between 
Asbestos 
(A) 
exposure, 
Smoking 
(S) 
and 
Cancer 
(C). 
A 
model 
describing 
this 
relationship 
is 
given 
by 


p(A, 
S, 
C)= 
p(CjA, 
S)p(A)p(S) 
(3.8.1) 


1. 
Is 
A 
.
??Sj;? 
2. 
Is 
A 
.
??SjC? 
3. 
How 
could 
you 
adjust 
the 
model 
to 
account 
for 
the 
fact 
that 
people 
who 
work 
in 
the 
building 
industry 
have 
a 
higher 
likelihood 
to 
also 
be 
smokers 
and 
also 
a 
higher 
likelihood 
to 
asbestos 
exposure? 
Exercise 
27. 
Consider 
the 
three 
variable 
distribution 


p(a, 
b, 
c)= 
p(ajb)p(bjc)p(c) 
(3.8.2) 


where 
all 
variables 
are 
binary. 
How 
many 
parameters 
are 
needed 
to 
specify 
distributions 
of 
this 
form? 


Exercise 
28. 


Consider 
the 
Belief 
Network 
on 
the 
right 
which 
represents 
Mr 
Holmes’ 
burglary 
worries 
as 
given 
in 
g(3.2a) 
: 
(B)urglar, 
(A)larm, 
(W)atson, 
Mrs 
(Gibbon). 


BA
G
W
All 
variables 
take 
the 
two 
states 
ftr, 
fag. 
The 
table 
entries 
are 


p(B 
= 
tr) 
=0:01 


p(A 
= 
trjB 
= 
tr) 
=0:99 
p(A 
= 
trjB 
= 
fa) 
=0:05 


(3.8.3)
p(W 
= 
trjA 
= 
tr) 
=0:9 
p(W 
= 
trjA 
= 
fa) 
=0:5 


p(G 
= 
trjA 
= 
tr) 
=0:7 
p(G 
= 
trjA 
= 
fa) 
=0:2 


1. 
Compute 
`by 
hand’ 
(i.e. 
show 
your 
working) 
: 
(a) 
p(B 
= 
trjW 
= 
tr) 
(b) 
p(B 
= 
trjW 
= 
tr;G 
= 
fa) 
2. 
Consider 
the 
same 
situation 
as 
above, 
except 
that 
now 
the 
evidence 
is 
uncertain. 
Mrs 
Gibbon 
thinks 
that 
the 
state 
is 
G 
= 
fa 
with 
probability 
0.9. 
Similarly, 
Dr 
Watson 
believes 
in 
the 
state 
W 
= 
fa 
with 
value 
0.7. 
Compute 
`by 
hand’ 
the 
posteriors 
under 
these 
uncertain 
(soft) 
evidences: 
(a) 
p(B 
= 
trjW~
) 
(b) 
p(B 
= 
tr| 
~G)
W, 
~

Exercise 
29. 
A 
doctor 
gives 
a 
patient 
a 
(D)rug 
(drug 
or 
no 
drug) 
dependent 
on 
their 
(A)ge 
(old 
or 
young) 
and 
(G)ender 
(male 
or 
female). 
Whether 
or 
not 
the 
patient 
(R)ecovers 
(recovers 
or 
doesn't 
recover) 
depends 
on 
all 
D, 
A, 
G. 
In 
addition 
A.
??Gj;. 


1. 
Write 
down 
the 
Belief 
Network 
for 
the 
above 
situation. 
DRAFT 
March 
9, 
2010 



Exercises 


2. 
Explain 
how 
to 
compute 
p(recoverjdrug). 
3. 
Explain 
how 
to 
compute 
p(recoverjdo(drug), 
young). 
Exercise 
30. 
Implement 
the 
Wet 
Grass 
scenario 
numerically 
using 
the 
BRMLtoolbox. 


Exercise 
31 
(LA 
Burglar). 
Consider 
the 
Burglar 
scenario, 
example(10). 
We 
now 
wish 
to 
model 
the 
fact 
that 
in 
Los 
Angeles 
the 
probability 
of 
being 
burgled 
increases 
if 
there 
is 
an 
earthquake. 
Explain 
how 
to 
include 
this 
eect 
in 
the 
model. 


Exercise 
32. 
Given 
two 
Belief 
Networks 
represented 
as 
DAGs 
with 
associated 
adjacency 
matrices 
A 
and 
B, 
write 
a 
MATLAB 
function 
MarkovEquiv(A,B).m 
that 
returns 
1 
if 
A 
and 
B 
are 
Markov 
equivalent, 
and 
zero 
otherwise. 


Exercise 
33. 
The 
adjacency 
matrices 
of 
two 
Belief 
Networks 
are 
given 
below 
(see 
ABmatrices.mat). 
State 
if 
they 
are 
Markov 
equivalent. 


1

0

1

. 


001101000 
001100000 


A 
= 


BBBBBBBBB
. 


001010000 


000000100 


000000011 


001000100 


000100010 


000000001 


CCCCCCCCC
. 


, 


B 
= 


BBBBBBBBB
. 


001000000 


000000100 


000000011 


011000100 


100100010 


000000001 


CCCCCCCCC
. 


(3.8.4) 
000000000 
000000000 
000000000 
000000000 


Exercise 
34. 
There 
are 
three 
computers 
indexed 
by 
i 
2f1, 
2, 
3g. 
Computer 
i 
can 
send 
a 
message 
in 
one 
timestep 
to 
computer 
j 
if 
Cij 
=1, 
otherwise 
Cij 
=0. 
There 
is 
a 
fault 
in 
the 
network 
and 
the 
task 
is 
to 
nd 
out 
some 
information 
about 
the 
communication 
matrix 
C 
(C 
is 
not 
necessarily 
symmetric). 
To 
do 
this, 
Thomas, 
the 
engineer, 
will 
run 
some 
tests 
that 
reveal 
whether 
or 
not 
computer 
i 
can 
send 
a 
message 
to 
computer 
j 
in 
t 
timesteps, 
t 
2f1, 
2g. 
This 
is 
expressed 
as 
Cij(t), 
with 
Cij(1) 
= 
Cij. 
For 
example, 
he 
might 
know 
that 
C13(2) 
= 
1, 
meaning 
that 
according 
to 
his 
test, 
a 
message 
sent 
from 
computer 
1 
will 
arrive 
at 
computer 
3 
in 
at 
most 
2 
timesteps. 
Note 
that 
this 
message 
could 
go 
via 
dierent 
routes 
– 
it 
might 
go 
directly 
from 
1 
to 
3 
in 
one 
timestep, 
or 
indirectly 
from 
1 
to 
2 
and 
then 
from 
2 
to 
3, 
or 
both. 
You 
may 
assume 
Cii 
=1. 
A 
priori 
Thomas 
thinks 
there 
is 
a 
10% 
probability 
that 
Cij 
=1. 
Given 
the 
test 
information 
C 
= 
fC12(2) 
= 
1;C23(2) 
= 
0g, 
compute 
the 
a 
posteriori 
probability 
vector 


[p(C12 
=1jC);p(C13 
=1jC);p(C23 
=1jC);p(C32 
=1jC);p(C21 
=1jC);p(C31 
=1jC)] 
(3.8.5) 


Exercise 
35. 
A 
Belief 
Network 
models 
the 
relation 
between 
the 
variables 
oil, 
inf, 
eh, 
bp, 
rt 
which 
stand 
for 
the 
price 
of 
oil, 
ination 
rate, 
economy 
health, 
British 
Petroleum 
Stock 
price, 
retailer 
stock 
price. 
Each 
variable 
takes 
the 
states 
low, 
high, 
except 
for 
bp 
which 
has 
states 
low, 
high, 
normal. 
The 
Belief 
Network 
model 
for 
these 
variables 
has 
tables 


p(eh=low)=0.2 
p(bp=lowjoil=low)=0.9 
p(bp=lowjoil=high)=0.1 
p(oil=lowjeh=low)=0.9 
p(rt=lowjinf=low,eh=low)=0.9 
p(rt=lowjinf=high,eh=low)=0.1 
p(inf=lowjoil=low,eh=low)=0.9 
p(inf=lowjoil=high,eh=low)=0.1 
p(bp=normaljoil=low)=0.1 
p(bp=normaljoil=high)=0.4 
p(oil=lowjeh=high)=0.05 
p(rt=lowjinf=low,eh=high)=0.1 
p(rt=lowjinf=high,eh=high)=0.01 
p(inf=lowjoil=low,eh=high)=0.1 
p(inf=lowjoil=high,eh=high)=0.01 


1. 
Draw 
a 
Belief 
Network 
for 
this 
distribution. 
2. 
Given 
that 
BP 
stock 
price 
is 
normal 
and 
the 
retailer 
stock 
price 
is 
high, 
what 
is 
the 
probability 
that 
ination 
is 
high? 
Exercise 
36. 
There 
are 
a 
set 
of 
C 
potentials 
with 
potential 
c 
dened 
on 
a 
subset 
of 
variables 
Xc. 
If 
Xc 
Xd 
then 
can 
merge 
(multiply) 
potentials 
c 
and 
d 
since 
c 
is 
contained 
within 
d. 
With 
reference 
to 
suitable 
graph 
structures, 
describe 
an 
ecient 
algorithm 
to 
merge 
a 
set 
of 
potentials 
so 
that 
for 
the 
new 
set 
of 
potentials 
no 
potential 
is 
contained 
within 
the 
other. 


DRAFT 
March 
9, 
2010 



Exercises 


DRAFT 
March 
9, 
2010 



CHAPTER 
4 


Graphical 
Models 


4.1 
Graphical 
Models 
Graphical 
Models 
(GMs) 
are 
depictions 
of 
independence/dependence 
relationships 
for 
distributions. 
Each 
form 
of 
GM 
is 
a 
particular 
union 
of 
graph 
and 
probability 
constructs 
and 
details 
the 
form 
of 
independence 
assumptions 
represented. 
GMs 
are 
useful 
since 
they 
provide 
a 
framework 
for 
studying 
a 
wide 
class 
of 
probabilistic 
models 
and 
associated 
algorithms. 
In 
particular 
they 
help 
to 
clarify 
modelling 
assumptions 
and 
provide 
a 
unied 
framework 
under 
which 
inference 
algorithms 
in 
dierent 
communities 
can 
be 
related. 


It 
needs 
to 
be 
emphasised 
that 
all 
forms 
of 
GM 
have 
a 
limited 
ability 
to 
graphically 
expresses 
conditional 
(in)dependence 
statements[265]. 
As 
we've 
seen, 
Belief 
Networks 
are 
useful 
for 
modelling 
ancestral 
conditional 
independence. 
In 
this 
chapter 
we'll 
introduce 
other 
types 
of 
GM 
that 
are 
more 
suited 
to 
representing 
dierent 
assumptions. 
Markov 
Networks, 
for 
example, 
are 
particularly 
suited 
to 
modelling 
marginal 
dependence 
and 
conditional 
independence. 
Here 
we'll 
focus 
on 
Markov 
Networks, 
Chain 
Graphs 
(which 
marry 
Belief 
and 
Markov 
networks) 
and 
Factor 
Graphs. 
There 
are 
many 
more 
inhabitants 
of 
the 
zoo 
of 
Graphical 
Models, 
see 
[70, 
293]. 


The 
general 
viewpoint 
we 
adopt 
is 
to 
describe 
the 
problem 
environment 
using 
a 
probabilistic 
model, 
after 
which 
reasoning 
corresponds 
to 
performing 
probabilistic 
inference. 
This 
is 
therefore 
a 
two 
part 
process 
: 


Modelling 
After 
identifying 
all 
potentially 
relevant 
variables 
of 
a 
problem 
environment, 
our 
task 
is 
to 
describe 
how 
these 
variables 
can 
interact. 
This 
is 
achieved 
using 
structural 
assumptions 
as 
to 
the 
form 
of 
the 
joint 
probability 
distribution 
of 
all 
the 
variables, 
typically 
corresponding 
to 
assumptions 
of 
independence 
of 
variables. 
Each 
class 
of 
graphical 
model 
corresponds 
to 
a 
factorisation 
property 
of 
the 
joint 
distribution. 


Inference 
Once 
the 
basic 
assumptions 
as 
to 
how 
variables 
interact 
with 
each 
other 
is 
formed 
(i.e. 
the 
probabilistic 
model 
is 
constructed) 
all 
questions 
of 
interest 
are 
answered 
by 
performing 
inference 
on 
the 
distribution. 
This 
can 
be 
a 
computationally 
non-trivial 
step 
so 
that 
coupling 
GMs 
with 
accurate 
inference 
algorithms 
is 
central 
to 
successful 
graphical 
modelling. 


Whilst 
not 
a 
strict 
separation, 
GMs 
tend 
to 
fall 
into 
two 
broad 
classes 
– 
those 
useful 
in 
modelling, 
and 
those 
useful 
in 
representing 
inference 
algorithms. 
For 
modelling, 
Belief 
Networks, 
Markov 
Networks, 
Chain 
Graphs 
and 
Inuence 
Diagrams 
are 
some 
of 
the 
most 
popular. 
For 
inference 
one 
typically 
`compiles’ 
a 
model 
into 
a 
suitable 
GM 
for 
which 
an 
algorithm 
can 
be 
readily 
applied. 
Such 
inference 
GMs 
include 
Factor 
Graphs, 
Junction 
Trees 
and 
Region 
Graphs. 


49 



x1
x2
x3
x4
x1
x2
x3
x4
x1
x2
x3
x4
x5
x6
x1
x2
x3
x4
x1
x2
x3
x4
x1
x2
x3
x4
x5
x6
Markov 
Networks 


Figure 
4.1: 
(a): 
pa 
= 
(x1;x2)(x2;x3)(x3;x4)(x4;x1)=Za. 
(b): 
pb 
= 
(x1;x2;x3;x4)=Zb. 
(c): 
pc 
= 
(x1;x2;x4)(x2;x3;x4)(x3;x5)(x3;x6)=Zc. 


(a) 
(b) 
(c) 
4.2 
Markov 
Networks 
Belief 
Networks 
correspond 
to 
a 
special 
kind 
of 
factorisation 
of 
the 
joint 
probability 
distribution 
in 
which 
each 
of 
the 
factors 
is 
itself 
a 
distribution. 
An 
alternative 
factorisation 
is, 
for 
example 


1 


p(a, 
b, 
c)= 
(a, 
b)(b, 
c) 
(4.2.1)

Z 
where 
(a, 
b) 
and 
(b, 
c) 
are 
potentials 
and 
Z 
is 
a 
constant 
which 
ensures 
normalisation, 
called 
the 
partition 
function 


X

Z 
=(a, 
b)(b, 
c) 
(4.2.2) 


a;b;c 


We 
will 
typically 
use 
the 
convention 
that 
the 
ordering 
of 
the 
variables 
in 
the 
potential 
is 
not 
relevant 
(as 
for 
a 
distribution) 
– 
the 
joint 
variables 
simply 
index 
an 
element 
of 
the 
potential 
table. 
Markov 
Networks 
are 
dened 
as 
products 
of 
non-negative 
functions 
dened 
on 
maximal 
cliques 
of 
an 
undirected 
graph 
– 
see 
g(4.1). 


Denition 
24 
(Potential). 
A 
potential 
(x) 
is 
a 
non-negative 
function 
of 
the 
variable 
x, 
(x) 
= 
0. 
A 
joint 
potential 
(x1;:::;xn) 
is 
a 
non-negative 
function 
of 
the 
set 
of 
variables. 
A 
distribution 
is 
a 
special 


PX

case 
of 
a 
potential 
satisfying 
normalisation, 
(x) 
= 
1. 
This 
holds 
similarly 
for 
continuous 
variables, 


x 


with 
summation 
replaced 
by 
integration. 


Denition 
25 
(Markov 
Network). 
For 
a 
set 
of 
variables 
X 
= 
fx1;:::;xn} 
a 
Markov 
network 
is 
dened 
as 
a 
product 
of 
potentials 
on 
subsets 
of 
the 
variables 
Xc 
X 
: 


Y

1 
C

p(x1;:::;xn)= 
c(Xc) 
(4.2.3)

Z 


c=1 


Graphically 
this 
is 
represented 
by 
an 
undirected 
graph 
G 
with 
Xc;c 
=1;:::;C 
being 
the 
maximal 
cliques 
of 
G. 
The 
constant 
Z 
ensures 
the 
distribution 
is 
normalised. 
The 
graph 
is 
said 
to 
satisfy 
the 
factorisation 
property. 
In 
the 
special 
case 
that 
the 
graph 
contains 
cliques 
of 
only 
size 
2, 
the 
distribution 
is 
called 
a 
pairwise 
Markov 
Network, 
with 
potentials 
dened 
on 
each 
link 
between 
two 
variables. 


For 
the 
case 
in 
which 
clique 
potentials 
are 
strictly 
positive, 
this 
is 
called 
a 
Gibbs 
distribution. 


Remark 
4 
(Pairwise 
Markov 
network). 
Whilst 
a 
Markov 
network 
is 
formally 
dened 
on 
maximal 
cliques, 
in 
practice 
authors 
often 
use 
the 
term 
to 
refer 
to 
non-maximal 
cliques. 


For 
example, 
in 
the 
graph 
on 
the 
right, 
the 
maximal 
cliques 
are 
x1;x2;x3 
and 
x2;x3;x4, 
so 
that 
the 
graph 
describes 
a 
distribution 
p(x2;x2;x3;x4)= 
(x1;x2;x3)(x2;x3;x4)=Z. 
In 
a 
pairwise 
network 
though 
the 
potentials 
are 
assumed 
to 
be 
over 
two-cliques, 
giving 
p(x2;x2;x3;x4)= 
(x1;x2)(x1;x3)(x2;x3)(x2;x4)(x3;x4)=Z. 


x1
x2x3
x4
DRAFT 
March 
9, 
2010 



Markov 
Networks 


Denition 
26 
(Properties 
of 
Markov 
Networks). 


AB
C
p(A, 
B, 
C)= 
AC 
(A, 
C)BC(B, 
C)=Z 
(4.2.4) 
A 
and 
B 
are 
unconditionally 
dependent 
: 
p(A, 
B)6

= 
p(A)p(B). 
A 
and 
B 
are 
conditionally 
independent 
on 
C 
: 
p(A, 
BjC)= 
p(AjC)p(BjC). 


AB
C
!6AB
Marginalising 
over 
C 
makes 
A 
and 
B 
(graphically) 
dependent. 


AB
C
!6AB
Conditioning 
on 
C 
makes 
A 
and 
B 
independent. 


4.2.1 
Markov 
properties 
We 
here 
state 
some 
of 
the 
most 
useful 
results. 
The 
reader 
is 
referred 
to 
[168] 
for 
proofs 
and 
more 
detailed 
discussion. 
Consider 
the 
Markov 
Network 
in 
g(4.2a). 
Here 
we 
use 
the 
shorthand 
p(1) 
6p(x1), 
(1, 
2, 
3) 
6(x1;x2;x3) 
etc. 
We 
will 
use 
this 
undirected 
graph 
to 
demonstrate 
conditional 
independence 
properties. 


Local 
Markov 
property 


Denition 
27 
(Local 
Markov 
Property). 


p(xjX6nx)= 
p(xjne 
(x)) 
(4.2.5) 


When 
conditioned 
on 
its 
neighbours, 
x 
is 
independent 
of 
the 
remaining 
variables 
of 
the 
graph. 


The 
conditional 
distribution 
p(4j1, 
2, 
3, 
5, 
6, 
7) 
is 


(1, 
2, 
3)(2, 
3, 
4)(4, 
5, 
6)(5, 
6, 
7)(2, 
3, 
4)(4, 
5, 
6)

p(4j1, 
2, 
3, 
5, 
6, 
7) 
= 
P= 
P= 
p(4j2, 
3, 
5, 
6)

(1, 
2, 
3)(2, 
3, 
4)(4, 
5, 
6)(5, 
6, 
7) 
(2, 
3, 
4)(4, 
5, 
6)

44 


(4.2.6) 
The 
last 
line 
above 
follows 
since 
the 
variable 
x4 
only 
appears 
in 
the 
cliques 
that 
border 
x4. 
The 
generalisation 
of 
the 
above 
example 
is 
clear: 
a 
MN 
with 
positive 
clique 
potentials 
, 
dened 
with 
respect 
to 
an 
undirected 
graph 
G 
entails1 
p(xijxni)= 
p(xijne 
(xi)). 


Pairwise 
Markov 
property 


Denition 
28 
(Pairwise 
Markov 
Property). 
For 
any 
non-adjacent 
vertices 
x 
and 
y 


x.
??yjXnfx, 
yg6(4.2.7) 


1The 
notation 
xni 
is 
shorthand 
for 
the 
set 
of 
all 
variables 
X 
excluding 
variable 
xi, 
namely 
Xnxi 
in 
set 
notation. 


DRAFT 
March 
9, 
2010 
51 



Markov 
Networks 


12345671234567
Figure 
4.2: 
(a): 
(1, 
2, 
3)(2, 
3, 
4)(4, 
5, 
6)(5, 
6, 
7). 
(b): 
By 
the 
global 
Markov 
property, 
since 


every 
path 
from 
1 
to 
7 
passes 
through 
4, 
then 
1 
.
??7j4. 


(a) 
(b) 
(1, 
2, 
3)(2, 
3, 
4)(4, 
5, 
6)(5, 
6, 
7)


(1, 
2, 
3)(2, 
3, 
4)(4, 
5, 
6)


p(1, 
4j2, 
3, 
5, 
6, 
7) 
=


 


=


 


(1, 
2, 
3)(2, 
3, 
4)(4, 
5, 
6)(5, 
6, 
7) 
(1, 
2, 
3)(2, 
3, 
4)(4, 
5, 
6)

1;41;4 


(4.2.8) 
= 
p(1j2, 
3, 
4, 
5, 
6, 
7)p(4j1, 
2, 
3, 
5, 
6, 
7) 
(4.2.9) 


where 
the 
last 
line 
follows 
since 
for 
xed 
2, 
3, 
5, 
6, 
7, 
the 
function 
(1, 
2, 
3)(2, 
3, 
4)(4, 
5, 
6) 
is 
a 
product 
of 
a 
function 
on 
1 
and 
a 
function 
on 
4, 
implying 
independence. 


Global 
Markov 
property 


Denition 
29 
(Separation). 
A 
subset 
S6separates 
a 
subset 
A6from 
a 
subset 
B6if 
every 
path 
from 
any 
member 
of 
A6to 
any 
member 
of 
B6passes 
though 
S. 


Denition 
30 
(Global 
Markov 
Property). 
For 
a 
disjoint 
subset 
of 
variables, 
(A, 
B, 
S) 
where 
S6separates 
A6from 
B6in 
G, 
then 
A.
??BjS. 


 


p(1, 
7j4) 
/p(1, 
2, 
3, 
4, 
5, 
6, 
7) 
(4.2.10) 


2;3;5;6 


X

=(1, 
2, 
3)(2, 
3, 
4)(4, 
5, 
6)(5, 
6, 
7) 
(4.2.11) 


X8<X

2;3;5;6 


9=

X8<X9=X

= 


(1, 
2, 
3)(2, 
3, 
4) 


(4, 
5, 
6)(5, 
6, 
7) 


(4.2.12)
.


.
.


.


2;35;6 


This 
implies 
that 
p(1, 
7j4) 
= 
p(1j4)p(7j4). 


Example 
16 
(Boltzmann 
machine). 
A 
Boltzmann 
machine 
is 
a 
MN 
on 
binary 
variables 
dom(xi)= 
f0, 
1gof 
the 
form 


PP

1 


p(x)= 
ei<j 
wij 
xixj 
+i 
bixi 
(4.2.13)

Z(w;b)

where 
the 
interactions 
wij 
are 
the 
`weights’ 
and 
the 
bi 
the 
biases. 
This 
model 
has 
been 
studied 
in 
the 
machine 
learning 
community 
as 
a 
basic 
model 
of 
distributed 
memory 
and 
computation[2]. 
The 
graphical 
model 
of 
the 
BM 
is 
an 
undirected 
graph 
with 
a 
link 
been 
nodes 
i 
and 
j 
for 
wij6

= 
0. 
Consequently, 
for 
all 
but 
specially 
constrained 
W, 
the 
graph 
is 
multiply-connected 
and 
inference 
will 
be 
typically 
intractable. 


4.2.2 
Gibbs 
networks 
For 
simplicity 
we 
assume 
that 
the 
potentials 
are 
strictly 
positive 
in 
which 
case 
MNs 
are 
also 
termed 
Gibbs 
Networks. 
In 
this 
case, 
a 
GN 
satises 
the 
following 
independence 
relations: 


52 
DRAFT 
March 
9, 
2010 



Markov 
Networks 


x1
x3x4
x2
x3x4
x1
x3x4
x2
x3x4
x1x2
x3x4
(a) 
(b) 
(c) 
(d) 
(e) 
Figure 
4.3: 
(a-d): 
Local 
distributions. 
(e): 
The 
Markov 
network 
consistent 
with 
the 
local 
distributions. 
If 
the 
local 
distributions 
are 
positive, 
by 
the 
Hammersley-Cliord 
theorem, 
the 
only 
joint 
distribution 
that 
can 
be 
consistent 
with 
the 
local 
distributions 
must 
be 
a 
Gibbs 
distribution 
with 
structure 
given 
by 
(e). 


4.2.3 
Markov 
random 
elds 
Denition 
31 
(Markov 
Random 
Field). 
A 
MRF 
is 
dened 
by 
a 
set 
of 
distributions 
p(xijne 
(xi)) 
where 
i 
2f1;:::;ng6indexes 
the 
distributions 
and 
ne 
(xi) 
are 
the 
neighbours 
of 
variable 
xi, 
namely 
that 
subset 
of 
the 
variables 
x1;:::;xn 
that 
the 
distribution 
of 
variable 
xi 
depends 
on. 
The 
term 
Markov 
indicates 
that 
this 
is 
a 
proper 
subset 
of 
the 
variables. 


A 
distribution 
is 
an 
MRF 
with 
respect 
to 
an 
undirected 
graph 
G 
if 


p(xijxni)= 
p(xijne 
(xi)) 
(4.2.14) 


where 
ne 
(xi) 
are 
the 
neighbouring 
variables 
of 
variable 
xi, 
according 
to 
the 
undirected 
graph 
G. 


Hammersley 
Cliord 
Theorem 


The 
Hammersley-Cliord 
theorem 
helps 
resolve 
questions 
as 
to 
when 
a 
set 
of 
positive 
local 
distributions 
p(xijne 
(xi)) 
could 
ever 
form 
a 
consistent 
joint 
distribution 
p(x1;:::;xn). 
Local 
distributions 
p(xijne 
(xi)) 
can 
form 
a 
consistent 
joint 
distribution 
if 
and 
only 
if 
p(x1;:::;xn) 
factorises 
according 
to 


1 


p(x1;:::;xn) 
= 
exp 
..6Vc(Xc) 
(4.2.15)

Z 


c 


where 
the 
sum 
is 
over 
all 
cliques 
and 
Vc(Xc) 
is 
a 
real 
function 
dened 
over 
the 
variables 
in 
the 
clique

a 


indexed 
by 
c. 
Equation 
(4.2.15) 
is 
equivalent 
to 
(Xc), 
namely 
a 
MN 
on 
positive 
clique 
potentials. 


c 


The 
graph 
over 
which 
the 
cliques 
are 
dened 
is 
an 
undirected 
graph 
with 
a 
link 
between 
xi 
and 
xj 
if 


p(xijxni)=6p(xijxn(i;j)) 
(4.2.16) 


That 
is, 
if 
xj 
has 
an 
eect 
on 
the 
conditional 
distribution 
of 
xi, 
then 
add 
an 
undirected 
link 
between 
xi 
and 
xj. 
This 
is 
then 
repeated 
over 
all 
the 
variables 
xi[35, 
203], 
see 
g(4.3). 
Note 
that 
the 
HC 
theorem 
does 
not 
mean 
that 
given 
a 
set 
of 
conditional 
distributions, 
we 
can 
always 
form 
a 
consistent 
joint 
distribution 
from 
them 
– 
rather 
it 
states 
what 
the 
functional 
form 
of 
a 
joint 
distribution 
for 
the 
conditionals 
to 
be 
consistent 
with 
the 
joint, 
see 
exercise(45). 


4.2.4 
Conditional 
independence 
using 
Markov 
networks 
For 
X6,Y,Z6each 
being 
collections 
of 
variables, 
in 
section(3.3.3) 
we 
discussed 
an 
algorithm 
to 
determine 
X6.
??YjZ. 
An 
alternative 
and 
more 
general 
method 
(since 
it 
handles 
directed 
and 
undirected 
graphs) 
uses 
the 
following 
steps: 
(see 
[74, 
169]) 


DRAFT 
March 
9, 
2010 
53 



Markov 
Networks 


ab
Figure 
4.4: 
(a): 
Belief 
Network 
for 
which 
we 
are 
interested 
in 
checking 
conditional 
independence 
a 
. 
??bjfd, 
ig. 
(b): 
Ancestral 
moralised 
graph 
for 
a 
. 
??bjfd, 
ig. 
Every 
path 
from 
a 
red 
to 
green 
node 
passes 
through 
a 
yellow 
node, 
so 
a 
and 
b 
are 
independent 
given 
d, 
i. 
Alternatively, 
if 
we 
consider 
a 
. 
??b 
| 
i, 
the 
variable 
d 
is 
uncoloured, 
and 
we 
can 
travel 
from 
the 
red 
to 
the 
green 
without 
encountering 
a 
yellow 
node 
(using 
the 
e 
- 
f 
path). 
In 
this 
case 
a 
is 
dependent 
on 
b, 
conditioned 
on 
i. 


cd
ef
i
(b) 
Ancestral 
Graph 
Remove 
from 
the 
DAG 
any 
node 
which 
is 
neither 
in 
X 
[Y[Z 
nor 
an 
ancestor 
of 
a 
node 
in 
this 
set, 
together 
with 
any 
edges 
in 
or 
out 
of 
such 
nodes. 


Moralisation 
Add 
a 
line 
between 
any 
two 
remaining 
nodes 
which 
have 
a 
common 
child, 
but 
are 
not 
already 
connected 
by 
an 
arrow. 
Then 
remove 
remaining 
arrowheads. 


Separation 
In 
the 
undirected 
graph 
so 
constructed, 
look 
for 
a 
path 
which 
joins 
a 
node 
in 
X 
to 
one 
in 
Y 
but 
does 
not 
intersect 
Z. 
If 
there 
is 
no 
such 
path 
deduce 
that 
X 
.
??YjZ. 


For 
Markov 
Networks 
only 
the 
nal 
separation 
criterion 
needs 
to 
be 
applied. 
See 
g(4.4) 
for 
an 
example. 


4.2.5 
Lattice 
Models 
Undirected 
models 
have 
a 
long 
history 
in 
dierent 
branches 
of 
science, 
especially 
statistical 
mechanics 
on 
lattices 
and 
more 
recently 
as 
models 
in 
visual 
processing 
in 
which 
the 
models 
encourage 
neighbouring 
variables 
to 
be 
in 
the 
same 
states[35, 
36, 
106]. 


Consider 
a 
model 
in 
which 
our 
desire 
is 
that 
states 
of 
the 
binary 
valued 
variables 
x1;:::;x9, 
arranged 
on 
a 
lattice 
(right) 
should 
prefer 
their 
neighbouring 
variables 
to 
be 
in 
the 
same 
state 


ab
cd
efg
i
jk
h
(a)
x1x2x3
Y

1 


p(x1;:::;x9)= 
ij(xi;xj) 
(4.2.17)

Z

ij 


x4
x5
x6
where 
i 
~ 
j 
denotes 
the 
set 
of 
indices 
where 
i 
and 
j 
are 
neighbours 
in 
the 
undirected 
graph. 


x7
x9
x8
The 
Ising 
model 


A 
set 
of 
potentials 
for 
equation 
(4.2.17) 
that 
encourages 
neighbouring 
variables 
to 
have 
the 
same 
state 
is 


- 
1 
(xi..xj 
)2 


ij(xi;xj)= 
e 
2T 
(4.2.18) 


This 
corresponds 
to 
a 
well-known 
model 
of 
the 
physics 
of 
magnetic 
systems, 
called 
the 
Ising 
model 
which 
consists 
of 
`mini-magnets’ 
which 
prefer 
to 
be 
aligned 
in 
the 
same 
state, 
depending 
on 
the 
temperature 


00.511.5200.51T/TcM
Figure 
4.5: 
Onsagar 
magnetisation. 
As 
the 
temperature 
T 
decreases 
towards 
the 
critical 
temperature 
Tc 
a 
phase 
transition 
occurs 
in 
which 
a 
large 
fraction 
of 
the 
variables 
become 
aligned 
in 
the 
same 
state. 


DRAFT 
March 
9, 
2010 



Chain 
Graphical 
Models 


ab
cd
ab
cd
a
bg
dc
ef
aedf
c
bg
(a) 
(b) 
(c) 
(d) 
Figure 
4.6: 
Chain 
graphs. 
The 
chain 
components 
are 
identied 
by 
deleting 
the 
directed 
edges 
and 
identifying 
the 
remaining 
connected 
components. 
(a): 
Chain 
components 
are 
(a),(b),(c, 
d), 
which 
can 
be 
written 
as 
a 
BN 
on 
the 
cluster 
variables 
in 
(b). 
(c): 
Chain 
components 
are 
(a, 
e, 
d, 
f), 
(b, 
g), 
(c), 
which 
has 
the 
cluster 
BN 
representation 
(d). 
(From 
[168]) 


T 
. 
For 
high 
T 
the 
variables 
behave 
independently 
so 
that 
no 
global 
magnetisation 
appears. 
For 
low 
T 
, 
there 
is 
a 
strong 
preference 
for 
neighbouring 
mini-magnets 
to 
become 
aligned, 
generating 
a 
strong 
macro-
magnet. 
Remarkably, 
one 
can 
show 
that, 
in 
a 
very 
large 
two-dimensional 
lattice, 
below 
the 
so-called 
Curie 
temperature, 
Tc 
˜ 
2:269 
(for 
1 
variables), 
the 
system 
admits 
a 
phase 
change 
in 
that 
a 
large 
fraction 
of 
the 
variables 
become 
aligned 
– 
above 
Tc, 
on 
average, 
the 
variables 
are 
unaligned. 
This 
is 
depicted 
in 
PN

g(4.5) 
where 
M 
= 
ji=1 
xij=N 
is 
the 
average 
alignment 
of 
the 
variables. 
That 
this 
phase 
change 
happens 
for 
non-zero 
temperature 
has 
driven 
considerable 
research 
in 
this 
and 
related 
areas[40]. 
Global 
coherence 
eects 
such 
as 
this 
that 
arise 
from 
weak 
local 
constraints 
are 
present 
in 
systems 
that 
admit 
emergent 
behaviour. 
Similar 
local 
constraints 
are 
popular 
in 
image 
restoration 
algorithms 
to 
clean 
up 
noise, 
under 
the 
assumption 
that 
noise 
will 
not 
show 
any 
local 
spatial 
coherence, 
whilst 
`signal’ 
will. 
An 
example 
is 
given 
in 
section(28.8) 
where 
we 
discuss 
algorithms 
for 
inference 
under 
special 
constraints 
on 
the 
MRF. 


4.3 
Chain 
Graphical 
Models 
Denition 
32 
(Chain 
Component). 
The 
chain 
components 
of 
a 
graph 
G 
are 
obtained 
by 
: 


1. 
Forming 
a 
graph 
G' 
with 
directed 
edges 
removed 
from 
G. 
2. 
Then 
each 
connected 
component 
in 
G' 
constitutes 
a 
chain 
component. 
Chain 
Graphs 
(CGs) 
contain 
both 
directed 
and 
undirected 
links. 
To 
develop 
the 
intuition, 
consider 
g(4.6a). 
The 
only 
terms 
that 
we 
can 
unambiguously 
specify 
from 
this 
depiction 
are 
p(a) 
and 
p(b) 
since 
there 
is 
no 
mixed 
interaction 
of 
directed 
and 
undirected 
edges 
at 
the 
a 
and 
b 
vertices. 
By 
probability, 
therefore, 
we 
must 
have 


p(a, 
b, 
c, 
d)= 
p(a)p(b)p(c, 
dja, 
b) 
(4.3.1) 


Looking 
at 
the 
graph, 
we 
might 
expect 
the 
interpretation 
to 
be 


p(c, 
dja, 
b)= 
(c, 
d)p(cja)p(djb) 
(4.3.2) 


However, 
to 
ensure 
normalisation, 
and 
also 
to 
retain 
generality, 
we 
interpret 
this 
chain 
component 
as 


X

p(c, 
dja, 
b)= 
(c, 
d)p(cja)p(djb)(a, 
b), 
with 
(a, 
b) 
= 
1=(c, 
d)p(cja)p(djb) 
(4.3.3) 


c;d 


This 
leads 
to 
the 
interpretation 
of 
a 
CG 
as 
a 
DAG 
over 
the 
chain 
components. 
Each 
chain 
component 
represents 
a 
distribution 
over 
the 
variables 
of 
the 
component, 
conditioned 
on 
the 
parental 
components. 
The 
conditional 
distribution 
is 
itself 
a 
product 
over 
the 
cliques 
of 
the 
undirected 
component 
and 
moralised 
parental 
components, 
including 
also 
a 
factor 
to 
ensure 
normalisation 
over 
the 
chain 
component. 


DRAFT 
March 
9, 
2010 
55 



Expressiveness 
of 
Graphical 
Models 


Denition 
33 
(Chain 
Graph 
distribution). 
The 
distribution 
associated 
with 
a 
chain 
graph 
G 
is 
found 
by 


rst 
identifying 
the 
chain 
components, 
. 
Then 
Yp(x) 
=
t 
p 
(Xt 
jpa 
(Xt 
)) 
(4.3.4) 
and 
Yp 
(Xt 
jpa 
(Xt 
)) 
/
c2Ct 
f 
(XCt 
) 
(4.3.5) 


where 
Ct 
denotes 
the 
union 
of 
the 
cliques 
in 
component 
t 
together 
with 
the 
moralised 
parental 
components 
of 
, 
with 
f 
being 
the 
associated 
functions 
dened 
on 
each 
clique. 
The 
proportionality 
factor 
is 
determined 
implicitly 
by 
the 
constraint 
that 
the 
distribution 
sums 
to 
1. 


BNs 
are 
CGs 
in 
which 
the 
connected 
components 
are 
singletons. 
MNs 
are 
CGs 
in 
which 
the 
chain 
components 
are 
simply 
the 
connected 
components 
of 
the 
undirected 
graph. 


CGs 
can 
be 
useful 
since 
they 
are 
more 
expressive 
of 
CI 
statements 
than 
either 
Belief 
Networks 
or 
Markov 
Networks 
alone. 
The 
reader 
is 
referred 
to 
[168] 
and 
[99] 
for 
further 
details. 


Example 
17 
(Chain 
Graphs 
are 
more 
expressive 
than 
Belief 
or 
Markov 
Networks). 
Consider 
the 
chain 
graph 
in 
g(4.7a), 
which 
has 
chain 
component 
decomposition 


p(a, 
b, 
c, 
d, 
e, 
f)= 
p(a)p(b)p(c, 
d, 
e, 
fja, 
b) 
(4.3.6) 


where 


p(c, 
d, 
e, 
fja, 
b)= 
(a, 
c)(c, 
e)(e, 
f)(d, 
f)(d, 
b)(a, 
b) 
(4.3.7) 


with 
the 
normalisation 
requirement 


X

(a, 
b) 
= 
1=(a, 
c)(c, 
e)(e, 
f)(d, 
f)(d, 
b) 
(4.3.8) 


c;d;e;f 


The 
marginal 
p(c, 
d, 
e, 
f) 
is 
given 
by 


X

(c, 
e)(e, 
f)(d, 
f)(a, 
b)p(a)p(b)(a, 
c)(d, 
b) 
(4.3.9) 


a;b 


(c;d) 


Since 
the 
marginal 
distribution 
of 
p(c, 
d, 
e, 
f) 
is 
an 
undirected 
4-cycle, 
no 
DAG 
can 
express 
the 
CI 
statements 
contained 
in 
the 
marginal 
p(c, 
d, 
e, 
f). 
Similarly 
no 
undirected 
distribution 
on 
the 
same 
skeleton 
as 
g(4.7a) 
could 
express 
that 
a 
and 
b 
are 
independent 
(unconditionally), 
i.e. 
p(a, 
b)= 
p(a)p(b). 


4.4 
Expressiveness 
of 
Graphical 
Models 
It 
is 
clear 
that 
directed 
distributions 
can 
be 
represented 
as 
undirected 
distributions 
since 
one 
can 
associate 
each 
(normalised) 
factor 
in 
a 
directed 
distribution 
with 
a 
potential. 
For 
example, 
the 
distribution 
p(ajb)p(bjc)p(c) 
can 
be 
factored 
as 
(a, 
b)(b, 
c), 
where 
(a, 
b)= 
p(ajb) 
and 
(b, 
c)= 
p(bjc)p(c), 
with 
Z 
= 
1. 
Hence 
every 
Belief 
Network 
can 
be 
represented 
as 
some 
MN 
by 
simple 
identication 
of 
the 
factors 
in 
the 
distributions. 
However, 
in 
general, 
the 
associated 
undirected 
graph 
(which 
corresponds 
to 
the 


56 
DRAFT 
March 
9, 
2010 



Expressiveness 
of 
Graphical 
Models 


ab
cd
ef
cd
ef
cd
ef
Figure 
4.7: 
The 
CG 
(a) 
expresses 
a 
. 
??b 
jØ 
and 
d 
. 
??e 
| 
(c, 
f). 
No 
directed 
graph 
could 
express 
both 
these 
conditions 
since 
the 
marginal 
distribution 
p(c, 
d, 
e, 
f) 
is 
an 
undirected 
four 
cycle, 
(b). 
Any 
DAG 
on 
a 
4 
cycle 
must 
contain 
a 
collider, 
as 
in 
(c) 
and 
therefore 
express 
a 
dierent 
set 
of 
CI 
statements 
than 
(b). 
Similarly, 
no 
connected 
Markov 
network 
can 
express 
unconditional 
independence 
and 
hence 


(a) 
(b) 
(c) 
(a) 
expresses 
CI 
statements 
that 
no 
Belief 
Network 
or 
Markov 
Network 
alone 
can 
express. 
moralised 
directed 
graph) 
will 
contain 
additional 
links 
and 
independence 
information 
can 
be 
lost. 
For 
example, 
the 
MN 
of 
p(cja, 
b)p(a)p(b) 
if 
a 
single 
clique 
(a, 
b, 
c) 
from 
which 
one 
cannot 
graphically 
infer 
that 
a.
??bj;. 


The 
converse 
question 
is 
whether 
every 
undirected 
model 
can 
be 
represented 
by 
a 
BN 
with 
a 
readily 
derived 
link 
structure? 
Consider 
the 
example 
in 
g(4.8). 
In 
this 
case, 
there 
is 
no 
directed 
model 
with 
the 
same 
link 
structure 
that 
can 
express 
the 
(in)dependencies 
in 
the 
undirected 
graph. 
Naturally, 
every 
probability 
distribution 
can 
be 
represented 
by 
some 
BN 
though 
it 
may 
not 
necessarily 
have 
a 
simple 
structure 
and 
be 
simply 
a 
`fully 
connected’ 
cascade 
style 
graph. 
In 
this 
sense 
the 
DAG 
cannot 
graphically 
represent 
the 
independence/dependence 
relations 
true 
in 
the 
distribution. 


Denition 
34 
(Independence 
Maps). 
A 
graph 
is 
an 
independence 
map 
(I-map) 
of 
a 
given 
distribution 
P 
if 
every 
conditional 
independence 
statement 
that 
one 
can 
derive 
from 
the 
graph 
G 
is 
true 
in 
the 
distribution 


P 
. 
That 
is 
X 
.
??YjZG 
. 
X 
.
??YjZP 
(4.4.1) 


for 
all 
disjoint 
sets 
X,Y,Z. 


Similarly, 
a 
graph 
is 
a 
dependence 
map 
(D-map) 
of 
a 
given 
distribution 
P 
if 
every 
conditional 
independence 
statement 
that 
one 
can 
derive 
from 
P 
is 
true 
in 
the 
graph 
G. 
That 
is 


X 
.
??YjZG 
. 
X 
.
??YjZP 
(4.4.2) 


for 
all 
disjoint 
sets 
X,Y,Z. 


A 
graph 
G 
which 
is 
both 
an 
I-map 
and 
a 
D-map 
for 
P 
is 
called 
a 
perfect 
map 
and 


X 
.
??YjZG 
. 
X 
.
??YjZP 
(4.4.3) 


for 
all 
disjoint 
sets 
X,Y,Z. 
In 
this 
case, 
the 
set 
of 
all 
conditional 
independence 
and 
dependence 
statements 
expressible 
in 
the 
graph 
G 
are 
consistent 
with 
P 
and 
vice 
versa. 


Due 
to 
Inverse 
Modus 
Ponens, 
example(5), 
a 
dependence 
map 
is 
equivalent 
to 


X>>YjZG 
. 
X>>YjZP 
(4.4.4) 


although 
this 
is 
less 
useful 
since 
standard 
graphical 
model 
representations 
cannot 
express 
dependence. 


Note 
that 
the 
above 
denitions 
are 
not 
dependent 
on 
the 
graph 
being 
directed 
or 
undirected. 
Indeed, 
some 
distributions 
may 
have 
a 
perfect 
directed 
map, 
but 
no 
perfect 
undirected 
map. 
For 
example 


p(x, 
y, 
z)= 
p(zjx, 
y)p(x)p(y) 
(4.4.5) 


DRAFT 
March 
9, 
2010 



Factor 
Graphs 


a
b
c
d
a
b
c
d
Figure 
4.8: 
(a): 
An 
undirected 
model 
for 
which 
we 
wish 
to 
nd 
a 
directed 
equivalent. 
(b): 
Every 
DAG 
with 
the 
same 
structure 
as 
the 
undirected 
model 
must 
have 
a 
situation 
where 
two 
arrows 
will 
point 
to 
a 
node, 
such 
as 
node 
d. 
Summing 
over 
the 
states 
of 
variable 
d 
will 
leave 
a 
DAG 
on 
the 
variables 
a, 
b, 
c 
with 
no 
link 
between 
a 
and 
c. 
This 
cannot 
represent 
the 
undirected 
model 


(a) 
(b) 
since 
when 
one 
marginals 
over 
d 
in 
the 
undirected 
this 
adds 
a 
link 
between 
a 
and 
c. 
has 
a 
directed 
perfect 
map 
x 
!6z 
 6y 
(assuming 
that 
p(zjx, 
y)6(x)y

= 
x(y)), 
but 
no 
perfect 
undirected 
map. 


Example 
18. 
Consider 
the 
distribution 
dened 
on 
variables 
t1;t2;y1;y2[232]: 


X

p(t1;t2;y1;y2)= 
p(t1)p(t2)p(y1jt1;h)p(y2jt2;h)p(h) 
(4.4.6) 


h 


The 
BN 


p(y2jy1;t2)p(y1jt1)p(t1)p(t2) 
(4.4.7) 


is 
an 
I-MAP 
for 
distribution 
(4.4.6) 
since 
every 
independence 
statement 
in 
the 
BN 
is 
true 
for 
the 
corresponding 
graph. 
However, 
it 
is 
not 
a 
D-MAP 
since 
t1l
>>t2jy2 
cannot 
be 
inferred 
from 
the 
BN. 
Similarly 
no 
undirected 
graph 
can 
represent 
all 
independence 
statements 
true 
in 
(4.4.6). 
In 
this 
case 
no 
perfect 
MAP 
(a 
BN 
or 
a 
MN) 
can 
represent 
(4.4.6). 


4.5 
Factor 
Graphs 
Factor 
Graphs 
(FGs) 
are 
mainly 
used 
as 
part 
of 
inference 
algorithms2 
. 


Denition 
35 
(Factor 
Graph). 
Given 
a 
function 


Y..

f(x1;:::;xn)= iX6i(4.5.1) 
i 


The 
FG 
has 
a 
node 
(represented 
by 
a 
square) 
for 
each 
factor 
 i, 
and 
a 
variable 
node 
(represented 
by 
a 
circle) 
for 
each 
variable 
xj. 
For 
each 
xj 
2X6i 
an 
undirected 
link 
is 
made 
between 
factor 
 i 
and 
variable 
xj. 


..

For 
a 
factor 
 iX6iwhich 
is 
a 
conditional 
distribution 
p(xijpa 
(xi)), 
we 
may 
use 
directed 
links 
from 
the 
parents 
to 
the 
factor 
node, 
and 
a 
directed 
link 
from 
the 
factor 
node 
to 
the 
child. 
This 
has 
the 
same 
structure 
as 
an 
(undirected) 
FG, 
but 
preserves 
the 
information 
that 
the 
factors 
are 
distributions. 


Factor 
Graphs 
are 
useful 
since 
they 
can 
preserve 
more 
information 
about 
the 
form 
of 
the 
distribution 
than 
either 
a 
Belief 
Network 
or 
a 
Markov 
Network 
(or 
Chain 
Graph) 
can 
do 
alone. 
Consider 
the 
distribution 


p(a, 
b, 
c)= 
(a, 
b)(a, 
c)(b, 
c) 
(4.5.2) 


2Formally 
a 
FG 
is 
an 
alternative 
graphical 
depiction 
of 
a 
hypergraph[81] 
in 
which 
the 
vertices 
represent 
variables, 
and 
a 
hyperedge 
a 
factor 
as 
a 
function 
of 
the 
variables 
associated 
with 
the 
hyperedge. 
A 
FG 
is 
therefore 
a 
hypergraph 
with 
the 
additional 
interpretation 
that 
the 
graph 
represents 
a 
function 
dened 
as 
products 
over 
the 
associated 
hyperedges. 
Many 
thanks 
to 
Robert 
Cowell 
for 
this 
observation. 


DRAFT 
March 
9, 
2010 



Exercises 


a
bc
abcabcabcabcabcd
(a) 
(b) 
(c) 
(d) 
(e) 
(f) 
Figure 
4.9: 
(a): 
(a, 
b, 
c). 
(b): 
(a, 
b)(b, 
c)(c, 
a). 
(c): 
(a, 
b, 
c). 
Both 
(a) 
and 
(b) 
have 
the 
same 
undirected 
graphical 
model, 
(c). 
(e): 
Directed 
FG 
of 
the 
BN 
in 
(d). 
(a) 
is 
an 
undirected 
FG 
of 
(d). 
The 
advantage 
of 
(e) 
over 
(a) 
is 
that 
information 
regarding 
the 
marginal 
independence 
of 
variables 
b 
and 
c 
is 
clear 
from 
graph 
(e), 
whereas 
one 
could 
only 
ascertain 
this 
by 
examination 
of 
the 
numerical 
entries 
of 
the 
factors 
in 
graph 
(a). 
(f): 
A 
partially 
directed 
FG 
of 
p(ajb, 
c)(d, 
c)(b, 
d). 
No 
directed, 
undirected 
or 
chain 
graph 
can 
represent 
both 
the 
conditional 
and 
marginal 
independence 
statements 
expressed 
by 
this 
graph 
and 
also 
the 
factored 
structure 
of 
the 
undirected 
terms. 


The 
MN 
representation 
is 
given 
in 
g(4.9c). 
However, 
g(4.9c) 
could 
equally 
represent 
some 
unfactored 
clique 
potential 
(a, 
b, 
c). 
In 
this 
sense, 
the 
FG 
representation 
in 
g(4.9b) 
more 
precisely 
conveys 
the 
form 
of 
distribution 
equation 
(4.5.2). 
An 
unfactored 
clique 
potential 
(a, 
b, 
c) 
is 
represented 
by 
the 
FG 
g(4.9a). 
Hence 
dierent 
FGs 
can 
have 
the 
same 
MN 
since 
information 
regarding 
the 
structure 
of 
the 
clique 
potential 
is 
lost 
in 
the 
MN. 


4.5.1 
Conditional 
independence 
in 
factor 
graphs 
A 
rule 
which 
works 
with 
both 
directed 
and 
undirected 
(and 
partially 
directed) 
FGs 
is 
as 
follows[96]. 
To 
determine 
whether 
two 
variables 
are 
independent 
given 
a 
set 
of 
conditioned 
variables, 
consider 
all 
paths 
connecting 
the 
two 
variables. 
If 
all 
paths 
are 
blocked, 
the 
variables 
are 
conditionally 
independent. 


A 
path 
is 
blocked 
if 
any 
one 
or 
more 
of 
the 
following 
conditions 
are 
satised: 


• 
One 
of 
the 
variables 
in 
the 
path 
is 
in 
the 
conditioning 
set. 
• 
One 
of 
the 
variables 
or 
factors 
in 
the 
path 
has 
two 
incoming 
edges 
that 
are 
part 
of 
the 
path, 
and 
neither 
the 
variable 
or 
factor 
nor 
any 
of 
its 
descendants 
are 
in 
the 
conditioning 
set. 
4.6 
Notes 
A 
detailed 
discussion 
of 
the 
axiomatic 
and 
logical 
basis 
of 
conditional 
independence 
is 
given 
in 
[45] 
and 
[264]. 


4.7 
Code 
condindep.m: 
Conditional 
Independence 
test 
p(X, 
Y 
jZ)= 
p(XjZ)p(Y 
jZ)? 


4.8 
Exercises 
Exercise 
37. 
1. 
Consider 
the 
pairwise 
Markov 
Network, 


p(x)= 
(x1;x2)(x2;x3)(x3;x4)(x4;x1) 
(4.8.1) 


Express 
in 
terms 
of 
f 
the 
following: 


p(x1jx2;x4);p(x2jx1;x3);p(x3jx2;x4);p(x4jx1;x3) 
(4.8.2) 


DRAFT 
March 
9, 
2010 
59 



Exercises 


2. 
For 
a 
set 
of 
local 
distributions 
dened 
as 
p1(x1jx2;x4);p2(x2jx1;x3);p3(x3jx2;x4);p4(x4jx1;x3) 
(4.8.3) 
is 
it 
always 
possible 
to 
nd 
a 
joint 
distribution 
p(x1;x2;x3;x4) 
consistent 
with 
these 
local 
conditional 
distributions? 
Exercise 
38. 
Consider 
the 
Markov 
network 


p(a, 
b, 
c)= 
ab(a, 
b)bc(b, 
c) 
(4.8.4) 
Nominally, 
by 
summing 
over 
b, 
the 
variables 
a 
and 
c 
are 
dependent. 
For 
binary 
b, 
explain 
a 
situation 
in 
which 
this 
is 
not 
the 
case, 
so 
that 
marginally, 
a 
and 
c 
are 
independent. 
Exercise 
39. 
Show 
that 
for 
the 
Boltzmann 
machine 


1 


xTWx+xTb 


p(x)= 
e 
(4.8.5)

Z(W, 
b)
one 
may 
assume, 
without 
loss 
of 
generality, 
W 
= 
WT 
. 
Exercise 
40. 


The 
restricted 
Boltzmann 
machine 
(or 
Harmonium[253]) 
is 
a 
specially 
constrained 


Boltzmann 
machine 
on 
a 
bipartite 
graph, 
consisting 
of 
a 
layer 
of 
visible 
variables 
v 
=(v1;:::;vV 
) 
and 
hidden 
variables 
h 
=(h1;:::;hH 
): 


1 
T

vTWh+av+bTh 


p(v, 
h)= 
e 
(4.8.6)

Z(W, 
a, 
b)

h1h2
v1v2v3
All 
variables 
are 
binary 
taking 
states 
0, 
1. 


1. 
Show 
that 
the 
distribution 
of 
hidden 
units 
conditional 
on 
the 
visible 
units 
factorises 
as 
01XYX

p(hjv)=p(hijv), 
with 
p(hijv)= 
s 
@bi 
+WjivjAX(4.8.7) 
ij 


where 
(x)= 
ex=(1 
+ 
ex). 


2. 
By 
symmetry 
arguments, 
write 
down 
the 
form 
of 
the 
conditional 
p(vjh). 
3. 
Is 
p(h) 
factorised? 
4. 
Can 
the 
partition 
function 
Z(W, 
a, 
b) 
be 
computed 
eciently 
for 
the 
RBM? 
Exercise 
41. 
Consider 
99

Y

p(x)= 
(x1;x100) 
(xi;xi+1) 
(4.8.8) 
i=1 


Is 
it 
possible 
to 
compute 
argmax 
p(x) 
eciently? 


x1;:::;x100 


Exercise 
42. 
You 
are 
given 
that 


x.
??yj(z, 
u) 
;u.
??zjØ 
(4.8.9) 


Derive 
the 
most 
general 
form 
of 
probability 
distribution 
p(x, 
y, 
z, 
u) 
consistent 
with 
these 
statements. 
Does 
this 
distribution 
have 
a 
simple 
graphical 
model? 


DRAFT 
March 
9, 
2010 



Exercises 


Exercise 
43. 
The 
undirected 
graph 
represents 
a 
Markov 
Network 
with 
nodes 
x1;x2;x3;x4;x5, 
counti
ng 
clockwise 
around 
the 
pentagon 
with 
potentials 
(xi;x1+mod(i;5)). 
Show 
that 
the 
joint 
distribution 
can 
be 
written 
as 


p(x1;x2;x5)p(x2;x4;x5)p(x2;x3;x4) 


p(x1;x2;x3;x4;x5) 
= 
(4.8.10) 


p(x2;x5)p(x2;x4) 


and 
express 
the 
marginal 
probability 
tables 
explicitly 
as 
functions 
of 
the 
potentials 
(xi;xj). 


Exercise 
44. 


Consider 
the 
Belief 
Network 
on 
the 
right. 


1. 
Write 
down 
a 
Markov 
Network 
of 
p(x1;x2;x3). 
2. 
Is 
your 
Markov 
Network 
a 
perfect 
map 
of 
p(x1;x2;x3)? 
h1h2
x1x2x3
Exercise 
45. 
Two 
research 
labs 
work 
independently 
on 
the 
relationship 
between 
discrete 
variables 
x 
and 


y. 
Lab 
A 
proudly 
announces 
that 
they 
have 
ascertained 
distribution 
pA(xjy) 
from 
data. 
Lab 
B 
proudly 
announces 
that 
they 
have 
ascertained 
pB(yjx) 
from 
data. 
1. 
Is 
it 
always 
possible 
to 
nd 
a 
joint 
distribution 
p(x, 
y) 
consistent 
with 
the 
results 
of 
both 
labs? 
PX

2. 
Is 
it 
possible 
to 
dene 
consistent 
marginals 
p(x) 
and 
p(y), 
in 
the 
sense 
that 
p(x)= 
pA(xjy)p(y)
y

PX

and 
p(y)= 
pB(yjx)p(x)? 
If 
so, 
explain 
how 
to 
nd 
such 
marginals. 
If 
not, 
explain 
why 
not. 


x 


Exercise 
46. 
Research 
lab 
A 
states 
its 
ndings 
about 
a 
set 
of 
variables 
x1;:::;xn 
as 
a 
list 
LA 
of 
conditional 
independence 
statements. 
Lab 
B 
similarly 
provides 
a 
list 
of 
conditional 
independence 
statements 
LB. 


1. 
Is 
it 
possible 
to 
nd 
a 
distribution 
which 
is 
consistent 
with 
LA 
and 
LB? 
2. 
If 
the 
lists 
also 
contain 
dependence 
statements, 
how 
could 
one 
attempt 
to 
nd 
a 
distribution 
that 
is 
consistent 
with 
both 
lists? 
Exercise 
47. 


Consider 
the 
distribution 


p(x, 
y, 
w, 
z)= 
p(zjw)p(wjx, 
y)p(x)p(y) 
(4.8.11) 


1. 
Write 
p(xjz) 
using 
a 
formula 
involving 
(all 
or 
some 
of) 
p(zjw), 
p(wjx, 
y), 
p(x), 
p(y). 
2. 
Write 
p(yjz) 
using 
a 
formula 
involving 
(all 
or 
some 
of) 
p(zjw), 
p(wjx, 
y), 
p(x), 
p(y). 
3. 
Using 
the 
above 
results, 
derive 
an 
explicit 
condition 
for 
x 
. 
??y| 
z 
and 
explain 
if 
this 
is 
satised 
for 
this 
distribution. 
Exercise 
48. 
Consider 
the 
distribution 


p(t1;t2;y1;y2;h)= 
p(y1jt1;h)p(y2jt2;h)p(t1)p(t2)p(h) 
(4.8.12) 


1. 
Draw 
a 
Belief 
Network 
for 
this 
distribution. 
2. 
Can 
the 
distribution 
X

p(t1;t2;y1;y2)=p(y1jt1;h)p(y2jt2;h)p(t1)p(t2)p(h) 
(4.8.13) 


h 


be 
written 
as 
a 
(`non-complete') 
Belief 
Network? 


3. 
Show 
that 
for 
p(t1;t2;y1;y2) 
as 
dened 
above 
t1 
.
??y2j;. 
DRAFT 
March 
9, 
2010 



Exercises 


Exercise 
49. 
Consider 
the 
distribution 


p(a, 
b, 
c, 
d)= 
ab(a, 
b)bc(b, 
c)cd(c, 
d)da(d, 
a) 
(4.8.14) 


where 
the 
f 
are 
potentials. 


1. 
Draw 
a 
Markov 
Network 
for 
this 
distribution. 
2. 
Explain 
if 
the 
distribution 
can 
be 
represented 
as 
a 
(`non-complete') 
Belief 
Network. 
3. 
Derive 
explicitly 
if 
a.
??cj;. 
Exercise 
50. 
Show 
how 
for 
any 
singly-connected 
Markov 
network, 
one 
may 
construct 
a 
Markov 
equivalent 
Belief 
Network. 
Exercise 
51. 
Consider 
a 
pairwise 
binary 
Markov 
network 
dened 
on 
variables 
si 
2f0, 
1g, 
i 
=1;:::;N,


Q

with 
p(s)=ij2E 
ij(si;sj), 
where 
E 
is 
a 
given 
edge 
set 
and 
the 
potentials 
ij 
are 
arbitrary. 
Explain 
how 
to 
translate 
such 
a 
Markov 
network 
into 
a 
Boltzmann 
machine. 


DRAFT 
March 
9, 
2010 



CHAPTER 
5 


Ecient 
Inference 
in 
Trees 


5.1 
Marginal 
Inference 
Given 
a 
distribution 
p(x1;:::;xn), 
inference 
is 
the 
process 
of 
computing 
functions 
of 
the 
distribution. 
For 
example, 
computing 
a 
marginal 
conditioned 
on 
a 
subset 
of 
variables 
being 
in 
a 
particular 
state 
would 
be 
an 
inference 
task. 
Similarly, 
computing 
the 
mean 
of 
a 
variable 
can 
be 
considered 
an 
inference 
task. 
The 
main 
focus 
of 
this 
chapter 
is 
on 
ecient 
inference 
algorithms 
for 
marginal 
inference 
in 
singly-connected 
structures. 
An 
ecient 
algorithm 
for 
multiply-connected 
graphs 
will 
be 
considered 
in 
chapter(6). 
Marginal 
inference 
is 
concerned 
with 
the 
computation 
of 
the 
distribution 
of 
a 
subset 
of 
variables, 
possibly 
conditioned 
on 
another 
subset. 
For 
example, 
given 
a 
joint 
distribution 
p(x1;x2;x3;x4;x5), 
a 
marginal 
inference 
given 
evidence 
calculation 
is 


X

p(x5jx1 
= 
tr) 
/p(x1 
= 
tr;x2;x3;x4;x5) 
(5.1.1) 


x2;x3;x4 


Marginal 
inference 
for 
discrete 
models 
involves 
summation 
and 
will 
be 
the 
focus 
of 
our 
development. 
In 
principle 
the 
algorithms 
carry 
over 
to 
continuous 
variable 
models 
although 
the 
lack 
of 
closure 
of 
most 
continuous 
distributions 
under 
marginalisation 
(the 
Gaussian 
being 
a 
notable 
exception) 
can 
make 
the 
direct 
transference 
of 
these 
algorithms 
to 
the 
continuous 
domain 
problematic. 


5.1.1 
Variable 
elimination 
in 
a 
Markov 
chain 
and 
message 
passing 
A 
key 
concept 
in 
ecient 
inference 
is 
message 
passing 
in 
which 
information 
from 
the 
graph 
is 
summarised 
by 
local 
edge 
information. 
To 
develop 
this 
idea, 
consider 
the 
four 
variable 
Markov 
chain 
(Markov 
chains 
are 
discussed 
in 
more 
depth 
in 
section(23.1)) 


p(a, 
b, 
c, 
d)= 
p(ajb)p(bjc)p(cjd)p(d) 
(5.1.2) 


as 
given 
in 
g(5.1), 
for 
which 
our 
task 
is 
to 
calculate 
the 
marginal 
p(a). 
For 
simplicity, 
we 
assume 
that 
each 
of 
the 
variables 
has 
domain 
f0, 
1g. 
Then 


XX

p(a 
= 
0) 
=p(a 
=0, 
b, 
c, 
d)=p(a 
=0jb)p(bjc)p(cjd)p(d) 
(5.1.3) 


b2f0;1g;c2f0;1g;d2f0;1} 
b2f0;1g;c2f0;1g;d2f0;1} 


a
bcd
Figure 
5.1: 
A 
Markov 
chain 
is 
of 
the 
form 
p(xT 
)QT 
..1 
p(xtjxt+1)

t=1 


for 
some 
assignment 
of 
the 
variables 
to 
labels 
xt. 
Variable 
Elimination 
can 
be 
carried 
out 
in 
time 
linear 
in 
the 
number 
of 
variables 
in 
the 
chain. 


63 



Marginal 
Inference 


We 
could 
carry 
out 
this 
computation 
by 
simply 
summing 
each 
of 
the 
probabilities 
for 
the 
2 
× 
2 
× 
2=8 
states 
of 
the 
variables 
b,c 
and 
d. 


A 
more 
ecient 
approach 
is 
to 
push 
the 
summation 
over 
d 
as 
far 
to 
the 
right 
as 
possible: 


XX

p(a 
= 
0) 
=p(a 
=0jb)p(bjc)p(cjd)p(d) 
(5.1.4) 


b2f0;1g;c2f0;1} 
d2f0;1g

|{z


d(c) 


where 
d 
(c) 
is 
a 
(two 
state) 
potential. 
Similarly, 
we 
can 
distribute 
the 
summation 
over 
c 
as 
far 
to 
the 
right 
as 
possible: 


XX

p(a 
= 
0) 
=p(a 
=0jb)p(bjc)d 
(c) 
(5.1.5) 


b2f0;1} 
c2f0;1g

|{zX


c(b) 


Then, 
nally, 


X

p(a 
= 
0) 
=p(a 
=0jb)c 
(b) 
(5.1.6) 


b2f0;1} 


By 
distributing 
the 
summations 
we 
have 
made 
2 
+ 
2 
+ 
2 
= 
6 
additions, 
compared 
to 
8 
from 
the 
naive 
approach. 
Whilst 
this 
saving 
may 
not 
appear 
much, 
the 
important 
point 
is 
that 
the 
number 
of 
computations 
for 
a 
chain 
of 
length 
T 
would 
scale 
linearly 
with 
T 
, 
as 
opposed 
to 
exponentially 
for 
the 
naive 
approach. 


This 
procedure 
is 
naturally 
enough 
called 
variable 
elimination, 
since 
each 
time 
we 
sum 
over 
the 
states 
of 
a 
variable, 
we 
eliminate 
it 
from 
the 
distribution. 
We 
can 
always 
perform 
variable 
elimination 
in 
a 
chain 
eciently 
since 
there 
is 
a 
natural 
way 
to 
distribute 
the 
summations, 
working 
inwards 
from 
the 
edges. 
Note 
that 
in 
the 
above 
case, 
the 
potentials 
are 
in 
fact 
always 
distributions 
– 
we 
are 
just 
recursively 
computing 
the 
marginal 
distribution 
of 
the 
right 
leaf 
of 
the 
chain. 


One 
can 
view 
the 
elimination 
of 
a 
variable 
as 
passing 
a 
message 
(information) 
to 
a 
neighbouring 
vertex 
on 
the 
graph. 
We 
can 
calculate 
a 
univariate-marginal 
of 
any 
singly-connected 
graph 
by 
starting 
at 
a 
leaf 
of 
the 
tree, 
eliminating 
the 
variable 
there, 
and 
then 
working 
inwards, 
nibbling 
off 
each 
time 
a 
leaf 
of 
the 
remaining 
tree. 
Provided 
we 
perform 
elimination 
from 
the 
leaves 
inwards, 
then 
the 
structure 
of 
the 
remaining 
graph 
is 
simply 
a 
subtree 
of 
the 
original 
tree, 
albeit 
with 
the 
conditional 
probability 
table 
entries 
modied 
to 
potentials 
which 
update 
under 
recursion. 
This 
is 
guaranteed 
to 
enable 
us 
to 
calculate 
any 
marginal 
p(xi) 
using 
a 
number 
of 
summations 
which 
scales 
linearly 
with 
the 
number 
of 
variables 
in 
the 
graph. 


Finding 
conditional 
marginals 
for 
a 
chain 


Consider 
the 
following 
inference 
problem, 
g(5.1) 
: 
Given 


p(a, 
b, 
c, 
d) 
= 
p(ajb)p(bjc)p(cjd)p(d), 
(5.1.7) 
nd 
p(dja). 
This 
can 
be 
computed 
using 
XXXXp(dja) 
/
b;c 
p(a, 
b, 
c, 
d) 
/
b;c 
p(ajb)p(bjc)p(cjd)p(d) 
/
cb 
p(ajb)p(bjc) 
|{zX}
p(cjd)p(d) 
= 
c 
(d) 
(5.1.8) 
b(c) 


The 
missing 
proportionality 
constant 
is 
found 
by 
repeating 
the 
computation 
for 
all 
states 
of 
variable 
d. 
Since 
we 
know 
that 
p(dja)= 
kc 
(d), 
where 
c 
(d) 
is 
the 
unnormalised 
result 
of 
the 
summation, 
we 
can 
use 


PPX

the 
fact 
that 
d 
p(dja) 
= 
1 
to 
infer 
that 
k 
=1=c 
(d).

d 


DRAFT 
March 
9, 
2010 



Marginal 
Inference 


In 
this 
example, 
the 
potential 
b 
(c) 
is 
not 
a 
distribution 
in 
c, 
nor 
is 
c 
(d). 
In 
general, 
one 
may 
view 
variable 
elimination 
as 
the 
passing 
of 
messages 
in 
the 
form 
of 
potentials 
from 
nodes 
to 
their 
neighbours. 
For 
Belief 
Networks, 
variable 
elimination 
passes 
messages 
that 
are 
distributions 
when 
following 
the 
direction 
of 
the 
edge, 
and 
non-normalised 
potentials 
when 
passing 
messages 
against 
the 
direction 
of 
the 
edge. 


Remark 
5. 
Variable 
elimination 
in 
trees 
as 
matrix 
multiplication 


Variable 
Elimination 
is 
related 
to 
the 
associativity 
of 
matrix 
multiplication. 
For 
equation 
(5.1.2) 
above, 
we 
can 
dene 
matrices 


[Mab]i;j 
= 
p(a 
= 
ijb 
= 
j), 
[Mbc]i;j 
= 
p(b 
= 
ijc 
= 
j), 


[Mcd]i;j 
= 
p(c 
= 
ijd 
= 
j), 
[Md]i 
= 
p(d 
= 
i), 
[Ma]i 
= 
p(a 
= 
i) 
(5.1.9) 
Then 
the 
marginal 
Ma 
can 
be 
written 


Ma 
= 
MabMbcMcdMd 
= 
Mab(Mbc(McdMd)) 
(5.1.10) 
since 
matrix 
multiplication 
is 
associative. 
This 
matrix 
formulation 
of 
calculating 
marginals 
is 
called 
the 
transfer 
matrix 
method, 
and 
is 
particularly 
popular 
in 
the 
physics 
literature[26]. 


Example 
19 
(Where 
will 
the 
y 
be?). 


You 
live 
in 
a 
house 
with 
three 
rooms, 
labelled 
1, 
2, 
3. 
There 
is 
a 
door 
between 
rooms 
1 
and 
2 
and 
another 
between 
rooms 
2 
and 
3. 
One 
cannot 
directly 
pass 
between 
rooms 
1 
and 
3 
in 
one 
time-step. 
An 
annoying 
y 
is 
buzzing 
from 
one 
room 
to 
another 
and 
there 
is 
some 
smelly 
cheese 
in 
room 
1 
which 
seems 
to 
attract 
the 
y 
more. 
Using 
xt 
for 
which 
room 
the 
y 
is 
in 
at 
time 
t, 
with 
dom(xt)= 
f1, 
2, 
3g, 
the 
movement 
of 
the 
y 
can 
be 
described 
by 
a 
transition 


p(xt+1 
= 
ijxt 
= 
j)= 
Mij 
(5.1.11) 


where 
M 
is 
a 
transition 
matrix 


0X0:7 
0:5 
0 
1XM 
= 
@X0:3 
0:3 
0:5 
AX(5.1.12) 
0 
0:2 
0:5 


The 
transition 
matrix 
is 
stochastic 
in 
the 
sense 
that, 
as 
required 
of 
a 
conditional 
probability 
distribution

P3 


= 
1. 
Given 
that 
the 
y 
is 
in 
room 
1 
at 
time 
1, 
what 
is 
the 
probability 
of 
room 
occupancy 
at 


i=1 
Mij 
time 
t 
= 
5? 
Assume 
a 
Markov 
chain 
which 
is 
dened 
by 
the 
joint 
distribution 


T 
..1

YX

p(x1;:::;xT 
)= 
p(x1) 
p(xt+1jxt) 
(5.1.13) 
t=1 


We 
are 
asked 
to 
compute 
p(x5jx1 
= 
1) 
which 
is 
given 
by

X

p(x5jx4)p(x4jx3)p(x3jx2)p(x2jx1 
= 
1) 
(5.1.14) 


x4;x3;x2 


Since 
the 
graph 
of 
the 
distribution 
is 
a 
Markov 
chain, 
we 
can 
easily 
distribute 
the 
summation 
over 
the 
terms. 
This 
is 
most 
easily 
done 
using 
the 
transfer 
matrix 
method, 
giving 


p(x5 
= 
ijx1 
= 
1) 
= 
[M4v]i 
(5.1.15) 


DRAFT 
March 
9, 
2010 
65 



Marginal 
Inference 


where 
v 
is 
a 
vector 
with 
components 
(1, 
0, 
0)T, 
reecting 
the 
evidence 
that 
at 
time 
1 
the 
y 
is 
in 
room 
1. 
Computing 
this 
we 
have 
(to 
4 
decimal 
places 
of 
accuracy) 


01X

0:5746 


M4v 
= 
@X0:3180 
AX(5.1.16) 


0:1074 


Similarly, 
after 
5 
time-steps, 
the 
occupancy 
probabilities 
are 
(0:5612, 
0:3215, 
0:1173). 
The 
room 
occupancy 
probability 
is 
converging 
to 
a 
particular 
distribution 
– 
the 
stationary 
distribution 
of 
the 
Markov 
chain. 
One 
might 
ask 
where 
the 
y 
is 
after 
an 
innite 
number 
of 
time-steps. 
That 
is, 
we 
are 
interested 
in 
the 
large 
t 
behaviour 
of 


X

p(xt+1)=p(xt+1jxt)p(xt) 
(5.1.17) 


xt 


At 
convergence 
p(xt+1)= 
p(xt). 
Writing 
p 
for 
the 
vector 
describing 
the 
stationary 
distribution, 
this 
means 


p 
= 
Mp 
(5.1.18) 


In 
other 
words, 
p 
is 
the 
eigenvector 
of 
M 
with 
eigenvalue 
1[122]. 
Computing 
this 
numerically, 
the 
stationary 
distribution 
is 
(0:5435, 
0:3261, 
0:1304). 
Note 
that 
software 
packages 
usually 
return 
eigenvectors 
with 
eTe 
= 
1 
– 
the 
unit 
eigenvector 
therefore 
will 
usually 
require 
normalisation 
to 
make 
this 
a 
probability. 


5.1.2 
The 
sum-product 
algorithm 
on 
factor 
graphs 
Both 
Markov 
and 
belief 
networks 
can 
be 
represented 
using 
factor 
graphs. 
For 
this 
reason 
it 
is 
convenient 
to 
derive 
a 
marginal 
inference 
algorithm 
for 
the 
FG 
since 
this 
then 
applies 
to 
both 
Markov 
and 
belief 
networks. 
This 
is 
termed 
the 
sum-product 
algorithm 
since 
to 
compute 
marginals 
we 
need 
to 
distribute 
the 
sum 
over 
variable 
states 
over 
the 
product 
of 
factors. 
In 
older 
texts, 
this 
is 
referred 
to 
as 
belief 
propagation. 


Non-branching 
graphs 
: 
variable 
to 
variable 
messages 


Consider 
the 
distribution 


p(a, 
b, 
c, 
d)= 
f1 
(a, 
b) 
f2 
(b, 
c) 
f3 
(c, 
d) 
f4 
(d) 
(5.1.19) 


which 
has 
the 
factor 
graph 
represented 
in 
g(5.2) 
with 
factors 
as 
dened 
by 
the 
f 
above. 
To 
compute 
the 
marginal 
p(a, 
b, 
c), 
since 
the 
variable 
d 
only 
occurs 
locally, 
we 
use 


XXX

p(a, 
b, 
c)=p(a, 
b, 
c, 
d)=f1 
(a, 
b) 
f2 
(b, 
c) 
f3 
(c, 
d) 
f4 
(d)= 
f1 
(a, 
b) 
f2 
(b, 
c)f3 
(c, 
d) 
f4 
(d) 
(5.1.20) 


dd 
d

|{zX}X

d!c(c) 


Similarly, 


XX

p(a, 
b)=p(a, 
b, 
c)= 
f1 
(a, 
b)f2 
(b, 
c) 
d!c 
(c) 
(5.1.21) 
cc

|{zX}X

c!b(b) 


Hence 


X

c!b 
(b)=f2 
(b, 
c) 
d!c 
(c) 
(5.1.22) 


It 
is 
clear 
how 
one 
can 
recurse 
this 
denition 
of 
messages 
so 
that 
for 
a 
chain 
of 
length 
n 
variables 
the 
marginal 
of 
the 
rst 
node 
can 
be 
computed 
in 
time 
linear 
in 
n. 
The 
term 
c!b 
(b) 
can 
be 
interpreted 
as 


DRAFT 
March 
9, 
2010 



Marginal 
Inference 


Figure 
5.2: 
For 
singly-connected 
structures 
with


a
f1 


b
f2 


c
f3 


d
f4 


out 
branches, 
simple 
messages 
from 
one 
variable 
to 
its 
neighbour 
may 
be 
dened 
to 
form 
an 
ecient 
marginal 
inference 
scheme. 


carrying 
marginal 
information 
from 
the 
graph 
beyond 
c. 


For 
any 
singly-connected 
structure 
the 
factors 
at 
the 
edge 
of 
the 
graph 
can 
be 
replaced 
with 
messages 
that 
reect 
marginal 
information 
from 
the 
graph 
beyond 
that 
factor. 
For 
simple 
linear 
structures 
with 
no 
branching, 
messages 
from 
variables 
to 
variables 
are 
sucient. 
However, 
as 
we 
will 
see 
below, 
it 
is 
useful 
in 
more 
general 
structures 
with 
branching 
to 
consider 
two 
types 
of 
messages, 
namely 
those 
from 
variables 
to 
factors 
and 
vice 
versa. 


General 
singly-connected 
factor 
graphs 


The 
slightly 
more 
complex 
example, 


p(ajb)p(bjc, 
d)p(c)p(d)p(ejd) 
(5.1.23) 


has 
the 
factor 
graph, 
g(5.3) 


f1 
(a, 
b) 
f2 
(b, 
c, 
d) 
f3 
(c) 
f4 
(d, 
e) 
f5 
(d) 
(5.1.24) 


If 
the 
marginal 
p(a, 
b) 
is 
to 
be 
represented 
by 
the 
amputated 
graph 
with 
messages 
on 
the 
edges, 
then 


XX

p(a, 
b)= 
f1 
(a, 
b)f2 
(b, 
c, 
d) 
f3 
(c) 
f5 
(d)f4 
(d, 
e) 
(5.1.25) 


c;d 
e 


|{z}X

f2!b(b) 


In 
this 
case 
it 
is 
natural 
to 
consider 
messages 
from 
factors 
to 
variables. 
Similarly, 
we 
can 
break 
the 
message 
from 
the 
factor 
f2 
into 
messages 
arriving 
from 
the 
two 
branches 
through 
c 
and 
d, 
namely 


XX

f2!b 
(b)=f2 
(b, 
c, 
d) 
f3 
(c) 
f5 
(d)f4 
(d, 
e) 
(5.1.26)

|{z}

c;d 
e 


c!f2 
(c) 
|{z}Xd!f2 
(d) 


Similarly, 
we 
can 
interpret 


X

d!f2 
(d)= 
f5 
(d) 
f4 
(d, 
e) 
(5.1.27)

|{z}X

e 


f5!d(d)|{z}X

f4!d(d) 


To 
complete 
the 
interpretation 
we 
identify 
c!f2 
(c) 
= 
f3!c 
(c). 
In 
a 
non-branching 
link, 
one 
can 
more 
simply 
use 
a 
variable 
to 
variable 
message. 


c
d
f3 


a
Figure 
5.3: 
For 
a 
branching 
singly-connected 
graph, 


f1 


b
f2 


e
it 
is 
useful 
to 
dene 
messages 
from 
both 
factors 
to 
variables, 
and 
variables 
to 
factors. 


f4 


f5 


DRAFT 
March 
9, 
2010 



Marginal 
Inference 


To 
compute 
the 
marginal 
p(a), 
we 
then 
have 


 


p(a)=f1 
(a, 
b) 
f2!b 
(b) 
(5.1.28) 


|b 
{z' 
f1!a(a) 


For 
consistency 
of 
interpretation, 
one 
also 
can 
view 
the 
above 
as 


X

f1!a 
(a)=f1 
(a, 
b) 
f2!b 
(b) 
(5.1.29)

|{z}

b 


b!f1 
(b) 


A 
convenience 
of 
this 
approach 
is 
that 
the 
messages 
can 
be 
reused 
to 
evaluate 
other 
marginal 
inferences. 
For 
example, 
it 
is 
clear 
that 
p(b) 
is 
given 
by 


X

p(b)=f1 
(a, 
b) 
f2!b 
(b) 
(5.1.30) 


a

|}

{z

f1!b(b) 


If 
we 
additionally 
desire 
p(c), 
we 
need 
to 
dene 
the 
message 
from 
f2 
to 
c, 


X

f2!c 
(c)=f2 
(b, 
c, 
d) 
b!f2 
(b) 
d!f2 
(d) 
(5.1.31) 


b;d 


where 
b!f2 
(b) 
= 
f1!b 
(b). 
This 
demonstrates 
the 
reuse 
of 
already 
computed 
message 
from 
d 
to 
f2 
to 
compute 
the 
marginal 
p(c). 


Denition 
36 
(Message 
schedule). 
A 
message 
schedule 
is 
a 
specied 
sequence 
of 
message 
updates. 
A 
valid 
schedule 
is 
that 
a 
message 
can 
be 
sent 
from 
a 
node 
only 
when 
that 
node 
has 
received 
all 
requisite 
messages 
from 
its 
neighbours. 
In 
general, 
there 
is 
more 
than 
one 
valid 
updating 
schedule. 


Sum-Product 
algorithm 


The 
sum-product 
algorithm 
is 
described 
below 
in 
which 
messages 
are 
updated 
as 
a 
function 
of 
incoming 
messages. 
One 
then 
proceeds 
by 
computing 
the 
messages 
in 
a 
schedule 
that 
allows 
the 
computation 
of 
a 
new 
message 
based 
on 
previously 
computed 
messages, 
until 
all 
messages 
from 
all 
factors 
to 
variables 
and 
vice-versa 
have 
been 
computed. 


Denition 
37 
(Sum-Product 
messages 
on 
Factor 
Graphs). 


Q..

1 
X 
f

Given 
a 
distribution 
dened 
as 
a 
product 
on 
subsets 
of 
the 
variables, 
p(X 
)= 
f, 
provided 
the 


Zf 


factor 
graph 
is 
singly-connected 
we 
can 
carry 
out 
summation 
over 
the 
variables 
eciently. 


Initialisation 
Messages 
from 
extremal 
(simplical) 
node 
factors 
are 
initialised 
to 
the 
factor. 
Messages 
from 
extremal 
(simplical) 
variable 
nodes 
are 
set 
to 
unity. 


f1 


Variable 
to 
Factor 
message 


x
!

x(

x
) 


x!f 
(x) 
f

Y

x!f 
(x)=g!x 
(x) 
f2 
(x)

f2!x 


)

(x

!

f3

g2fne(x)nf} 


f3 


DRAFT 
March 
9, 
2010 



Marginal 
Inference 


y1
y1!f(1)
Factor 
to 
Variable 
message 


XY

f!x 


(x)=f 
(X 
f 
)y!f 
(y) 
ff!x 
(x) 


x
y2X 
f 
nxy2fne(f)nx} 


y2
y2!f 
(y2) 


P

We 
writeto 
emphasise 
that 
we 
sum 
over 
all 
states 
in 


y2X 
f 
nx 


the 
set 
of 
variables 
X 
f 
nx. 


y3
f1!x(x)
f1

Marginal 


x
f3

Y

x f3 
(x)

p(x) 
/f!x 
(x) 


f2

f2ne(x) 


For 
marginal 
inference, 
the 
important 
information 
is 
the 
relative 
size 
of 
the 
message 
states 
so 
that 
we 
may 
renormalise 
messages 
as 
we 
wish. 
Since 
the 
marginal 
will 
be 
proportional 
to 
the 
incoming 
messages 
for 
that 
variable, 
the 
normalisation 
constant 
is 
trivially 
obtained 
using 
the 
fact 
that 
the 
marginal 
must 
sum 
to 
1. 
However, 
if 
we 
wish 
to 
also 
compute 
any 
normalisation 
constant 
using 
these 
messages, 
we 
cannot 
normalise 
the 
messages 
since 
this 
global 
information 
will 
then 
be 
lost. 
To 
resolve 
this 
one 
may 
work 
with 
log 
messages 
to 
avoid 
numerical 
under/overow 
problems. 


The 
sum-product 
algorithm 
is 
able 
to 
perform 
ecient 
marginal 
inference 
in 
both 
Belief 
and 
Markov 
Networks, 
since 
both 
are 
expressible 
as 
factor 
graphs. 
This 
is 
the 
reason 
for 
the 
preferred 
use 
of 
the 
Factor 
Graph 
since 
it 
requires 
only 
a 
single 
algorithm 
and 
is 
agnostic 
to 
whether 
or 
not 
the 
graph 
is 
a 
locally 
or 
globally 
normalised 
distribution. 


5.1.3 
Computing 
the 
marginal 
likelihood 
..

For 
a 
distribution 
dened 
as 
products 
over 
potentials 
fX 
f



Y

1 


p(x)= 
fX 
f(5.1.32)

Z

f 


the 
normalisation 
is 
given 
by 




XY

Z 
=fX 
f(5.1.33) 
Xf 


To 
compute 
this 
summation 
eciently 
we 
take 
the 
product 
of 
all 
incoming 
messages 
to 
an 
arbitrarily 
chosen 
variable 
x 
and 
then 
sum 
over 
the 
states 
of 
that 
variable: 


XY

Z 
=f!x 
(x) 
(5.1.34) 
f2ne(x) 


If 
the 
factor 
graph 
is 
derived 
from 
setting 
a 
subset 
of 
variables 
of 
a 
BN 
in 
evidential 
states 


p(X 
jV) 


p(X 
, 
V) 
= 
(5.1.35) 


p(V) 


then 
the 
summation 
over 
all 
non-evidential 
variables 
will 
yield 
the 
marginal 
on 
the 
visible 
(evidential) 
variables, 
p(V). 


For 
this 
method 
to 
work, 
the 
absolute 
(not 
relative) 
values 
of 
the 
messages 
are 
required, 
which 
prohibits 
renormalisation 
at 
each 
stage 
of 
the 
message 
passing 
procedure. 
However, 
without 
normalisation 
the 


DRAFT 
March 
9, 
2010 



Marginal 
Inference 


a
f1 


b
f1 


a
Figure 
5.4: 
(a) 
Factor 
graph 
with 
a 
loop. 
(b) 
Eliminating 


b
the 
variable 
d 
adds 
an 
edge 
between 
a 
and 
c, 
demonstrating 


f4 


f2 


f2 
f5 


c
that, 
in 
general, 
one 
cannot 
perform 
marginal 
inference 
in 


d
f3 


c
loopy 
graphs 
by 
simply 
passing 
messages 
along 
existing 
edges 
in 
the 
original 
graph. 


(a) 
(b) 
numerical 
value 
of 
messages 
can 
become 
very 
small, 
particularly 
for 
large 
graphs, 
and 
numerical 
precision 
issues 
can 
occur. 
A 
remedy 
in 
this 
situation 
is 
to 
work 
with 
log 
messages, 


. 
= 
log 
µ 
(5.1.36) 


For 
this, 
the 
variable 
to 
factor 
messages 


Y

x!f 
(x)=g!x 
(x) 
(5.1.37) 
g2fne(x)nf} 


become 
simply 


X

x!f 
(x)=g!x 
(x) 
(5.1.38) 
g2fne(x)nf} 


More 
care 
is 
required 
for 
the 
factors 
to 
variable 
messages, 
which 
are 
dened 
by 


XY

f!x 
(x)=f 
(X 
f 
)y!f 
(y) 
(5.1.39) 
y2X 
f 
nxy2fne(f)nx} 


Naively, 
one 
may 
write 


0. 
X 


y!f 
(y)

f!x 
(x) 
= 
log 
@f 
(X 
f 
)ey2fne(f)nx} 
AX(5.1.40) 
y2X 
f 
nx 


However, 
the 
exponentiation 
of 
the 
log 
messages 
will 
cause 
potential 
numerical 
precision 
problems. 
A 
solution 
to 
this 
numerical 
diculty 
is 
obtained 
by 
nding 
the 
largest 
value 
of 
the 
incoming 
log 
messages, 


. 
* 
y!f 
= 
max 
y!f 
(y) 
(5.1.41) 
y2fne(f)nx} 


Then 


01XXP

y!f 
(y)..* 


f!x 
(x)= 
y
!f 
+ 
log 
@f 
(X 
f 
)ey2fne(f)nx} 
y!f 
AX(5.1.42) 
y2X 
f 
nx 


P

y!f 
(y)..* 


By 
construction 
the 
terms 
ey2fne(f)nx} 
y!f 
will 
be 
= 
1. 
This 
ensures 
that 
the 
dominant 
numerical 
contributions 
to 
the 
summation 
are 
computed 
accurately. 


Log 
marginals 
are 
readily 
found 
using 


X

log 
p(x)=f!x 
(x) 
(5.1.43) 
f2fne(x)} 


DRAFT 
March 
9, 
2010 



Other 
Forms 
of 
Inference 


5.1.4 
The 
problem 
with 
loops 
Loops 
cause 
a 
problem 
with 
variable 
elimination 
(or 
message 
passing) 
techniques 
since 
once 
a 
variable 
is 
eliminated 
the 
structure 
of 
the 
`amputated’ 
graph 
in 
general 
changes. 
For 
example, 
consider 
the 
FG 


p(a, 
b, 
c, 
d)= 
f1 
(a, 
b) 
f2 
(b, 
c) 
f3 
(c, 
d) 
f4 
(a, 
d) 
(5.1.44) 
The 
marginal 
p(a, 
b, 
c) 
is 
given 
by 


X

p(a, 
b, 
c)= 
f1 
(a, 
b) 
f2 
(b, 
c)f3 
(c, 
d) 
f4 
(a, 
d) 
(5.1.45) 


|d 
{zXf
5(a;c) 


which 
adds 
a 
link 
ac 
in 
the 
amputated 
graph, 
see 
g(5.4). 
This 
means 
that 
one 
cannot 
account 
for 
information 
from 
variable 
d 
by 
simply 
updating 
potentials 
on 
links 
in 
the 
original 
graph 
– 
one 
needs 
to 
account 
for 
the 
fact 
that 
the 
structure 
of 
the 
graph 
changes. 
The 
Junction 
Tree 
algorithm, 
chapter(6) 
is 
a 
widely 
used 
technique 
to 
deal 
with 
this 
and 
essentially 
combines 
variables 
together 
in 
order 
to 
make 
a 
new 
singly-connected 
graph 
for 
which 
the 
graph 
structure 
remains 
singly-connected 
under 
variable 
elimination. 


5.2 
Other 
Forms 
of 
Inference 
5.2.1 
Max-Product 
A 
common 
interest 
is 
the 
most 
likely 
state 
of 
distribution. 
That 
is 


argmax 
p 
(x1;x2;:::;xn) 
(5.2.1) 


x1;x2;:::;xn 


To 
compute 
this 
eciently 
we 
exploit 
any 
factorisation 
structure 
of 
the 
distribution, 
analogous 
to 
the 
sum-product 
algorithm. 
That 
is, 
we 
aim 
to 
distribute 
the 
maximization 
so 
that 
only 
local 
computations 
are 
required. 


To 
develop 
the 
algorithm, 
consider 
a 
function 
which 
can 
be 
represented 
as 
an 
undirected 
chain, 


f(x1;x2;x3;x4)= 
(x1;x2)(x2;x3)(x3;x4) 
(5.2.2) 


for 
which 
we 
wish 
to 
nd 
the 
joint 
state 
x 




which 
maximises 
f. 
Firstly, 
we 
calculate 
the 
maximum 
value 


of 
f. 
Since 
potentials 
are 
non-negative, 
we 
may 
write 
max 
f(x) 
= 
max 
(x1;x2)(x2;x3)(x3;x4) 
= 
max 
(x1;x2)(x2;x3) 
max 
(x3;x4) 


x 
x1;x2;x3;x4 
x1;x2;x3 
x4

' 
{zX


(x3) 


= 
max 
(x1;x2) 
max 
(x2;x3)(x3) 
= 
max 
(x1;x2)(x2) 
= 
max 
max 
(x1;x2)(x2) 


x1;x2 
x3 
x1;x2 
x1 
x2

|X{z- 
|X{z


(x2) 
(x1) 


The 
nal 
equation 
corresponds 
to 
solving 
a 
single 
variable 
optimisation 
and 
determines 
both 
the 
optimal 


* 


1 
= 
argmax 


(x1). 
Given 
x 


* 


1

, 
the 
optimal 
x2 
is 
given 


value 
of 
the 
function 
f 
and 
also 
the 
optimal 
state 
x 


x1 




* 


1;x2)(x2), 
and 
similarly 
x 
2;x3)(x3), 
and 
so 
on. 
This 
procedure 
is 


x2 
x3 


called 
backtracking. 
Note 
that 
we 
could 
have 
equally 
started 
at 
the 
other 
end 
of 
the 
chain 
by 
dening 
messages 
. 
that 
pass 
information 
from 
xi 
to 
xi+1. 


The 
chain 
structure 
of 
the 
function 
ensures 
that 
the 
maximal 
value 
(and 
its 
state) 
can 
be 
computed 
in 
time 
which 
scales 
linearly 
with 
the 
number 
of 
factors 
in 
the 
function. 
There 
is 
no 
requirement 
here 
that 
the 
function 
f 
corresponds 
to 
a 
probability 
distribution 
(though 
the 
factors 
must 
be 
non-negative). 


DRAFT 
March 
9, 
2010 






by 
x 


(x 


(x


2 
= 
argmax 


3 
= 
argmax 



Other 
Forms 
of 
Inference 


Example 
20. 
Consider 
a 
distribution 
dened 
over 
binary 
variables: 


p(a, 
b, 
c) 
= 
p(ajb)p(bjc)p(c) 
(5.2.3) 
with 


p(a 
= 
trjb 
= 
tr)=0:3;p(a 
= 
trjb 
= 
fa)=0:2;p(b 
= 
trjc 
= 
tr)=0:75 


p(b 
= 
trjc 
= 
fa)=0:1;p(c 
= 
tr)=0:4 
What 
is 
the 
most 
likely 
joint 
conguration, 
argmax 
p(a, 
b, 
c)? 


a;b;c 


Naively, 
we 
could 
evaluate 
p(a, 
b, 
c) 
over 
all 
the 
8 
joint 
states 
of 
a, 
b, 
c 
and 
select 
that 
states 
with 
highest 
probability. 
A 
message 
passing 
approach 
is 
to 
dene 


(b) 
= 
max 
p(bjc)p(c) 
(5.2.4) 


c 


For 
the 
state 
b 
= 
tr, 


p(b 
= 
trjc 
= 
tr)p(c 
= 
tr)=0:75 
× 
0:4;p(b 
= 
trjc 
= 
fa)p(c 
= 
fa)=0:1 
× 
0:6 
(5.2.5) 
Hence, 
(b 
= 
tr)=0:75 
× 
0:4=0:3. 
Similarly, 
for 
b 
= 
fa, 


p(b 
= 
fajc 
= 
tr)p(c 
= 
tr)=0:25 
× 
0:4 
p(b 
= 
fajc 
= 
fa)p(c 
= 
fa)=0:9 
× 
0:6 
(5.2.6) 
Hence, 
(b 
= 
fa)=0:9 
× 
0:6=0:54. 
We 
now 
consider 


(a) 
= 
max 
p(ajb)(b) 
(5.2.7) 


b 


For 
a 
= 
tr, 
the 
state 
b 
= 
tr 
has 
value 


p(a 
= 
trjb 
= 
tr)(b 
= 
tr)=0:3 
× 
0:3=0:09 
(5.2.8) 
and 
state 
b 
= 
fa 
has 
value 


p(a 
= 
trjb 
= 
fa)(b 
= 
fa)=0:2 
× 
0:54 
= 
0:108 
(5.2.9) 
Hence 
(a 
= 
tr)=0:108. 
Similarly, 
for 
a 
= 
fa, 
the 
state 
b 
= 
tr 
has 
value 


p(a 
= 
fajb 
= 
tr)(b 
= 
tr)=0:7 
× 
0:3=0:21 
(5.2.10) 
and 
state 
b 
= 
fa 
has 
value 


p(a 
= 
fajb 
= 
fa)(b 
= 
fa)=0:8 
× 
0:54 
= 
0:432 
(5.2.11) 
giving 
(a 
= 
fa)=0:432. 
Now 
we 
can 
compute 
the 
optimal 
state 


* 


a 
= 
argmax 
(a)= 
fa 
(5.2.12) 


a 


Given 
this 
optimal 
state, 
we 
can 
backtrack, 
giving 


* 


b 
* 
= 
argmax 
p(a 
= 
fajb)(b)= 
fa;c 
= 
argmax 
p(b 
= 
fajc)p(c)= 
fa 
(5.2.13) 


bc 


Note 
that 
in 
the 
backtracking 
process, 
we 
already 
have 
all 
the 
information 
required 
from 
the 
computation 
of 
the 
messages 
. 


DRAFT 
March 
9, 
2010 



Other 
Forms 
of 
Inference 


Using 
a 
factor 
graph 


One 
can 
also 
use 
the 
factor 
graph 
to 
compute 
the 
joint 
most 
probable 
state. 
Provided 
that 
a 
full 
schedule 
of 
message 
passing 
has 
occurred, 
the 
product 
of 
messages 
into 
a 
variable 
equals 
the 
maximum 
value 
of 
the 
joint 
function 
with 
respect 
to 
all 
other 
variables. 
One 
can 
then 
simply 
read 
off 
the 
most 
probable 
state 
by 
maximising 
this 
local 
potential. 


One 
then 
proceeds 
in 
computing 
the 
messages 
in 
a 
schedule 
that 
allows 
the 
computation 
of 
a 
new 
message 
based 
on 
previously 
computed 
messages, 
until 
all 
messages 
from 
all 
factors 
to 
variables 
and 
vice-versa 
have 
been 
computed. 
The 
message 
updates 
are 
given 
below. 


Denition 
38 
(Max-Product 
messages 
on 
Factor 
Graphs). 


Q..

1 
X 
f

Given 
a 
distribution 
dened 
as 
a 
product 
on 
subsets 
of 
the 
variables, 
p(X 
)= 
f, 
provided 
the 


Zf 


factor 
graph 
is 
singly-connected 
we 
can 
carry 
out 
maximisation 
over 
the 
variables 
eciently. 


Initialisation 
Messages 
from 
extremal 
(simplical) 
node 
factors 
are 
initialised 
to 
the 
factor. 
Messages 
from 
extremal 
(simplical) 
variable 
nodes 
are 
set 
to 
unity. 


f1 


Variable 
to 
Factor 
message 


x
!

x(

x
) 


x!f 
(x) 
f

Y

x!f 
(x)=g!x 
(x) 
f2 
(x)

f2!x 


)

(x

!

f3

g2fne(x)nf} 


f3 


FactortoVariablemessage
f!x(x)=max
y2Xfnx
f(Xf)Yy2fne(f)nxg
y!f(y)
x
y1
y2
y3
ff!x(x)
y1!f(y1)
y2!f(y2)
y3!f(y3)
MaximalState
x=argmax
xYf!x(x)
f1
f2
f3x
f1!x(x)
f2!x(x)x f3(x)
f2ne(x) 


In 
earlier 
literature, 
this 
algorithm 
is 
called 
belief 
revision. 


5.2.2 
Finding 
the 
N 
most 
probable 
states 
It 
is 
often 
of 
interest 
to 
calculate 
not 
just 
the 
most 
likely 
joint 
state, 
but 
the 
N 
most 
probable 
states, 
particularly 
in 
cases 
where 
the 
optimal 
state 
is 
only 
slightly 
more 
probable 
than 
other 
states. 
This 
is 
an 
interesting 
problem 
in 
itself 
and 
can 
be 
tackled 
with 
a 
variety 
of 
methods. 
A 
general 
technique 
is 
given 
by 
Nilson[210] 
which 
is 
based 
on 
the 
Junction 
Tree 
formalism, 
chapter(6), 
and 
the 
construction 
of 
candidate 
lists, 
see 
for 
example 
[69]. 


DRAFT 
March 
9, 
2010 



Other 
Forms 
of 
Inference 


43 



Figure 
5.5: 
State 
transition 
diagram 
(weights 
not 
shown). 
The 
shortest 



7 
(unweighted) 
path 
from 
state 
1 
to 
state 
7 
is 
1 
- 
2 
- 
7. 
Considered 
as 
a 
1 


2 



8
Markov 
chain 
(random 
walk), 
the 
most 
probable 
path 
from 
state 
1 
to 
state 
7 
is 
1 
- 
8 
- 
9 
- 
7. 
The 
latter 
path 
is 
longer 
but 
more 
probable 
since 
for 
the 


65 



path 
1 
- 
2 
- 
7, 
the 
probability 
of 
exiting 
from 
state 
2 
into 
state 
7 
is 
1/5 
(assuming 
each 
transition 
is 
equally 
likely). 
See 
demoMostProbablePath.m

9 


For 
singly-connected 
structures, 
several 
approaches 
have 
been 
developed. 
For 
the 
hidden 
Markov 
model, 
section(23.2) 
a 
simple 
algorithm 
is 
the 
N-Viterbi 
approach 
which 
stores 
the 
N-most 
probable 
messages 
at 
each 
stage 
of 
the 
propagation, 
see 
for 
example 
[256]. 
A 
special 
case 
of 
Nilsson's 
approach 
is 
available 
for 
hidden 
Markov 
models[211] 
which 
is 
the 
particularly 
ecient 
for 
large 
state 
spaces. 


For 
more 
general 
singly-connected 
graphs 
one 
can 
extend 
the 
max-product 
algorithm 
to 
an 
N-max-product 
algorithm 
by 
retaining 
at 
each 
stage 
the 
N 
most 
probable 
messages, 
see 
below. 
These 
techniques 
require 
N 
to 
be 
specied 
a-priori 
compared 
to 
anytime 
alternatives, 
[298]. 
An 
alternative 
approach 
for 
singly-
connected 
networks 
was 
developed 
in 
[269]. 
Of 
particular 
interest 
is 
the 
application 
of 
the 
singly-connected 
algorithms 
as 
an 
approximation 
when 
for 
example 
Nilsson's 
approach 
on 
a 
multiply-connected 
graph 
is 
intractable[298]. 


N-max-product 


The 
algorithm 
for 
N-max-product 
is 
a 
straightforward 
modication 
of 
the 
standard 
algorithms. 
Computationally, 
a 
straightforward 
way 
to 
accomplish 
this 
is 
to 
introduce 
an 
additional 
variable 
for 
each 
message 
that 
is 
used 
to 
index 
the 
most 
likely 
messages. 
For 
example, 
consider 
the 
distribution 


p(a, 
b, 
c, 
d)= 
(a, 
b)(b, 
c)(b, 
d) 
(5.2.14) 


for 
which 
we 
wish 
to 
nd 
the 
two 
most 
probable 
values. 
Using 
the 
notation 


i 


max 
f(x) 
(5.2.15) 


x 


for 
the 
ith 
highest 
value 
of 
f(x), 
the 
maximisation 
over 
d 
can 
be 
expressed 
using 
the 
message 


12 


d(b, 
1) 
= 
max 
(b, 
d);d(b, 
2) 
= 
max 
(b, 
d) 
(5.2.16) 


dd 


Using 
a 
similar 
message 
for 
the 
maximisation 
over 
c, 
the 
2 
most 
likely 
states 
of 
p(a, 
b, 
c, 
d) 
can 
be 
computed 
using 


1:2 
1:2 
max 
= 
max 
(a, 
b)d(b, 
md)c(b, 
mc) 
(5.2.17) 


a;b;c;d 
a;b;mb;md 


where 
mc 
and 
md 
index 
the 
highest 
values. 
At 
the 
nal 
stage 
we 
now 
have 
a 
table 
with 
dim 
a 
× 
dim 
b 
× 
4 
entries, 
from 
which 
we 
compute 
the 
highest 
two 
states. 


The 
generalisation 
of 
this 
to 
the 
factor 
graph 
formalism 
is 
straightforward 
and 
contained 
in 
maxNprodFG.m. 
Essentially 
the 
only 
modication 
required 
is 
to 
dene 
extended 
messages 
which 
contain 
the 
N-most 
likely 
messages 
computed 
at 
each 
stage. 
At 
a 
junction 
of 
the 
factor 
graph, 
all 
the 
messages 
from 
the 
neighbours, 
along 
with 
their 
N-most 
probable 
tables 
are 
multiplied 
together 
into 
a 
large 
table. 
For 
a 
factor 
to 
variable 
message 
the 
N-most 
probable 
messages 
are 
retained, 
see 
maxNprodFG.m. 
The 
N-most 
probable 
states 
for 
each 
variable 
can 
then 
be 
read 
off 
by 
nding 
the 
variable 
state 
that 
maximises 
the 
product 
of 
incoming 
extended 
messages. 


DRAFT 
March 
9, 
2010 



Other 
Forms 
of 
Inference 


5.2.3 
Most 
probable 
path 
and 
shortest 
path 
What 
is 
the 
most 
likely 
path 
from 
state 
a 
to 
state 
b 
for 
an 
N 
state 
Markov 
chain? 
Note 
that 
this 
is 
not 
necessarily 
the 
same 
as 
the 
shortest 
path, 
as 
explained 
in 
g(5.5). 


If 
assume 
that 
a 
length 
T 
path 
exists, 
this 
has 
probability 


p(s2js1 
= 
a)p(s3js2) 
:::p(sT 
= 
bjsT 
..1) 
(5.2.18) 


Finding 
the 
most 
probable 
path 
can 
then 
be 
readily 
solved 
using 
the 
max-product 
(or 
max-sum 
algorithm 
for 
the 
log-transitions) 
on 
a 
simple 
serial 
factor 
graph. 
To 
deal 
with 
the 
issue 
that 
we 
don't 
know 
the 
optimal 
T 
, 
one 
approach 
is 
to 
redene 
the 
probability 
transitions 
such 
that 
the 
desired 
state 
b 
is 
an 
abs
orbing 
state 
of 
the 
chain 
(that 
is, 
one 
can 
enter 
this 
state 
but 
not 
leave 
it). 
With 
this 
redenition, 
the 
most 
probable 
joint 
state 
will 
correspond 
to 
the 
most 
probable 
state 
on 
the 
product 
of 
N 
transitions 
– 
once 
the 
absorbing 
state 
is 
reached 
the 
chain 
will 
stay 
in 
this 
state, 
and 
hence 
the 
most 
probable 
path 
can 
be 
read 
off 
from 
the 
sequence 
of 
states 
up 
to 
the 
rst 
time 
the 
chain 
hits 
the 
absorbing 
state. 
This 
approach 
is 
demonstrated 
in 
demoMostProbablePath.m, 
along 
with 
the 
more 
direct 
approaches 
described 
below. 


An 
alternative, 
cleaner 
approach 
is 
as 
follows: 
for 
the 
Markov 
chain 
we 
can 
dispense 
with 
variable-to-factor 
and 
factor-to-variable 
messages 
and 
use 
only 
variable-to-variable 
messages. 
If 
we 
want 
to 
nd 
the 
most 
likely 
set 
of 
states 
a;s2;:::;sT 
..1, 
b 
to 
get 
us 
there, 
then 
this 
can 
be 
computed 
by 
dening 
the 
maximal 
path 
probability 
E 
(a 
. 
b;T 
) 
to 
get 
from 
a 
to 
b 
in 
T 
-timesteps: 


E 
(a 
. 
b;T 
) 
= 
max 
p(s2js1 
= 
a)p(s3js2)p(s4js3) 
:::p(sT 
= 
bjsT 
..1) 
(5.2.19) 


s2;:::;sT 
..1 


= 
max 
max 
p(s2js1 
= 
a)p(s3js2) 
p(s4js3) 
:::p(sT 
= 
bjsT 
..1) 
(5.2.20) 


s3;:::;sT 
..1 
s2 


2!3(s3) 


To 
compute 
this 
eciently 
we 
dene 
messages 


t!t+1 
(st+1) 
= 
max 
t..1!t 
(st) 
p(st+1jst);t 
= 
2;1!2 
(s2)= 
p(s2js1 
= 
a) 
(5.2.21) 


st 


until 
the 
point 


E 
(a 
. 
b;T 
) 
= 
max 
T 
..2!T 
..1 
(sT 
..1) 
p(sT 
= 
bjsT 
..1)= 
T 
..1!T 
(sT 
= 
b) 
(5.2.22) 


sT 
..1 


We 
can 
now 
proceed 
to 
nd 
the 
maximal 
path 
probability 
for 
timestep 
T 
+ 
1. 
Since 
the 
messages 
up 
to 
time 
T 
- 
1 
will 
be 
the 
same 
as 
before, 
we 
need 
only 
compute 
one 
additional 
message, 
T 
..1!T 
(sT 
), 
from 
which 


E 
(a 
. 
b;T 
+ 
1) 
= 
max 
T 
..1!T 
(sT 
) 
p(sT 
+1 
= 
bjsT 
)= 
T 
!T 
+1 
(sT 
+1 
= 
b) 
(5.2.23) 


sT 


We 
can 
proceed 
in 
this 
manner 
until 
we 
reach 
E 
(a 
. 
b;N) 
where 
N 
is 
the 
number 
of 
nodes 
in 
the 
graph. 
We 
don't 
need 
to 
go 
beyond 
this 
number 
of 
steps 
since 
those 
that 
do 
must 
necessarily 
contain 
non-simple 
paths. 
(A 
simple 
path 
is 
one 
that 
does 
not 
include 
the 
same 
state 
more 
than 
once.) 
The 
optimal 
time 
t 
* 
is 
then 
given 
by 
which 
of 
E 
(a 
. 
b, 
2) 
;:::;E 
(a 
. 
b;N) 
is 
maximal. 
Given 
t 
* 
one 
can 
begin 
to 
backtrack1 
. 
Since 


E 
(a 
. 
b;t 
) 
= 
max 
t 
..2!t 
..1 
(st 
..1) 
p(st 
* 
= 
bjst 
..1) 
(5.2.24) 


st 
..1 


we 
know 
the 
optimal 
state 


* 


st 
..1 
= 
argmax 
t 
..2!t 
..1 
(st 
..1) 
p(st 
* 
= 
bjst 
..1) 
(5.2.25) 


st 
..1 


1An 
alternative 
to 
nding 
t 
* 
is 
to 
dene 
self-transitions 
with 
probability 
1, 
and 
then 
use 
a 
xed 
time 
T 
= 
N. 
Once 
the 
desired 
state 
b 
is 
reached, 
the 
self-transition 
then 
preserves 
the 
chain 
in 
state 
b 
for 
the 
remaining 
timesteps. 
This 
procedure 
is 
used 
in 
mostprobablepathmult.m 


DRAFT 
March 
9, 
2010 
75 



Other 
Forms 
of 
Inference 


We 
can 
then 
continue 
to 
backtrack: 


* 
* 


st 
..2 
= 
argmax 
t 
..3!t 
..2 
(st 
..2) 
p(st 
..1jst 
..2) 
(5.2.26) 


st 
..2 


and 
so 
on. 
See 
mostprobablepath.m. 


• 
In 
the 
above 
derivation 
we 
do 
not 
use 
any 
properties 
of 
probability, 
except 
that 
p 
must 
be 
nonnegative 
(otherwise 
sign 
changes 
can 
ip 
a 
whole 
sequence 
`probability’ 
and 
the 
local 
message 
recursion 
no 
longer 
applies). 
One 
can 
consider 
the 
algorithm 
as 
nding 
the 
optimal 
`product’ 
path 
from 
a 
to 
b. 
• 
It 
is 
straightforward 
to 
modify 
the 
algorithm 
to 
solve 
the 
(single-source, 
single-sink) 
shortest 
weighted 
path 
problem. 
One 
way 
to 
do 
this 
is 
to 
replace 
the 
Markov 
transition 
probabilities 
with 
edge 
weights 
exp(..u(stjst..1)), 
where 
u(stjst..1) 
is 
innite 
if 
there 
is 
no 
edge 
from 
st..1 
to 
st. 
This 
approach 
is 
taken 
in 
shortestpath.m 
which 
is 
able 
to 
deal 
with 
either 
positive 
or 
negative 
edge 
weights. 
This 
method 
is 
therefore 
more 
general 
than 
the 
well-known 
Dijkstra's 
algorithm 
[111] 
which 
requires 
weights 
to 
be 
positive. 
If 
a 
negative 
edge 
cycle 
exists, 
the 
code 
returns 
the 
shortest 
weighted 
length 
N 
path, 
where 
N 
is 
the 
number 
of 
nodes 
in 
the 
graph. 
See 
demoShortestPath.m. 
• 
The 
above 
algorithm 
is 
ecient 
for 
the 
single-source, 
single-sink 
scenario, 
since 
the 
messages 
contain 
only 
N 
states, 
meaning 
that 
the 
overall 
storage 
is 
O(N2). 
• 
As 
it 
stands, 
the 
algorithm 
is 
numerically 
impractical 
since 
the 
messages 
are 
recursively 
multiplied 
by 
values 
usually 
less 
than 
1 
(at 
least 
for 
the 
case 
of 
probabilities). 
One 
will 
therefore 
quickly 
run 
into 
numerical 
underow 
(or 
possibly 
overow 
in 
the 
case 
of 
non-probabilities) 
with 
this 
method. 
To 
x 
the 
nal 
point 
above, 
it 
is 
best 
to 
work 
by 
dening 
the 
logarithm 
of 
E. 
Since 
this 
is 
a 
monotonic 
transformation, 
the 
most 
probable 
path 
dened 
through 
log 
E 
is 
the 
same 
as 
that 
obtained 
from 
E. 
In 
this 
case 


L 
(a 
. 
b;T 
) 
= 
max 
log[p(s2js1 
= 
a)p(s3js2)p(s4js3) 
:::p(sT 
= 
bjsT 
..1)] 
(5.2.27) 


s2;:::;sT 
..1 


 
#

T 
..1

X

= 
max 
log 
p(s2js1 
= 
a) 
+ 
log 
p(stjst..1) 
+ 
log 
p(sT 
= 
bjsT 
..1)(5.2.28) 


s2;:::;sT 
..1
t=2 


We 
can 
therefore 
dene 
new 
messages 


t!t+1 
(st+1) 
= 
max 
[t..1!t 
(st) 
+ 
log 
p(st+1jst)] 
(5.2.29) 
st 


One 
then 
proceeds 
as 
before 
by 
nding 
the 
most 
probable 
t 
* 
dened 
on 
L, 
and 
backtracks. 


Remark 
6. 
A 
possible 
confusion 
is 
that 
optimal 
paths 
can 
be 
eciently 
found 
`when 
the 
graph 
is 
loopy'. 
Note 
that 
the 
graph 
in 
g(5.5) 
is 
a 
state-transition 
diagram, 
not 
a 
graphical 
model. 
The

QX

graphical 
model 
corresponding 
to 
this 
simple 
Markov 
chain 
is 
the 
Belief 
Network 
p(stjst..1), 
a 


t 


linear 
serial 
structure. 
Hence 
the 
underlying 
graphical 
model 
is 
a 
simple 
chain, 
which 
explains 
why 
computation 
is 
ecient. 


Most 
probable 
path 
(multiple-source, 
multiple-sink) 


If 
we 
need 
the 
most 
probable 
path 
between 
all 
states 
a 
and 
b, 
one 
could 
re-run 
the 
above 
single-sourcesingle-
sink 
algorithm 
for 
all 
a 
and 
b. 
A 
computationally 
more 
ecient 
approach 
is 
to 
observe 
that 
one 
can 
dene 
a 
message 
for 
each 
starting 
state 
a: 


t!t+1 
(st+1ja) 
= 
max 
t..1!t 
(stja) 
p(st+1jst) 
(5.2.30) 


st 


DRAFT 
March 
9, 
2010 



Other 
Forms 
of 
Inference 


Q

Algorithm 
1 
Compute 
marginal 
p(x1jevidence) 
from 
distribution 
p(x)=f 
(fxgf 
). 
Assumes 
non-

f 


evidential 
variables 
are 
ordered 
x1;:::;xn. 


Q

1: 
procedure 
Bucket 
Elimination(p(x)=f 
(fxgf 
).)
f 


2: 
Initialize 
all 
Bucket 
potentials 
to 
unity. 
I 
Fill 
Buckets 
3: 
while 
There 
are 
potentials 
left 
in 
the 
distribution 
do 
4: 
For 
each 
potential 
f 
, 
its 
highest 
variable 
xj 
(according 
to 
the 
ordering). 
5: 
Multiply 
f 
with 
the 
potential 
in 
Bucket 
j 
and 
remove 
f 
the 
distribution. 
6: 
end 
while 
7: 
for 
i 
= 
Bucket 
n 
to 
1 
do 
I 
Empty 
Buckets 
8: 
For 
Bucket 
i 
sum 
over 
the 
states 
of 
variable 
xi 
and 
call 
this 
potential 
i 
9: 
Identify 
the 
highest 
variable 
xh 
of 
potential 
i 
10: 
Multiply 
the 
existing 
potential 
in 
Bucket 
h 
by 
i 
11: 
end 
for 
12: 
The 
marginal 
p(x1jevidence) 
is 
proportional 
to 
1. 
13: 
return 
p(x1jevidence) 
I 
The 
conditional 
marginal. 
14: 
end 
procedure 
and 
continue 
until 
we 
nd 
the 
maximal 
path 
probability 
matrix 
for 
getting 
from 
any 
state 
a 
to 
any 
state 
b 
in 
T 
timesteps: 


E 
(a 
. 
b;T 
) 
= 
max 
T 
..2!T 
..1 
(sT 
..1ja) 
p(sT 
= 
bjsT 
..1) 
(5.2.31) 
sT 
..1 


Since 
we 
know 
the 
message 
T 
..2!T 
..1 
(sT 
..1ja) 
for 
all 
states 
a, 
we 
can 
readily 
compute 
the 
most 
probable 
path 
from 
all 
starting 
states 
a 
to 
all 
states 
b 
after 
T 
steps. 
This 
requires 
passing 
an 
N 
N 
matrix 
message 
. 
We 
can 
then 
proceed 
to 
the 
next 
timestep 
T 
+ 
1. 
Since 
the 
messages 
up 
to 
time 
T 
- 
1 
will 
be 
the 
same 
as 
before, 
we 
need 
only 
compute 
one 
additional 
message, 
T 
..1!T 
(sT 
), 
from 
which 


E 
(a 
. 
b;T 
+ 
1) 
= 
max 
T 
..1!T 
(sT 
ja) 
p(sT 
+1 
= 
bjsT 
) 
(5.2.32) 
sT 


In 
this 
way 
one 
can 
then 
eciently 
compute 
the 
optimal 
path 
probabilities 
for 
any 
starting 
state 
a 
and 
end 
state 
b 
after 
t 
timesteps. 
To 
nd 
the 
optimal 
corresponding 
path, 
backtracking 
proceeds 
as 
before, 
see 
mostprobablepathmult.m. 
One 
can 
also 
use 
the 
same 
algorithm 
to 
solve 
the 
multiple-source, 
multiple 
sink 
shortest 
path 
problem. 
This 
algorithm 
is 
a 
variant 
of 
the 
Floyd-Warshall-Roy 
algorithm[111] 
for 
nding 
shortest 
weighted 
summed 
paths 
on 
a 
directed 
graph 
(the 
above 
algorithm 
enumerates 
through 
time, 
whereas 
the 
FWR 
algorithm 
enumerates 
through 
states). 


5.2.4 
Mixed 
inference 
An 
often 
encountered 
situation 
is 
to 
infer 
the 
most 
likely 
state 
of 
a 
joint 
marginal, 
possibly 
given 
some 
evidence. 
For 
example, 
given 
a 
distribution 
p(x1;:::;xn), 
nd 


X

argmax 
p(x1;x2;:::;xm) 
= 
argmax 
p(x1;:::;xn) 
(5.2.33) 


x1;x2;:::;xm 
x1;x2;:::;xm

xm+1;:::;xn 


In 
general, 
even 
for 
tree 
structured 
p(x1;:::;xn), 
the 
optimal 
marginal 
state 
cannot 
be 
computed 
eciently. 
One 
way 
to 
see 
this 
is 
that 
due 
to 
the 
summation 
the 
resulting 
joint 
marginal 
does 
not 
have 
a 
structured 
factored 
form 
as 
products 
of 
simpler 
functions 
of 
the 
marginal 
variables. 
Finding 
the 
most 
probable 
joint 
marginal 
then 
requires 
a 
search 
over 
all 
the 
joint 
marginal 
states 
– 
a 
task 
exponential 
in 
m. 
An 
approximate 
solution 
is 
provided 
by 
the 
EM 
algorithm 
(see 
section(11.2) 
and 
exercise(57)). 


DRAFT 
March 
9, 
2010 
77 



Inference 
in 
Multiply-Connected 
Graphs 


e
c
b
g
a
d
f
p(e)p(gjd, 
e) 
p(cja) 
p(b)p(dja, 
b) 
p(b)p(dja, 
b) 
E 
(d, 
g) 
E 
(d, 
g) 
p(a) 
p(a) 


p(a)B 
(d, 
a) 
p(a)B 
(d, 
a) 
p(fjd) 
p(fjd) 
p(fjd) 


p(fjd)G 
(d) 



p(fjd)G 
(d) 
A 
(d) 



D 
(f) 
Figure 
5.6: 
The 
bucket 
elimination 
algorithm 
applied 
to 
the 
graph 
g(2.1). 
At 
each 
stage, 
at 
least 
one 


PX

node 
is 
eliminated 
from 
the 
graph. 
The 
second 
stage 
of 
eliminating 
c 
is 
trivial 
since 
p(cja)=1 
and 


c 


has 
therefore 
been 
skipped 
over 
since 
this 
bucket 
does 
not 
send 
any 
message. 


5.3 
Inference 
in 
Multiply-Connected 
Graphs 
5.3.1 
Bucket 
elimination 
We 
consider 
here 
a 
general 
conditional 
marginal 
variable 
elimination 
method 
that 
works 
for 
any 
distribution 
(including 
multiply 
connected 
graphs). 
The 
algorithm 
assumes 
the 
distribution 
is 
in 
the 
form 


Y

p(x1;:::;xn) 
/(Xf 
) 
(5.3.1) 
f 


and 
that 
the 
task 
is 
to 
compute 
p(x1jevidence). 
For 
example, 
for 


p(x1;x2;x3;x4)= 
p(x1jx2)p(x2jx3)p(x3jx4)p(x4) 
(5.3.2) 
we 
could 
use 


1(x1;x2)= 
p(x1jx2);2(x2;x3)= 
p(x2jx3);3(x3;x4)= 
p(x3jx4)p(x4) 
(5.3.3) 
The 
sets 
of 
variables 
here 
are 
X1 
=(x1;x2), 
X2 
=(x2;x3), 
X3 
=(x3;x4). 
In 
general, 
the 
construction 
of 
potentials 
for 
a 
distribution 
is 
not 
unique. 
The 
task 
of 
computing 
a 
marginal 
in 
which 
a 
set 
of 
variables 
xn+1;::. 
are 
clamped 
to 
their 
evidential 
states 
is 


XY

p(x1jevidence) 
. 
p(x1, 
evidence)=f 
(Xf 
) 
(5.3.4) 


x2;:::;xnf 


The 
algorithm 
is 
given 
in 
algorithm(11) 
and 
can 
be 
considered 
a 
way 
to 
organise 
the 
distributed 
summation[79]. 
The 
algorithm 
is 
best 
explained 
by 
a 
simple 
example, 
as 
given 
below. 


Example 
21 
(Bucket 
Elimination). 
Consider 
the 
problem 
of 
calculating 
the 
marginal 
p(f) 
of 


p(a, 
b, 
c, 
d, 
e, 
f, 
g)= 
p(fjd)p(gjd, 
e)p(cja)p(dja, 
b)p(a)p(b)p(e), 
(5.3.5) 
see 
g(2.1). 


XX

p(f)=p(a, 
b, 
c, 
d, 
e, 
f, 
g)=p(fjd)p(gjd, 
e)p(cja)p(dja, 
b)p(a)p(b)p(e) 
(5.3.6) 


a;b;c;d;e;g 
a;b;c;d;e;g 


78 
DRAFT 
March 
9, 
2010 



Inference 
in 
Multiply-Connected 
Graphs 


We 
can 
distribute 
the 
summation 
over 
the 
various 
terms 
as 
follows: 
e,b 
and 
c 
are 
end 
nodes, 
so 
that 
we 
can 
sum 
over 
their 
values: 


 ! ! 
!

X 
XX

p(f)=p(fjd)p(a)p(dja, 
b)p(b)p(cja)p(gjd, 
e)p(e) 
(5.3.7) 


a;d;g 
b 
ce 


 
P

For 
convenience, 
lets 
write 
the 
terms 
in 
the 
brackets 
asb 
p(dja, 
b)p(b) 
= 
B 
(a, 
d),p(gjd, 
e)p(e) 
= 


e

P

E 
(d, 
g). 
The 
termp(cja) 
is 
equal 
to 
unity, 
and 
we 
therefore 
eliminate 
this 
node 
directly. 
Rearranging

c 


terms, 
we 
can 
write 


X

p(f)=p(fjd)p(a)B 
(a, 
d) 
E 
(d, 
g) 
(5.3.8) 
a;d;g 


If 
we 
think 
of 
this 
graphically, 
the 
eect 
of 
summing 
over 
b, 
c, 
e 
is 
eectively 
to 
remove 
or 
`eliminate’ 
those 
variables. 
We 
can 
now 
carry 
on 
summing 
over 
a 
and 
g 
since 
these 
are 
end 
points 
of 
the 
new 
graph: 


 
! !

X 
X

p(f)=p(fjd)p(a)B 
(a, 
d)E 
(d, 
g)(5.3.9) 
da 
g 


Again, 
this 
denes 
new 
functions 
A 
(d), 
G 
(d), 
so 
that 
the 
nal 
answer 
can 
be 
found 
from 


X

p(f)=p(fjd)A 
(d) 
G 
(d) 
(5.3.10) 
d 


We 
illustrate 
this 
in 
g(5.6). 
Initially, 
we 
dene 
an 
ordering 
of 
the 
variables, 
beginning 
with 
the 
one 
that 
we 
wish 
to 
nd 
the 
marginal 
for 
– 
a 
suitable 
ordering 
is 
therefore, 
f, 
d, 
a, 
g, 
b, 
c, 
e. 
Then 
starting 
with 
the 
highest 
bucket 
e 
(according 
to 
our 
ordering 
f, 
d, 
a, 
g, 
b, 
c, 
e), 
we 
put 
all 
the 
functions 
that 
mention 
e 
in 
the 
e 
bucket. 
Continuing 
with 
the 
next 
highest 
bucket, 
c, 
we 
put 
all 
the 
remaining 
functions 
that 
mention 
c 
in 
this 
c 
bucket, 
etc. 
The 
result 
of 
this 
initialisation 
procedure 
is 
that 
terms 
(conditional 
distributions) 
in 
the 
DAG 
are 
distributed 
over 
the 
buckets, 
as 
shown 
in 
the 
left 
most 
column 
of 
g(5.6). 
Eliminating 
then 
the 
highest 
bucket 
e, 
we 
pass 
a 
message 
to 
node 
g. 
Immediately, 
we 
can 
also 
eliminate 
bucket 
c 
since 
this 
sums 
to 
unity. 
In 
the 
next 
column, 
we 
have 
now 
two 
less 
buckets, 
and 
we 
eliminate 
the 
highest 
remaining 
bucket, 
this 
time 
b, 
passing 
a 
message 
to 
bucket 
a. 


There 
are 
some 
important 
observations 
we 
can 
make 
about 
bucket 
elimination: 


1. 
To 
compute 
say 
p(x2jevidence) 
we 
need 
to 
re-order 
the 
variables 
(so 
that 
the 
required 
marginal 
variable 
is 
labelled 
x1) 
and 
repeat 
Bucket 
Elimination. 
Hence 
each 
query 
(calculation 
of 
a 
marginal 
in 
this 
case) 
requires 
re-running 
the 
algorithm. 
It 
would 
be 
more 
ecient 
to 
reuse 
messages, 
rather 
than 
recalculating 
them 
each 
time. 
2. 
In 
general, 
Bucket 
Elimination 
constructs 
multi-variable 
messages 
. 
from 
Bucket 
to 
Bucket. 
The 
storage 
requirements 
of 
a 
multi-variable 
message 
are 
exponential 
in 
the 
number 
of 
variables 
of 
the 
message. 
3. 
For 
trees 
we 
can 
always 
choose 
a 
variable 
ordering 
to 
render 
the 
computational 
complexity 
to 
be 
linear 
in 
the 
number 
of 
variables. 
Such 
an 
ordering 
is 
called 
perfect, 
denition(49), 
and 
indeed 
it 
can 
be 
shown 
that 
a 
perfect 
ordering 
can 
always 
easily 
be 
found 
for 
singly-connected 
graphs 
(see 
[86]). 
However, 
orderings 
exist 
for 
which 
Bucket 
Elimination 
will 
be 
extremely 
inecient. 
5.3.2 
Loop-cut 
conditioning 
For 
distributions 
which 
contain 
a 
loop 
(there 
is 
more 
than 
one 
path 
between 
two 
nodes 
in 
the 
graph 
when 
the 
directions 
are 
removed), 
we 
run 
into 
some 
diculty 
with 
the 
message 
passing 
routines 
such 
as 
the 
sum-product 
algorithm 
which 
are 
designed 
to 
work 
on 
singly-connected 
graphs 
only. 
One 
way 
to 
solve 


DRAFT 
March 
9, 
2010 
79 



Notes 


d
a
c
b
e
fg
d
a
c
b
e
fg
Figure 
5.7: 
A 
multiply-
connected 
graph 
(a) 
reduced 
to 
a 
singly-connected 
graph 
(b) 
by 
conditioning 
on 
the 
variable 
c. 


(a) 
(b) 
the 
diculties 
of 
multiply 
connected 
(loopy) 
graphs 
is 
to 
identify 
nodes 
that, 
when 
removed, 
would 
reveal 
a 
singly-connected 
subgraph[219]. 
Consider 
the 
example 
if 
g(5.7). 
Imagine 
that 
we 
wish 
to 
calculate 
a 
marginal, 
say 
p(d). 
Then 


XX

p(d)=p(cja)p(a) 
p(dja, 
b)p(b) 
p(fjc, 
d) 
p(gjd, 
e) 
(5.3.11)

|{z}X|{z}

ca;b;e;f;g 


p 
(a) 
p 
(fjd) 




where 
the 
p 
denitions 
are 
not 
necessarily 
distributions. 
For 
each 
state 
of 
c, 
the 
form 
of 
the 
products 
of 
factors 
remaining 
as 
a 
function 
of 
a, 
b, 
e, 
f, 
g 
is 
singly-connected, 
so 
that 
standard 
singly-connected 
message 
passing 
can 
be 
used 
to 
perform 
inference. 
We 
will 
need 
to 
do 
perform 
inference 
for 
each 
state 
of 
variable 
c, 
each 
state 
dening 
a 
new 
singly-connected 
graph 
(with 
the 
same 
structure) 
but 
with 
modied 
potentials. 


More 
generally, 
we 
can 
dene 
a 
set 
of 
variables 
C, 
called 
the 
loop 
cut 
set 
and 
run 
singly-connected 
inference 
for 
each 
joint 
state 
of 
the 
cut-set 
variables 
C. 
This 
can 
also 
be 
used 
for 
nding 
the 
most 
likely 
state 
of 
a 
multiply-connected 
joint 
distribution 
as 
well. 
Hence, 
for 
a 
computational 
price 
exponential 
in 
the 
loop-
cut 
size, 
we 
can 
calculate 
the 
marginals 
(or 
the 
most 
likely 
state) 
for 
a 
multiply-connected 
distribution. 
However, 
determining 
a 
small 
cut 
set 
is 
in 
general 
dicult, 
and 
there 
is 
no 
guarantee 
that 
this 
will 
anyway 
be 
small 
for 
a 
given 
graph. 
Whilst 
this 
method 
is 
able 
to 
handle 
loops 
in 
a 
general 
manner, 
it 
is 
not 
particularly 
elegant 
since 
the 
concept 
of 
messages 
now 
only 
applies 
conditioned 
on 
the 
cut 
set 
variables, 
and 
how 
to 
re-use 
messages 
for 
inference 
of 
additional 
quantities 
of 
interest 
becomes 
unclear. 
We 
will 
discuss 
an 
alternative 
method 
for 
handling 
multiply 
connected 
distributions 
in 
chapter(6). 


5.4 
Message 
Passing 
for 
Continuous 
Distributions 
For 
parametric 
continuous 
distributions 
p(xjx), 
message 
passing 
corresponds 
to 
passing 
parameters 
. 
of 
the 
distributions. 
For 
the 
sum-product 
algorithm, 
this 
requires 
that 
the 
operations 
of 
multiplication 
and 
integration 
over 
the 
variables 
are 
closed 
with 
respect 
to 
the 
family 
of 
distributions. 
This 
is 
the 
case, 
for 
example, 
for 
the 
Gaussian 
distribution 
– 
the 
marginal 
(integral) 
of 
a 
Gaussian 
is 
another 
Gaussian, 
and 
the 
product 
of 
two 
Gaussians 
is 
a 
Gaussian, 
see 
section(8.6). 
This 
means 
that 
we 
can 
then 
implement 
the 
sum-product 
algorithm 
based 
on 
passing 
mean 
and 
covariance 
parameters. 
To 
implement 
this 
requires 
some 
tedious 
algebra 
to 
compute 
the 
appropriate 
message 
parameter 
updates. 
At 
this 
stage, 
the 
complexities 
from 
performing 
such 
calculations 
are 
a 
potential 
distraction, 
though 
the 
interested 
reader 
may 
refer 
to 
demoSumprodGaussMoment.m, 
demoSumprodGaussCanon.m 
and 
demoSumprodGaussCanonLDS.m 
and 
also 
chapter(24) 
for 
examples 
of 
message 
passing 
with 
Gaussians. 
For 
more 
general 
exponential 
family 
distributions, 
message 
passing 
is 
essentially 
straightforward, 
though 
again 
the 
specics 
of 
the 
updates 
may 
be 
tedious 
to 
work 
out. 
In 
cases 
where 
the 
operations 
of 
marginalisation 
and 
products 
are 
not 
closed 
within 
the 
family, 
the 
distributions 
need 
to 
be 
projected 
back 
to 
the 
chosen 
message 
family. 
Expectation 
propagation, 
section(28.7) 
is 
relevant 
in 
this 
case. 


5.5 
Notes 
A 
take-home 
message 
from 
this 
chapter 
is 
that 
(non-mixed) 
inference 
in 
singly-connected 
structures 
is 
usually 
computationally 
tractable. 
Notable 
exceptions 
are 
when 
the 
message 
passing 
operations 
are 
not-
closed 
within 
the 
message 
family, 
or 
representing 
messages 
explicitly 
requires 
an 
exponential 
amount 
of 
space. 
This 
happens 
for 
example 
when 
the 
distribution 
can 
contain 
both 
discrete 
and 
continuous 
variables, 


80 
DRAFT 
March 
9, 
2010 



Code 


such 
as 
the 
Switching 
Linear 
Dynamical 
system, 
which 
we 
discuss 
in 
chapter(25). 


Broadly 
speaking, 
inference 
in 
multiply-connected 
structures 
is 
more 
complex 
and 
may 
be 
intractable. 
However, 
we 
do 
not 
want 
to 
give 
the 
impression 
that 
this 
is 
always 
the 
case. 
Notable 
exceptions 
are: 
nding 
the 
MAP 
state 
in 
an 
attractive 
pairwise 
MRF, 
section(28.8); 
nding 
the 
MAP 
and 
MPM 
state 
in 
a 
binary 
planar 
MRF 
with 
pure 
interactions, 
see 
for 
example 
[115, 
243]. 
For 
N 
variables 
in 
the 
graph, 


..e 


a 
naive 
use 
of 
the 
junction 
tree 
algorithm 
for 
these 
inferences 
would 
result 
in 
an 
O 
2N 
computation,

..e 


whereas 
clever 
algorithms 
are 
able 
to 
return 
the 
exact 
results 
in 
ON3 
operations. 
Of 
interest 
is 
bond 
propagation[177] 
which 
is 
an 
intuitive 
node 
elimination 
method 
to 
arrive 
at 
the 
MPM 
inference 
in 
pure-
interaction 
Ising 
models. 


5.6 
Code 
The 
code 
below 
implements 
message 
passing 
on 
a 
tree 
structured 
factor 
graph. 
The 
FG 
is 
stored 
as 
an 
adjacency 
matrix 
with 
the 
message 
between 
FG 
node 
i 
and 
FG 
node 
j 
given 
in 
Ai;j. 
FactorGraph.m: 
Return 
a 
Factor 
Graph 
adjacency 
matrix 
and 
message 
numbers 
sumprodFG.m: 
Sum-Product 
algorithm 
on 
a 
Factor 
Graph 


In 
general 
it 
is 
recommended 
to 
work 
in 
log-space 
in 
the 
Max-Product 
case, 
particularly 
for 
large 
graphs 
since 
the 
produce 
of 
messages 
can 
become 
very 
small. 
The 
code 
provided 
does 
not 
work 
in 
log 
space 
and 
as 
such 
may 
not 
work 
on 
large 
graphs; 
writing 
this 
using 
log-messages 
is 
straightforward 
but 
leads 
to 
less 
readable 
code. 
An 
implementation 
based 
on 
log-messages 
is 
left 
as 
an 
exercise 
for 
the 
interested 
reader. 
maxprodFG.m: 
Max-Product 
algorithm 
on 
a 
Factor 
Graph 
maxNprodFG.m: 
N-Max-Product 
algorithm 
on 
a 
Factor 
Graph 


5.6.1 
Factor 
graph 
examples 
For 
the 
distribution 
from 
g(5.3), 
the 
following 
code 
nds 
the 
marginals 
and 
most 
likely 
joint 
states. 
The 
number 
of 
states 
of 
each 
variable 
is 
chosen 
at 
random. 
demoSumprod.m: 
Test 
the 
Sum-Product 
algorithm 
demoMaxprod.m: 
Test 
the 
Max-Product 
algorithm 
demoMaxNprod.m: 
Test 
the 
Max-N-Product 
algorithm 


5.6.2 
Most 
probable 
and 
shortest 
path 
mostprobablepath.m: 
Most 
Probable 
Path 
demoMostProbablePath.m: 
Most 
probable 
versus 
shortest 
path 
demo 


The 
shortest 
path 
demo 
works 
for 
both 
positive 
and 
negative 
edge 
weights. 
If 
negative 
weight 
cycles 
exist, 
the 
code 
nds 
the 
best 
length 
N 
shortest 
path. 
demoShortestPath.m: 
Shortest 
path 
demo 
mostprobablepathmult.m: 
Most 
Probable 
Path 
– 
multi-source, 
multi-sink 
demoMostProbablePathMult.m: 
Demo 
of 
most 
probable 
path 
– 
multi-source, 
multi-sink 


5.6.3 
Bucket 
elimination 
The 
ecacy 
of 
Bucket 
Elimination 
depends 
critically 
on 
the 
elimination 
sequence 
chosen. 
In 
the 
demonstration 
below 
we 
nd 
the 
marginal 
of 
a 
variable 
in 
the 
Chest 
Clinic 
exercise 
using 
a 
randomly 
chosen 
elimination 
order. 
The 
desired 
marginal 
variable 
is 
specied 
as 
the 
last 
to 
be 
eliminated. 
For 
comparison 
we 
use 
an 
elimination 
sequence 
based 
on 
decimating 
a 
triangulated 
graph 
of 
the 
model, 
as 
discussed 
in 
section(6.5.1), 
again 
under 
the 
constraint 
that 
the 
last 
variable 
to 
be 
`decimated’ 
is 
the 
marginal 
variable 
of 


DRAFT 
March 
9, 
2010 
81 



Exercises 


interest. 
For 
this 
smarter 
choice 
of 
elimination 
sequence, 
the 
complexity 
of 
computing 
this 
single 
marginal 
is 
roughly 
the 
same 
as 
that 
for 
the 
Junction 
Tree 
algorithm, 
using 
the 
same 
triangulation. 
bucketelim.m: 
Bucket 
Elimination 
demoBucketElim.m: 
Demo 
Bucket 
Elimination 


5.6.4 
Message 
passing 
on 
Gaussians 
The 
following 
code 
hints 
at 
how 
message 
passing 
may 
be 
implemented 
for 
continuous 
distributions. 
The 
reader 
is 
referred 
to 
the 
BRMLtoolbox 
for 
further 
details 
and 
also 
section(8.6) 
for 
the 
algebraic 
manipulations 
required 
to 
perform 
marginalisation 
and 
products 
of 
Gaussians. 
The 
same 
principal 
holds 
for 
any 
family 
of 
distributions 
which 
is 
closed 
under 
products 
and 
marginalisation, 
and 
the 
reader 
may 
wish 
to 
implement 
specic 
families 
following 
the 
method 
outlined 
for 
Gaussians. 
demoSumprodGaussMoment.m: 
Sum-product 
message 
passing 
based 
on 
Gaussian 
Moment 
parameterisation 


5.7 
Exercises 
Exercise 
52. 
Given 
a 
pairwise 
singly 
connected 
Markov 
Network 
of 
the 
form 


Y

1 


p(x)= 
f 
(xi;xj) 
(5.7.1)

Z 


ij 


explain 
how 
to 
eciently 
compute 
the 
normalisation 
factor 
(also 
called 
the 
partition 
function) 
Z 
as 
a 
function 
of 
the 
potentials 
. 


Exercise 
53. 
You 
are 
employed 
by 
a 
web 
start 
up 
company 
that 
designs 
virtual 
environments, 
in 
which 
players 
can 
move 
between 
rooms. 
The 
rooms 
which 
are 
accessible 
from 
another 
in 
one 
time 
step 
is 
given 
by 
the 
100 
× 
100 
matrix 
M, 
stored 
in 
virtualworlds.mat, 
where 
Mij 
=1 
means 
that 
there 
is 
a 
door 
between 
rooms 
i 
and 
j 
(Mij 
= 
Mji). 
Mij 
=0 
means 
that 
there 
is 
no 
door 
between 
rooms 
i 
and 
j. 
Mii 
=1 
meaning 
that 
in 
one 
time 
step, 
one 
can 
stay 
in 
the 
same 
room. 
You 
can 
visualise 
this 
matrix 
by 
typing 
imagesc(M). 


1. 
Write 
a 
list 
of 
rooms 
which 
cannot 
be 
reached 
from 
room 
2 
after 
10 
time 
steps. 
2. 
The 
manager 
complains 
that 
takes 
at 
least 
13 
time 
steps 
to 
get 
from 
room 
1 
to 
room 
100. 
Is 
this 
true? 
3. 
Find 
the 
most 
likely 
path 
(sequence 
of 
rooms) 
to 
get 
from 
room 
1 
to 
room 
100. 
4. 
If 
a 
single 
player 
were 
to 
jump 
randomly 
from 
one 
room 
to 
another 
(or 
stay 
in 
the 
same 
room), 
with 
no 
preference 
between 
rooms, 
what 
is 
the 
probability 
at 
time 
t 
» 
1 
the 
player 
will 
be 
in 
room 
1? 
Assume 
that 
eectively 
an 
innite 
amount 
of 
time 
has 
passed 
and 
the 
player 
began 
in 
room 
1 
at 
t 
=1. 
5. 
If 
two 
players 
are 
jumping 
randomly 
between 
rooms 
(or 
staying 
in 
the 
same 
room), 
explain 
how 
to 
compute 
the 
probability 
that, 
after 
an 
innite 
amount 
of 
time, 
at 
least 
one 
of 
them 
will 
be 
in 
room 
1? 
Assume 
that 
both 
players 
begin 
in 
room 
1. 
Exercise 
54. 
Consider 
the 
hidden 
Markov 
model: 


T

T 


p(v1;:::;vT 
;h1;:::;hT 
)= 
p(h1)p(v1jh1) 
p(vtjht)p(htjht..1) 
(5.7.2) 
t=2 


in 
which 
dom(ht)= 
f1;:::;H} 
and 
dom(vt)= 
f1;:::;V 
} 
for 
all 
t 
=1;:::;T 
. 


1. 
Draw 
a 
Belief 
Network 
representation 
of 
the 
above 
distribution. 
2. 
Draw 
a 
Factor 
Graph 
representation 
of 
the 
above 
distribution. 
DRAFT 
March 
9, 
2010 



Exercises 


3. 
Use 
the 
Factor 
Graph 
to 
derive 
a 
Sum-Product 
algorithm 
to 
compute 
marginals 
p(htjv1;:::;vT 
). 
Explain 
the 
sequence 
order 
of 
messages 
passed 
on 
your 
Factor 
Graph. 
4. 
Explain 
how 
to 
compute 
p(ht;ht+1jv1;:::;vT 
). 
Exercise 
55. 
For 
a 
singly 
connected 
Markov 
Network, 
p(x)= 
p(x1;:::;xn), 
the 
computation 
of 
a 
marginal 


* 


p(xi) 
can 
be 
carried 
out 
eciently. 
Similarly, 
the 
most 
likely 
joint 
state 
x 
= 
arg 
maxx1;:::;xn 
p(x) 
can 
be 
computed 
eciently. 
Explain 
when 
the 
most 
likely 
joint 
state 
of 
a 
marginal 
can 
be 
computed 
eciently, 


i.e. 
under 
what 
circumstances 
could 
one 
eciently 
(in 
O 
(m) 
time) 
compute 
argmax 
p(x1;:::;xm) 
for 
x1;x2;:::;xm 


m<n? 


Exercise 
56. 
Consider 
the 
internet 
with 
webpages 
labelled 
1;:::;N. 
If 
webpage 
j 
has 
a 
link 
to 
webpage 
i, 
then 
we 
place 
an 
element 
of 
the 
matrix 
Lij 
=1, 
otherwise 
Lij 
=0. 
By 
considering 
a 
random 
jump 
from 
webpage 
j 
to 
webpage 
i 
to 
be 
given 
by 
the 
transition 
probability 


Lij

Mij 
= 
= 
(5.7.3) 
i 


Lij 


what 
is 
the 
probability 
that 
after 
an 
innite 
amount 
of 
random 
surng, 
one 
ends 
up 
on 
webpage 
i? 
How 
could 
you 
relate 
this 
to 
the 
potential 
`relevance’ 
of 
a 
webpage 
in 
terms 
of 
a 
search 
engine? 


Exercise 
57. 
A 
special 
time-homogeneous 
hidden 
Markov 
model 
is 
given 
by 


T

T 


p(x1;:::;xT 
;y1;:::;yT 
;h1;:::;hT 
)= 
p(x1jh1)p(y1jh1)p(h1) 
p(htjht..1)p(xtjht)p(ytjht) 
(5.7.4) 
t=2 


The 
variable 
xt 
has 
4 
states, 
dom(xt)= 
fA, 
C, 
G, 
T} 
(numerically 
labelled 
as 
states 
1,2,3,4). 
The 
variable 
yt 
has 
4 
states, 
dom(yt)= 
fA, 
C, 
G, 
Tg. 
The 
hidden 
or 
latent 
variable 
ht 
has 
5 
states, 
dom(ht)= 
f1;:::, 
5g. 
The 
HMM 
models 
the 
following 
(ctitious) 
process: 


In 
humans, 
Z-factor 
proteins 
are 
a 
sequence 
on 
states 
of 
the 
variables 
x1;x2;:::;xT 
. 
In 
bananas 
Z-
factor 
proteins 
are 
also 
present, 
but 
represented 
by 
a 
dierent 
sequence 
y1;y2;:::;yT 
. 
Given 
a 
sequence 
x1;:::;xT 
from 
a 
human, 
the 
task 
is 
to 
nd 
the 
corresponding 
sequence 
y1;:::;yT 
in 
the 
banana 
by 
rst 
nding 
the 
most 
likely 
joint 
latent 
sequence, 
and 
then 
the 
most 
likely 
banana 
sequence 
given 
this 
optimal 
latent 
sequence. 
That 
is, 
we 
require 


argmax 
p(y1;:::;yT 
jh1* 
;:::;h 
* 
) 
(5.7.5)

T 


y1;:::;yT 


where 


h 
* 
1;:::;h 
* 
= 
argmax 
p(h1;:::;hT 
jx1;:::;xT 
) 
(5.7.6)

T 
h1;:::;hT 


The 
le 
banana.mat 
contains 
the 
emission 
distributions 
pxgh 
(p(xjh)), 
pygh 
(p(yjh)) 
and 
transition 
phtghtm 
(p(htjht..1)). 
The 
initial 
hidden 
distribution 
is 
given 
in 
ph1 
(p(h1)). 
The 
observed 
x 
sequence 
is 
given 
in 
x. 


1. 
Explain 
mathematically 
and 
in 
detail 
how 
to 
compute 
the 
optimal 
y-sequence, 
using 
the 
two-stage 
procedure 
as 
stated 
above. 
2. 
Write 
a 
MATLAB 
routine 
that 
computes 
and 
displays 
the 
optimal 
y-sequence, 
given 
the 
observed 
x-sequence. 
Your 
routine 
must 
make 
use 
of 
the 
Factor 
Graph 
formalism. 
3. 
Explain 
whether 
or 
not 
it 
is 
computationally 
tractable 
to 
compute 
arg 
max 
p(y1;:::;yT 
jx1;:::;xT 
) 
(5.7.7) 


y1;:::;yT 


4. 
Bonus 
question: 
By 
considering 
y1;:::;yT 
as 
parameters, 
explain 
how 
the 
EM 
algorithm 
may 
be 
used 
to 
nd 
most 
likely 
marginal 
states. 
Implement 
this 
approach 
with 
a 
suitable 
initialisation 
for 
the 
optimal 
parameters 
y1;:::;yT 
. 
DRAFT 
March 
9, 
2010 



Exercises 


DRAFT 
March 
9, 
2010 



CHAPTER 
6 


The 
Junction 
Tree 
Algorithm 


6.1 
Clustering 
Variables 
In 
chapter(5) 
we 
discussed 
ecient 
inference 
for 
singly-connected 
graphs, 
for 
which 
variable 
elimination 
and 
message 
passing 
schemes 
are 
appropriate. 
In 
the 
multiply-connected 
case, 
however, 
one 
cannot 
in 
general 
perform 
inference 
by 
passing 
messages 
only 
along 
existing 
links 
in 
the 
graph. 
The 
idea 
behind 
the 
junction 
tree 
algorithm 
is 
to 
form 
a 
new 
representation 
of 
the 
graph 
in 
which 
variables 
are 
clustered 
together, 
resulting 
in 
a 
singly-connected 
graph 
in 
the 
cluster 
variables 
(albeit 
on 
a 
dierent 
graph). 
The 
main 
focus 
of 
the 
development 
will 
be 
on 
marginal 
inference, 
though 
similar 
techniques 
apply 
to 
dierence 
inferences, 
such 
as 
nding 
the 
maximal 
state 
of 
the 
distribution. 


At 
this 
stage 
it 
is 
important 
to 
point 
out 
that 
the 
junction 
tree 
algorithm 
is 
not 
a 
magic 
method 
to 
deal 
with 
intractabilities 
resulting 
from 
multiply 
connected 
graphs; 
it 
is 
simply 
a 
way 
to 
perform 
correct 
inference 
on 
a 
multiply 
connected 
graph 
by 
transforming 
to 
a 
singly 
connected 
structure. 
Carrying 
out 
the 
inference 
on 
the 
resulting 
junction 
tree 
may 
still 
be 
computationally 
intractable. 
For 
example, 
the 
junction 
tree 
representation 
of 
a 
general 
two-dimensional 
Ising 
model 
is 
a 
single 
supernode 
containing 
all 
the 
variables. 
Inference 
in 
this 
case 
is 
exponentially 
complex 
in 
the 
number 
of 
variables. 
Nevertheless, 
even 
in 
cases 
where 
implementing 
the 
JTA 
(or 
any 
other 
exact 
inference 
algorithm) 
may 
be 
intractable, 
the 
JTA 
provides 
useful 
insight 
into 
the 
representation 
of 
distributions 
that 
can 
form 
the 
basis 
for 
approximate 
inference. 
In 
this 
sense 
the 
JTA 
is 
key 
to 
understanding 
issues 
related 
to 
representations 
and 
complexity 
of 
inference 
and 
is 
central 
to 
the 
development 
of 
ecient 
inference 
algorithms. 


6.1.1 
Reparameterisation 
Consider 
the 
chain 
p(a, 
b, 
c, 
d) 
= 
p(ajb)p(bjc)p(cjd)p(d) 
(6.1.1) 
Using 
Bayes’ 
rule, 
we 
can 
reexpress 
this 
as 
p(a, 
b, 
c, 
d) 
= 
p(a, 
b) 
p(b) 
p(b, 
c) 
p(c) 
p(c, 
d) 
p(d) 
p(d) 
= 
p(a, 
b)p(b, 
c)p(c, 
d) 
p(b)p(c) 
(6.1.2) 


A 
useful 
insight 
is 
that 
the 
distribution 
can 
therefore 
be 
written 
as 
a 
product 
of 
marginal 
distributions, 
divided 
by 
a 
product 
of 
the 
intersection 
of 
the 
marginal 
distributions: 
Looking 
at 
the 
numerator 
p(a, 
b)p(b, 
c)p(c, 
d) 
this 
cannot 
be 
a 
distribution 
over 
a, 
b, 
c, 
d 
since 
we 
are 
overcounting 
b 
and 
c, 
where 
this 
overcounting 
of 
b 
arises 
from 
the 
overlap 
between 
the 
sets 
a, 
b 
and 
b, 
c, 
which 
have 
b 
as 
their 
intersection. 
Similarly, 
the 
overcounting 
of 
c 
arises 
from 
the 
overlap 
between 
the 
sets 
b, 
c 
and 
c, 
d. 
Roughly 
speaking 
we 
need 
to 
correct 
for 
this 
overcounting 
by 
dividing 
by 
the 
distribution 
on 
the 
intersections. 
Given 
the 
transformed 
representation, 
a 
marginal 
such 
as 
p(a, 
b) 
can 
be 
read 
off 
directly 
from 
the 
factors 
in 
the 
new 


85 



Clique 
Graphs 


d
b
c
a
abcbcbcd
Figure 
6.1: 
(a) 
Markov 
network 
(a, 
b, 
c)(b, 
c, 
d). 
(b) 
Equivalent 
clique 
graph 
of 
(a). 


(a) 
(b) 
expression. 


The 
aim 
of 
the 
junction 
tree 
algorithm 
is 
to 
form 
a 
representation 
of 
the 
distribution 
which 
contains 
the 
marginals 
explicitly. 
We 
want 
to 
do 
this 
in 
a 
way 
that 
works 
for 
Belief 
and 
Markov 
networks, 
and 
also 
deals 
with 
the 
multiply-connected 
case. 
In 
order 
to 
do 
so, 
an 
appropriate 
way 
to 
parameterise 
the 
distribution 
is 
in 
terms 
of 
a 
clique 
graph, 
as 
described 
in 
the 
next 
section. 


6.2 
Clique 
Graphs 
Denition 
39 
(Clique 
Graph). 
A 
clique 
graph 
consists 
of 
a 
set 
of 
potentials, 
1(X 
1);:::;n(X 
n) 
each 
dened 
on 
a 
set 
of 
variables 
X 
i 
. 
For 
neighbouring 
cliques 
on 
the 
graph, 
dened 
on 
sets 
of 
variables 
X 
i 
and 
X 
j, 
the 
intersection 
X 
s 
= 
X 
i 
\X 
j 
is 
called 
the 
separator 
and 
has 
a 
corresponding 
potential 
s(X 
s). 


A 
clique 
graph 
represents 
the 
function
Qc 
c(X 
c)
Qs 
s(X 
s) 
(6.2.1) 
For 
notational 
simplicity 
we 
will 
usually 
drop 
the 
clique 
potential 
index 
c. 
Graphically 
clique 
potentials 
are 
represented 
by 
circles/ovals, 
and 
separator 
potentials 
by 
squares. 


X1X1\X2X2
The 
graph 
on 
the 
left 
represents 
(X 
1)(X 
2)=(X 
1 
\X 
2). 


Clique 
graphs 
translate 
Markov 
networks 
into 
structures 
convenient 
for 
carrying 
out 
inference. 
Consider 
the 
Markov 
network 
in 
g(6.1a) 


(a, 
b, 
c)(b, 
c, 
d) 


p(a, 
b, 
c, 
d) 
= 
(6.2.2)

Z 


which 
contains 
two 
clique 
potentials 
sharing 
the 
variables 
b, 
c. 
An 
equivalent 
representation 
is 
given 
by 
the 
clique 
graph 
in 
g(6.1b), 
dened 
as 
the 
product 
of 
the 
numerator 
clique 
potentials, 
divided 
by 
the 
product 
of 
the 
separator 
potentials. 
In 
this 
case 
the 
separator 
potential 
may 
be 
set 
to 
the 
normalisation 
constant 
Z. 
By 
summing 
we 
have 


XX

Zp(a, 
b, 
c)= 
(a, 
b, 
c)(b, 
c, 
d), 
Zp(b, 
c, 
d)= 
(b, 
c, 
d)(a, 
b, 
c) 
(6.2.3) 


da 


Multiplying 
the 
two 
expressions, 
we 
have 


 ! !

XXX

Z2 
p(a, 
b, 
c)p(b, 
c, 
d)=(a, 
b, 
c)(b, 
c, 
d)(b, 
c, 
d)(a, 
b, 
c)= 
Z2 
p(a, 
b, 
c, 
d)p(a, 
b, 
c, 
d) 


d 
a 
a;d 


(6.2.4) 
In 
other 
words 


p(a, 
b, 
c)p(b, 
c, 
d) 


p(a, 
b, 
c, 
d) 
= 
(6.2.5) 


p(c, 
b) 


DRAFT 
March 
9, 
2010 



Clique 
Graphs 


The 
important 
observation 
is 
that 
the 
distribution 
can 
be 
written 
in 
terms 
of 
its 
marginals 
on 
the 
variables 
in 
the 
original 
cliques 
and 
that, 
as 
a 
clique 
graph, 
it 
has 
the 
same 
structure 
as 
before. 
All 
that 
has 
changed 
is 
that 
the 
original 
clique 
potentials 
have 
been 
replaced 
by 
the 
marginals 
of 
the 
distribution 
and 
the 
separator 
by 
the 
marginal 
dened 
on 
the 
separator 
variables 
(a, 
b, 
c) 
. 
p(a, 
b, 
c), 
(b, 
c, 
d) 
. 
p(b, 
c, 
d), 
Z 
. 
p(c, 
b). 
The 
usefulness 
of 
this 
representation 
is 
that 
if 
we 
are 
interested 
in 
the 
marginal 
p(a, 
b, 
c), 
this 
can 
be 
read 
off 
from 
the 
transformed 
clique 
potential. 
To 
make 
use 
of 
this 
representation, 
we 
require 
a 
systematic 
way 
of 
transforming 
the 
clique 
graph 
potentials 
so 
that 
at 
the 
end 
of 
the 
transformation 
the 
new 
potentials 
contain 
the 
marginals 
of 
the 
distribution. 


Remark 
7. 
Note 
that, 
whilst 
visually 
similar, 
a 
Factor 
Graph 
and 
a 
Clique 
Graph 
are 
dierent 
representations. 
In 
a 
Clique 
Graph 
the 
nodes 
contain 
sets 
of 
variables, 
which 
may 
share 
variables 
with 
other 
nodes. 


6.2.1 
Absorption 
Consider 
neighbouring 
cliques 
V 
and 
W, 
sharing 
the 
variables 
S 
in 
common. 
In 
this 
case, 
the 
distribution 
on 
the 
variables 
X 
= 
V[W 
is 


. 
(V). 
(W) 


p(X 
) 
= 
(6.2.6)

. 
(S) 


and 
our 
aim 
is 
to 
nd 
a 
new 
representation 


 * 
(V) * 
(W) 


p(X 
) 
= 
(6.2.7)

 * 
(S) 


in 
which 
the 
potentials 
are 
given 
by 


. 
* 
(V)= 
p(V);. 
* 
(W)= 
p(W);. 
* 
(S)= 
p(S) 
(6.2.8) 


In 
this 
example, 
we 
can 
explicitly 
work 
out 
the 
new 
potentials 
as 
function 
of 
the 
old 
potentials 
by 
computing 
the 
marginals 
as 
follows: 


P

XX

. 
(V). 
(W) 
VnS 
. 
(V) 


p(W)=p(X 
)== 
. 
(W)(6.2.9)

. 
(S) 
. 
(S)

VnS 
VnS 


and 


P

XX

. 
(V). 
(W) 
WnS 
. 
(W) 


p(V)=p(X 
)== 
. 
(V)(6.2.10)

. 
(S) 
. 
(S)

WnS 
WnS 


There 
is 
a 
symmetry 
present 
in 
the 
two 
equations 
above 
– 
they 
are 
the 
same 
under 
interchanging 
V 
and 


W. 
One 
way 
to 
describe 
these 
equations 
is 
through 
`absorption'. 
We 
say 
that 
the 
cluster 
W 
`absorbs’ 
information 
from 
cluster 
V 
by 
the 
following 
updating 
procedure. 
First 
we 
dene 
a 
new 
separator 
X

. 
* 
(S)=. 
(V) 
(6.2.11) 


VnS 


and 
the 
rene 
the 
W 
potential 
using 


 * 
(S)

. 
* 
(W)= 
. 
(W)(6.2.12)

. 
(S) 


The 
advantage 
of 
this 
interpretation 
is 
that 
the 
new 
representation 
is 
still 
a 
valid 
clique 
graph 
representation 
of 
the 
distribution 
since 


 (V) (S) (W)
 (V) (S) (W)
. 
(V). 
(W) (S)

. 
(V) * 
(W) 
 (S) 
. 
(V). 
(W)

= 
== 
p(X 
) 
(6.2.13)

 * 
(S) 
 * 
(S) 
. 
(S) 


DRAFT 
March 
9, 
2010 



Junction 
Trees 


Figure 
6.2: 
An 
example 
absorption 
schedule 
on 
a 
clique 
tree. 
Many 
valid 
schedules 
exist 
under 
the 
con-


AB
CD
EF
1!2!
3! 4
 56 
7!
8! 9
 10
straint 
that 
messages 
can 
only 
be 
passed 
to 
a 
neighbour 
when 
all 
other 
messages 
have 
been 
received. 


After 
W 
absorbs 
information 
from 
V 
then 
 * 
(W) 
contains 
the 
marginal 
p(W). 
Similarly, 
after 
V 
absorbs 
information 
from 
W 
then 
 * 
(V) 
contains 
the 
marginal 
p(V). 
After 
the 
separator 
S 
has 
participated 
in 
absorption 
along 
both 
directions, 
then 
the 
separator 
potential 
will 
contain 
p(S) 
(this 
is 
not 
the 
case 
after 
only 
a 
single 
absorption). 
To 
see 
this, 
consider 


XXX

. 
(W) * 
(S) 
. 
(W). 
(V)

. 
* 
(S)=. 
* 
(W)=== 
p(S) 
(6.2.14)

. 
(S) 
. 
(S)

WnS 
WnS 
fW[VgnS 


Continuing, 
we 
have 
the 
new 
potential 
 * 
(V) 
given 
by 


PP

. 
(V) * 
(S) 
. 
(V). 
(W) * 
(S)=. 
(s) 
. 
(V). 
(W)

WnS 
WnS 


. 
* 
(V)= 
= 
== 
p(V) 
(6.2.15)

 * 
(S) 
 * 
(S) 
. 
(S) 


and 
 * 
(W) 
with 
. 
* 
(S) 
=X. 
(V) 
. 
* 
(W) 
= 
. 
(W) * 
(S) 
. 
(S) 
(6.2.16) 
VnS 


 (V) (S) (W)
Denition 
40 
(Absorption). 


Let 
V 
and 
W 
be 
neighbours 
in 
a 
clique 
graph, 
let 
S 
be 
their 
separator, 
and 
let 
. 
(V), 
. 
(W) 
and 
. 
(S) 
be 
their 
potentials. 
Absorption 
from 
V 
to 
W 
through 
S 
replaces 
the 
tables 
 * 
(S) 


We 
say 
that 
clique 
W 
absorbs 
information 
from 
clique 
V. 


6.2.2 
Absorption 
schedule 
on 
clique 
trees 
Having 
dened 
the 
local 
message 
propagation 
approach, 
we 
need 
to 
dene 
an 
update 
ordering 
for 
absorption. 
In 
general, 
a 
node 
V 
can 
send 
exactly 
one 
message 
to 
a 
neighbour 
W, 
and 
it 
may 
only 
be 
sent 
when 
V 
has 
received 
a 
message 
from 
each 
of 
its 
other 
neighbours. 
We 
continue 
this 
sequence 
of 
absorptions 
until 
a 
message 
has 
been 
passed 
in 
both 
directions 
along 
every 
link. 
See, 
for 
example, 
g(6.2). 
Note 
that 
the 
message 
passing 
scheme 
is 
not 
unique. 


Denition 
41 
(Absorption 
Schedule). 
A 
clique 
can 
send 
a 
message 
to 
a 
neighbour, 
provided 
it 
has 
already 
received 
messages 
from 
all 
other 
neighbours. 


6.3 
Junction 
Trees 
There 
are 
a 
few 
stages 
we 
need 
to 
go 
through 
in 
order 
to 
transform 
a 
distribution 
into 
an 
appropriate 
structure 
for 
inference. 
Initially 
we 
explain 
how 
to 
do 
this 
for 
singly-connected 
structures 
before 
moving 


88 
DRAFT 
March 
9, 
2010 



Junction 
Trees 


x1
x2x3
x4
x1;x4
x2;x4x3;x4
x4x4
x4
x1;x4
x2;x4x3;x4
x4x4
(a) 
(b) 
(c) 
Figure 
6.3: 
(a): 
Singly-connected 
Markov 
network. 
(b): 
Clique 
graph. 
(c): 
Clique 
tree. 


onto 
the 
multiply-connected 
case. 


Consider 
the 
singly-connected 
Markov 
network, 
g(6.3a) 


p(x1;x2;x3;x4)= 
(x1;x4)(x2;x4)(x3;x4) 
(6.3.1) 


The 
clique 
graph 
of 
this 
singly-connected 
Markov 
network 
is 
multiply-connected, 
g(6.3b), 
where 
the 
separator 
potentials 
are 
all 
set 
to 
unity. 
Nevertheless, 
let's 
try 
to 
reexpress 
the 
Markov 
network 
in 
terms 
of 
marginals. 
First 
we 
have 
the 
relations 


XXX

p(x1;x4)=p(x1;x2;x3;x4)= 
(x1;x4)(x2;x4)(x3;x4) 
(6.3.2) 


x2;x3 
x2 
x3 


XXX

p(x2;x4)=p(x1;x2;x3;x4)= 
(x2;x4)(x1;x4)(x3;x4) 
(6.3.3) 


x1;x3 
x1 
x3 


XXX

p(x3;x4)=p(x1;x2;x3;x4)= 
(x3;x4)(x1;x4)(x2;x4) 
(6.3.4) 


x1;x2 
x1 
x2 


Taking 
the 
product 
of 
the 
three 
marginals, 
we 
have 


 !2

XXX

p(x1;x4)p(x2;x4)p(x3;x4)= 
(x1;x4)(x2;x4)(x3;x4) 
(x1;x4)(x2;x4)(x3;x4) 


x1 
x2 
x3

|{z}X

p(x4)2 


(6.3.5) 
This 
means 
that 
the 
Markov 
network 
can 
be 
expressed 
in 
terms 
of 
marginals 
as 


p(x1;x4)p(x2;x4)p(x3;x4) 


p(x1;x2;x3;x4) 
= 
(6.3.6) 


p(x4)p(x4) 


Hence 
a 
valid 
clique 
graph 
is 
also 
given 
by 
the 
representation 
g(6.3c). 
Indeed, 
if 
a 
variable 
(here 
x4) 
occurs 
on 
every 
separator 
in 
a 
clique 
graph 
loop, 
one 
can 
remove 
that 
variable 
from 
an 
arbitrarily 
chosen 
separator 
in 
the 
loop. 
This 
leaves 
an 
empty 
separator, 
which 
we 
can 
simply 
remove. 
This 
shows 
that 
in 
such 
cases 
we 
can 
transform 
the 
clique 
graph 
into 
a 
clique 
tree 
(i.e. 
a 
singly-connected 
clique 
graph). 
Provided 
that 
the 
original 
Markov 
network 
is 
singly-connected, 
one 
can 
always 
form 
a 
clique 
tree 
in 
this 
manner. 


6.3.1 
The 
running 
intersection 
property 
Sticking 
with 
the 
above 
example, 
consider 
the 
clique 
tree 
in 
g(6.3) 


(x3;x4)(x1;x4)(x2;x4) 


(6.3.7)
1(x4)2(x4) 


DRAFT 
March 
9, 
2010 
89 



Junction 
Trees 


as 
a 
representation 
of 
the 
distribution 
(6.3.1) 
where 
we 
set 
1(x4)= 
2(x4) 
= 
1 
to 
make 
this 
match. 
Now 
perform 
absorption 
on 
this 
clique 
tree: 


We 
absorb 
(x3, 
x4) 
' 
(x1, 
x4). 
The 
new 
separator 
is 
Xf 
* 
1(x4) 
=(x3, 
x4) 
(6.3.8) 
x3 
and 
the 
new 
potential 
is 
f 
(x1, 
x4) 
= 
(x1, 
x4)* 
1(x4) 
1(x4) 
= 
(x1, 
x4)f 
* 
1(x4) 
(6.3.9) 
Now 
(x1, 
x4) 
' 
(x2, 
x4). 
The 
new 
separator 
is 
Xf 
* 
2(x4) 
=f 
(x1, 
x4) 
(6.3.10) 
x1 
and 
the 
new 
potential 
is 
f 
(x2, 
x4) 
= 
(x2, 
x4)* 
2(x4) 
2(x4) 
= 
(x2, 
x4)f 
* 
2(x4) 
(6.3.11) 


Since 
we've 
`hit 
the 
buers’ 
in 
terms 
of 
message 
passing, 
the 
potential 
(x2;x4) 
cannot 
be 
updated 
further. 
Let's 
examine 
more 
carefully 
the 
value 
of 
this 
new 
potential, 


X

f 
(x2;x4)= 
(x2;x4)f 
(x4)= 
(x2;x4)f 
(x1;x4) 
(6.3.12)

2

x1

XXX

= 
(x2;x4)(x1;x4)(x3;x4)=p(x1;x2;x3;x4)= 
p(x2;x4) 
(6.3.13) 


x1 
x3 
x1;x3 


Hence 
the 
new 
potential 
(x2;x4) 
contains 
the 
marginal 
p(x2;x4). 


To 
complete 
a 
full 
round 
of 
message 
passing 
we 
need 
to 
have 
passed 
messages 
in 
a 
valid 
schedule 
along 
both 
directions 
of 
each 
separator. 
To 
do 
so, 
we 
continue 
as 
follows: 


We 
absorb 
(x2;x4) 
' 
(x1;x4). 
The 
new 
separator 
is 


X

f 


(x4)=f 
(x2;x4) 
(6.3.14)

2 


x2 


and 




(x4)

2

f 
(x1;x4)= 
f 
(x1;x4)(6.3.15)

(x4)

2

PP

Note 
that 
(x4)=(x2;x4)=p(x2;x4)= 
p(x4) 
so 
that 
now, 
after 
absorbing 
through 
both 


2 
x2 
x2 
directions, 
the 
separator 
contains 
the 
marginal 
p(x4). 
The 
reader 
may 
show 
that 
(x1;x4)= 
p(x1;x4). 


Finally, 
we 
absorb 
(x1;x4) 
' 
(x3;x4). 
The 
new 
separator 
is 


X

f 


(x4)=f 
(x1;x4)= 
p(x4) 
(6.3.16)

1 


x1 


and 




(x4)

1

f 
(x3;x4)= 
(x3;x4)= 
p(x3;x4) 
(6.3.17)

(x4)

1

Hence, 
after 
a 
full 
round 
of 
message 
passing, 
the 
new 
potentials 
all 
contain 
the 
correct 
marginals. 


DRAFT 
March 
9, 
2010 



Junction 
Trees 


The 
new 
representation 
is 
consistent 
in 
the 
sense 
that 
for 
any 
(not 
necessarily 
neighbouring) 
cliques 
V 
and 
W 
with 
intersection 
I, 
and 
corresponding 
potentials 
. 
(V) 
and 
. 
(W),

XX

. 
(V)=. 
(W) 
(6.3.18) 


VnI 
WnI 


Note 
that 
bidirectional 
absorption 
guarantees 
consistency 
for 
neighbouring 
cliques, 
as 
in 
the 
example 
above, 
provided 
that 
we 
started 
with 
a 
clique 
tree 
which 
is 
a 
correct 
representation 
of 
the 
distribution. 


In 
general, 
the 
only 
possible 
source 
of 
non-consistency 
is 
if 
a 
variable 
occurs 
in 
two 
non-neighbouring 
cliques 
and 
is 
not 
present 
in 
all 
cliques 
on 
any 
path 
connection 
them. 
An 
extreme 
example 
would 
be 
if 
we 
removed 
the 
link 
between 
cliques 
(x3;x4) 
and 
(x1;x4). 
In 
this 
case 
this 
is 
still 
a 
Clique 
Tree; 
however 
global 
consistency 
could 
not 
be 
guaranteed 
since 
the 
information 
required 
to 
make 
clique 
(x3;x4) 
consistent 
with 
the 
rest 
of 
the 
graph 
cannot 
reach 
this 
clique. 


Formally, 
the 
requirement 
for 
the 
propagation 
of 
local 
to 
global 
consistency 
is 
that 
the 
clique 
tree 
is 
a 
junction 
tree, 
as 
dened 
below. 


Denition 
42 
(Junction 
Tree). 
A 
Clique 
Tree 
is 
a 
Junction 
Tree 
if, 
for 
each 
pair 
of 
nodes, 
V 
and 
W, 
all 
nodes 
on 
the 
path 
between 
V 
and 
W 
contain 
the 
intersection 
V\W. 
This 
is 
also 
called 
the 
running 
intersection 
property. 


From 
this 
denition 
local 
consistency 
will 
be 
passed 
on 
to 
any 
neighbours 
and 
the 
distribution 
will 
be 
globally 
consistent. 
Proofs 
for 
these 
results 
are 
contained 
in 
[148]. 


Example 
22 
(A 
consistent 
Junction 
Tree). 
To 
gain 
some 
intuition 
about 
the 
meaning 
of 
consistency, 
consider 
the 
junction 
tree 
in 
g(6.4d). 
After 
a 
full 
round 
of 
message 
passing 
on 
this 
tree, 
each 
link 
is 
consistent, 
and 
the 
product 
of 
the 
potentials 
divided 
by 
the 
product 
of 
the 
separator 
potentials 
is 
just 
the 
original 
distribution 
itself. 
Imagine 
that 
we 
are 
interested 
in 
calculating 
the 
marginal 
for 
the 
node 
abc. 
That 
requires 
summing 
over 
all 
the 
other 
variables, 
defgh. 
If 
we 
consider 
summing 
over 
h 
then, 
because 
the 
link 
is 
consistent,

X

. 
* 
(e, 
h)= 
. 
* 
(e) 
(6.3.19) 


h 


P

 (e;h)

so 
that 
the 
ratiois 
unity, 
and 
the 
eect 
of 
summing 
over 
node 
h 
is 
that 
the 
link 
between 
eh

h (e)

and 
dce 
can 
be 
removed, 
along 
with 
the 
separator. 
The 
same 
happens 
for 
the 
link 
between 
node 
eg 
and 
dce, 
and 
also 
for 
cf 
to 
abc. 
The 
only 
nodes 
remaining 
are 
now 
dce 
and 
abc 
and 
their 
separator 
c, 
which 
have 
so 
far 
been 
unaected 
by 
the 
summations. 
We 
still 
need 
to 
sum 
out 
over 
d 
and 
e. 
Again, 
because 
the 
link 
is 
consistent,

X

. 
* 
(d, 
c, 
e)= 
. 
* 
(c) 
(6.3.20) 


de 


P

 (d;c;e)

so 
that 
the 
ratio= 
1. 
The 
result 
of 
the 
summation 
of 
all 
variables 
not 
in 
abc 
therefore

de 
 (c)

produces 
unity 
for 
the 
cliques 
and 
their 
separators, 
and 
the 
summed 
potential 
representation 
reduces 
simply 
to 
the 
potential 
 * 
(a, 
b, 
c) 
which 
is 
the 
marginal 
p(a, 
b, 
c). 
It 
is 
clear 
that 
a 
similar 
eect 
will 
happen 
for 
other 
nodes. 
We 
can 
then 
obtain 
the 
marginals 
for 
individual 
variables 
by 
simple 
brute 
force 


P

summation 
over 
the 
other 
variables 
in 
that 
potential, 
for 
example 
p(f)= * 
(c, 
f).

c 


DRAFT 
March 
9, 
2010 



Constructing 
a 
Junction 
Tree 
for 
Singly-Connected 
Distributions 


6.4 
Constructing 
a 
Junction 
Tree 
for 
Singly-Connected 
Distributions 
6.4.1 
Moralisation 
For 
Belief 
Networks, 
an 
initial 
step 
is 
required, 
which 
is 
not 
required 
in 
the 
case 
of 
undirected 
graphs. 


Denition 
43 
(Moralisation). 
For 
each 
variable 
x 
add 
an 
undirected 
link 
between 
all 
parents 
of 
x 
and 
replace 
the 
directed 
link 
from 
x 
to 
its 
parents 
by 
undirected 
links. 
This 
creates 
a 
`moralised’ 
Markov 
network. 


6.4.2 
Forming 
the 
clique 
graph 
The 
clique 
graph 
is 
formed 
by 
identifying 
the 
cliques 
in 
the 
Markov 
network 
and 
adding 
a 
link 
between 
cliques 
that 
have 
a 
non-empty 
intersection. 
Add 
a 
separator 
between 
the 
intersecting 
cliques. 


6.4.3 
Forming 
a 
junction 
tree 
from 
a 
clique 
graph 
For 
a 
singly-connected 
distribution, 
any 
maximal 
weight 
spanning 
tree 
of 
a 
clique 
graph 
is 
a 
junction 
tree. 


Denition 
44 
(Junction 
Tree). 
A 
junction 
tree 
is 
obtained 
by 
nding 
a 
maximal 
weight 
spanning 
tree 
of 
the 
clique 
graph. 
The 
weight 
of 
the 
tree 
is 
dened 
as 
the 
sum 
of 
all 
the 
separator 
weights 
of 
the 
tree, 
where 
the 
separator 
weight 
is 
the 
number 
of 
variables 
in 
the 
separator. 


If 
the 
clique 
graph 
contains 
loops, 
then 
all 
separators 
on 
the 
loop 
contain 
the 
same 
variable. 
By 
continuing 
to 
remove 
loop 
links 
until 
you 
have 
a 
tree 
is 
revealed, 
we 
obtain 
a 
junction 
tree. 


Example 
23 
(Forming 
a 
Junction 
Tree). 
Consider 
the 
Belief 
Network 
in 
g(6.4a). 
The 
moralisation 
procedure 
gives 
g(6.4b). 
Identifying 
the 
cliques 
in 
this 
graph, 
and 
linking 
them 
together 
gives 
the 
clique 
graph 
in 
g(6.4c). 
There 
are 
several 
possible 
junction 
trees 
one 
could 
obtain 
from 
this 
clique 
graph, 
and 
one 
is 
given 
in 
g(6.4d). 


6.4.4 
Assigning 
potentials 
to 
cliques 
Denition 
45 
(Clique 
Potential 
Assignment). 
Given 
a 
junction 
tree 
and 
a 
function 
dened 
as 
the 
product 


..e 


of 
a 
set 
of 
potentials 
 X 
1 
;:::;. 
(X 
n), 
a 
valid 
clique 
potential 
assignment 
places 
potentials 
in 
JT 
cliques 
whose 
variables 
can 
contain 
them 
such 
that 
the 
product 
of 
the 
JT 
clique 
potentials, 
divided 
by 
the 
JT 
separator 
potentials, 
is 
equal 
to 
the 
function. 


A 
simple 
way 
to 
achieve 
this 
assignment 
is 
to 
list 
all 
the 
potentials 
and 
order 
the 
JT 
cliques 
arbitrarily. 
Then, 
for 
each 
potential, 
search 
through 
the 
JT 
cliques 
until 
the 
rst 
is 
encountered 
for 
which 
the 
potential 
variables 
are 
a 
subset 
of 
the 
JT 
clique 
variables. 
Subsequently 
the 
potential 
on 
each 
JT 
clique 
is 
taken 
as 
the 
product 
of 
all 
clique 
potentials 
assigned 
to 
the 
JT 
clique. 
Lastly, 
we 
assign 
all 
JT 
separators 
to 
unity. 
This 
approach 
is 
taken 
in 
jtassignpot.m. 


DRAFT 
March 
9, 
2010 



Junction 
Trees 
for 
Multiply-Connected 
Distributions 


c
ab
d
ef
gh
c
ab
d
ef
gh
(a) 
(b) 
dce
abc
cf
egeh
ccceee
dce
abc
cf
egeh
ccee
(c) 
(d) 
Figure 
6.4: 
(a): 
Belief 
Network. 
(b): 
Moralised 
version 
of 
(a). 
(c): 
Clique 
Graph 
of 
(b). 
(d):A 
junction 
tree. 
This 
satises 
the 
running 
intersection 
property 
that 
for 
any 
two 
nodes 
which 
contain 
a 
variable 
in 
common, 
any 
clique 
on 
the 
path 
linking 
the 
two 
nodes 
also 
contains 
that 
variable. 


Example 
24. 
For 
the 
belief 
network 
of 
g(6.4a), 
we 
wish 
to 
assign 
its 
potentials 
to 
the 
junction 
tree 
g(6.4d). 
In 
this 
case 
the 
assignment 
is 
unique 
and 
is 
given 
by 


. 
(abc)= 
p(a)p(b)p(cja, 
b) 
. 
(dce)= 
p(d)p(ejd, 
c) 
. 
(cf)= 
p(fjc) 
(6.4.1) 
. 
(eg)= 
p(gje) 
. 
(eh)= 
p(hje) 


All 
separator 
potentials 
are 
initialised 
to 
unity. 
Note 
that 
in 
some 
instances 
it 
can 
be 
that 
a 
junction 
tree 
clique 
is 
assigned 
to 
unity. 


6.5 
Junction 
Trees 
for 
Multiply-Connected 
Distributions 
When 
the 
distribution 
contains 
loops, 
the 
construction 
outlined 
in 
section(6.4) 
does 
not 
result 
in 
a 
junction 
tree. 
The 
reason 
is 
that, 
due 
to 
the 
loops, 
variable 
elimination 
changes 
the 
structure 
of 
the 
remaining 
graph. 
To 
see 
this, 
consider 
the 
following 
distribution, 


p(a, 
b, 
c, 
d)= 
(a, 
b)(b, 
c)(c, 
d)(d, 
a) 
(6.5.1) 


as 
shown 
in 
g(6.5a). 
Let's 
rst 
try 
to 
make 
a 
clique 
graph. 
We 
have 
a 
choice 
about 
which 
variable 
rst 
to 
marginalise 
over. 
Let's 
choose 
d: 


X

p(a, 
b, 
c)= 
(a, 
b)(b, 
c)(c, 
d)(d, 
a) 
(6.5.2) 


d 


DRAFT 
March 
9, 
2010 



Junction 
Trees 
for 
Multiply-Connected 
Distributions 


ab
cd
ab
c
ab
cd
ab
cdabcacdac
(a) 
(b) 
(c) 
(d) 
(e) 
Figure 
6.5: 
(a): 
An 
undirected 
graph 
with 
a 
loop. 
(b): 
Eliminating 
node 
d 
adds 
a 
link 
between 
a 
and 
c 
in 
the 
subgraph. 
(c): 
The 
induced 
representation 
for 
the 
graph 
in 
(a). 
(d): 
Equivalent 
induced 
representation. 
(e): 
Junction 
tree 
for 
(a). 


The 
remaining 
subgraph 
therefore 
has 
an 
extra 
connection 
between 
a 
and 
c, 
see 
g(6.5b). 
We 
can 
express 
the 
joint 
in 
terms 
of 
the 
marginals 
using 


p(a, 
b, 
c)

p(a, 
b, 
c, 
d)= 
P(c, 
d)(d, 
a) 
(6.5.3)

(c, 
d)(d, 
a)

d 


To 
continue 
the 
transformation 
into 
marginal 
form, 
let's 
try 
to 
replace 
the 
numerator 
terms 
with 
proba


bilities. 
We 
can 
do 
this 
by 
considering 
Xp(a, 
c, 
d) 
= 
(c, 
d)(d, 
a)(a, 
b)(b, 
c) 
(6.5.4) 
b 
Plugging 
this 
into 
the 
above 
equation, 
we 
have 
p(a, 
b, 
c, 
d) 
= 
p(a, 
b, 
c)p(a, 
c, 
d)
Pd 
(c, 
d)(d, 
a)Pb 
(a, 
b)(b, 
c) 
(6.5.5) 
We 
recognise 
that 
the 
denominator 
is 
simply 
p(a, 
c), 
hence 
p(a, 
b, 
c, 
d) 
= 
p(a, 
b, 
c)p(a, 
c, 
d) 
p(a, 
c) 
. 
(6.5.6) 


This 
means 
that 
a 
valid 
clique 
graph 
for 
the 
distribution 
g(6.5a) 
must 
contain 
cliques 
larger 
than 
those 
in 
the 
original 
distribution. 
To 
form 
a 
JT 
based 
on 
products 
of 
cliques 
divided 
by 
products 
of 
separators, 
we 
could 
start 
from 
the 
induced 
representation 
g(6.5c). 
Alternatively, 
we 
could 
have 
marginalised 
over 
variables 
a 
and 
c, 
and 
ended 
up 
with 
the 
equivalent 
representation 
g(6.5d). 


Generally, 
the 
result 
from 
variable 
elimination 
and 
re-representation 
in 
terms 
of 
the 
induced 
graph 
is 
that 
a 
link 
is 
added 
between 
any 
two 
variables 
on 
a 
loop 
(of 
length 
4 
or 
more) 
which 
does 
not 
have 
a 
chord. 
This 
is 
called 
triangulation. 
A 
Markov 
network 
on 
a 
triangulated 
graph 
can 
always 
be 
written 
in 
terms 
of 
the 
product 
of 
marginals 
divided 
by 
the 
product 
of 
separators. 
Armed 
with 
this 
new 
induced 
representation, 
we 
can 
form 
a 
junction 
tree. 


abc
def
abc
def
Figure 
6.6: 
(a): 
Loopy 
`ladder’ 
Markov 
network. 
(b): 
Induced 
representation. 


(a) 
(b) 
DRAFT 
March 
9, 
2010 



Junction 
Trees 
for 
Multiply-Connected 
Distributions 


abcde
fghi
jk
l
abcde
fghi
jk
l
abcde
fghi
jk
l
(a) 
(b) 
(c) 
abcde
fghi
jk
l
abcde
fghi
jk
l
abcde
fghi
jk
l
abcde
fghi
jk
l
(d) 
(e) 
(f) 
Figure 
6.7: 
(a): 
Markov 
network 
for 
which 
we 
seek 
a 
triangulation 
via 
greedy 
variable 
elimination. 
We 
rst 
eliminate 
the 
simplical 
nodes 
a, 
e, 
l. 
(b): 
We 
then 
eliminate 
variables 
b, 
d 
since 
these 
only 
add 
a 
single 
extra 
link 
to 
the 
induced 
graph. 
(c): 
f 
and 
i 
are 
now 
simplical 
and 
are 
eliminated. 
(d): 
We 
eliminate 
g 
and 
h 
since 
this 
adds 
only 
single 
extra 
links. 
(e): 
The 
remaining 
variables 
fc, 
j, 
kgmay 
be 
eliminated 
in 
any 
order. 
(f): 
Final 
triangulation. 
The 
variable 
elimination 
(partial) 
order 
is 
fa, 
e, 
l} 
, 
fb, 
d} 
, 
ff, 
i} 
, 
fg, 
h} 
, 
fc, 
j, 
k} 
where 
the 
brackets 
indicate 
that 
the 
order 
in 
which 
the 
variables 
inside 
the 
bracket 
are 
eliminated 
is 
irrelevant. 
Compared 
with 
the 
triangulation 
produced 
by 
the 
max-
cardinality 
checking 
approach 
in 
g(6.9d), 
this 
triangulation 
is 
more 
parsimonious. 


Example 
25. 
A 
slightly 
more 
complex 
loopy 
distribution 
is 
depicted 
in 
g(6.6a), 


p(a, 
b, 
c, 
d, 
e, 
f)= 
(a, 
b)(b, 
c)(c, 
d)(d, 
e)(e, 
f)(a, 
f)(b, 
e) 
(6.5.7) 


There 
are 
dierent 
induced 
representations 
depending 
on 
which 
variables 
we 
decide 
to 
eliminate. 
The 
reader 
may 
convince 
herself 
that 
one 
such 
induced 
representation 
is 
given 
by 
g(6.6b). 


Denition 
46 
(Chord). 
This 
is 
a 
link 
joining 
two 
non-consecutive 
vertices 
of 
a 
loop. 


Denition 
47 
(Triangulated 
(Decomposable) 
Graph). 
An 
undirected 
graph 
is 
triangulated 
if 
every 
loop 
of 
length 
4 
or 
more 
has 
a 
chord. 
An 
equivalent 
term 
is 
that 
the 
graph 
is 
decomposable 
or 
chordal. 
An 
undirected 
graph 
is 
triangulated 
if 
and 
only 
if 
its 
clique 
graph 
has 
a 
junction 
tree. 


6.5.1 
Triangulation 
algorithms 
When 
a 
variable 
is 
eliminated 
from 
a 
graph, 
links 
are 
added 
between 
all 
the 
neighbours 
of 
the 
eliminated 
variable. 
A 
triangulation 
algorithm 
is 
one 
that 
produces 
a 
graph 
for 
which 
there 
exists 
a 
variable 
elimination 
order 
that 
introduces 
no 
extra 
links 
in 
the 
graph. 


DRAFT 
March 
9, 
2010 



Junction 
Trees 
for 
Multiply-Connected 
Distributions 


abfbcfgcdhideibfdi
cfgjchik
cjk
jkl
cfgchi
cjck
jk
Figure6.8:Junctiontreeformedfromthetriangulation
g(6.7)f.Oneverifythatthissatisestherunningintersectionproperty.
For 
discrete 
variables 
the 
complexity 
of 
inference 
scales 
exponentially 
with 
clique 
sizes 
in 
the 
triangulated 
graph 
since 
absorption 
requires 
computing 
tables 
on 
the 
cliques. 
It 
is 
therefore 
of 
some 
interest 
to 
nd 
a 
triangulated 
graph 
with 
small 
clique 
sizes. 
However, 
nding 
the 
triangulated 
graph 
with 
the 
smallest 
maximal 
clique 
is 
an 
NP-hard 
problem 
for 
a 
general 
graph, 
and 
heuristics 
are 
unavoidable. 
Below 
we 
describe 
two 
simple 
algorithms 
that 
are 
generically 
reasonable, 
although 
there 
may 
be 
cases 
where 
an 
alternative 
algorithm 
may 
be 
considerably 
more 
ecient[53, 
28, 
191]. 


Remark 
8 
(Triangulation 
does 
not 
mean 
putting 
`triangles’ 
on 
the 
original 
graph). 
Note 
that 
a 
triangulated 
graph 
is 
not 
one 
in 
which 
`squares 
in 
the 
original 
graph 
have 
triangles 
within 
them 
in 
the 
triangulated 
graph'. 
Whilst 
this 
is 
the 
case 
for 
g(6.6b), 
this 
is 
not 
true 
for 
g(6.9d). 
The 
term 
triangulation 
refers 
to 
the 
fact 
that 
every 
`square’ 
(i.e. 
loop 
of 
length 
4) 
must 
have 
a 
`triangle', 
with 
edges 
added 
until 
this 
criterion 
is 
satised. 


Greedy 
variable 
elimination 


An 
intuitive 
way 
to 
think 
of 
triangulation 
is 
to 
rst 
start 
with 
simplical 
nodes, 
namely 
those 
which, 
when 
eliminated 
do 
not 
introduce 
any 
extra 
links 
in 
the 
remaining 
graph. 
Next 
consider 
a 
non-simplical 
node 
of 
the 
remaining 
graph 
that 
has 
the 
minimal 
number 
of 
neighbours. 
Then 
add 
a 
link 
between 
all 
neighbours 
of 
this 
node 
and 
nally 
eliminate 
this 
node 
from 
the 
graph. 
Continue 
until 
all 
nodes 
have 
been 
eliminated. 
(This 
procedure 
corresponds 
to 
Rose-Tarjan 
Elimination[233] 
with 
a 
particular 
node 
elimination 
choice). 
By 
labelling 
the 
nodes 
eliminated 
in 
sequence, 
we 
obtain 
a 
perfect 
ordering 
(see 
below) 
in 
reverse. 
In 
the 
case 
that 
(discrete) 
variables 
have 
dierent 
numbers 
of 
states, 
a 
more 
rened 
version 
is 
to 
choose 
the 
non-simplical 
node 
i 
which, 
when 
eliminated, 
leaves 
the 
smallest 
clique 
table 
size 
(the 
product 
of 
the 
size 
of 
all 
the 
state 
dimensions 
of 
the 
neighbours 
of 
node 
i). 
See 
g(6.7) 
for 
an 
example. 


Denition 
48 
(Variable 
Elimination). 
In 
Variable 
Elimination, 
one 
simply 
picks 
any 
non-deleted 
node 
x 
in 
the 
graph, 
and 
then 
adds 
links 
to 
all 
the 
neighbours 
of 
x. 
Node 
x 
is 
then 
deleted. 
One 
repeats 
this 
until 
all 
nodes 
have 
been 
deleted[233]. 


Whilst 
this 
procedure 
guarantees 
a 
triangulated 
graph, 
its 
eciency 
depends 
heavily 
on 
the 
sequence 
of 
nodes 
chosen 
to 
be 
eliminated. 
Several 
heuristics 
for 
this 
have 
been 
proposed, 
including 
the 
one 
below, 
which 
corresponds 
to 
choosing 
x 
to 
be 
the 
node 
with 
the 
minimal 
number 
of 
neighbours. 


Maximum 
cardinality 
checking 


Algorithm(2) 
terminates 
with 
success 
if 
the 
graph 
is 
triangulated. 
Not 
only 
is 
this 
a 
sucient 
condition 
for 
a 
graph 
to 
be 
triangulated, 
but 
is 
also 
necessary 
[271]. 
It 
processes 
each 
node 
and 
the 
time 
to 
process 
a 
node 
is 
quadratic 
in 
the 
number 
of 
adjacent 
nodes. 
This 
triangulation 
checking 
algorithm 
also 
suggests 


DRAFT 
March 
9, 
2010 



The 
Junction 
Tree 
Algorithm 


125710348961112571034896111257103489611
(a) 
(b) 
(c) 
(d) 
Figure 
6.9: 
Starting 
with 
the 
Markov 
network 
in 
(a), 
the 
maximum 
cardinality 
check 
algorithm 
proceeds 
until 
(b). 
where 
an 
additional 
link 
is 
required 
(c). 
One 
continues 
the 
algorithm 
until 
the 
fully 
triangulated 
graph 
(d) 
is 
found. 


a 
triangulation 
construction 
algorithm 
– 
we 
simply 
add 
a 
link 
between 
the 
two 
neighbours 
that 
caused 
the 
algorithm 
to 
FAIL, 
and 
then 
restart 
the 
algorithm. 
The 
algorithm 
is 
restarted 
from 
the 
beginning, 
not 
just 
continued 
from 
the 
current 
node. 
This 
is 
important 
since 
the 
new 
link 
may 
change 
the 
connectivity 
between 
previously 
labelled 
nodes. 
See 
g(6.9) 
for 
an 
example1 
. 


Denition 
49 
(Perfect 
Elimination 
Order). 
Let 
the 
n 
variables 
in 
a 
Markov 
network 
be 
ordered 
from 
1 
to 
n. 
The 
ordering 
is 
perfect 
if, 
for 
each 
node 
i, 
the 
neighbours 
of 
i 
that 
are 
later 
in 
the 
ordering, 
and 
i 
itself, 
form 
a 
(maximal) 
clique. 
This 
means 
that 
when 
we 
eliminate 
the 
variables 
in 
sequence 
from 
1 
to 
n, 
no 
additional 
links 
are 
induced 
in 
the 
remaining 
marginal 
graph. 
A 
graph 
which 
admits 
a 
perfect 
elimination 
order 
is 
decomposable, 
and 
vice 
versa. 


Algorithm 
2 
A 
check 
if 
a 
graph 
is 
decomposable 
(triangulated). 
The 
graph 
is 
triangulated 
if, 
after 
cycling 
through 
all 
the 
n 
nodes 
in 
the 
graph, 
the 
FAIL 
criterion 
is 
not 
encountered. 


1: 
Choose 
any 
node 
in 
the 
graph 
and 
label 
it 
1. 
2: 
for 
i 
= 
2 
to 
n 
do 
3: 
Choose 
the 
node 
with 
the 
most 
labeled 
neighbours 
and 
label 
it 
i. 
4: 
If 
any 
two 
labeled 
neighbours 
of 
i 
are 
not 
adjacent 
to 
each 
other, 
FAIL. 
5: 
end 
for 


Where 
there 
is 
more 
than 
one 
node 
with 
the 
most 
labeled 
neighbours, 
the 
tie 
may 
be 
broken 
arbitrarily. 


6.6 
The 
Junction 
Tree 
Algorithm 
We 
now 
have 
all 
the 
steps 
required 
for 
inference 
in 
multiply-connected 
graphs: 


Moralisation 
Marry 
the 
parents. 
This 
is 
required 
only 
for 
directed 
distributions. 


Triangulation 
Ensure 
that 
every 
loop 
of 
length 
4 
or 
more 
has 
a 
chord. 


Junction 
Tree 
Form 
a 
junction 
tree 
from 
cliques 
of 
the 
triangulated 
graph, 
removing 
any 
unnecessary 
links 
in 
a 
loop 
on 
the 
cluster 
graph. 
Algorithmically, 
this 
can 
be 
achieved 
by 
nding 
a 
tree 
with 
maximal 
spanning 
weight 
with 
weight 
wij 
given 
by 
the 
number 
of 
variables 
in 
the 
separator 
between 
cliques 
i 
and 
j. 
Alternatively, 
given 
a 
clique 
elimination 
order 
(with 
the 
lowest 
cliques 
eliminated 
rst), 
one 
may 
connect 
each 
clique 
i 
to 
the 
single 
neighbouring 
clique 
j>i 
with 
greatest 
edge 
weight 


wij. 


1This 
example 
is 
due 
to 
David 
Page 
www.cs.wisc.edu/dpage/cs731 


DRAFT 
March 
9, 
2010 
97 



The 
Junction 
Tree 
Algorithm 


a
a
bd
c
bd
efg
hi
(a)
Figure 
6.10: 
(a): 
Original 
loopy 
Belief 
Network. 
(b): 
The 
moralisation 
links 
(dashed) 
are 
between 
nodes 
e 
and 
f 
and 
between 
nodes 
f 
and 
g. 
The 
other 
additional 
links 
come 
from 
triangulation. 
The 
clique 
size 
of 
the 
resulting 
clique 
tree 
(not 
shown) 
is 
four. 
From 
[55]. 


c
efg
hi
(b) 
Potential 
Assignment 
Assign 
potentials 
to 
junction 
tree 
cliques 
and 
set 
the 
separator 
potentials 
to 
unity. 


Message 
Propagation 
Carry 
out 
absorption 
until 
updates 
have 
been 
passed 
along 
both 
directions 
of 
every 
link 
on 
the 
JT. 


The 
clique 
marginals 
can 
then 
be 
read 
off 
from 
the 
JT. 
An 
example 
is 
given 
in 
g(6.10). 


6.6.1 
Remarks 
on 
the 
JTA 
• 
The 
algorithm 
provides 
an 
upper 
bound 
on 
the 
computation 
required 
to 
calculate 
marginals 
in 
the 
graph. 
There 
may 
exist 
more 
ecient 
algorithms 
in 
particular 
cases, 
although 
generally 
it 
is 
believed 
that 
there 
cannot 
be 
much 
more 
ecient 
approaches 
than 
the 
JTA 
since 
every 
other 
approach 
must 
perform 
a 
triangulation[147, 
173]. 
One 
particular 
special 
case 
is 
that 
of 
marginal 
inference 
for 
a 
binary 
variable 
MRF 
on 
a 
two-dimensional 
lattice 
containing 
only 
pure 
quadratic 
interactions. 
In 
..r 


3

this 
case 
the 
complexity 
of 
computing 
a 
marginal 
inference 
is 
Onwhere 
n 
is 
the 
number 
of 
variables 
in 
the 
distribution. 
This 
is 
in 
contrast 
to 
the 
pessimistic 
exponential 
complexity 
suggested 
by 
the 
JTA. 
Such 
cases 
are 
highly 
specialised 
and 
it 
is 
unlikely 
that 
a 
general 
purpose 
algorithm 
that 
could 
consistently 
outperform 
the 
JTA 
exists. 


• 
One 
might 
think 
that 
the 
only 
class 
of 
distributions 
for 
which 
essentially 
a 
linear 
time 
algorithm 
is 
available 
are 
singly-connected 
distributions. 
However, 
there 
are 
decomposable 
graphs 
for 
which 
the 
cliques 
have 
limited 
size 
meaning 
that 
inference 
is 
tractable. 
For 
example 
an 
extended 
version 
of 
the 
`ladder’ 
in 
g(6.6a) 
has 
a 
simple 
induced 
decomposable 
representation 
g(6.6b), 
for 
which 
marginal 
inference 
would 
be 
linear 
in 
the 
number 
of 
rungs 
in 
the 
ladder. 
Eectively 
these 
structures 
are 
hyper 
trees 
in 
which 
the 
complexity 
is 
then 
related 
to 
the 
tree 
width 
of 
the 
graph[81]. 
• 
Ideally, 
we 
would 
like 
to 
nd 
a 
triangulated 
graph 
which 
has 
minimal 
clique 
size. 
However, 
it 
can 
be 
shown 
to 
be 
a 
hard-computation 
problem 
(NP 
-hard) 
to 
nd 
the 
most 
ecient 
triangulation. 
In 
practice, 
most 
general 
purpose 
triangulation 
algorithms 
are 
somewhat 
heuristic 
and 
chosen 
to 
provide 
reasonable, 
but 
clearly 
not 
optimal, 
generic 
performance. 
• 
Numerical 
over/under 
ow 
issues 
can 
occur 
in 
large 
cliques, 
where 
many 
probability 
values 
are 
multiplied 
together. 
Similarly 
in 
long 
chains 
since 
absorption 
will 
tend 
to 
reduce 
the 
numerical 
size 
of 
potential 
entries 
in 
a 
clique. 
If 
we 
only 
care 
about 
marginals 
we 
can 
avoid 
numerical 
diculties 
by 
normalising 
potentials 
at 
each 
step; 
these 
missing 
normalisation 
constants 
can 
always 
be 
found 
under 
the 
normalisation 
constraint. 
If 
required 
one 
can 
always 
store 
the 
values 
of 
these 
local 
renormalisations, 
should, 
for 
example, 
the 
global 
normalisation 
constant 
of 
a 
distribution 
be 
required, 
see 
section(6.6.2). 
• 
After 
clamping 
variables 
in 
evidential 
states, 
running 
the 
JTA 
returns 
the 
joint 
distribution 
on 
the 
non-evidential 
variables 
in 
a 
clique 
with 
all 
the 
evidential 
variables 
clamped 
in 
their 
evidential 
states. 
From 
this 
conditionals 
are 
straightforward 
to 
calculate. 
• 
Imagine 
that 
we 
have 
run 
the 
JT 
algorithm 
and 
want 
to 
afterwards 
nd 
the 
marginal 
p(Xjevidence). 
We 
could 
do 
so 
by 
clamping 
the 
evidential 
variables. 
However, 
if 
both 
X 
and 
the 
set 
of 
evidential 
98 
DRAFT 
March 
9, 
2010 



The 
Junction 
Tree 
Algorithm 


variables 
are 
all 
contained 
within 
a 
single 
clique 
of 
the 
JT, 
then 
we 
may 
use 
the 
consistent 
JT 
cliques 
to 
compute 
p(Xjevidence). 
The 
reason 
is 
that 
since 
the 
JT 
clique 
contains 
the 
marginal 
on 
the 
set 
of 
variables 
which 
includes 
X 
and 
the 
evidential 
variables, 
we 
can 
obtain 
the 
required 
marginal 
by 
considering 
the 
single 
JT 
clique 
alone. 


• 
Representing 
the 
marginal 
distribution 
of 
a 
set 
of 
variables 
X 
which 
are 
not 
contained 
within 
a 
single 
clique 
is 
in 
general 
computationally 
dicult. 
Whilst 
the 
probability 
of 
any 
state 
of 
p(X 
) 
may 
be 
computed 
eciently, 
there 
are 
in 
general 
an 
exponential 
number 
of 
such 
states. 
A 
classical 
example 
in 
this 
regard 
is 
the 
HMM, 
section(23.2) 
with 
singly-connected 
joint 
distribution 
p(V, 
H). 
However 
the 
marginal 
distribution 
p(H) 
is 
fully 
connected. 
This 
means 
that 
for 
example 
whilst 
the 
entropy 
of 
p(V, 
H) 
is 
straightforward 
to 
compute, 
the 
entropy 
of 
the 
marginal 
p(H) 
is 
intractable. 
6.6.2 
Computing 
the 
normalisation 
constant 
of 
a 
distribution 
For 
a 
Markov 
network 


Y

1 


p(X 
)= 
(Xi) 
(6.6.1)

Z

i 


Q

how 
can 
we 
nd 
Z 
eciently? 
If 
we 
used 
the 
JTA 
on 
the 
unnormalised 
distribution(Xi), 
we 
would 


i 


have 
the 
equivalent 
representation: 


Q

1 
(XC)

C 


p(X 
)= 
(6.6.2)

Z(XS)

S 


Since 
the 
distribution 
must 
normalise, 
we 
can 
obtain 
Z 
from 


Q

X

(XC)

C

Z 
= 
(6.6.3)

(XS)

xS 


For 
a 
consistent 
JT, 
summing 
rst 
over 
the 
variables 
of 
a 
simplical 
JT 
clique 
(not 
including 
the 
separator 
variables), 
the 
marginal 
clique 
will 
cancel 
with 
the 
corresponding 
separator 
to 
give 
a 
unity 
term 
so 
that 
the 
clique 
and 
separator 
can 
be 
removed. 
This 
forms 
a 
new 
JT 
for 
which 
we 
then 
eliminate 
another 
simplical 
clique. 
Continuing 
in 
this 
manner 
we 
will 
be 
left 
with 
a 
single 
numerator 
potential 
so 
that 


X

Z 
=(XC 
) 
(6.6.4) 


XC 


This 
is 
true 
for 
any 
clique 
C, 
so 
it 
makes 
sense 
to 
choose 
one 
with 
a 
small 
number 
of 
states 
so 
that 
the 
resulting 
raw 
summation 
is 
ecient. 
Hence 
in 
order 
to 
compute 
the 
normalisation 
constant 
of 
a 
distribution 
one 
runs 
the 
JT 
algorithm 
on 
an 
unnormalised 
distribution 
and 
the 
global 
normalisation 
is 
then 
given 
by 
the 
local 
normalisation 
of 
any 
clique. 
Note 
that 
if 
the 
graph 
is 
disconnected 
(there 
are 
isolated 
cliques), 
the 
normalisation 
is 
the 
product 
of 
the 
connected 
component 
normalisation 
constants. 
A 
computationally 
convenient 
way 
to 
nd 
this 
is 
to 
compute 
the 
product 
of 
all 
clique 
normalisations 
divided 
by 
the 
product 
of 
all 
separator 
normalisations. 


6.6.3 
The 
marginal 
likelihood 
Our 
interest 
here 
is 
the 
computation 
of 
p(V) 
where 
V 
is 
a 
subset 
of 
the 
full 
variable 
set. 
Naively, 
one 
could 
carry 
out 
this 
computation 
by 
summing 
over 
all 
the 
non-evidential 
variables 
(hidden 
variables 
H 
= 
X 
nV) 
explicitly. 
In 
cases 
where 
this 
is 
computationally 
impractical 
an 
alternative 
is 
to 
use 


p(V, 
H) 


p(HjV) 
= 
(6.6.5) 


p(V) 


One 
can 
view 
this 
as 
a 
product 
of 
clique 
potentials 
divided 
by 
the 
normalisation 
p(V), 
for 
which 
the 
general 
method 
of 
section(6.6.2) 
may 
be 
directly 
applied. 
See 
demoJTree.m. 


DRAFT 
March 
9, 
2010 
99 



The 
Junction 
Tree 
Algorithm 


Example 
26 
(A 
simple 
example 
of 
the 
JTA). 


Consider 
running 
the 
JTA 
on 
the 
simple 
graph 


p(a, 
b, 
c)= 
p(ajb)p(bjc)p(c) 
(6.6.6) 
The 
moralisation 
and 
triangulation 
steps 
are 
trivial, 
and 
the 
JTA 
is 
given 
immediately 
by 
the 
gure 
on 
the 
right. 
A 
valid 
assignment 
is 


abc
abbcb
. 
(a, 
b)= 
p(ajb), 
. 
(b) 
= 
1, 
. 
(b, 
c)= 
p(bjc)p(c) 
(6.6.7) 


To 
nd 
a 
marginal 
p(b) 
we 
rst 
run 
the 
JTA: 


PP

• 
Absorbing 
from 
ab 
through 
b, 
the 
new 
separator 
is 
 * 
(b)=. 
(a, 
b)=p(ajb) 
= 
1. 
aa 


• 
The 
new 
potential 
on 
(b, 
c) 
is 
given 
by 
. 
(b, 
c) * 
(b) 
p(bjc)p(c) 
× 
1 


. 
* 
(b, 
c) 
= 
= 
(6.6.8)

. 
(b)1 


• 
Absorbing 
from 
bc 
through 
b, 
the 
new 
separator 
is 
XX

. 
* 
(b)=. 
* 
(b, 
c)=p(bjc)p(c) 
(6.6.9) 


cc 


• 
The 
new 
potential 
on 
(a, 
b) 
is 
given 
by 
P

. 
(a, 
b) * 
(b) 
p(ajb)p(bjc)p(c)

c

. 
* 
(a, 
b) 
= 
= 
(6.6.10)

 * 
(b)1 


P

This 
is 
therefore 
indeed 
equal 
to 
the 
marginal 
sincep(a, 
b, 
c)= 
p(a, 
b). 


c 


The 
new 
separator 
 * 
(b) 
contains 
the 
marginal 
p(b) 
since 


XX

. 
* 
(b)=p(bjc)p(c)=p(b, 
c)= 
p(b) 
(6.6.11) 


cc 


Example 
27 
(Finding 
a 
conditional 
marginal). 
Continuing 
with 
the 
distribution 
in 
example(26), 
we 
consider 
how 
to 
compute 
p(bja 
=1;c 
= 
1). 
First 
we 
clamp 
the 
evidential 
variables 
in 
their 
states. 
Then 
we 
claim 
that 
the 
eect 
of 
running 
the 
JTA 
is 
to 
produce 
on 
a 
set 
of 
clique 
variables 
X 
the 
marginals 
on 
the 
cliques 
p(X 
, 
V). 
We 
demonstrate 
this 
below: 


PP

• 
In 
general, 
the 
new 
separator 
is 
given 
by 
 * 
(b)=. 
(a, 
b)=p(ajb) 
= 
1. 
However, 
since 
aa 


a 
is 
clamped 
in 
state 
a 
= 
1, 
then 
the 
summation 
is 
not 
carried 
out 
over 
a, 
and 
we 
have 
instead 
 * 
(b)= 
p(a 
=1jb). 


• 
The 
new 
potential 
on 
the 
(b, 
c) 
clique 
is 
given 
by 
. 
(b, 
c) * 
(b) 
p(bjc 
= 
1)p(c 
= 
1)p(a 
=1jb)

. 
* 
(b, 
c) 
= 
= 
(6.6.12)

. 
(b)1 


• 
The 
new 
separator 
is 
normally 
given 
by 
XX

. 
* 
(b)=. 
* 
(b, 
c)=p(bjc)p(c) 
(6.6.13) 


cc 


However, 
since 
c 
is 
clamped 
in 
state 
1, 
we 
have 
instead 


. 
* 
(b)= 
p(bjc 
= 
1)p(c 
= 
1)p(a 
=1jb) 
(6.6.14) 


DRAFT 
March 
9, 
2010 



Finding 
the 
Most 
Likely 
State 


• 
The 
new 
potential 
on 
(a, 
b) 
is 
given 
by 
. 
(a, 
b) * 
(b) 
p(a 
=1jb)p(bjc 
= 
1)p(c 
= 
1)p(a 
=1jb)

. 
* 
(a, 
b)= 
= 
= 
p(a 
=1jb)p(bjc 
= 
1)p(c 
= 
1) 


 * 
(b) 
p(a 
=1jb) 


(6.6.15) 
The 
eect 
of 
clamping 
a 
set 
of 
variables 
V 
in 
their 
evidential 
states 
and 
running 
the 
JTA 
is 
that, 
for 
a 
clique 
i 
which 
contains 
the 
set 
of 
non-evidential 
variables 
Hi, 
the 
consistent 
potential 
from 
the 
JTA 
contains 
the 
marginal 
p(Hi 
, 
V). 
Finding 
a 
conditional 
marginal 
is 
then 
straightforward 
by 
ensuring 
normalisation. 


Example 
28 
(nding 
the 
likelihood 
p(a 
=1;c 
= 
1)). 
The 
eect 
of 
clamping 
the 
variables 
in 
their 
evidential 
states 
and 
running 
the 
JTA 
produces 
the 
joint 
marginals, 
such 
as 
 * 
(a, 
b)= 
p(a 
=1, 
b, 
c 
= 
1). 
Then 
calculating 
the 
likelihood 
is 
easy 
since 
we 
just 
sum 
out 
over 
the 
non-evidential 
variables 
of 
any 


Ps 


converged 
potential 
: 
p(a 
=1;c 
= 
1) 
= 
 * 
(a, 
b)= 
b 
p(a 
=1, 
b, 
c 
= 
1). 


b 


6.7 
Finding 
the 
Most 
Likely 
State 
A 
quantity 
of 
interest 
is 
the 
most 
likely 
joint 
state 
of 
a 
distribution: 


argmax 
p(X 
) 
(6.7.1) 


x1;:::;xn 


and 
it 
is 
natural 
to 
wonder 
how 
this 
can 
be 
eciently 
computed 
in 
the 
case 
of 
a 
loopy 
distribution. 
Since 
the 
development 
of 
the 
JTA 
is 
based 
around 
a 
variable 
elimination 
procedure 
and 
the 
max 
operator 
distributes 
over 
the 
distribution 
as 
well, 
eliminating 
a 
variable 
by 
maximising 
over 
that 
variable 
will 
have 
the 
same 
eect 
on 
the 
graph 
structure 
as 
summation 
did. 
This 
means 
that 
a 
junction 
tree 
is 
again 
an 
appropriate 
structure 
on 
which 
to 
perform 
max 
operations. 
Once 
a 
JT 
has 
been 
constructed, 
one 
then 
uses 
the 
Max 
Absorption 
procedure 
(see 
below), 
to 
perform 
maximisation 
over 
the 
variables. 
After 
a 
full 
round 
of 
absorption 
has 
been 
carried 
out, 
the 
cliques 
contain 
the 
distribution 
on 
the 
variables 
of 
the 
clique 
with 
all 
remaining 
variables 
set 
to 
their 
optimal 
states. 
The 
optimal 
local 
states 
can 
be 
found 
by 
explicit 
optimisation 
of 
each 
clique 
potential 
separately. 


Note 
that 
this 
procedure 
holds 
also 
for 
non-distributions 
– 
in 
this 
sense 
this 
is 
an 
example 
of 
a 
more 
general 
dynamic 
programming 
procedure 
applied 
in 
a 
case 
where 
the 
underlying 
graph 
is 
multiply-connected. 
This 
demonstrates 
how 
to 
eciently 
compute 
the 
optimum 
of 
a 
multiply-connected 
function 
dened 
as 
the 
product 
on 
potentials. 


Denition 
50 
(Max 
Absorption). 


Let 
V 
and 
W 
be 
neighbours 
in 
a 
clique 
graph, 
let 
S 
be 
their 
separator, 
and 
let 
. 
(V), 
. 
(W) 
and 
. 
(S) 
be 
their 
potentials. 
Absorption 
replaces 
the 
tables 
. 
(S) 
and 
. 
(W) 
with 


 * 
(S)

. 
* 
(S) 
= 
max 
. 
(V) 
. 
* 
(W)= 
. 
(W)

VnS 
. 
(S) 


 (V) (S) (W)
Once 
messages 
have 
been 
passed 
in 
both 
directions 
over 
all 
separators, 
according 
to 
a 
valid 
schedule, 
the 
most-likely 
joint 
state 
can 
be 
read 
off 
from 
maximising 
the 
state 
of 
the 
clique 
potentials. 
This 
is 
implemented 
in 
absorb.m 
and 
absorption.m 
where 
a 
ag 
is 
used 
to 
switch 
between 
either 
sum 
or 
max 
absorption. 


DRAFT 
March 
9, 
2010 



Reabsorption 
: 
Converting 
a 
Junction 
Tree 
to 
a 
Directed 
Network 


dce
abc
cf
egeh
ccee
dce
abc
cf
egeh
ccee
abc
edf
gh
(a) 
(b) 
(c) 
Figure 
6.11: 
(a): 
Junction 
tree. 
(b): 
Directed 
junction 
tree 
in 
which 
all 
edges 
are 
consistently 
oriented 
away 
from 
the 
clique 
(abc). 
(c): 
A 
set 
chain 
formed 
from 
the 
junction 
tree 
by 
reabsorbing 
each 
separator 
into 
its 
child 
clique. 


6.8 
Reabsorption 
: 
Converting 
a 
Junction 
Tree 
to 
a 
Directed 
Network 
It 
is 
sometimes 
useful 
to 
be 
able 
to 
convert 
the 
JT 
back 
to 
a 
BN 
of 
a 
desired 
form. 
For 
example, 
if 
one 
wishes 
to 
draw 
samples 
from 
a 
Markov 
network, 
this 
can 
be 
achieved 
by 
ancestral 
sampling 
on 
an 
equivalent 
directed 
structure, 
see 
section(27.2.2). 


Revisiting 
the 
example 
from 
g(6.4), 
we 
have 
the 
JT 
given 
in 
g(6.11a). 
To 
nd 
a 
valid 
directed 
representation 
we 
rst 
orient 
the 
JT 
edges 
consistently 
away 
from 
a 
chosen 
root 
node 
(see 
singleparenttree.m), 
thereby 
forming 
a 
directed 
JT 
which 
has 
the 
property 
that 
each 
clique 
has 
at 
most 
one 
parent 
clique. 


Denition 
51 
(Reabsorption). 


VSW
. 


VW
Let 
V 
and 
W 
be 
neighbouring 
cliques 
in 
a 
directed 
JT 
in 
which 
each 
clique 
in 
the 
tree 
has 
at 
most 
one 
parent. 
Furthermore, 
let 
S 
be 
their 
separator, 
and 
. 
(V), 
. 
(W) 
and 
. 
(S) 
be 
the 
potentials. 
Reabsorption 
into 
W 
removes 
the 
separator 
and 
forms 
a 
(set) 
conditional 
distribution 


. 
(W) 


p(WjV) 
= 
(6.8.1)

. 
(S) 
We 
say 
that 
clique 
W 
reabsorbs 
the 
separator 
S. 


In 
g(6.11) 
where 
one 
amongst 
many 
possible 
directed 
representations 
is 
formed 
from 
the 
JT. 
Specically, 
g(6.11a) 
represents 


p(e, 
g)p(d, 
c, 
e)p(a, 
b, 
c)p(c, 
f)p(e, 
h) 


p(a, 
b, 
c, 
d, 
e, 
f, 
g, 
h) 
= 
(6.8.2) 


p(e)p(c)p(c)p(e) 
We 
now 
have 
many 
choices 
as 
to 
which 
clique 
re-absorbs 
a 
separator. 
One 
such 
choice 
would 
give 
p(a, 
b, 
c, 
d, 
e, 
f, 
g, 
h)= 
p(gje)p(d, 
ejc)p(a, 
b, 
c, 
)p(fjc)p(hje) 
(6.8.3) 
This 
can 
be 
represented 
using 
a 
so-called 
set 
chain[170] 
in 
g(6.11c) 
(set 
chains 
generalise 
Belief 
Networks 
to 
a 
product 
of 
clusters 
of 
variables 
conditioned 
on 
parents). 
By 
writing 
each 
of 
the 
set 
conditional 
probabilities 
as 
local 
conditional 
BNs, 
one 
may 
also 
write 
full 
BN. 
For 
example, 
one 
such 
would 
be 
given 
from 
the 
decomposition 
p(cja, 
b)p(bja)p(a)p(gje)p(fjc)p(hje)p(dje, 
c)p(ejc) 
(6.8.4) 


DRAFT 
March 
9, 
2010 



Code 


d1d2d3d4d5
s1s2s3
Figure 
6.12: 
5 
diseases 
giving 
rise 
to 
3 
symptoms. 
Assuming 
the 
symptoms 
are 
all 
instantiated, 
the 
triangulated 
graph 
of 
the 
diseases 
is 
a 
5 
clique. 


6.9 
The 
Need 
For 
Approximations 
The 
JTA 
provides 
an 
upper 
bound 
on 
the 
complexity 
of 
(marginal) 
inference 
and 
attempts 
to 
exploit 
the 
structure 
of 
the 
graph 
to 
reduce 
computations. 
However, 
in 
a 
great 
deal 
of 
interesting 
applications 
the 
use 
of 
the 
JTA 
algorithm 
would 
result 
in 
clique-sizes 
in 
the 
triangulated 
graph 
that 
are 
prohibitively 
large. 


A 
classical 
situation 
in 
which 
this 
can 
arise 
are 
disease-symptom 
networks. 
For 
example, 
for 
the 
graph 
in 
g(6.12), 
the 
triangulated 
graph 
of 
the 
diseases 
is 
fully 
coupled, 
meaning 
that 
no 
simplication 
can 
occur 
in 
general. 
This 
situation 
is 
common 
in 
such 
bipartite 
networks, 
even 
when 
the 
children 
only 
have 
a 
small 
number 
of 
parents. 
Intuitively, 
as 
one 
eliminates 
each 
parent, 
links 
are 
added 
between 
other 
parents, 
mediated 
via 
the 
common 
children. 
Unless 
the 
graph 
is 
highly 
regular, 
analogous 
to 
a 
form 
of 
hidden 
Markov 
model, 
this 
ll-in 
eect 
rapidly 
results 
in 
large 
cliques 
and 
intractable 
computations. 


Dealing 
with 
large 
clique 
in 
the 
triangulated 
graph 
is 
an 
active 
research 
topic 
and 
we'll 
discuss 
strategies 
to 
approximate 
the 
computations 
in 
chapter(28). 


6.9.1 
Bounded 
width 
junction 
trees 
In 
some 
applications 
we 
may 
be 
at 
liberty 
to 
choose 
the 
structure 
of 
the 
Markov 
network. 
For 
example, 
if 
we 
wish 
to 
t 
a 
Markov 
network 
to 
data, 
we 
may 
wish 
to 
use 
as 
complex 
a 
Markov 
network 
as 
we 
can 
computationally 
aord. 
In 
such 
cases 
we 
desire 
that 
the 
clique 
sizes 
of 
the 
resulting 
triangulated 
Markov 
network 
are 
smaller 
than 
a 
specied 
`tree 
width’ 
(considering 
the 
corresponding 
junction 
tree 
as 
a 
hypertree). 
Constructing 
such 
bounded 
width 
or 
`thin’ 
junction 
trees 
is 
an 
active 
research 
topic. 
A 
simple 
way 
to 
do 
this 
is 
to 
start 
with 
a 
graph 
and 
include 
a 
randomly 
chosen 
edge 
provided 
that 
the 
size 
of 
all 
cliques 
in 
the 
resulting 
triangulated 
graph 
is 
below 
a 
specied 
maximal 
width. 
See 
demoThinJT.m 
and 
makeThinJT.m 
which 
assumes 
an 
initial 
graph 
G 
and 
a 
graph 
of 
candidate 
edges 
C, 
iteratively 
expanding 
G 
until 
a 
maximal 
tree 
width 
limit 
is 
reached. 
See 
also 
[11] 
for 
a 
discussion 
on 
learning 
an 
appropriate 
Markov 
structure 
based 
on 
data. 


6.10 
Code 
absorb.m: 
Absorption 
update 
V!S!W 
absorption.m: 
Full 
Absorption 
Schedule 
over 
Tree 
jtree.m: 
Form 
a 
Junction 
Tree 
triangulate.m: 
Triangulation 
based 
on 
simple 
node 
elimination 


6.10.1 
Utility 
routines 
Knowing 
if 
an 
undirected 
graph 
is 
a 
tree, 
and 
returning 
a 
valid 
elimination 
sequence 
is 
useful. 
A 
connected 
graph 
is 
a 
tree 
if 
the 
number 
of 
edges 
plus 
1 
is 
equal 
to 
the 
number 
of 
nodes. 
However, 
for 
a 
possibly 
disconnected 
graph 
this 
is 
not 
the 
case. 
The 
code 
deals 
with 
the 
possibly 
disconnected 
case, 
returning 
a 
valid 
elimination 
sequence 
if 
the 
graph 
is 
singly-connected. 
The 
routine 
is 
based 
on 
the 
observation 
that 
any 
singly-connected 
graph 
must 
always 
possess 
a 
simplical 
node 
which 
can 
be 
eliminated 
to 
reveal 
a 
smaller 
singly-connected 
graph. 
istree.m: 
If 
graph 
is 
singly 
connected 
return 
1 
and 
elimination 
sequence 
elimtri.m: 
Vertex/Node 
Elimination 
on 
a 
Triangulated 
Graph, 
with 
given 
end 
node 
demoJTree.m: 
Junction 
Tree 
: 
Chest 
Clinic 


DRAFT 
March 
9, 
2010 
103 



Exercises 


6.11 
Exercises 
1234
Exercise 
58. 
Show 
that 
the 
Markov 
network 
is 
not 
perfect 
elimination 
ordered 
and 
give 
a 
perfect 
elimination 
labelling 
for 
this 
graph. 
Exercise 
59. 
Consider 
the 
following 
distribution: 
p(x1, 
x2, 
x3, 
x4) 
= 
(x1, 
x2)(x2, 
x3)(x3, 
x4) 
(6.11.1) 


1. 
Draw 
a 
clique 
graph 
that 
represents 
this 
distribution 
and 
indicate 
the 
separators 
on 
the 
graph. 
2. 
Write 
down 
an 
alternative 
formula 
for 
the 
distribution 
p(x1;x2;x3;x4) 
in 
terms 
of 
the 
marginal 
probabilities 
p(x1;x2), 
p(x2;x3), 
p(x3;x4), 
p(x2), 
p(x3) 
Exercise 
60. 
Consider 
the 
distribution 


p(x1;x2;x3;x4)= 
(x1;x2)(x2;x3)(x3;x4)(x4;x1) 
(6.11.2) 


1. 
Write 
down 
a 
junction 
tree 
for 
the 
above 
graph. 
2. 
Carry 
out 
the 
absorption 
procedure 
and 
demonstrate 
that 
this 
gives 
the 
correct 
result 
for 
the 
marginal 
p(x1). 


Exercise 
61. 
Consider 
the 
distribution 


p(a, 
b, 
c, 
d, 
e, 
f, 
g, 
h, 
i)= 
p(a)p(bja)p(cja)p(dja)p(ejb)p(fjc)p(gjd)p(hje, 
f)p(ijf, 
g) 
(6.11.3) 


1. 
Draw 
the 
Belief 
Network 
for 
this 
distribution. 
2. 
Draw 
the 
moralised 
graph. 
3. 
Draw 
the 
triangulated 
graph. 
Your 
triangulated 
graph 
should 
contain 
cliques 
of 
the 
smallest 
size 
possible. 
4. 
Draw 
a 
junction 
tree 
for 
the 
above 
graph 
and 
verify 
that 
it 
satises 
the 
running 
intersection 
property. 
5. 
Describe 
a 
suitable 
initialisation 
of 
clique 
potentials. 
6. 
Describe 
the 
absorption 
procedure 
and 
write 
down 
an 
appropriate 
message 
updating 
schedule. 
Exercise 
62. 
This 
question 
concerns 
the 
distribution 


p(a, 
b, 
c, 
d, 
e, 
f)= 
p(a)p(bja)p(cjb)p(djc)p(ejd)p(fja, 
e) 
(6.11.4) 


1. 
Draw 
the 
Belief 
Network 
for 
this 
distribution. 
2. 
Draw 
the 
moralised 
graph. 
3. 
Draw 
the 
triangulated 
graph. 
Your 
triangulated 
graph 
should 
contain 
cliques 
of 
the 
smallest 
size 
possible. 
4. 
Draw 
a 
junction 
tree 
for 
the 
above 
graph 
and 
verify 
that 
it 
satises 
the 
running 
intersection 
property. 
5. 
Describe 
a 
suitable 
initialisation 
of 
clique 
potentials. 
6. 
Describe 
the 
Absorption 
procedure 
and 
an 
appropriate 
message 
updating 
schedule. 
7. 
Show 
that 
the 
distribution 
can 
be 
expressed 
in 
the 
form 
p(ajf)p(bja, 
c)p(cja, 
d)p(dja, 
e)p(eja, 
f)p(f) 
(6.11.5) 


DRAFT 
March 
9, 
2010 



Exercises 


Exercise 
63. 


For 
the 
undirected 
graph 
on 
the 
square 
lattice 
as 
shown, 
draw 
a 
triangulated 
graph 
with 
the 
smallest 
clique 
sizes 
possible. 



Exercise 
64. 


Q

Consider 
a 
binary 
variable 
Markov 
Random 
Field 
p(x)= 
Z..1
i>j 
(xi;xj), 
dened 
on 
the 
n 
× 
n 
lattice 
with 
(xi;xj)= 
eI[xi=xj 
] 
for 
i 
a 
neighbour 
of 
j 
on 
the 
lattice 
and 
i>j. 
A 
naive 
way 
to 
perform 
inference 
is 
to 
rst 
stack 
all 
the 
variables 
in 
the 
tth 


column 
and 
call 
this 
cluster 
variable 
Xt, 
as 
shown. 
The 
resulting 
graph 
is 
then 
singly-connected. 
What 
is 
the 
complexity 
of 
computing 
the 
normalisation 
constant 
based 
on 
this 
cluster 
representation? 
Compute 
log 
Z 
for 
n 
= 
10. 



X1X2X3
Exercise 
65. 
Given 
a 
consistent 
junction 
tree 
on 
which 
a 
full 
round 
of 
message 
passing 
has 
occurred, 
explain 
how 
to 
form 
a 
belief 
network 
from 
the 
junction 
tree. 


Exercise 
66. 
The 
le 
diseaseNet.mat 
contains 
the 
potentials 
for 
a 
disease 
bi-partite 
belief 
network, 
with 
20 
diseases 
d1;:::;d20 
and 
40 
symptoms, 
s1;:::;s40. 
Each 
disease 
and 
symptom 
is 
a 
binary 
variable, 
and 
each 
symptom 
connects 
to 
3 
parent 
diseases. 


1. 
Using 
the 
BRMLtoolbox, 
construct 
a 
junction 
tree 
for 
this 
distribution 
and 
use 
it 
to 
compute 
all 
the 
marginals 
of 
the 
symptoms, 
p(si 
= 
1). 
2. 
On 
most 
standard 
computers, 
computing 
the 
marginal 
p(si 
= 
1) 
by 
raw 
summation 
of 
the 
joint 
distribution 
is 
computationally 
infeasible. 
Explain 
how 
to 
compute 
the 
marginals 
p(si 
= 
1) 
in 
a 
tractable 
way 
without 
using 
the 
junction 
tree 
formalism. 
By 
implementing 
this 
method, 
compare 
it 
with 
the 
results 
from 
the 
junction 
tree 
algorithm. 
3. 
Consider 
the 
(unlikely) 
scenario 
in 
which 
all 
the 
40 
symptom 
variables 
are 
instantiated. 
Using 
the 
junction 
tree, 
estimate 
an 
upper 
bound 
on 
the 
number 
of 
seconds 
that 
computing 
a 
marginal 
p(d1js1:40) 
takes, 
assuming 
that 
for 
a 
two 
clique 
table 
containing 
S 
(joint) 
states, 
absorption 
takes 
O 
(S) 
seconds, 
for 
an 
unspecied 
. 
Compare 
this 
estimate 
with 
the 
time 
required 
to 
compute 
the 
marginal 
by 
raw 
summation 
of 
the 
instantiated 
Belief 
network. 
DRAFT 
March 
9, 
2010 



Exercises 


DRAFT 
March 
9, 
2010 



CHAPTER 
7 


Making 
Decisions 


7.1 
Expected 
Utility 
This 
chapter 
concerns 
situations 
in 
which 
decisions 
need 
to 
be 
taken 
under 
uncertainty. 
Consider 
the 
following 
scenario 
: 
you 
are 
asked 
if 
you 
wish 
to 
take 
a 
bet 
on 
the 
outcome 
of 
tossing 
a 
fair 
coin. 
If 
you 
bet 
and 
win, 
you 
gain 
$100. 
If 
you 
bet 
and 
lose, 
you 
lose 
$200. 
If 
you 
don't 
bet, 
the 
cost 
to 
you 
is 
zero. 
We 
can 
set 
this 
up 
using 
a 
two 
state 
variable 
x, 
with 
dom(x)= 
fwin, 
loseg, 
a 
decision 
variable 
d 
with 
dom(d)= 
fbet, 
no 
bet} 
and 
utilities 
as 
follows: 


U(win, 
bet) 
= 
100;U(lose, 
bet)= 
..200;U(win, 
no 
bet)=0;U(lose, 
no 
bet) 
= 
0 
(7.1.1) 


Since 
we 
don't 
know 
the 
state 
of 
x, 
in 
order 
to 
make 
a 
decision 
about 
whether 
or 
not 
to 
bet, 
arguably 
the 
best 
we 
can 
do 
is 
work 
out 
our 
expected 
winnings/losses 
under 
the 
situations 
of 
betting 
and 
not 
betting[240]. 
If 
we 
bet, 
we 
would 
expect 
to 
gain 


U(bet)= 
p(win) 
× 
U(win, 
bet)+ 
p(lose) 
× 
U(lose, 
bet)=0:5 
× 
100 
- 
0:5 
× 
200 
= 
..50 


If 
we 
don't 
bet, 
the 
expected 
gain 
is 
zero, 
U(no 
bet) 
= 
0. 
Based 
on 
taking 
the 
decision 
which 
maximises 
expected 
utility, 
we 
would 
therefore 
be 
advised 
not 
to 
bet. 


Denition 
52 
(Subjective 
Expected 
Utility). 
The 
utility 
of 
a 
decision 
is 


U(d)= 
hU(d, 
x)ip(x) 
(7.1.2) 


where 
p(x) 
is 
the 
distribution 
of 
the 
outcome 
x 
and 
d 
represents 
the 
decision. 


7.1.1 
Utility 
of 
money 
You 
are 
a 
wealthy 
individual, 
with 
$1, 
000, 
000 
in 
your 
bank 
account. 
You 
are 
asked 
if 
you 
would 
like 
to 
participate 
in 
a 
fair 
coin 
tossing 
bet 
in 
which, 
if 
you 
win, 
your 
bank 
account 
will 
become 
$1, 
000, 
000, 
000. 
However, 
if 
you 
lose, 
your 
bank 
account 
will 
contain 
only 
$1000. 
Assuming 
the 
coin 
is 
fair, 
should 
you 
take 
the 
bet? 


If 
we 
take 
the 
bet 
our 
expected 
bank 
balance 
would 
be 


U(bet)=0:5 
× 
1, 
000, 
000, 
000 
+ 
0:5 
× 
1000 
= 
500, 
000, 
500:00 
(7.1.3) 


If 
we 
don't 
bet, 
our 
bank 
balance 
will 
remain 
at 
$1, 
000, 
000. 
Based 
on 
expected 
utility, 
we 
are 
therefore 
advised 
to 
take 
the 
bet. 
(Note 
that 
if 
one 
considers 
instead 
the 
amount 
one 
will 
win 
or 
lose, 
one 
may 
show 


107 



Decision 
Trees 


that 
the 
dierence 
in 
expected 
utility 
between 
betting 
and 
not 
betting 
is 
the 
same, 
exercise(73)). 


Whilst 
the 
above 
is 
a 
correct 
mathematical 
derivation, 
few 
people 
who 
are 
millionaires 
are 
likely 
to 
be 
willing 
to 
risk 
losing 
almost 
everything 
in 
order 
to 
become 
a 
billionaire. 
This 
means 
that 
the 
subjective 
utility 
of 
money 
is 
not 
simply 
the 
quantity 
of 
money. 
In 
order 
to 
better 
reect 
the 
situation, 
the 
utility 
of 
money 
would 
need 
to 
be 
a 
non-linear 
function 
of 
money, 
growing 
slowly 
for 
large 
quantities 
of 
money 
and 
decreasing 
rapidly 
for 
small 
quantities 
of 
money, 
exercise(68). 


7.2 
Decision 
Trees 
Decision 
trees 
are 
a 
way 
to 
graphically 
organise 
a 
sequential 
decision 
process. 
A 
decision 
tree 
contains 
decision 
nodes, 
each 
with 
branches 
for 
each 
of 
the 
alternative 
decisions. 
Chance 
nodes 
(random 
variables) 
also 
appear 
in 
the 
tree, 
with 
the 
utility 
of 
each 
branch 
computed 
at 
the 
leaf 
of 
each 
branch. 
The 
expected 
utility 
of 
any 
decision 
can 
then 
be 
computed 
on 
the 
basis 
of 
the 
weighted 
summation 
of 
all 
branches 
from 
the 
decision 
to 
all 
leaves 
from 
that 
branch. 


Example 
29 
(Party). 
Consider 
the 
decision 
problem 
as 
to 
whether 
or 
not 
to 
go 
ahead 
with 
a 
fund-raising 
garden 
party. 
If 
we 
go 
ahead 
with 
the 
party 
and 
it 
subsequently 
rains, 
then 
we 
will 
lose 
money 
(since 
very 
few 
people 
will 
show 
up); 
on 
the 
other 
hand, 
if 
we 
don't 
go 
ahead 
with 
the 
party 
and 
it 
doesn't 
rain 
we're 
free 
to 
go 
and 
do 
something 
else 
fun. 
To 
characterise 
this 
numerically, 
we 
use: 


p(Rain 
= 
rain)=0:6, 
p(Rain 
= 
no 
rain)=0:4 
(7.2.1) 


The 
utility 
is 
dened 
as 


U 
(party, 
rain)= 
..100;U 
(party, 
no 
rain) 
= 
500;U 
(no 
party, 
rain)=0;U 
(no 
party, 
no 
rain) 
= 
50 


(7.2.2) 
We 
represent 
this 
situation 
in 
g(7.1). 
The 
question 
is, 
should 
we 
go 
ahead 
with 
the 
party? 
Since 
we 
don't 
know 
what 
will 
actually 
happen 
to 
the 
weather, 
we 
compute 
the 
expected 
utility 
of 
each 
decision: 


X

U 
(party)=U(party, 
Rain)p(Rain)= 
..100 
× 
0:6 
+ 
500 
× 
0:4 
= 
140 
(7.2.3) 


Rain 


X

U 
(no 
party)=U(no 
party, 
Rain)p(Rain)=0 
× 
0:6 
+ 
50 
× 
0:4 
= 
20 
(7.2.4) 


Rain 


Based 
on 
expected 
utility, 
we 
are 
therefore 
advised 
to 
go 
ahead 
with 
the 
party. 
The 
maximal 
expected 
utility 
is 
given 
by 
(see 
demoDecParty.m) 


X

max 
p(Rain)U(P 
arty, 
Rain) 
= 
140 
(7.2.5) 


P 
arty

Rain 


Example 
30 
(Party-Friend). 
An 
extension 
of 
the 
Party 
problem 
is 
that 
if 
we 
decide 
not 
to 
go 
ahead 
with 
the 
party, 
we 
have 
the 
opportunity 
to 
visit 
a 
friend. 
However, 
we're 
not 
sure 
if 
this 
friend 
will 
be 
in. 
The 
question 
is 
should 
we 
still 
go 
ahead 
with 
the 
party? 


We 
need 
to 
quantify 
all 
the 
uncertainties 
and 
utilities. 
If 
we 
go 
ahead 
with 
the 
party, 
the 
utilities 
are 
as 
before: 


Uparty 
(party, 
rain)= 
..100;Uparty 
(party, 
no 
rain) 
= 
500 
(7.2.6) 


108 
DRAFT 
March 
9, 
2010 



Decision 
Trees 


Figure 
7.1: 
A 
decision 
tree 
containing 
chance 
nodes 
(denoted 


Rain
Rain
Party
..100500050
yes
no
(0:6)yes
(0:4)no
(0:6)yes
(0:4)no
with 
ovals), 
decision 
nodes 
(denoted 
with 
squares) 
and 
utility 
nodes 
(denoted 
with 
diamonds). 
Note 
that 
a 
decision 
tree 
is 
not 
a 
graphical 
representation 
of 
a 
Belief 
Network 
with 
additional 
nodes. 
Rather, 
a 
decision 
tree 
is 
an 
explicit 
enumeration 
of 
the 
possible 
choices 
that 
can 
be 
made, 
beginning 
with 
the 
leftmost 


decision 
node, 
with 
probabilities 
on 
the 
links 
out 
of 
`chance’ 


nodes. 


with 


p(Rain 
= 
rain)=0:6;p(Rain 
= 
no 
rain)=0:4 
(7.2.7) 


If 
we 
decide 
not 
to 
go 
ahead 
with 
the 
party, 
we 
will 
consider 
going 
to 
visit 
a 
friend. 
In 
making 
the 
decision 
not 
to 
go 
ahead 
with 
the 
party 
we 
have 
utilities 


Uparty 
(no 
party, 
rain)=0;Uparty 
(no 
party, 
no 
rain) 
= 
50 
(7.2.8) 


The 
probability 
that 
the 
friend 
is 
in 
depends 
on 
the 
weather 
according 
to 


p(F 
riend 
= 
injrain)=0:8;p(F 
riend 
= 
injno 
rain)=0:1, 
(7.2.9) 


The 
other 
probabilities 
are 
determined 
by 
normalisation. 
We 
additionally 
have 


Uvisit 
(friend 
in, 
visit) 
= 
200;Uvisit 
(friend 
out, 
visit)= 
..100 
(7.2.10) 


with 
the 
remaining 
utilities 
zero. 
The 
two 
sets 
of 
utilities 
add 
up 
so 
that 
the 
overall 
utility 
of 
any 
decision 
sequence 
is 
Uparty 
+ 
Uvisit. 
The 
decision 
tree 
for 
the 
Party-Friend 
problem 
is 
shown 
is 
g(7.2). 
For 
each 
decision 
sequence 
the 
utility 
of 
that 
sequence 
is 
given 
at 
the 
corresponding 
leaf 
of 
the 
DT. 
Note 
that 
the 
leaves 
contain 
the 
total 
utility 
Uparty 
+ 
Uvisit. 
Solving 
the 
DT 
corresponds 
to 
nding 
for 
each 
decision 
node 
the 
maximal 
expected 
utility 
possible 
(by 
optimising 
over 
future 
decisions). 
At 
any 
point 
in 
the 
tree 
choosing 
that 
action 
which 
leads 
to 
the 
child 
with 
highest 
expected 
utility 
will 
lead 
to 
the 
optimal 
strategy. 
Using 
this, 
we 
nd 
that 
the 
optimal 
expected 
utility 
has 
value 
140 
and 
is 
given 
by 
going 
ahead 
with 
the 
party, 
see 
demoDecPartyFriend.m. 


• 
In 
DTs 
the 
same 
nodes 
are 
often 
repeated 
throughout 
the 
tree. 
For 
a 
longer 
sequence 
of 
decisions, 
the 
number 
of 
branches 
in 
the 
tree 
can 
grow 
exponentially 
with 
the 
number 
of 
decisions, 
making 
this 
representation 
impractical. 
• 
In 
this 
example 
the 
DT 
is 
asymmetric 
since 
if 
we 
decide 
to 
go 
ahead 
with 
the 
party 
we 
will 
not 
visit 
the 
friend, 
curtailing 
the 
further 
decisions 
present 
in 
the 
lower 
half 
of 
the 
tree. 
Mathematically, 
we 
can 
express 
the 
optimal 
expected 
utility 
U 
for 
the 
Party-Visit 
example 
by 
summing 
over 
un-revealed 
variables 
and 
optimising 
over 
future 
decisions: 


max 
p(Rain) 
maxp(F 
riendjRain)[Uparty(P 
arty, 
Rain)+ 
Uvisit(V 
isit, 
F 
riend)I[P 
arty 
= 
no]]

P 
arty 
V 
isit 


Rain 
F 
riend 


(7.2.11) 
where 
the 
term 
I[P 
arty 
= 
no] 
has 
the 
eect 
of 
curtailing 
the 
DT 
if 
the 
party 
goes 
ahead. 
To 
answer 
the 
question 
as 
to 
whether 
or 
not 
to 
go 
ahead 
with 
the 
party, 
we 
take 
that 
state 
of 
P 
arty 
that 
corresponds 
to 


DRAFT 
March 
9, 
2010 



Decision 
Trees 


-100 
-100 


Figure 
7.2: 
Solving 
a 
Decision 
Tree. 
(a): 
Decision 
Tree 
for 
the 
Party-Visit, 
example(30). 
(b): 
Solving 
the 
DT 
cor


Rain(140)
yes
no
Party
Rain
no
yes
(0.4)no
responds 
to 
making 
the 
decision 
with 
the 
highest 
expected 
future 
utility. 
This 
can 


be 
achieved 
by 
starting 
at 
the 
leaves 
(utilities). 
For 
a 
chance 
parent 
node 
x, 
the 
utility 
of 
the 
parent 
is 
the 
expected 
utility 
of 
that 
variable. 
For 
example, 
at 
the 



500 



top 
of 
the 
DT 
we 
have 
the 
Rain 
vari


200 


200 



able 
with 
the 
children 
..100 
(probability 
0.6) 
and 
500 
(probability 
0.4). 
Hence 


Rain
Visit
Friend
Friend
Visit
Friend
Friend
..100(0.6)yes
(0.4)nono
yes
no(0.2)out
(0.8)in
(0.2)out
(0.8)inyes
(0.9)out
(0.1)in
(0.9)out
(0.1)in
Visit(140)
Friend(0)
Friend(140)
noout
in
out
inye
s 


the 
expected 
utility 
of 
the 
Rain 
node 
is 
..100 
..100 
× 
0:6 
+ 
500 
× 
0:4 
= 
140. 
For 
a 
decision 
node, 
the 
value 
of 
the 
node 
is 


Party(140)
0 



0 



the 
optimum 
of 
its 
child 
values. 
One 
re


0 


0 


curses 
thus 
backwards 
from 
the 
leaves 
to 


the 
root. 
For 
example, 
the 
value 
of 
the 
Rain 
chance 
node 
in 
the 
lower 
branch 
is 
given 
by 
140 
× 
0:6 
+ 
50 
× 
0:4 
= 
104. 
The 


Rain(104)
optimal 
decision 
sequence 
is 
then 
given 
at 


250 


250 



each 
decision 
node 
by 
nding 
which 
child 
node 
has 
the 
maximal 
value. 
Hence 
the 
overall 
best 
decision 
is 
to 
decide 
to 
go 
ahead 
with 
the 
party. 
If 
we 
decided 
not 
to 
do 
so, 
and 
it 
does 
not 
rain, 
then 
the 
best 
decision 
we 
could 
take 
would 
be 
to 
not 
visit 


Visit(50)
Friend(50)
Friend(..20)
no
yes
out
in
out
in
..50
..50 


50
50
the 
friend 
(which 
has 
an 
expected 
utility 


of 
50). 
A 
more 
compact 
description 
of 
this 
problem 
is 
given 
by 
the 
inuence 
diagram, 
g(7.4). 
See 
also 
demoDecPartyFriend.m. 


50 
50 


(a) 
(b) 
the 
maximal 
expected 
utility 
above. 
The 
way 
to 
read 
equation 
(7.2.11) 
is 
to 
start 
from 
the 
last 
decision 
that 
needs 
to 
be 
taken, 
in 
this 
case 
V 
isit. 
When 
we 
are 
at 
the 
V 
isit 
stage 
we 
assume 
that 
we 
will 
have 
previously 
made 
a 
decision 
about 
P 
arty 
and 
also 
will 
have 
observed 
whether 
or 
not 
is 
is 
raining. 
However, 
we 
don't 
know 
whether 
or 
not 
our 
friend 
will 
be 
in, 
so 
we 
compute 
the 
expected 
utility 
by 
averaging 
over 
this 
unknown. 
We 
then 
take 
the 
optimal 
decision 
by 
maximising 
over 
V 
isit. 
Subsequently 
we 
move 
to 
the 
next-to-last 
decision, 
assuming 
that 
what 
we 
will 
do 
in 
the 
future 
is 
optimal. 
Since 
in 
the 
future 
we 
will 
have 
taken 
a 
decision 
under 
the 
uncertain 
F 
riend 
variable, 
the 
current 
decision 
can 
then 
be 
taken 
under 
uncertainty 
about 
Rain 
and 
maximising 
this 
expected 
optimal 
utility 
over 
P 
arty. 
Note 
that 
the 
sequence 
of 
maximisations 
and 
summations 
matters 
– 
changing 
the 
order 
will 
in 
general 
result 
in 
a 
dierent 
problem 
with 
a 
dierent 
expected 
utility1 
. 


1If 
one 
only 
had 
a 
sequence 
of 
summations, 
the 
order 
of 
the 
summations 
is 
irrelevant 
– 
likewise 
for 
the 
case 
of 
all 
maximisations. 
However, 
summation 
and 
maximisation 
operators 
do 
not 
in 
general 
commute. 


110 
DRAFT 
March 
9, 
2010 



Extending 
Bayesian 
Networks 
for 
Decisions 


RainParty
Utility
Figure 
7.3: 
An 
inuence 
diagram 
which 
contains 
random 
variables 
(denoted 
with 
ovals/circles) 
Decision 
nodes 
(denoted 
with 
squares) 
and 
Utility 
nodes 
(denoted 
with 
diamonds). 
Contrasted 
with 
g(7.1) 
this 
is 
a 
more 
compact 
representation 
of 
the 
structure 
of 
the 
problem. 
The 
diagram 
represents 
the 
expression 
p(rain)u(party, 
rain). 
In 
addition 
the 
diagram 
denotes 
an 
ordering 
of 
the 
variables 
with 
party 
rain 
(according 
to 
the 
convention 
given 
by 
equation 
(7.3.1)). 


7.3 
Extending 
Bayesian 
Networks 
for 
Decisions 
An 
inuence 
diagram 
is 
a 
Bayesian 
Network 
with 
additional 
Decision 
nodes 
and 
Utility 
nodes 
[137, 
148, 
162]. 
The 
decision 
nodes 
have 
no 
associated 
distribution; 
the 
utility 
nodes 
are 
deterministic 
functions 
of 
their 
parents. 
The 
utility 
and 
decision 
nodes 
can 
be 
either 
continuous 
or 
discrete; 
for 
simplicity, 
in 
the 
examples 
here 
the 
decisions 
will 
be 
discrete. 


A 
benet 
of 
decision 
trees 
is 
that 
they 
are 
general 
and 
explicitly 
encode 
the 
utilities 
and 
probabilities 
associated 
with 
each 
decision 
and 
event. 
In 
addition, 
we 
can 
readily 
solve 
small 
decision 
problems 
using 
decision 
trees. 
However, 
when 
the 
sequence 
of 
decisions 
increases, 
the 
number 
of 
leaves 
in 
the 
decision 
tree 
grows 
and 
representing 
the 
tree 
can 
become 
an 
exponentially 
complex 
problem. 
In 
such 
cases 
it 
can 
be 
useful 
to 
use 
an 
Inuence 
Diagram 
(ID). 
An 
ID 
states 
which 
information 
is 
required 
in 
order 
to 
make 
each 
decision, 
and 
the 
order 
in 
which 
these 
decisions 
are 
to 
be 
made. 
The 
details 
of 
the 
probabilities 
and 
rewards 
are 
not 
specied 
in 
the 
ID, 
and 
this 
can 
enable 
a 
more 
compact 
description 
of 
the 
decision 
problem. 


7.3.1 
Syntax 
of 
inuence 
diagrams 
Information 
Links 
An 
information 
link 
from 
a 
random 
variable 
into 
a 
decision 
node: 


XD
indicates 
that 
the 
state 
of 
the 
variable 
X 
will 
be 
known 
before 
decision 
D 
is 
taken. 
Information 
links 
from 
another 
decision 
node 
d 
in 
to 
D 
similarly 
indicate 
that 
decision 
d 
is 
known 
before 
decision 
D 
is 
taken. 
We 
use 
a 
dashed 
link 
to 
denote 
that 
decision 
D 
is 
not 
functionally 
related 
to 
its 
parents. 


Random 
Variables 
Random 
Variables 
may 
depend 
on 
the 
states 
of 
parental 
random 
variables 
(as 
in 
Belief 
Networks), 
but 
also 
Decision 
Node 
states: 


YXD
As 
decisions 
are 
taken, 
the 
states 
of 
some 
random 
variables 
will 
be 
revealed. 
To 
emphasise 
this 
we 
typically 
shade 
a 
node 
to 
denote 
that 
its 
state 
will 
be 
revealed 
during 
the 
sequential 
decision 
process. 


Utilities 
A 
utility 
node 
is 
a 
deterministic 
function 
of 
its 
parents. 
The 
parents 
can 
be 
either 
random 
variables 
or 
decision 
nodes. 


In 
the 
party 
example, 
the 
BN 
trivially 
consists 
of 
a 
single 
node, 
and 
the 
Inuence 
Diagram 
is 
given 
in 
g(7.3). 
The 
more 
complex 
Party-Friend 
problem 
is 
depicted 
in 
g(7.4). 
The 
ID 
generally 
provides 
a 
more 
compact 
representation 
of 
the 
structure 
of 
problem 
than 
a 
DT, 
although 
details 
about 
the 
specic 
probabilities 
and 
utilities 
are 
not 
present 
in 
the 
ID. 


DRAFT 
March 
9, 
2010 



Extending 
Bayesian 
Networks 
for 
Decisions 


Rain
Party
Uparty
Visit
Friend
Uvisit
Figure 
7.4: 
An 
inuence 
diagram 
for 
the 
party-visit 
problem, 
example(30). 
The 
partial 
ordering 
is 
P 
arty* 
Rain 
V 
isit* 
F 
riend. 
The 
dashed-link 
from 
party 
to 
visit 
is 
not 
strictly 
necessary 
but 
retained 
in 
order 
to 
satisfy 
the 
convention 
that 
there 
is 
a 
directed 
path 
connecting 
all 
decision 
nodes. 


Partial 
ordering 


An 
ID 
denes 
a 
partial 
ordering 
of 
the 
nodes. 
We 
begin 
by 
writing 
those 
variables 
X0 
whose 
states 
are 
known 
(evidential 
variables) 
before 
the 
rst 
decision 
D1. 
We 
then 
nd 
that 
set 
of 
variables 
X1 
whose 
states 
are 
revealed 
before 
the 
second 
decision 
D2. 
Subsequently 
the 
set 
of 
variables 
Xt 
is 
revealed 
before 
decision 
Dt+1. 
The 
remaining 
fully-unobserved 
variables 
are 
placed 
at 
the 
end 
of 
the 
ordering: 


X0 
D1 
X1 
D2;:::, 
Xn..1 
Dn 
Xn 
(7.3.1) 


with 
Xk 
being 
the 
variables 
revealed 
between 
decision 
Dk 
and 
Dk+1. 
The 
term 
`partial’ 
refers 
to 
the 
fact 
that 
there 
is 
no 
order 
implied 
amongst 
the 
variables 
within 
the 
set 
Xn. 
For 
notational 
clarity, 
at 
points 




below 
we 
will 
indicate 
decision 
variables 
with 
to 
reinforce 
that 
we 
maximise 
over 
these 
variables, 
and 
sum 
over 
the 
non-starred 
variables. 
Where 
the 
sets 
are 
empty 
we 
omit 
writing 
them. 
For 
example, 
in 
g(7.5a) 
the 
ordering 
is 
T 
est 
* 
Seismic 
Drill 
* 
Oil. 


The 
optimal 
rst 
decision 
D1 
is 
determined 
by 
computing 


XXXYX

U(D1jX0) 
max 
:::max 
p 
(xijpa 
(xi))Uj 
(pa 
(uj)) 
(7.3.2)
D2 
Dn

X1 
Xn..1 
Xni2I 
j2J 


for 
each 
state 
of 
the 
decision 
D1, 
given 
X0. 
In 
equation 
(7.3.2) 
above 
I 
denotes 
the 
set 
of 
indices 
for 
the 
random 
variables, 
and 
J 
the 
indices 
for 
the 
utility 
nodes. 
For 
each 
state 
of 
the 
conditioning 
variables, 
the 
optimal 
decision 
D1 
is 
found 
using 


argmax 
U(D1jX0) 
(7.3.3) 


D1 


Remark 
9 
(Reading 
off 
the 
partial 
ordering). 
Sometimes 
it 
can 
be 
tricky 
to 
read 
the 
partial 
ordering 
from 
the 
ID. 
A 
method 
is 
to 
identify 
the 
rst 
decision 
D1 
and 
then 
any 
variables 
X0 
that 
need 
to 
be 
observed 
to 
make 
that 
decision. 
Then 
identify 
the 
next 
decision 
D2 
and 
the 
variables 
X1 
that 
are 
revealed 
after 
decision 
D1 
is 
taken 
and 
before 
decision 
D2 
is 
taken, 
etc. 
This 
gives 
the 
partial 
ordering 
X0 
D1 
X1 
D2;:::. 
Place 
any 
unrevealed 
variables 
at 
the 
end 
of 
the 
ordering. 


Implicit 
and 
explicit 
information 
links 


The 
information 
links 
are 
a 
potential 
source 
of 
confusion. 
An 
information 
link 
species 
explicitly 
which 
quantities 
are 
known 
before 
that 
decision 
is 
taken2 
. 
We 
also 
implicitly 
assume 
the 
no 
forgetting 
principle 
that 
all 
past 
decisions 
and 
revealed 
variables 
are 
available 
at 
the 
current 
decision 
(the 
revealed 
variables 
are 
necessarily 
the 
parents 
of 
all 
past 
decision 
nodes). 
If 
we 
were 
to 
include 
all 
such 
information 
links, 
IDs 
would 
get 
potentially 
rather 
messy. 
In 
g(7.5), 
both 
explicit 
and 
implicit 
information 
links 
are 
demonstrated. 
We 
call 
an 
information 
link 
fundamental 
if 
its 
removal 
would 
alter 
the 
partial 
ordering. 


2

Some 
authors 
prefer 
to 
write 
all 
information 
links 
where 
possible, 
and 
others 
prefer 
to 
leave 
them 
implicit. 
Here 
we 
largely 
take 
the 
implicit 
approach. 
For 
the 
purposes 
of 
computation, 
all 
that 
is 
required 
is 
a 
partial 
ordering; 
one 
can 
therefore 
view 
this 
as 
`basic’ 
and 
the 
information 
links 
as 
supercial 
(see 
[69]). 


112 
DRAFT 
March 
9, 
2010 



Extending 
Bayesian 
Networks 
for 
Decisions 


Test
DrillU1
U2Oil
Seismic
Test
DrillU1
U2Oil
Seismic
(a) 
(b) 
Figure 
7.5: 
(a): 
The 
partial 
ordering 
is 
T 
est 
* 
Seismic 
Drill 
* 
Oil. 
The 
explicit 
information 
links 
from 
T 
est 
to 
Seismic 
and 
from 
Seismic 
to 
Drill 
are 
both 
fundamental 
in 
the 
sense 
that 
removing 
either 
results 
in 
a 
dierent 
partial 
ordering. 
The 
shaded 
node 
emphasises 
that 
the 
state 
of 
this 
variable 
will 
be 
revealed 
during 
the 
sequential 
decision 
process. 
Conversely, 
the 
non-shaded 
node 
will 
never 
be 
observed. 
(b): 
Based 
on 
the 
ID 
in 
(a), 
there 
is 
an 
implicit 
link 
from 
T 
est 
to 
Drill 
since 
the 
decision 
about 
T 
est 
is 
taken 
before 
Seismic 
is 
revealed. 


Causal 
consistency 


For 
an 
Inuence 
Diagram 
to 
be 
consistent 
a 
current 
decision 
cannot 
aect 
the 
past. 
This 
means 
that 
any 
random 
variable 
descendants 
of 
a 
decision 
D 
in 
the 
ID 
must 
come 
later 
in 
the 
partial 
ordering. 
Assuming 
the 
no-forgetting 
principle, 
this 
means 
that 
for 
any 
valid 
ID 
there 
must 
be 
a 
directed 
path 
connecting 
all 
decisions. 
This 
can 
be 
a 
useful 
check 
on 
the 
consistency 
of 
an 
ID. 


Asymmetry 


IDs 
are 
most 
convenient 
when 
the 
corresponding 
DT 
is 
symmetric. 
However, 
some 
forms 
of 
asymmetry 
are 
relatively 
straightforward 
to 
deal 
with 
in 
the 
ID 
framework. 
For 
our 
party-visit 
example, 
the 
DT 
is 
asymmetric. 
However, 
this 
is 
easily 
dealt 
with 
in 
the 
ID 
by 
using 
a 
link 
from 
P 
arty 
to 
Uvisit 
which 
removes 
the 
contribution 
from 
Uvisit 
when 
the 
party 
goes 
ahead. 


More 
complex 
issues 
arise 
when 
the 
set 
of 
variables 
than 
can 
be 
observed 
depends 
on 
the 
decision 
sequence 
taken. 
In 
this 
case 
the 
DT 
is 
asymmetric. 
In 
general, 
Inuence 
Diagrams 
are 
not 
well 
suited 
to 
modelling 
such 
asymmetries, 
although 
some 
eects 
can 
be 
mediated 
either 
by 
careful 
use 
of 
additional 
variables, 
or 
extending 
the 
ID 
notation. 
See 
[69] 
and 
[148] 
for 
further 
details 
of 
these 
issues 
and 
possible 
resolutions. 


Example 
31 
(Should 
I 
do 
a 
PhD?). 
Consider 
a 
decision 
whether 
or 
not 
to 
do 
PhD 
as 
part 
of 
our 
education 
(E). 
Taking 
a 
PhD 
incurs 
costs, 
UC 
both 
in 
terms 
of 
fees, 
but 
also 
in 
terms 
of 
lost 
income. 
However, 
if 
we 
have 
a 
PhD, 
we 
are 
more 
likely 
to 
win 
a 
Nobel 
Prize 
(P 
), 
which 
would 
certainly 
be 
likely 
to 
boost 
our 
Income 
(I), 
subsequently 
benetting 
our 
nances 
(UB). 
This 
setup 
is 
depicted 
in 
g(7.6a). 
The 
ordering 
is 
(eliding 
empty 
sets) 


E 
* 
fI;P 
} 
(7.3.4) 


and 


dom(E)=(do 
PhD, 
no 
PhD) 
, 
dom(I)=(low, 
average, 
high) 
, 
dom(P 
)=(prize, 
no 
prize) 
(7.3.5) 


The 
probabilities 
are 


p(win 
Nobel 
prizejno 
PhD)=0:0000001 
p(win 
Nobel 
prizejdo 
PhD)=0:001 
(7.3.6) 


p(lowjdo 
PhD, 
no 
prize)=0:1 
p(averagejdo 
PhD, 
no 
prize)=0:5 
p(highjdo 
PhD, 
no 
prize)=0:4 


p(lowjno 
PhD, 
no 
prize)=0:2 
p(averagejno 
PhD, 
no 
prize)=0:6 
p(highjno 
PhD, 
no 
prize)=0:2 


p(lowjdo 
PhD, 
prize)=0:01 
p(averagejdo 
PhD, 
prize)=0:04 
p(highjdo 
PhD, 
prize)=0:95 


p(lowjno 
PhD, 
prize)=0:01 
p(averagejno 
PhD, 
prize)=0:04 
p(highjno 
PhD, 
prize)=0:95 


DRAFT 
March 
9, 
2010 



Extending 
Bayesian 
Networks 
for 
Decisions 


E
UC
P
I
UB
SE
UC
P
USI
UB
Figure 
7.6: 
(a): 
Education 
E 
incurs 
some 
cost, 
but 
also 
gives 
a 
chance 
to 
win 
a 
prestigious 
science 
prize. 
Both 
of 
these 
affect 
our 
likely 
Incomes, 
with 
corresponding 
long 
term 
nancial 
benets. 
(b): 
The 
start-up 
scenario. 


(a) 
(b) 
(7.3.7) 
The 
utilities 
are 


UC 
(do 
PhD)= 
..50000;UC 
(no 
PhD)=0, 
(7.3.8) 


UB 
(low) 
= 
100000;UB 
(average) 
= 
200000;UB 
(high) 
= 
500000 
(7.3.9) 


The 
expected 
utility 
of 
Education 
is 


U(E)= 
p(IjE, 
P 
)p(P 
jE)[UC(E)+ 
UB(I)] 
(7.3.10) 


I;P 


so 
that 
U(do 
phd) 
= 
260174:000, 
whilst 
not 
taking 
a 
PhD 
is 
U(no 
phd) 
= 
240000:0244, 
making 
it 
on 
average 
benecial 
to 
do 
a 
PhD. 
See 
demoDecPhD.m. 


Example 
32 
(PhDs 
and 
Start-up 
companies). 
Inuence 
Diagrams 
are 
particularly 
useful 
when 
a 
sequence 
of 
decisions 
is 
taken. 
For 
example, 
in 
g(7.6b) 
we 
model 
a 
new 
situation 
in 
which 
someone 
has 
rst 
decided 
whether 
or 
not 
to 
take 
a 
PhD. 
Ten 
years 
later 
in 
their 
career 
they 
decide 
whether 
or 
not 
to 
make 
a 
start-up 
company. 
This 
decision 
is 
based 
on 
whether 
or 
not 
they 
won 
the 
Nobel 
Prize. 
The 
start-up 
decision 
is 
modelled 
by 
S 
with 
dom(S)=(tr, 
fa). 
If 
we 
make 
a 
start-up, 
this 
will 
cost 
some 
money 
in 
terms 
of 
investment. 
However, 
the 
potential 
benet 
in 
terms 
of 
our 
income 
could 
be 
high. 


We 
model 
this 
with 
(the 
other 
required 
table 
entries 
being 
taken 
from 
example(31)): 


p(lowjstart 
up, 
no 
prize)=0:1 
p(averagejstart 
up, 
no 
prize)=0:5 
p(highjstart 
up, 
no 
prize)=0:4 
p(lowjno 
start 
up, 
no 
prize)=0:2 
p(averagejno 
start 
up, 
no 
prize)=0:6 
p(highjno 
start 
up, 
no 
prize)=0:2 
p(lowjstart 
up, 
prize)=0:005 
p(averagejstart 
up, 
prize)=0:005 
p(highjstart 
up, 
prize)=0:99 
p(lowjno 
start 
up, 
prize)=0:05 
p(averagejno 
start 
up, 
prize)=0:15 
p(highjno 
start 
up, 
prize)=0:8 


(7.3.11) 
and 


US 
(start 
up)= 
..200000;US 
(no 
start 
up) 
= 
0 
(7.3.12) 


Our 
interest 
is 
to 
advise 
whether 
or 
not 
it 
is 
desirable 
(in 
terms 
of 
expected 
utility) 
to 
take 
a 
PhD, 
now 
bearing 
in 
mind 
that 
later 
one 
may 
or 
may 
not 
win 
the 
Nobel 
Prize, 
and 
may 
or 
may 
not 
make 
a 
start-up 
company. 


The 
ordering 
is 
(eliding 
empty 
sets) 


E 
* 
P 
S 
* 
I 
(7.3.13) 


DRAFT 
March 
9, 
2010 



Solving 
Inuence 
Diagrams 


Figure 
7.7: 
Markov 
Decision 
Process. 
These 
can 
be 
used 
to 
model 
planning 
problems 
of 
the 
form 
`how 
do 
I 
get 
to 
where 
I 
want 
to 
be 
incurring 
the 
lowest 
total 
cost?'. 
They 
are 
readily 
solvable 
using 
a 
message 
passing 
algorithm. 


x1x2x3x4
d1d2d3
u2 
u3 
u4 


The 
expected 
optimal 
utility 
for 
any 
state 
of 
E 
is 


XX

U(E) 
=max 
p(IjS, 
P 
)p(P 
jE)[US(S)+ 
UC(E)+ 
UB(I)] 
(7.3.14) 


S

PI 


where 
we 
assume 
that 
the 
optimal 
decisions 
are 
taken 
in 
the 
future. 
Computing 
the 
above, 
we 
nd 


U(do 
PhD) 
= 
190195:00;U(no 
PhD) 
= 
240000:02 
(7.3.15) 


Hence, 
we 
are 
better 
off 
not 
doing 
a 
PhD. 
See 
demoDecPhd.m. 


7.4 
Solving 
Inuence 
Diagrams 
Solving 
an 
inuence 
diagram 
means 
computing 
the 
optimal 
decision 
or 
sequence 
of 
decisions. 
Here 
we 
focus 
on 
nding 
the 
optimal 
rst 
decision. 
The 
direct 
approach 
is 
to 
take 
equation 
(7.3.2) 
and 
perform 
the 
required 
sequence 
of 
summations 
and 
maximisations 
explicitly. 
However, 
we 
may 
be 
able 
to 
exploit 
the 
structure 
of 
the 
problem 
to 
for 
computational 
eciency. 
To 
develop 
this 
we 
rst 
derive 
an 
ecient 
algorithm 
for 
a 
highly 
structured 
ID, 
the 
Markov 
Decision 
Process, 
which 
we 
will 
discuss 
further 
in 
section(7.5). 


7.4.1 
Ecient 
inference 
Consider 
the 
following 
function 
from 
the 
ID 
of 
g(7.7) 


(x4;x3;d3)(x3;x2;d2)(x2;x1;d1)(u(x2)+ 
u(x3)+ 
u(x4)) 
(7.4.1) 


where 
the 
f 
represent 
conditional 
probabilities 
and 
the 
u 
are 
utilities. 
We 
write 
this 
in 
terms 
of 
potentials 
since 
this 
will 
facilitate 
the 
generalisation 
to 
other 
cases. 
Our 
task 
is 
to 
take 
the 
optimal 
rst 
decision, 
based 
on 
the 
expected 
optimal 
utility 


XXX

U(d1) 
=max 
max 
(x4;x3;d3)(x3;x2;d2)(x2;x1;d1)(u(x2)+ 
u(x3)+ 
u(x4)) 
(7.4.2) 


d2d3

x2 
x3 
x4 


Whilst 
we 
could 
carry 
out 
the 
sequence 
of 
maximisations 
and 
summations 
naively, 
our 
interest 
is 
to 
derive 
a 
computationally 
ecient 
approach. 
Let's 
see 
how 
to 
distribute 
these 
operations 
`by 
hand'. 
Since 
only 
u(x4) 
depends 
on 
x4 
explicitly 
we 
can 
write 


XXX

U(d1)=(x2;x1;d1) 
max 
(x3;x2;d2) 
max 
(x4;x3;d3)u(x4) 


d2d3

x2 
x3 
x4

XXX

+(x2;x1;d1) 
max 
(x3;x2;d2)u(x3) 
max 
(x4;x3;d3) 


d2d3

x2 
x3 
x4

XXX

+(x2;x1;d1)u(x2) 
max 
(x3;x2;d2) 
max 
(x4;x3;d3) 
(7.4.3) 


d2d3

x2 
x3 
x4 


DRAFT 
March 
9, 
2010 



Solving 
Inuence 
Diagrams 


Starting 
with 
the 
rst 
line 
and 
carrying 
out 
the 
summation 
over 
x4 
and 
max 
over 
d3, 
this 
gives 
a 
new 
function 
of 
x3, 


X

u3 4(x3) 
= 
max 
(x4;x3;d3)u(x4) 
(7.4.4) 


d3

x4 


In 
addition 
we 
dene 
the 
message 
(which 
in 
our 
particular 
example 
will 
be 
unity) 


X

3 4(x3) 
= 
max 
(x4;x3;d3) 
(7.4.5) 


d3

x4 


Using 
this 
we 
can 
write 


XX

U(d1)=(x2;x1;d1) 
max 
(x3;x2;d2)[u(x3)3 4(x3)+ 
u3 4(x3)] 


d2

x2 
x3 


XX

+(x2;x1;d1)u(x2) 
max 
(x3;x2;d2)3 4(x3) 
(7.4.6) 


d2

x2 
x3 


Now 
we 
carry 
out 
the 
sum 
over 
x3 
and 
max 
over 
d2 
for 
the 
rst 
row 
above 
and 
dene 
a 
utility 
message 


X

u2 3(x2) 
= 
max 
(x3;x2;d2)[u(x3)3 4(x3)+ 
u3 4(x3)] 
(7.4.7) 


d2

x3 


and 
probability 
message3 


X

2 3(x2) 
= 
max 
(x3;x2;d2)3 4(x3) 
(7.4.8) 


d2

x3 


The 
optimal 
decision 
for 
d1 
can 
be 
obtained 
from 


X

U(d1)=(x2;x1;d1)[u(x2)2 3(x2)+ 
u2 3(x2)] 


x2 


Since 
the 
probability 
message 
2 3(x2) 
represents 
information 
about 
the 
distribution 
passed 
to 
x2 
via 
x3, 
it 
is 
more 
intuitive 
to 
write 




X

u2 3(x2)

U(d1)=(x2;x1;d1)2 3(x2)u(x2)+ 
2 3(x2)


x2 


which 
has 
the 
interpretation 
of 
the 
average 
of 
a 
utility 
with 
respect 
to 
a 
distribution. 


It 
is 
intuitively 
clear 
that 
we 
can 
continue 
along 
this 
line 
for 
richer 
structures 
than 
chains. 
Indeed, 
provided 
we 
have 
formed 
an 
appropriate 
junction 
tree, 
we 
can 
pass 
potential 
and 
utility 
messages 
from 
clique 
to 
neighbouring 
clique, 
as 
described 
in 
the 
following 
section. 


7.4.2 
Using 
a 
junction 
tree 
In 
complex 
IDs 
computational 
eciency 
in 
carrying 
out 
the 
series 
of 
summations 
and 
maximisations 
may 
be 
an 
issue 
and 
one 
therefore 
seeks 
to 
exploit 
structure 
in 
the 
ID. 
It 
is 
intuitive 
that 
some 
form 
of 
junction 
tree 
style 
algorithm 
is 
applicable. 
We 
can 
rst 
represent 
an 
ID 
using 
decision 
potentials 
which 
consist 
of 
two 
parts, 
as 
dened 
below. 


Denition 
53 
(Decision 
Potential). 
A 
decision 
potential 
on 
a 
clique 
C 
contains 
two 
potentials: 
a 
proba
bility 
potential 
C 
and 
a 
utility 
potential 
C 
. 
The 
joint 
potentials 
for 
the 
junction 
tree 
are 
dened 
as 


YX

. 
=C 
;µ 
=C 
(7.4.9) 
C2C 
C2C 


with 
the 
junction 
tree 
representing 
the 
term 
. 


3For 
our 
MDP 
example 
all 
these 
probability 
messages 
are 
unity. 


DRAFT 
March 
9, 
2010 



Solving 
Inuence 
Diagrams 


In 
this 
case 
there 
are 
constraints 
on 
the 
triangulation, 
imposed 
by 
the 
partial 
ordering 
which 
restricts 
the 
variables 
elimination 
sequence. 
This 
results 
in 
a 
so-called 
strong 
Junction 
Tree. 
The 
treatment 
here 
is 
inspired 
by 
[146]; 
a 
related 
approach 
which 
deals 
with 
more 
general 
chain 
graphs 
is 
given 
in 
[69]. 
The 
sequence 
of 
steps 
required 
to 
construct 
a 
JT 
for 
an 
ID 
is 
as 
follows: 


Remove 
Information 
Edges 
Parental 
links 
of 
decision 
nodes 
are 
removed4 
. 


Moralization 
Marry 
all 
parents 
of 
the 
remaining 
nodes. 


Remove 
Utility 
Nodes 
Remove 
the 
utility 
nodes 
and 
their 
parental 
links. 


Strong 
Triangulation 
Form 
a 
triangulation 
based 
on 
an 
elimination 
order 
which 
obeys 
the 
partial 
ordering 
of 
the 
variables. 


Strong 
Junction 
Tree 
From 
the 
strongly 
triangulated 
graph, 
form 
a 
junction 
tree 
and 
orient 
the 
edges 
towards 
the 
strong 
root 
(the 
clique 
that 
appears 
last 
in 
the 
elimination 
sequence). 


The 
cliques 
are 
ordered 
according 
to 
the 
sequence 
in 
which 
they 
are 
eliminated. 
The 
separator 
probability 
cliques 
are 
initialised 
to 
the 
identity, 
with 
the 
separator 
utilities 
initialised 
to 
zero. 
The 
probability 
cliques 
are 
then 
initialised 
by 
placing 
conditional 
probability 
factors 
into 
the 
lowest 
available 
clique 
(according 
to 
the 
elimination 
order) 
that 
can 
contain 
them, 
and 
similarly 
for 
the 
utilities. 
Remaining 
probability 
cliques 
are 
set 
to 
the 
identity 
and 
utility 
cliques 
to 
zero. 


Example 
33 
(Junction 
Tree). 
An 
example 
of 
a 
junction 
tree 
for 
an 
ID 
is 
given 
in 
g(7.8a). 
The 
moralisation 
and 
triangulation 
links 
are 
given 
in 
g(7.8b). 
The 
orientation 
of 
the 
edges 
follows 
the 
partial 
ordering 
with 
the 
leaf 
cliques 
being 
the 
rst 
to 
disappear 
under 
the 
sequence 
of 
summations 
and 
maximisations. 


A 
by-product 
of 
the 
above 
steps 
is 
that 
the 
cliques 
describe 
the 
fundamental 
dependencies 
on 
previous 
decisions 
and 
observations. 
In 
g(7.8a), 
for 
example, 
the 
information 
link 
from 
f 
to 
D2 
is 
not 
present 
in 
the 
moralised-triangulated 
graph 
g(7.8b), 
nor 
in 
the 
associated 
cliques 
of 
g(7.8c). 
This 
is 
because 
once 
e 
is 
revealed, 
the 
utility 
U4 
is 
independent 
of 
f, 
giving 
rise 
to 
the 
two-branch 
structure 
in 
g(7.8b). 
Nevertheless, 
the 
information 
link 
from 
f 
to 
D2 
is 
fundamental 
since 
it 
species 
that 
f 
will 
be 
revealed 
– 
removing 
this 
link 
would 
therefore 
change 
the 
partial 
ordering. 


Absorption 


By 
analogy 
with 
the 
denition 
of 
messages 
in 
section(7.4.1), 
for 
two 
neighbouring 
cliques 
C1 
and 
C2, 
where 
C1 
is 
closer 
to 
the 
strong 
root 
of 
the 
JT 
(the 
last 
clique 
dened 
through 
the 
elimination 
order), 
we 
dene 




XX

S 
= 
C2 
;S 
= 
C2 
C2 
(7.4.10) 
C2nSC2nS 


new 
new

S;µ 
+ 
S 
(7.4.11)

C1 
= 
C1 
C1 
= 
C1 


S 


P

In 
the 
aboveis 
a 
`generalised 
marginalisation’ 
operation 
– 
it 
sums 
over 
those 
elements 
of 
clique 
C

C 


which 
are 
random 
variables 
and 
maximises 
over 
the 
decision 
variables 
in 
the 
clique. 
The 
order 
of 
this 
sequence 
of 
sums 
and 
maximisations 
follows 
the 
partial 
ordering 
dened 
by 
. 


Absorption 
is 
then 
computed 
from 
the 
leaves 
inwards 
to 
the 
root 
of 
the 
strong 
Junction 
Tree. 
The 
optimal 
setting 
of 
a 
decision 
D1 
can 
then 
be 
computed 
from 
the 
root 
clique. 
Subsequently 
backtracking 
may 
be 


4Note 
that 
for 
the 
case 
in 
which 
the 
domain 
is 
dependent 
on 
the 
parental 
variables, 
such 
links 
must 
remain. 


DRAFT 
March 
9, 
2010 



Solving 
Inuence 
Diagrams 


a
b
c
d
e
f
g
D2
D1
U1
D4
i
lU4
h
j
k
D3
U2
U3
a
b
c
d
e
f
g
D2
D1
D4
i
l
h
j
k
D3
(a) 
(b) 
b;D1;e;f;d
b;e;d;c
b;c;a
f;D3;hD3;h;kh;k;j
e;D2;gD2;g;D4;iD4;i;l
b;c
b;e;de
D2;gD4;i
fD3;hh;k
(c) 
Figure 
7.8: 
(a): 
Inuence 
Diagram, 
adapted 
from 
[146]. 
Causal 
consistency 
is 
satised 
since 
there 
is 
a 
directed 
path 
linking 
the 
all 
decisions 
in 
sequence. 
The 
partial 
ordering 
is 
b 
D1 
(e, 
f) 
D2 
() 
D
3 
g 
D4 
(a, 
c, 
d, 
h, 
i, 
j, 
k, 
l). 
(b): 
Moralised 
and 
strongly 
triangulated 
graph. 
Moralisation 
links 
are 
in 
green, 
strong 
triangulation 
links 
are 
in 
red. 
(c): 
Strong 
Junction 
Tree. 
Absorption 
passes 
information 
from 
the 
leaves 
of 
the 
tree 
towards 
the 
root. 


applied 
to 
infer 
the 
optimal 
decision 
trajectory. 
The 
optimal 
decision 
for 
D 
can 
be 
obtained 
by 
working 
with 
the 
clique 
containing 
D 
which 
is 
closest 
to 
the 
strong 
root 
and 
setting 
any 
previously 
taken 
decisions 
and 
revealed 
observations 
into 
their 
evidential 
states. 
See 
demoDecAsia.m 
for 
an 
example. 


Example 
34 
(Absorption 
on 
a 
chain). 
For 
the 
ID 
of 
g(7.7), 
the 
moralisation 
and 
triangulation 
steps 
are 
trivial 
and 
give 
the 
JT: 


3:x1;x2;d1x22:x2;x3;d2x31:x3;x4;d3
where 
the 
cliques 
are 
indexed 
according 
the 
elimination 
order. 
The 
probability 
and 
utility 
cliques 
are 
initialised 
to 


3 
(x1;x2;d1)= 
p(x2jx1;d1) 
3 
(x1;x2;d1)=0 
2 
(x2;x3;d2)= 
p(x3jx2;d2) 
2 
(x2;x3;d2)= 
u(x2) 
(7.4.12) 
1 
(x3;x4;d3)= 
p(x4jx3;d3) 
1 
(x3;x4;d3)= 
u(x3)+ 
u(x4) 


DRAFT 
March 
9, 
2010 



Solving 
Inuence 
Diagrams 


with 
the 
separator 
cliques 
initialised 
to 


1..2 
(x3)=1 
1..2 
(x3)=0 


(7.4.13)
2..3 
(x2)=1 
2..3 
(x2)=0 


Updating 
the 
separator 
we 
have 
the 
new 
probability 
potential 


X

1..2 
(x3)* 
= 
max 
1 
(x3;x4;d3) 
= 
1 
(7.4.14) 


d3

x4 


and 
utility 
potential 


XX

1..2 
(x3)* 
= 
max 
1 
(x3;x4;d3) 
1 
(x3;x4;d3) 
= 
max 
p(x4jx3;d3)(u(x3)+ 
u(x4)) 
(7.4.15) 


d3d3

x4 
x4 


X

= 
max 
u(x3)+p(x4jx3;d3)u(x4) 
(7.4.16) 


d3 


x4 


At 
the 
next 
step 
we 
update 
the 
probability 
potential 


2 
(x2;x3;d2)* 
= 
2 
(x2;x3;d2) 
1..2 
(x3)* 
= 
1 
(7.4.17) 


and 
utility 
potential 


X

1..2 
(x3)* 


2 
(x2;x3;d2)* 
= 
2 
(x2;x3;d2)+ 
= 
u(x2) 
+ 
max 
u(x3)+p(x4jx3;d3)u(x4) 
(7.4.18)
1..2 
(x3) 
d3 


x4 


The 
next 
separator 
decision 
potential 
is 


X

2..3 
(x2)* 
= 
max 
2 
(x2;x3;d2)* 
= 
1 
(7.4.19) 


d2

x3 


X

2..3 
(x2)* 
= 
max 
2 
(x2;x3;d2) 
2 
(x2;x3;d2)* 
(7.4.20) 


d2x3 
  X!!X

XX

= 
max 
p(x3jx2;d2) 
u(x2) 
+ 
max 
u(x3)+p(x4jx3;d3)u(x4) 
(7.4.21) 


d2d3 


x3 
x4 


Finally 
we 
end 
up 
with 
the 
root 
decision 
potential 


3 
(x1;x2;d1)* 
= 
3 
(x1;x2;d1) 
2..3 
(x2)* 
= 
p(x2jx1;d1) 
(7.4.22) 


and 


2..3 
(x2)* 


3 
(x1;x2;d1)* 
= 
3 
(x2;x1;d1) 
+ 
(7.4.23)

2..3 
(x2)* 


XX

= 
max 
p(x3jx2;d2) 
u(x2) 
+ 
max 
u(x3)+p(x4jx3;d3)u(x4) 
(7.4.24) 


d2d3 


x3 
x4 


From 
the 
nal 
decision 
potential 
we 
have 
the 
expression 


3 
(x1;x2;d1)* 
3 
(x1;x2;d1)* 
(7.4.25) 


which 
is 
equivalent 
to 
that 
which 
would 
be 
obtained 
by 
simply 
distributing 
the 
summations 
and 
maximisations 
over 
the 
original 
ID. 
At 
least 
for 
this 
special 
case, 
we 
therefore 
have 
veried 
that 
the 
JT 
approach 
yields 
the 
correct 
root 
clique 
potentials. 


DRAFT 
March 
9, 
2010 



Markov 
Decision 
Processes 


7.5 
Markov 
Decision 
Processes 
Consider 
a 
Markov 
chain 
with 
transition 
probabilities 
p(xt+1 
= 
jjxt 
= 
i). 
At 
each 
time 
t 
we 
consider 
an 
action 
(decision), 
which 
aects 
the 
state 
at 
time 
t 
+ 
1. 
We 
describe 
this 
by 


p(xt+1 
= 
ijxt 
= 
j, 
dt 
= 
k) 
(7.5.1) 


Associated 
with 
each 
state 
xt 
= 
i 
is 
a 
utility 
u(xt 
= 
i), 
and 
is 
schematically 
depicted 
in 
g(7.7). 
One 
use 
of 
such 
an 
environment 
model 
would 
be 
to 
help 
plan 
a 
sequence 
of 
actions 
(decisions) 
required 
to 
reach 
a 
goal 
state 
in 
minimal 
total 
summed 
cost. 


More 
generally 
one 
could 
consider 
utilities 
that 
depend 
on 
transitions 
and 
decisions, 
u(xt+1 
= 
i, 
xt 
= 
j, 
dt 
= 
k) 
and 
also 
time 
dependent 
versions 
of 
all 
of 
these, 
pt(xt+1 
= 
ijxt 
= 
j, 
dt 
= 
k), 
ut(xt+1 
= 
i, 
xt 
= 
j, 
dt 
= 
k). 
We'll 
stick 
with 
the 
time-independent 
(stationary) 
case 
here 
since 
the 
generalisations 
are 
conceptually 
straightforward 
at 
the 
expense 
of 
notational 
complexity. 


MDPs 
can 
be 
used 
to 
solve 
planning 
tasks 
such 
as 
how 
can 
one 
get 
to 
a 
desired 
goal 
state 
as 
quickly 
as 
possible. 
By 
dening 
the 
utility 
of 
being 
in 
the 
goal 
state 
as 
high, 
and 
being 
in 
the 
non-goal 
state 
as 
a 
low 
value, 
at 
each 
time 
t, 
we 
have 
a 
utility 
u(xt) 
of 
being 
in 
state 
xt. 
For 
positive 
utilities, 
the 
total 
utility 
of 
any 
state-decision 
path 
x1:T 
;d1:T 
is 
dened 
as 
(assuming 
we 
know 
the 
initial 
state 
x1) 


T

X

U(x1:T 
) 
= 
u(xt) 
(7.5.2) 


t=2 
and 
the 
probability 
with 
which 
this 
happens 
is 
given 
by 


T 
..1

YX

p(x2:T 
jx1;d1:T 
..1)= 
p(xt+1jxt;dt) 
(7.5.3) 


t=1 
At 
time 
t 
= 
1 
we 
want 
to 
make 
that 
decision 
d1 
that 
will 
lead 
to 
maximal 
expected 
total 
utility 


XXXX

U(d1) 
max 
max 
::. 
max 
p(x2:T 
jx1;d1:T 
..1)U(x1:T 
) 
(7.5.4) 


d2d3dT 
..1

x2 
x3 
x4 
xT 


Our 
task 
is 
to 
compute 
U(d1) 
for 
each 
state 
of 
d1 
and 
then 
choose 
that 
state 
with 
maximal 
expected 
total 
utility. 
To 
carry 
out 
the 
summations 
and 
maximisations 
eciently, 
we 
could 
use 
the 
junction 
tree 
approach, 
as 
described 
in 
the 
previous 
section. 
However, 
in 
this 
case, 
the 
ID 
is 
suciently 
simple 
that 
a 
direct 
message 
passing 
approach 
can 
be 
used 
to 
compute 
the 
expected 
utility. 


7.5.1 
Maximising 
expected 
utility 
by 
message 
passing 
Consider 
the 
MDP 


T 
..1T

YXX

p(xt+1jxt;dt) 
u(xt) 
(7.5.5) 


t=1 
t=2 
For 
the 
specic 
example 
in 
g(7.7) 
the 
joint 
model 
of 
the 
BN 
and 
utility 
is 


p(x4jx3;d3)p(x3jx2;d2)p(x2jx1;d1)(u(x2)+ 
u(x3)+ 
u(x4)) 
(7.5.6) 


To 
decide 
on 
how 
to 
take 
the 
rst 
optimal 
decision, 
we 
need 
to 
compute 


XXX

U(d1) 
=max 
max 
p(x4jx3;d3)p(x3jx2;d2)p(x2jx1;d1)(u(x2)+ 
u(x3)+ 
u(x4)) 
(7.5.7) 


d2d3

x2 
x3 
x4 


Since 
only 
u(x4) 
depends 
on 
x4 
explicitly, 
we 
can 
write 


XXX

U(d1) 
=max 
max 
p(x4jx3;d3)p(x3jx2;d2)p(x2jx1;d1)u(x4) 


d2d3

x2 
x3 
x4

XX

+max 
p(x3jx2;d2)p(x2jx1;d1)u(x3) 


d2

x2 
x3

X

+p(x2jx1;d1)u(x2) 
(7.5.8) 


x2 


120 
DRAFT 
March 
9, 
2010 



Markov 
Decision 
Processes 


For 
each 
line 
we 
distribute 
the 
operations: 


X

U(d1)=

x2

X

+

x2

X

+

x2 


p(x2jx1;d1)u(x2) 


We 
now 
start 
with 
the 
rst 
line 
and 
carry 
out 
the 
summation 
over 
x4 
gives 
a 
new 
function 
of 
x3, 


X

u3 4(x3) 
= 
max 
p(x4jx3;d3)u(x4) 


d3

x4 


which 
we 
can 
incorporate 
in 
the 
next 
line 


XX

U(d1)=p(x2jx1;d1) 
max 
p(x3jx2;d2)[u(x3)+ 
u3 4(x3)] 


d2

x2 
x3

X

+p(x2jx1;d1)u(x2) 


x2 


p(x2jx1;d1) 
max 


d2

p(x2jx1;d1) 
max 


d2

XX

p(x3jx2;d2) 
max 
p(x4jx3;d3)u(x4) 


d3

x3 
x4

X

p(x3jx2;d2)u(x3) 


x3 


(7.5.9) 
and 
maximisation 
over 
d3. 
This 
(7.5.10) 
(7.5.11) 
Similarly, 
we 
can 
now 
carry 
out 
the 
sum 
over 
x3 
and 
max 
over 
d2 
to 
dene 
a 
new 
function 


X

u2 3(x2) 
= 
max 
p(x3jx2;d2)[u(x3)+ 
u3 4(x3)] 
(7.5.12) 


d2

x3 


to 
give 


X

U(d1)=p(x2jx1;d1)[u(x2)+ 
u2 3(x2)] 
(7.5.13) 


x2 


Given 
U(d1) 
above, 
we 
can 
then 
nd 
the 
optimal 
decision 
d1 
by 


d 
* 
1 
= 
argmax 
U(d1) 
(7.5.14) 


d1 


What 
about 
d* 
2? 
Bear 
in 
mind 
that 
when 
we 
come 
to 
make 
decision 
d2 
we 
will 
have 
observed 
x1;x2 
and 
d2. 
We 
can 
then 
nd 
d* 
by 


2 


X

argmax 
p(x3jx2;d2)[u(x3)+ 
u3 4(x3)] 
(7.5.15) 


d2

x3 


Subsequently 
we 
can 
backtrack 
further 
to 
nd 
d* 
3. 
In 
general, 
the 
optimal 
decision 
is 
given 
by 


X

d 
* 
t..1 
= 
argmax 
p(xtjxt..1;dt..1)[u(xt)+ 
ut t+1(xt)] 
(7.5.16) 
dt..1

xt 


7.5.2 
Bellman's 
equation 
In 
a 
Markov 
Decision 
Process, 
as 
above, 
we 
can 
dene 
utility 
messages 
recursively 
as 


X

ut..1 t(xt..1) 
= 
max 
p(xtjxt..1;dt..1)[u(xt)+ 
ut t+1(xt)] 
(7.5.17) 
dt..1

xt 


It 
is 
more 
common 
to 
dene 
the 
value 
of 
being 
in 
state 
xt 
as 


vt(xt) 
= 
u(xt)+ 
ut t+1(xt);vT 
(xT 
)= 
u(xT 
) 
(7.5.18) 


and 
write 
then 
the 
equivalent 
recursion 


X

vt..1(xt..1)= 
u(xt..1) 
+ 
max 
p(xtjxt..1;dt..1)vt(xt) 
(7.5.19) 
dt..1

xt 


DRAFT 
March 
9, 
2010 
121 



Temporally 
Unbounded 
MDPs 


The 
optimal 
decision 
d* 
is 
then 
given 
by 


t 


X

d 
* 
= 
argmax 
p(xt+1jxt;dt)v(xt+1) 
(7.5.20)

t 
dt


xt+1 


Equation(7.5.19) 
is 
called 
Bellman's 
equation[30]5 
. 


7.6 
Temporally 
Unbounded 
MDPs 
In 
the 
previous 
discussion 
about 
MDPs 
we 
assumed 
a 
given 
end 
time, 
T 
, 
from 
which 
one 
can 
propagate 
messages 
back 
from 
the 
end 
of 
the 
chain. 
The 
innite 
T 
case 
would 
appear 
to 
be 
ill-dened 
since 
the 
sum 
of 
utilities 


u(x1)+ 
u(x2)+ 
::. 
+ 
u(xT 
) 
(7.6.1) 




will 
in 
general 
be 
unbounded. 
There 
is 
a 
simple 
way 
to 
avoid 
this 
diculty. 
If 
we 
let 
u 
= 
maxs 
u(s) 
be 
the 
largest 
value 
of 
the 
utility 
and 
consider 
the 
sum 
of 
modied 
utilities 
for 
a 
chosen 
discount 
factor 
0 
<< 
1 


TT1 
- 
T

XX

* 


t 
u(xt) 
= 
ut 
= 
u 
* 
(7.6.2)

1 
- 
. 


t=1 
t=1 


where 
we 
used 
the 
result 
for 
a 
geometric 
series. 
In 
the 
limit 
T 
!8 
this 
means 
that 
the 
summed 
modied 
utility 
tu(xt) 
is 
nite. 
The 
only 
modication 
required 
to 
our 
previous 
discussion 
is 
to 
include 
a 
factor 
. 
in 
the 
message 
denition. 
Assuming 
that 
we 
are 
at 
convergence, 
we 
dene 
a 
value 
v(xt 
= 
s) 
dependent 
only 
on 
the 
state 
s, 
and 
not 
the 
time. 
This 
means 
we 
replace 
the 
time-dependent 
Bellman's 
value 
recursion 
equation 
(7.5.19) 
with 
the 
stationary 
equation 


X

v(s) 
= 
u(s)+ 
. 
max 
p(xt 
= 
s0jxt..1 
= 
s;dt..1 
= 
d)v(s0) 
(7.6.3) 


d

s' 


We 
then 
need 
to 
solve 
equation 
(7.6.3) 
for 
the 
value 
v(s) 
for 
all 
states 
s. 
The 
optimal 
decision 
policy 
when 
one 
is 
in 
state 
xt 
= 
s 
is 
then 
given 
by 


X

d 
(s) 
= 
argmax 
p(xt+1 
= 
s0jxt 
= 
s;dt 
= 
d)v(s0) 
(7.6.4) 


d

s' 


For 
a 
deterministic 
transition 
p 
(i.e. 
for 
each 
decision 
d, 
only 
one 
state 
s/ 
is 
available), 
this 
means 
that 
the 
best 
decision 
is 
the 
one 
that 
takes 
us 
to 
the 
accessible 
state 
with 
highest 
value. 


Equation(7.6.3) 
seems 
straightforward 
to 
solve. 
However, 
the 
max 
operation 
means 
that 
the 
equations 
are 
non-linear 
in 
the 
value 
v 
and 
no 
closed 
form 
solution 
is 
available. 
Two 
popular 
techniques 
for 
solving 
equation 
(7.6.3), 
are 
Value 
and 
Policy 
iteration, 
which 
we 
describe 
below. 
When 
the 
number 
of 
states 
S 
is 
very 
large, 
approximate 
solutions 
are 
required. 
Sampling 
and 
state-dimension 
reduction 
techniques 
are 
described 
in 
[58]. 


7.6.1 
Value 
iteration 
A 
naive 
procedure 
is 
to 
iterate 
equation 
(7.6.3) 
until 
convergence, 
assuming 
some 
initial 
guess 
for 
the 
values 
(say 
uniform). 
One 
can 
show 
that 
this 
value 
iteration 
procedure 
is 
guaranteed 
to 
converge 
to 
a 
unique 
optimum[34]. 
The 
convergence 
rate 
depends 
somewhat 
on 
the 
discount 
. 
– 
the 
smaller 
. 
is, 
the 
faster 
is 
the 
convergence. 
An 
example 
of 
value 
iteration 
is 
given 
in 
g(7.10). 


5The 
continuous-time 
analog 
has 
a 
long 
history 
in 
physics 
and 
is 
called 
the 
Hamilton-Jacobi 
equation 
and 
enables 
one 
to 
solve 
MDPs 
by 
message 
passing, 
this 
being 
a 
special 
case 
of 
the 
more 
general 
junction 
tree 
approach 
described 
earlier 
in 
section(7.4.2). 


122 
DRAFT 
March 
9, 
2010 



Temporally 
Unbounded 
MDPs 


1020314050607080901001101201301401501601701811902002102202302402502602702802903003103203303403503603703803904004104204304404504
1470480490500511520530540550560570580590600610620630640650660670680690701710720731740750760770780790800810820830840850860870880909019109219309409509609709809901000
Figure 
7.9: 
States 
dened 
on 
a 
two 
dimensional 
grid. 
In 
each 
square 
the 
top 
left 
value 
is 
the 
state 
number, 
and 
the 
bottom 
right 
is 
the 
utility 
of 
being 
in 
that 
state. 
An 
`agent’ 
can 
move 
from 
a 
state 
to 
a 
neighbouring 
state, 
as 
indicated. 
The 
task 
is 
to 
solve 
this 
problem 
such 
that 
for 
any 
position 
(state) 
one 
knows 
how 
to 
move 
optimally 
to 
maximise 
the 
expected 
utility. 
This 
means 
that 
we 
need 
to 
move 
towards 
the 
goal 
states 
(states 
with 
nonzero 
utility). 
See 
demoMDP. 


123456789101112131415123456789101112131415024
Figure 
7.10: 
Value 
Iteration 
on 
a 
set 
of 
225 
states, 
corresponding 
to 
a 
15 
× 
15 
two 
dimensional 
grid. 
Deterministic 
transitions 
are 
allowed 
to 
neighbours 
on 
the 
grid, 
fstay, 
left, 
right, 
up, 
downg. 
There 
are 
three 
goal 
states, 
each 
with 
utility 
1 
– 
all 
other 
states 
have 
utility 
0. 
Plotted 
is 
the 
value 
v(s) 
for 
. 
=0:9 
after 
30 
updates 
of 
Value 
Iteration, 
where 
the 
states 
index 
a 
point 
on 
the 
x 
- 
y 
grid. 
The 
optimal 
decision 
for 
any 
state 
on 
the 
grid 
is 
to 
go 
to 
the 
neighbouring 
state 
with 
highest 
value. 
See 
demoMDP. 


7.6.2 
Policy 
iteration 
In 
policy 
iteration 
we 
rst 
assume 
we 
know 
the 
optimal 
decision 
d 
(s) 
for 
any 
state 
s. 
We 
may 
use 
this 
in 
equation 
(7.6.3) 
to 
give 


X

v(s)= 
u(s)+ 
p(xt 
= 
s0jxt..1 
= 
s, 
d 
(s))v(s0) 
(7.6.5) 


s' 


The 
maximisation 
over 
d 
has 
disappeared 
since 
we 
have 
assumed 
we 
already 
know 
the 
optimal 
decision 
for 
each 
state 
s. 
For 
xed 
d 
(s), 
equation 
(7.6.5) 
is 
now 
linear 
in 
the 
value. 
Dening 
the 
value 
v 
and 
utility 
u 
vectors 
and 
transition 
matrix 
P, 


[v]= 
v(s), 
[u]= 
u(s), 
[P]= 
p(s'js, 
d 
* 
(s)) 
(7.6.6)

ss 
s' 
;s 


in 
matrix 
notation, 
equation 
(7.6.5) 
becomes 


..1 


v 
= 
u 
+ 
PTv 
,I 
- 
PTv 
= 
u 
. 
v 
=I 
- 
PTu 
(7.6.7) 


These 
linear 
equations 
are 
readily 
solved 
with 
Gaussian 
Elimination. 
Using 
this, 
the 
optimal 
policy 
is 
recomputed 
using 
equation 
(7.6.4). 
The 
two 
steps 
of 
solving 
for 
the 
value, 
and 
recomputing 
the 
policy 
are 
iterated 
until 
convergence. 


In 
Policy 
Iteration 
we 
guess 
an 
initial 
d 
(s), 
then 
solve 
the 
linear 
equations 
(7.6.5) 
for 
the 
value, 
and 
then 
recompute 
the 
optimal 
decision. 
See 
demoMDP.m 
for 
a 
comparison 
of 
value 
and 
policy 
iteration, 
and 
also 
an 
EM 
style 
approach 
which 
we 
discuss 
in 
the 
next 
section. 


Example 
35 
(A 
grid-world 
MDP). 
A 
set 
of 
states 
dened 
on 
a 
grid, 
utilities 
for 
being 
in 
a 
grid 
state 
is 
given 
in 
g(7.9), 
for 
which 
the 
agent 
deterministically 
moves 
to 
a 
neighbouring 
grid 
state 
at 
each 
time 
step. 
After 
initialising 
the 
value 
of 
each 
grid 
state 
to 
unity, 
the 
converged 
value 
for 
each 
state 
is 
given 
in 
g(7.10). 
The 
optimal 
policy 
is 
then 
given 
by 
moving 
to 
the 
neighbouring 
grid 
state 
with 
highest 
value. 


DRAFT 
March 
9, 
2010 
123 



Probabilistic 
Inference 
and 
Planning 


x1x2x3
u3
d1d2
x1x2x3
u3
d1d2Figure7.11:(a):AMarkovDecisionPro-
cess.(b):Thecorrespondingprobabilis-
ticinferenceplanner.
(a) 
(b) 
7.6.3 
A 
curse 
of 
dimensionality 
Consider 
the 
following 
Tower 
of 
Hanoi 
problem. 
There 
are 
4 
pegs 
a, 
b, 
c, 
d 
and 
10 
disks 
numbered 
from 
1 
to 
10. 
You 
may 
move 
a 
single 
disk 
from 
one 
peg 
to 
another 
– 
however, 
you 
are 
not 
allowed 
to 
put 
a 
bigger 
numbered 
disk 
on 
top 
of 
a 
smaller 
numbered 
disk. 
Starting 
with 
all 
disks 
on 
peg 
a, 
how 
can 
you 
move 
them 
all 
to 
peg 
d 
in 
the 
minimal 
number 
of 
moves? 


This 
would 
appear 
to 
be 
a 
straightforward 
Markov 
decision 
process 
in 
which 
the 
transitions 
are 
allowed 
disk 
moves. 
If 
we 
use 
x 
to 
represent 
the 
state 
of 
the 
disks 
on 
the 
4 
pegs, 
this 
has 
410 
= 
1048576 
states 
(some 
are 
equivalent 
up 
to 
permutation 
of 
the 
pegs, 
which 
reduces 
this 
by 
a 
factor 
of 
2). 
This 
large 
number 
of 
states 
renders 
this 
naive 
approach 
computationally 
problematic. 


Many 
interesting 
real-world 
problems 
suer 
from 
this 
large 
number 
of 
states 
issue 
so 
that 
a 
naive 
approach 
based 
as 
we've 
described 
is 
computationally 
infeasible. 
Finding 
ecient 
exact 
and 
also 
approximate 
state 
representations 
is 
a 
key 
aspect 
to 
solving 
large 
scale 
MDPs, 
see 
for 
example 
[193]. 


7.7 
Probabilistic 
Inference 
and 
Planning 
An 
alternative 
to 
the 
classical 
MDP 
solution 
methods 
is 
to 
make 
use 
of 
the 
standard 
methods 
for 
training 
probabilistic 
models, 
such 
as 
the 
Expectation-Maximisation 
algorithm. 
In 
order 
to 
do 
so 
we 
rst 
need 
to 
write 
the 
problem 
of 
maximising 
expected 
utility 
in 
a 
form 
that 
is 
suitable. 
To 
do 
this 
we 
rst 
discuss 
how 
a 
MDP 
can 
be 
expressed 
as 
the 
maximisation 
of 
a 
form 
of 
Belief 
Network 
in 
which 
the 
parameters 
to 
be 
found 
relate 
to 
the 
policy. 


7.7.1 
Non-stationary 
Markov 
Decision 
Process 
Consider 
the 
MDP 
in 
g(7.11a) 
in 
which, 
for 
simplicity, 
we 
assume 
we 
know 
the 
initial 
state 
x1 
= 
x1. 
Our 
task 
is 
then 
to 
nd 
the 
decisions 
that 
maximise 
the 
expected 
utility, 
based 
on 
a 
sequential 
decision 
process. 
The 
rst 
decision 
d1 
is 
given 
by 
maximising 
the 
expected 
utility: 


XXU(d1) 
=
x2 
p(x2jx1, 
d1) 
max 
d2x3 
p(x3jx2, 
d2)u3(x3) 
(7.7.1) 
More 
generally, 
this 
utility 
can 
be 
computed 
eciently 
using 
a 
standard 
message 
passing 
routine: 
Xut t+1(xt) 
= 
max 
dtxt+1 
p(xt+1jxt, 
dt)ut+1 t+2(xt+1) 
(7.7.2) 


where 
uT 
 T 
+1(xT 
) 
= 
uT 
(xT 
) 
(7.7.3) 
124 
DRAFT 
March 
9, 
2010 



Probabilistic 
Inference 
and 
Planning 


7.7.2 
Non-stationary 
probabilistic 
inference 
planner 
As 
an 
alternative 
to 
the 
above 
MDP 
description, 
consider 
the 
Belief 
Network 
g(7.11b) 
in 
which 
we 
have 
a 
utility 
associated 
with 
the 
last 
time-point[279]. 
Then 
the 
expected 
utility 
is 
given 
by 


X

U(1;2)=p(d1jx1;1)p(x2jx1;d1)p(d2jx2;2)p(x3jx2;d2)u3(x3) 
(7.7.4) 


d1;d2;x2;x3 


XXXX

=p(d1jx1;1)p(x2jx1;d1)p(d2jx2;2)p(x3jx2;d2)u3(x3) 
(7.7.5) 


d1 
x2 
d2 
x3 


Here 
the 
terms 
p(dtjxt;t) 
are 
the 
`policy 
distributions’ 
that 
we 
wish 
to 
learn 
and 
t 
are 
the 
parameters 
of 
the 
tth 
policy 
distribution. 
Let's 
assume 
that 
we 
have 
one 
per 
time 
so 
that 
t 
is 
a 
function 
that 
maps 
a 
state 
x 
to 
a 
probability 
distribution 
over 
decisions. 
Our 
interest 
is 
to 
nd 
the 
policy 
distributions 
1;2 
that 
maximise 
the 
expected 
utility. 
Since 
each 
time-step 
has 
its 
own 
t 
and 
for 
each 
state 
x2 
= 
x2 
we 
have 
a 
separate 
unconstrained 
distribution 
p(d2jx2;2) 
to 
optimise 
over 
and 
we 
can 
write 


XXXX

max 
U(1;2) 
= 
maxp(d1jx1;1)p(x2jx1;d1) 
maxp(d2jx2;2)p(x3jx2;d2)u3(x3) 
(7.7.6) 


1;2 
12
d1 
x2 
d2 
x3 


This 
shows 
that 
provided 
there 
are 
no 
constraints 
on 
the 
policy 
distributions 
(there 
is 
a 
separate 
one 
for 
each 
timepoint), 
we 
are 
allowed 
to 
distribute 
the 
maximisations 
over 
the 
individual 
policies 
inside 
the 
summation. 


More 
generally, 
for 
a 
nite 
time 
T 
one 
can 
dene 
messages 
to 
solve 
for 
the 
optimal 
policy 
distributions 


XX

ut t+1(xt) 
= 
max 
p(dtjxt;t)p(xt+1jxt;dt)ut+1 t+2(xt+1) 
(7.7.7)

t
dt 
xt+1 


with 


uT 
+1 T 
(xT 
)= 
uT 
(xT 
) 
(7.7.8) 


Deterministic 
policy 


For 
a 
deterministic 
policy, 
only 
a 
single 
state 
is 
allowed, 
so 
that 


p(dtjxt;t)= 
d 
(dt;d 
(xt)) 
(7.7.9)

t 


where 
d(x) 
is 
a 
policy 
function 
that 
maps 
a 
state 
x 
to 
a 
single 
decision 
d. 
Since 
we 
have 
a 
separate 
policy 


t 


function 
for 
each 
time 
t 
equation 
(7.7.7) 
reduces 
to 


X

ut t+1(xt) 
= 
max 
p(xt+1jxt;d 
* 
t 
(xt))ut+1 t+2(xt+1) 
(7.7.10) 


d

(xt)

t 


xt+1 


which 
is 
equivalent 
to 
equation 
(7.7.2). 


This 
shows 
that 
solving 
the 
MDP 
is 
equivalent 
to 
maximising 
a 
standard 
expected 
utility 
dened 
in 
terms 
of 
a 
Belief 
Network 
under 
the 
assumption 
that 
each 
time 
point 
has 
its 
own 
policy 
distribution, 
and 
that 
this 
is 
deterministic. 


7.7.3 
Stationary 
planner 
If 
we 
reconsider 
our 
simple 
example, 
g(7.11b) 
but 
now 
constrain 
the 
policy 
distributions 
to 
be 
the 
same 
for 
all 
time, 
p(dtjxt;t)= 
p(dtjxt;) 
(or 
more 
succinctly 
t 
= 
), 
then 
equation 
(7.7.5) 
becomes 


XXXX

U()=p(d1jx1;)p(x2jx1;d1)p(d2jx2;)p(x3jx2;d2)u3(x3) 
(7.7.11) 


d1 
x2 
d2 
x3 


In 
this 
case 
we 
cannot 
distribute 
the 
maximisation 
over 
the 
policy 
p 
over 
the 
individual 
terms 
of 
the 
product. 
However, 
computing 
the 
expected 
utility 
for 
any 
given 
policy 
p 
is 
straightforward, 
using 
message 
passing. 
One 
may 
thus 
optimise 
the 
expected 
utility 
using 
standard 
numerical 
optimisation 
procedures, 
or 
alternatively 
an 
EM 
style 
approach 
as 
we 
discuss 
below. 


DRAFT 
March 
9, 
2010 
125 



Probabilistic 
Inference 
and 
Planning 


A 
variational 
training 
approach 


Without 
loss 
of 
generality, 
we 
assume 
that 
the 
utility 
is 
positive 
and 
dene 
a 
distribution 


p(d1jx1;)p(x2jx1;d1)p(d2jx2;)p(x3jx2;d2)u3(x3)

p~(d1;d2;d3;x2;x3)= 
P(7.7.12) 


p(d1jx1;)p(x2jx1;d1)p(d2jx2;)p(x3jx2;d2)u3(x3)

d1;d2;d3;x2;x3 


Then 
for 
any 
variational 
distribution 
q(d1;d2;d3;x2;x3), 


KL(q(d1;d2;d3;x2;x3)jp~(d1;d2;d3;x2;x3)) 
= 
hlog 
q(d1;d2;d3;x2;x3)) 


q(d1;d2;d3;x2;x3) 


..hlog 
~p(d1;d2;d3;x2;x3)i= 
0 
(7.7.13)

q(d1;d2;d3;x2;x3) 


Using 
the 
denition 
of 
~p(d1;d2;d3;x2;x3) 
and 
the 
fact 
that 
the 
denominator 
in 
equation 
(7.7.12) 
is 
equal 
to 
U() 
we 
obtain 
the 
bound 


log 
U() 
..hlog 
q(d1;d2;d3;x2;x3)i

q(d1;d2;d3;x2;x3) 


+ 
hlog 
p(d1jx1;)p(x2jx1;d1)p(d2jx2;)p(x3jx2;d2)u3(x3)i(7.7.14)
q(d1;d2;d3;x2;x3) 


This 
then 
gives 
a 
two-stage 
EM 
style 
procedure: 


M-step 
Isolating 
the 
dependencies 
on 
, 
for 
a 
given 
variational 
distribution 
qold, 
maximising 
the 
bound 
equation 
(7.7.14) 
is 
equivalent 
to 
maximising 


E() 
hlog 
p(d1jx1;)) 
old(d1) 
+ 
hlog 
p(d2jx2;)) 
old(d2;x2) 
(7.7.15)

qq

One 
then 
nds 
a 
policy 
new 
which 
maximises 
E(): 


new 


= 
argmax 
E() 
(7.7.16) 


p 


E-step 
For 
xed 
p 
the 
best 
q 
is 
given 
by 
the 
update 


q 
new 
. 
p(d1jx1;)p(x2jx1;d1)p(d2jx2;)p(x3jx2;d2)u3(x3) 
(7.7.17) 


From 
this 
joint 
distribution, 
in 
order 
to 
determine 
the 
M-step 
updates, 
we 
only 
require 
the 
marginals 
q(d1) 
and 
q(d2;x2), 
both 
of 
which 
are 
straightforward 
to 
obtain 
since 
q 
is 
simply 
a 
rst 
order 
Markov 
Chain 
in 
the 
joint 
variables 
xt;dt. 
For 
example 
one 
may 
write 
the 
q-distribution 
as 
a 
simple 
chain 
Factor 
Graph 
for 
which 
marginal 
inference 
can 
be 
performed 
readily 
using 
the 
sum-product 
algorithm. 


This 
procedure 
is 
analogous 
to 
the 
standard 
EM 
procedure, 
section(11.2). 
The 
usual 
guarantees 
therefore 
carry 
over 
so 
that 
nding 
a 
policy 
that 
increases 
E() 
is 
guaranteed 
to 
improve 
the 
expected 
utility. 


In 
complex 
situations 
in 
which, 
for 
reasons 
of 
storage, 
the 
optimal 
q 
cannot 
be 
used, 
a 
structured 
constrained 
variational 
approximation 
may 
be 
applied. 
In 
this 
case, 
as 
in 
generalised 
EM, 
only 
a 
guaranteed 
improvement 
on 
the 
lower 
bound 
of 
the 
expected 
utility 
is 
achieved. 
Nevertheless, 
this 
may 
be 
of 
considerable 
use 
in 
practical 
situations, 
for 
which 
general 
techniques 
of 
approximate 
inference 
may 
be 
applied. 


The 
deterministic 
case 


For 
the 
special 
case 
that 
the 
policy 
p 
is 
deterministic, 
p 
simply 
maps 
each 
state 
x 
to 
single 
decision 
d. 
Writing 
this 
policy 
map 
as 
d(x) 
equation 
(7.7.11) 
reduces 
to 


XX

U(d 
)=p(x2jx1;d 
(x1))p(x3jx2;d 
(x2))u3(x3) 
(7.7.18) 


x2 
x3 


We 
now 
dene 
a 
variational 
distribution 
only 
over 
x2;x3, 
q(x2;x3) 
. 
p(x2jx1;d 
(x1))p(x3jx2;d 
(x2))u3(x3) 
(7.7.19) 


DRAFT 
March 
9, 
2010 



Probabilistic 
Inference 
and 
Planning 


and 
the 
`energy’ 
term 
becomes 


E(d 
) 
hlog 
p(x2jx1;d 
(x1))i+hlog 
p(x3jx2;d 
(x2))i(7.7.20)

q(x2) 
q(x2;x3) 


For 
a 
more 
general 
problem 
in 
which 
the 
utility 
is 
at 
the 
last 
time 
point 
T 
and 
no 
starting 
state 
is 
given 
we 
have 
(for 
a 
stationary 
transition 
p(xt+1jxt;dt)) 


 !

XX

E(d(s)) 
q(xt 
= 
s;xt+1 
= 
s0)log 
p(x/ 
= 
s0jx 
= 
s;d(s)= 
d) 
(7.7.21) 


s0t 


and 


d 
(s) 
= 
argmax 
E(d(s)) 
(7.7.22) 


d(s) 


This 
shows 
how 
to 
train 
a 
stationary 
MDP 
using 
EM 
in 
which 
there 
is 
a 
utility 
dened 
only 
at 
the 
last 
time-point. 
Below 
we 
generalise 
this 
to 
the 
case 
of 
utilities 
at 
each 
time 
for 
both 
the 
stationary 
and 
non-stationary 
cases. 


7.7.4 
Utilities 
at 
each 
timestep 
Consider 
a 
generalisation 
in 
which 
we 
have 
an 
additive 
utility 
associated 
with 
each 
time-point. 


Non-stationary 
policy 


To 
help 
develop 
the 
approach, 
let's 
look 
at 
simply 
including 
utilities 
at 
times 
t 
=1, 
2 
for 
the 
previous 
example. 
The 
expected 
utility 
is 
given 
by 


X

U(1;2)=p(d1jx1;1)p(x2jx1;d1)p(d2jx2;2)p(x3jx2;d2)u3(x3) 
(7.7.23) 


d1;d2;x1;x2;x3 


X

+p(d1jx1;1)p(x2jx1;d1)u2(x2)+ 
u1(x1) 


d1;x2 


01XXX

= 
u1(x1)+p(d1jx1;1)p(x2jx1) 
@u2(x2)+p(d2jx2;2)p(x3jx2;d2)u3(x3)AX

d1;x2 
d2;x3 


(7.7.24) 
Dening 
value 
messages 


X

v2 
(x2)= 
u2(x2)+p(d2jx2;2)p(x3jx2;d2)u3(x3) 
(7.7.25) 


d2;x3 


and 


X

v1 
(x1)= 
u1(x1)+p(d1jx1;1)p(x2jx1;d1)v2 
(x2) 
(7.7.26) 


d1;x2 


U(1;2)= 
v1 
(x1) 
(7.7.27) 


For 
a 
more 
general 
case 
dened 
over 
T 
timesteps, 
we 
have 
analogously 
an 
expected 
utility 
U(1:T 
), 
and 
our 
interest 
is 
to 
maximise 
this 
expected 
utility 
with 
respect 
to 
all 
the 
policies 


max 
U(1:T 
) 
(7.7.28)

1:T 


As 
before, 
since 
each 
timestep 
has 
its 
own 
policy 
distribution 
for 
each 
state, 
we 
may 
distribute 
the 
maximisation 
using 
the 
recursion 


X

vt;t+1(xt) 
iut(xt) 
+ 
maxp(dtjxt;t)p(xt+1jxt;dt)vt+1;t+2(xt+1) 
(7.7.29)

t
dt;xt+1 


with 


vT;T 
+1(xT 
) 
iu(xT 
) 
(7.7.30) 


DRAFT 
March 
9, 
2010 
127 



Probabilistic 
Inference 
and 
Planning 


Stationary 
deterministic 
policy 


For 
an 
MDP 
the 
optimal 
policy 
is 
deterministic[267], 
so 
that 
methods 
which 
explicitly 
seek 
for 
deterministic 
policies 
are 
of 
interest. 
For 
a 
stationary 
deterministic 
policy 
p 
we 
have 
the 
expected 
utility 


Tt

XXXY

U()= 
ut(xt) 
p(xt 
jx..1;d(x..1)) 
(7.7.31) 
t=1 
xt 
x1:t..1 
=1 


with 
the 
convention 
p(x1jx0;d(x0)) 
= 
p(x1). 
Viewed 
as 
a 
Factor 
Graph, 
this 
is 
simply 
a 
chain, 
so 
that 
for 
any 
policy 
d, 
the 
expected 
utility 
can 
be 
computed 
easily. 
In 
principle 
one 
could 
then 
attempt 
to 
optimise 
U 
with 
respect 
to 
the 
decisions 
directly. 
An 
alternative 
is 
to 
use 
an 
EM 
style 
procedure[100]. 
To 
do 
this 
we 
need 
to 
dene 
a 
(trans-dimensional) 
distribution 


t

ut(xt) 
Yp^(x1:t;t)= 
p(xt 
jx..1;d(x..1)) 
(7.7.32)

Z(d) 


=1 


The 
normalisation 
constant 
Z(d) 
of 
this 
distribution 
is 


TtTt

XXYXXY

ut(xt) 
p(xt 
jx..1;d(x..1)) 
= 
ut(xt) 
p(xt 
jx..1;d(x..1)) 
= 
U() 
(7.7.33) 
t=1 
x1:t 
=1 
t=1 
x1:t 
=1 


If 
we 
now 
dene 
a 
variational 
distribution 
q(x1:t;t), 
and 
consider 


KL(q(x1:t;t)jp^(x1:t;t)) 
= 
0 
(7.7.34) 


this 
gives 
the 
lower 
bound 


t

Y

log 
U() 
..H(q(x1:T 
;t))+ 
log 
ut(xt) 
p(xt 
jx..1;d(x..1)) 
(7.7.35) 
=1 


q(x1:t;t) 


In 
terms 
of 
an 
EM 
algorithm, 
the 
M-step 
requires 
the 
dependency 
on 
d 
alone, 
which 
is 


Tt

XX

E(d)= 
hlog 
p(xt 
jx..1;d(x..1))i(7.7.36)

q(xt 
;x..1;t) 
t=1 
=1 
Tt


XX

= 
q(xt 
= 
s0;x..1 
= 
s;t) 
log 
p(xt 
= 
s0jx..1 
= 
s;d(x..1)= 
d) 
(7.7.37) 
t=1 
=1 


(7.7.38) 
For 
each 
given 
state 
s 
we 
now 
attempt 
to 
nd 
the 
optimal 
decision 
d, 
which 
corresponds 
to 
maximising 


Tt

XXX

^0

E(djs)= 
q(xt 
= 
s 
;x..1 
= 
s;t) 
log 
p(s 
0js, 
d) 
(7.7.39) 


s. 
t=1 
=1 


Dening 


Tt

XX

q(s0js) 
. 
q(xt 
= 
s0;x..1 
= 
s;t) 
(7.7.40) 


t=1 
=1 


^

we 
see 
that 
for 
given 
s, 
up 
to 
a 
constant, 
E(djs) 
is 
the 
Kullback-Leibler 
divergence 
between 
q(s0js) 
and 
p(s0js, 
d) 
so 
that 
the 
optimal 
decision 
d 
is 
given 
by 
the 
index 
of 
the 
distribution 
p(s0js, 
d) 
most 
closely 
aligned 
with 
q(s0js): 


..

d 
(s) 
= 
argmin 
KLq(s0js)jp(s0js, 
d)(7.7.41) 


d 


DRAFT 
March 
9, 
2010 



Further 
Topics 


h1h2h3h4
v1v2v3v4
u2u3u4
d1d2d3
Figure 
7.12: 
An 
example 
Partially 
Observable 
Markov 
Decision 
Process 
(POMDP). 
The 
`hidden’ 
variables 
h 
are 
never 
observed. 
In 
solving 
the 
In
uence 
Diagram 
we 
are 
required 
to 
rst 
sum 
over 
variables 
that 
are 
never 
observed; 
doing 
so 
will 
couple 
together 
all 
past 
observed 
variables 
and 
decisions 
that 
means 
any 
decision 
at 
time 
t 
will 
depend 
on 
all 
previous 
decisions. 
Note 
that 
the 
no-forgetting 
principle 
means 
that 
we 
do 
not 
need 
to 
explicitly 
write 
that 
each 
decision 
depends 
on 
all 
previous 
observa


tions 
– 
this 
is 
implicitly 
assumed. 


The 
E-step 
concerns 
the 
computation 
of 
the 
marginal 
distributions 
required 
in 
the 
M-step. 
The 
optimal 
q 
distribution 
is 
proportional 
to 
^p 
evaluated 
at 
the 
previous 
decision 
function 
d: 


t

t 


q(x1:t;t) 
. 
ut(xt) 
p(xt 
jx..1;d(x..1)) 
(7.7.42) 
=1 


For 
a 
constant 
discount 
factor 
. 
at 
each 
time-step 
and 
an 
otherwise 
stationary 
utility6 


=1 


ut(xt) 
= 
t 
u(xt) 
(7.7.43) 
using 
this 
q(x1:t, 
t) 
. 
t 
u(xt) 
tt 
p(xt 
jx..1, 
d(x..1)) 
(7.7.44) 


For 
each 
t 
this 
is 
a 
simple 
Markov 
chain 
for 
which 
the 
pairwise 
transition 
marginals 
required 
for 
the 
M-step, 
equation 
(7.7.40) 
are 
straightforward. 
This 
requires 
inference 
in 
a 
series 
of 
Markov 
models 
of 
dierent 
lengths. 
This 
can 
be 
done 
eciently 
using 
a 
single 
forward 
and 
backward 
pass[279]. 
See 
MDPemDeterministicPolicy.m 
which 
also 
deals 
with 
the 
more 
general 
case 
of 
utilities 
dependent 
on 
the 
decision(action) 
as 
well 
as 
the 
state. 


Note 
that 
this 
EM 
algorithm 
formally 
fails 
in 
the 
case 
of 
a 
deterministic 
environment 
(the 
transition 
p(xtjxt..1;dt..1) 
is 
deterministic) 
– 
see 
exercise(75) 
for 
an 
explanation 
and 
exercise(76) 
for 
a 
possible 
resolution. 


7.8 
Further 
Topics 
7.8.1 
Partially 
observable 
MDPs 
In 
a 
POMDP 
there 
are 
states 
that 
are 
not 
observed. 
This 
seemingly 
innocuous 
extension 
of 
the 
MDP 
case 
can 
lead 
however 
to 
computational 
diculties. 
Let's 
consider 
the 
situation 
in 
g(7.12), 
and 
attempt 
to 
compute 
the 
optimal 
expected 
utility 
based 
on 
the 
sequence 
of 
summations 
and 
maximisations: 


U 
= 
max 
max 
max 
p(h4jh3;d3)p(v3jh3)p(h3jh2;d2)p(v2jh2)p(h2jh1;d1)p(v1jh1)p(h1) 


d1 
d2 
d3 
v2 
v3 
h1:3 


The 
sum 
over 
the 
hidden 
variables 
h1:3 
couples 
all 
the 
decisions 
and 
observations, 
meaning 
that 
we 
no 
longer 
have 
a 
simple 
chain 
structure 
for 
the 
remaining 
maximisations. 
For 
a 
POMDP 
of 
length 
t, 
this 
leads 
to 
intractable 
problem 
with 
complexity 
exponential 
in 
t. 
An 
alternative 
view 
is 
to 
recognise 
that 
all 
past 
decisions 
and 
observations 
v1:t;d1:t..1, 
can 
be 
summarised 
in 
terms 
of 
a 
belief 
in 
the 
current 
latent 
state, 
p(htjv1:t;d1:t..1). 
This 
suggests 
that 
instead 
of 
having 
an 
actual 
state, 
as 
in 
the 
MDP 
case, 
we 
need 


6In 
the 
standard 
MDP 
framework 
it 
is 
more 
common 
to 
dene 
ut(xt)= 
t..1 
u(xt) 
so 
that 
for 
comparison 
with 
the 
standard 
Policy/Value 
routines 
one 
needs 
to 
divide 
the 
expected 
utility 
by 
. 


DRAFT 
March 
9, 
2010 
129 



Further 
Topics 


to 
use 
a 
distribution 
over 
states 
to 
represent 
our 
current 
knowledge. 
One 
can 
therefore 
write 
down 
an 
eective 
MDP 
albeit 
over 
belief 
distributions, 
as 
opposed 
to 
nite 
states. 
Approximate 
techniques 
are 
required 
to 
solve 
the 
resulting 
`innite’ 
state 
MDPs, 
and 
the 
reader 
is 
referred 
to 
more 
specialised 
texts 
for 
a 
study 
of 
approximation 
procedures. 
See 
for 
example 
[148, 
151]. 


7.8.2 
Restricted 
utility 
functions 
An 
alternative 
to 
solving 
MDPs 
is 
to 
consider 
restricted 
utilities 
such 
that 
the 
policy 
can 
be 
found 
easily. 
Recently 
ecient 
solutions 
have 
been 
developed 
for 
classes 
of 
MDPs 
with 
utilities 
restricted 
to 
Kullback-
Leibler 
divergences 
[152, 
278]. 


7.8.3 
Reinforcement 
learning 
Reinforcement 
Learning 
deals 
mainly 
with 
stationary 
Markov 
Decision 
Processes. 
The 
added 
twist 
is 
that 
the 
transition 
p(s0js, 
d) 
(and 
possibly 
the 
utility) 
is 
unknown. 
Initially 
an 
`agent’ 
begins 
to 
explore 
the 
set 
of 
states 
and 
utilities 
(rewards) 
associated 
with 
taking 
decisions. 
The 
set 
of 
accessible 
states 
and 
their 
rewards 
populates 
as 
the 
agent 
traverses 
its 
environment. 
Consider 
for 
example 
a 
maze 
problem 
with 
a 
given 
start 
and 
goal 
state, 
though 
with 
an 
unknown 
maze 
structure. 
The 
task 
is 
to 
get 
from 
the 
start 
to 
the 
goal 
in 
the 
minimum 
number 
of 
moves 
on 
the 
maze. 
Clearly 
there 
is 
a 
balance 
required 
between 
curiosity 
and 
acting 
to 
maximise 
the 
expected 
reward. 
If 
we 
are 
too 
curious 
(don't 
take 
optimal 
decisions 
given 
the 
currently 
available 
information 
about 
the 
maze 
structure) 
and 
continue 
exploring 
the 
possible 
maze 
routes, 
this 
may 
be 
bad. 
On 
the 
other 
hand, 
if 
we 
don't 
explore 
the 
possible 
maze 
states, 
we 
might 
never 
realise 
that 
there 
is 
a 
much 
more 
optimal 
short-cut 
to 
follow 
than 
that 
based 
on 
our 
current 
knowledge. 
This 
exploration-exploitation 
tradeoff 
is 
central 
to 
the 
diculties 
of 
RL. 
See 
[267] 
for 
an 
extensive 
discussion 
of 
reinforcement 
learning. 


For 
a 
given 
set 
of 
environment 
data 
Xi(observed 
transitions 
and 
utilities) 
one 
aspect 
of 
RL 
problem 
can 
be 
considered 
as 
nding 
the 
policy 
that 
maximises 
expected 
reward, 
given 
only 
a 
prior 
belief 
about 
the 
environment 
and 
observed 
decisions 
and 
states. 
If 
we 
assume 
we 
know 
the 
utility 
function 
but 
not 
the 


transition, 
we 
may 
write 
U(jXi) 
=hU(j)ip(jX 
) 
where 
. 
represents 
the 
environment 
state 
transition, 
(7.8.1) 
. 
= 
p(xt+1jxt, 
dt) 
Given 
a 
set 
of 
observed 
states 
and 
decisions, 
(7.8.2) 
p(jXi) 
/ip(Xij)p() 
(7.8.3) 


where 
p() 
is 
a 
prior 
on 
the 
transition. 
Similar 
techniques 
to 
the 
EM 
style 
training 
can 
be 
carried 
through 
in 
this 
case 
as 
well[77, 
279]. 
Rather 
than 
the 
policy 
being 
a 
function 
of 
the 
state 
and 
the 
environment 
, 
optimally 
one 
needs 
to 
consider 
a 
policy 
p(dtjxt;b()) 
as 
a 
function 
of 
the 
state 
and 
the 
belief 
in 
the 
environment. 
This 
means 
that, 
for 
example, 
if 
the 
belief 
in 
the 
environment 
has 
high 
entropy, 
the 
agent 
can 
recognise 
this 
and 
explicitly 
carry 
out 
decisions/actions 
to 
explore 
the 
environment. 
A 
further 
complication 
in 
RL 
is 
that 
the 
data 
collected 
Xidepends 
on 
the 
policy 
. 
If 
we 
write 
t 
for 
an 
`episode’ 
in 
which 
policy 
t 
is 
followed 
and 
data 
Xt 
collected, 
then 
the 
utility 
of 
the 
policy 
p 
given 
all 
the 
historical 
information 
is 


U(j1:t, 
X1:t)=hU(j)ip(jX1:t;1:t) 
(7.8.4) 


Depending 
on 
the 
priors 
on 
the 
environment, 
and 
also 
on 
how 
long 
each 
episode 
is, 
we 
will 
have 
dierent 
posteriors 
for 
the 
environment 
parameters. 
If 
we 
then 
set 


t+1 
= 
argmax 
U(j1:t, 
X1:t) 
(7.8.5) 


p 


this 
aects 
the 
data 
we 
collect 
at 
the 
next 
episode 
Xt+1. 
In 
this 
way, 
the 
trajectory 
of 
policies 
1;2;::. 
can 
be 
very 
dierent 
depending 
on 
these 
episode 
lengths 
and 
priors. 


DRAFT 
March 
9, 
2010 



Code 


7.9 
Code 
7.9.1 
Sum/Max 
under 
a 
partial 
order 
maxsumpot.m: 
Generalised 
elimination 
operation 
according 
to 
a 
partial 
ordering 
sumpotID.m: 
Sum/max 
an 
ID 
with 
probability 
and 
decision 
potentials 
demoDecParty.m: 
Demo 
of 
summing/maxing 
an 
ID 


7.9.2 
Junction 
trees 
for 
inuence 
diagrams 
There 
is 
no 
need 
to 
specify 
the 
information 
links 
provided 
that 
a 
partial 
ordering 
is 
given. 
In 
the 
code 
jtreeID.m 
no 
check 
is 
made 
that 
the 
partial 
ordering 
is 
consistent 
with 
the 
inuence 
diagram. 
In 
this 
case, 
the 
rst 
step 
of 
the 
junction 
tree 
formulation 
in 
section(7.4.2) 
is 
not 
required. 
Also 
the 
moralisation 
and 
removal 
of 
utility 
nodes 
is 
easily 
dealt 
with 
by 
dening 
utility 
potentials 
and 
including 
them 
in 
the 
moralisation 
process. 


The 
strong 
triangulation 
is 
found 
by 
a 
simple 
variable 
elimination 
scheme 
which 
seeks 
to 
eliminate 
a 
variable 
with 
the 
least 
number 
of 
neighbours, 
provided 
that 
the 
variable 
may 
be 
eliminated 
according 
to 
the 
specied 
partial 
ordering. 


The 
junction 
tree 
is 
constructed 
based 
only 
on 
the 
elimination 
clique 
sequence 
C1;:::, 
CN 
. 
obtained 
from 
the 
triangulation 
routine. 
The 
junction 
tree 
is 
then 
obtained 
by 
connecting 
a 
clique 
CI 
to 
the 
rst 
clique 
j>i 
that 
is 
connected 
to 
this 
clique. 
Clique 
Ci 
is 
then 
eliminated 
from 
the 
graph. 
In 
this 
manner 
a 
junction 
tree 
of 
connected 
cliques 
is 
formed. 
We 
do 
not 
require 
the 
separators 
for 
the 
inuence 
diagram 
absorption 
since 
these 
can 
be 
computed 
and 
discarded 
on 
the 
y. 


Note 
that 
the 
code 
only 
computes 
messages 
from 
the 
leaves 
to 
the 
root 
of 
the 
junction 
tree, 
which 
is 
sucient 
for 
taking 
decisions 
at 
the 
root. 
If 
one 
desires 
an 
optimal 
decision 
at 
a 
non-root, 
one 
would 
need 
to 
absorb 
probabilities 
into 
a 
clique 
which 
contains 
the 
decision 
required. 
These 
extra 
forward 
probability 
absorptions 
are 
required 
because 
information 
about 
any 
unobserved 
variables 
can 
be 
aected 
by 
decisions 
and 
observations 
in 
the 
past. 
This 
extra 
forward 
probability 
schedule 
is 
not 
given 
in 
the 
code 
and 
left 
as 
an 
exercise 
for 
the 
interested 
reader. 


jtreeID.m: 
Junction 
Tree 
for 
an 
Inuence 
Diagram 
absorptionID.m: 
Absorption 
on 
an 
Inuence 
Diagram 
triangulatePorder.m: 
Triangulation 
based 
on 
a 
partial 
ordering 
demoDecPhD.m: 
Demo 
for 
utility 
of 
Doing 
PhD 
and 
Startup 


7.9.3 
Party-Friend 
example 
The 
code 
below 
implements 
the 
Party-Friend 
example 
in 
the 
text. 
To 
deal 
with 
the 
asymmetry 
the 
V 
isit 
utility 
is 
zero 
if 
P 
arty 
is 
in 
state 
yes. 
demoDecPartyFriend.m: 
Demo 
for 
Party-Friend 


7.9.4 
Chest 
Clinic 
with 
Decisions 
The 
table 
for 
the 
Chest 
Clinic 
Decision 
network, 
g(7.13) 
is 
taken 
from 
exercise(24), 
see 
[119, 
69]. 
There 
is 
a 
slight 
modication 
however 
to 
the 
p(xje) 
table. 
If 
an 
x-ray 
is 
taken, 
then 
information 
about 
x 
is 
available. 
However, 
if 
the 
decision 
is 
not 
to 
take 
an 
x-ray 
no 
information 
about 
x 
is 
available. 
This 
is 
a 
form 
of 
asymmetry. 
A 
straightforward 
approach 
in 
this 
case 
is 
to 
make 
dx 
a 
parent 
of 
the 
x 
variable 
and 


DRAFT 
March 
9, 
2010 
131 



Code 


xd
e
tdx
Ux
ldh
Uh
b
a
s
s 
= 
Smoking 
x 
= 
Positive 
X-ray 
d 
= 
Dyspnea 
(Shortness 
of 
breath) 
e 
= 
Either 
Tuberculosis 
or 
Lung 
Cancer 
t 
= 
Tuberculosis 


l 
= 
Lung 
Cancer 
b 
= 
Bronchitis 
a 
= 
Visited 
Asia 


dh 
= 
Hospitalise? 
dx 
= 
Take 
X-ray? 


Figure 
7.13: 
Inuence 
Diagram 
for 
the 
`Chest 
Clinic’ 
Decision 
example. 


set 
the 
distribution 
of 
x 
to 
be 
uninformative 
if 
dx 
= 
fa. 


p(a 
= 
tr)=0:01 
p(s 
= 
tr)=0:5 
p(t 
= 
trja 
= 
tr)=0:05 
p(t 
= 
trja 
= 
fa)=0:01 
p(l 
= 
trjs 
= 
tr)=0:1 
p(l 
= 
trjs 
= 
fa)=0:01 
p(b 
= 
trjs 
= 
tr)=0:6 
p(b 
= 
trjs 
= 
fa)=0:3 


(7.9.1)
p(x 
= 
trje 
= 
tr;dx 
= 
tr)=0:98 
p(x 
= 
trje 
= 
fa;dx 
= 
tr)=0:05 
p(x 
= 
trje 
= 
tr;dx 
= 
fa)=0:5 
p(x 
= 
trje 
= 
fa;dx 
= 
fa)=0:5 
p(d 
= 
trje 
= 
tr;b 
= 
tr)=0:9 
p(d 
= 
trje 
= 
tr;b 
= 
fa)=0:3 
p(d 
= 
trje 
= 
fa;b 
= 
tr)=0:2 
p(d 
= 
trje 
= 
fa;b 
= 
fa)=0:1 


The 
two 
utilities 
are 
designed 
to 
reect 
the 
costs 
and 
benets 
of 
taking 
an 
x-ray 
and 
hospitalising 
a 
patient: 


dh 
= 
tr 
t 
= 
tr 
l 
= 
tr 
dh 
= 
tr 
t 
= 
tr 
l 
= 
fa 
dh 
= 
tr 
t 
= 
fa 
l 
= 
tr 
dh 
= 
tr 
t 
= 
fa 
l 
= 
fa 
dh 
= 
fa 
t 
= 
tr 
l 
= 
tr 
dh 
= 
fa 
t 
= 
tr 
l 
= 
fa 
dh 
= 
fa 
t 
= 
fa 
l 
= 
tr 
dh 
= 
fa 
t 
= 
fa 
l 
= 
fa 


180 
120 
160 
dx 
= 
tr 
t 
= 
tr 
15 
2 
(7.9.2) 
dx 
= 
tr 
dx 
= 
fa 
t 
= 
fa 
t 
= 
tr 
4 
dx 
= 
fa 
t 
= 
fa 
0 
40 


0 
1 


(7.9.3)
10 
10 


We 
assume 
that 
we 
know 
whether 
or 
not 
the 
patient 
has 
been 
to 
Asia, 
before 
deciding 
on 
taking 
an 
x-ray. 
The 
partial 
ordering 
is 
then 


a 
dx 
fd, 
xgdh 
fb, 
e, 
l, 
s, 
t} 
(7.9.4) 


The 
demo 
demoDecAsia.m 
produces 
the 
results: 


utility 
table: 
asia 
= 
yes 
takexray 
= 
yes 
49.976202 
asia 
= 
no 
takexray 
= 
yes 
46.989441 
asia 
= 
yes 
takexray 
= 
no 
48.433043 
asia 
= 
no 
takexray 
= 
no 
47.460900 


which 
shows 
that 
optimally 
one 
should 
take 
an 
x-ray 
only 
if 
the 
patient 
has 
been 
to 
Asia. 
demoDecAsia.m: 
Junction 
Tree 
Inuence 
Diagram 
demo 


DRAFT 
March 
9, 
2010 



Exercises 


7.9.5 
Markov 
decision 
processes 
In 
demoMDP.m 
we 
consider 
a 
simple 
two 
dimensional 
grid 
in 
which 
an 
`agent’ 
can 
move 
to 
a 
grid 
square 
either 
above, 
below, 
left, 
right 
of 
the 
current 
square, 
or 
stay 
in 
the 
current 
square. 
We 
dened 
goal 
states 
(grid 
squares) 
that 
have 
high 
utility, 
with 
others 
having 
zero 
utility. 
demoMDPclean.m: 
Demo 
of 
Value 
and 
Policy 
Iteration 
for 
a 
simple 
MDP 
MDPsolve.m: 
MDP 
solver 
using 
Value 
or 
Policy 
Iteration 


MDP 
solver 
using 
EM 
and 
assuming 
a 
deterministic 
policy 


The 
following 
code7 
is 
not 
fully 
documented 
in 
the 
text, 
although 
the 
method 
is 
reasonably 
straightforward 
and 
follows 
that 
described 
in 
section(7.7.3). 
The 
inference 
is 
carried 
out 
using 
a 
simple 
..ß 
style 
recursion. 
This 
could 
also 
be 
implemented 
using 
the 
general 
Factor 
Graph 
code, 
but 
was 
coded 
explicitly 
for 
reasons 
of 
speed. 
The 
code 
also 
handles 
the 
more 
general 
case 
of 
utilities 
(rewards) 
as 
a 
function 
of 
both 
the 
state 
and 
the 
action 
u(xt;dt). 
MDPemDeterministicPolicy.m: 
MDP 
solver 
using 
EM 
and 
assuming 
a 
deterministic 
policy 
EMqTranMarginal.m: 
Marginal 
information 
required 
for 
the 
transition 
term 
of 
the 
energy 
EMqUtilMarginal.m: 
Marginal 
information 
required 
for 
the 
utility 
term 
of 
the 
energy 
EMTotalBetaMessage.m: 
Backward 
information 
required 
for 
inference 
in 
the 
MDP 
EMminimizeKL.m: 
Find 
the 
optimal 
decision 
EMvalueTable.m: 
Return 
the 
expected 
value 
of 
the 
policy 


7.10 
Exercises 
Exercise 
67. 
You 
play 
a 
game 
in 
which 
you 
have 
a 
probability 
p 
of 
winning. 
If 
you 
win 
the 
game 
you 
gain 
an 
amount 
$S 
and 
if 
you 
lose 
the 
game 
you 
lose 
an 
amount 
$S. 
Show 
that 
the 
expected 
gain 
from 
playing 
the 
game 
is 
$(2p 
- 
1)S. 


Exercise 
68. 
It 
is 
suggested 
that 
the 
utility 
of 
money 
is 
based, 
not 
on 
the 
amount, 
but 
rather 
how 
much 
we 
have 
relative 
to 
other 
peoples. 
Assume 
a 
distribution 
p(i), 
i 
=1;:::, 
10 
of 
incomes 
using 
a 
histogram 
with 
10 
bins, 
each 
bin 
representing 
an 
income 
range. 
Use 
a 
histogram 
to 
roughly 
reect 
the 
distribution 
of 
incomes 
in 
society, 
namely 
that 
most 
incomes 
are 
around 
the 
average 
with 
few 
very 
wealthy 
and 
few 
extremely 
poor 
people. 
Now 
dene 
the 
utility 
of 
an 
income 
x 
as 
the 
chance 
that 
income 
x 
will 
be 
higher 
than 
a 
randomly 
chosen 
income 
y 
(under 
the 
distribution 
you 
dened) 
and 
relate 
this 
to 
the 
cumulative 
distribution 
of 
p. 
Write 
a 
program 
to 
compute 
this 
probability 
and 
plot 
the 
resulting 
utility 
as 
a 
function 
of 
income. 
Now 
repeat 
the 
coin 
tossing 
bet 
of 
section(7.1.1) 
so 
that 
if 
one 
wins 
the 
bet 
one's 
new 
income 
will 
be 
placed 
in 
the 
top 
histogram 
bin, 
whilst 
if 
one 
loses 
one's 
new 
income 
is 
in 
the 
lowest 
bin. 
Compare 
the 
optimal 
expect 
utility 
decisions 
under 
the 
situations 
in 
which 
one's 
original 
income 
is 
(i) 
average, 
and 


(ii) 
much 
higher 
than 
average. 
Exercise 
69. 


Derive 
a 
partial 
ordering 
for 
the 
ID 
on 
the 
right, 
and 
explain 
how 
this 
ID 
diers 
from 
that 
of 
g(7.5). 


Test
DrillU1
U2Oil
Seismic
Exercise 
70. 
This 
question 
follows 
closely 
demoMDP.m, 
and 
represents 
a 
problem 
in 
which 
a 
pilot 
wishes 
to 
land 
an 
airplane. 


The 
matrix 
U(x, 
y) 
in 
the 
le 
airplane.mat 
contains 
the 
utilities 
of 
being 
in 
position 
x, 
y 
and 
is 
a 
very 
crude 
model 
of 
a 
runway 
and 
taxiing 
area. 


7Thanks 
to 
Tom 
Furmston 
for 
coding 
this. 


DRAFT 
March 
9, 
2010 



Exercises 


The 
airspace 
is 
represented 
by 
an 
1815 
grid 
(Gx 
= 
18, 
Gy 
= 
15 
in 
the 
notation 
employed 
in 
demoMDP.m). 
The 
matrix 
U(8, 
4) 
= 
2 
represents 
that 
position 
(8, 
4) 
is 
the 
desired 
parking 
bay 
of 
the 
airplane 
(the 
vertical 
height 
of 
the 
airplane 
is 
not 
taken 
in 
to 
account). 
The 
positive 
values 
in 
U 
represent 
runway 
and 
areas 
where 
the 
airplane 
is 
allowed. 
Zero 
utilities 
represent 
neutral 
positions. 
The 
negative 
values 
represent 
unfavourable 
positions 
for 
the 
airplane. 
By 
examining 
the 
matrix 
U 
you 
will 
see 
that 
the 
airplane 
should 
preferably 
not 
veer 
off 
the 
runway, 
and 
also 
should 
avoid 
two 
small 
villages 
close 
to 
the 
airport. 


At 
each 
timestep 
the 
plane 
can 
perform 
one 
of 
the 
following 
actions 
stay 
up 
down 
left 
right: 
For 
stay, 
the 
airplane 
stays 
in 
the 
same 
x, 
y 
position. 
For 
up, 
the 
airplane 
moves 
to 
the 
x, 
y 
+1 
position. 
For 
down, 
the 
airplane 
moves 
to 
the 
x, 
y 
- 
1 
position. 
For 
left, 
the 
airplane 
moves 
to 
the 
x 
- 
1;y 
position. 
For 
right, 
the 
airplane 
moves 
to 
the 
x 
+1;y 
position. 


A 
move 
that 
takes 
the 
airplane 
out 
of 
the 
airspace 
is 
not 
allowed. 


1. 
The 
airplane 
begins 
in 
at 
point 
x 
=1;y 
= 
13. 
Assuming 
that 
an 
action 
deterministically 
results 
in 
the 
intended 
grid 
move, 
nd 
the 
optimal 
xt;yt 
sequence 
for 
times 
t 
=1;:::, 
for 
the 
position 
of 
the 
aircraft. 
2. 
The 
pilot 
tells 
you 
that 
there 
is 
a 
fault 
with 
the 
airplane. 
When 
the 
pilot 
instructs 
the 
plane 
to 
go 
right 
with 
probability 
0.1 
it 
actually 
goes 
up 
(provided 
this 
remains 
in 
the 
airspace). 
Assuming 
again 
that 
the 
airplane 
begins 
at 
point 
x 
=1;y 
= 
13, 
return 
the 
optimal 
xt;yt 
sequence 
for 
times 
t 
=1;:::, 
for 
the 
position 
of 
the 
aircraft. 
Exercise 
71. 


The 
inuence 
diagram 
depicted 
describes 
the 
rst 
stage 
of 
a 
game. 
The 
decision 
variable 
dom(d1)= 
fplay, 
not 
playg, 
indicates 
the 
decision 
to 
either 
play 
the 
rst 
stage 
or 
not. 
If 
you 
decide 
to 
play, 
there 
is 
a 
cost 
c1(play)= 
C1, 
but 
no 
cost 
otherwise, 
c1(no 
play)=0. 
The 
variable 
x1 
describes 
if 
you 
win 
or 
lose 
the 
game, 
dom(x1)= 
fwin, 
loseg, 
with 
probabilities: 


p(x1 
= 
winjd1 
= 
play)= 
p1;p(x1 
= 
winjd1 
= 
no 
play) 
= 
0 
(7.10.1) 


The 
utility 
of 
winning/losing 
is 


x1
u1
c1
d1
u1(x1 
= 
win)= 
W1;u1(x1 
= 
lose) 
= 
0 
(7.10.2) 


Show 
that 
the 
expected 
utility 
gain 
of 
playing 
this 
game 
is 


U(d1 
= 
play)= 
p1W1 
- 
C1 
(7.10.3) 


Exercise 
72. 
Exercise(71) 
above 
describes 
the 
rst 
stage 
of 
a 
new 
two-stage 
game. 
If 
you 
win 
the 
rst 
stage 
x1 
= 
win, 
you 
have 
to 
make 
a 
decision 
d2 
as 
to 
whether 
or 
not 
play 
in 
the 
second 
stage 
dom(d2)= 
fplay, 
not 
playg. 
If 
you 
do 
not 
win 
the 
rst 
stage, 
you 
cannot 
enter 
the 
second 
stage. 


If 
you 
decide 
to 
play 
the 
second 
stage, 
you 
win 
with 
probability 
p2: 
p(x2 
= 
winjx1 
= 
win;d2 
= 
play)= 
p2 
(7.10.4) 


If 
you 
decide 
not 
to 
play 
the 
second 
stage 
there 
is 
no 
chance 
to 
win: 


p(x2 
= 
winjx1 
= 
win;d2 
= 
not 
play) 
= 
0 
(7.10.5) 


The 
cost 
of 
playing 
the 
second 
stage 
is 


c2(d2 
= 
play)= 
C2;c2(d2 
= 
no 
play) 
= 
0 
(7.10.6) 


DRAFT 
March 
9, 
2010 



Exercises 


and 
the 
utility 
of 
winning/losing 
the 
second 
stage 
is 


u2(x2 
= 
win)= 
W2;u2(x2 
= 
lose) 
= 
0 
(7.10.7) 


1. 
Draw 
an 
Inuence 
Diagram 
that 
describes 
this 
two-stage 
game. 
2. 
A 
gambler 
needs 
to 
decide 
if 
he 
should 
even 
enter 
the 
rst 
stage 
of 
this 
two-stage 
game. 
Show 
that 
based 
on 
taking 
the 
optimal 
future 
decision 
d2 
the 
expected 
utility 
based 
on 
the 
rst 
decision 
is: 


p1(p2W2 
- 
C2)+ 
p1W1 
- 
C1 
if 
p2W2 
- 
C2 
= 
0 


U(d1 
= 
play) 
=(7.10.8)

p1W1 
- 
C1 
if 
p2W2 
- 
C2 
= 
0 


Exercise 
73. 
You 
have 
$B 
in 
your 
bank 
account. 
You 
are 
asked 
if 
you 
would 
like 
to 
participate 
in 
a 
bet 
in 
which, 
if 
you 
win, 
your 
bank 
account 
will 
become 
$W 
. 
However, 
if 
you 
lose, 
your 
bank 
account 
will 
contain 
only 
$L. 
You 
win 
the 
bet 
with 
probability 
pw. 


1. 
Assuming 
that 
the 
utility 
is 
given 
by 
the 
number 
of 
pounds 
in 
your 
bank 
account, 
write 
down 
a 
formula 
for 
the 
expected 
utility 
of 
taking 
the 
bet, 
U(bet) 
and 
also 
the 
expected 
utility 
of 
not 
taking 
the 
bet, 
U(no 
bet). 
2. 
The 
above 
situation 
can 
be 
formulated 
dierently. 
If 
you 
win 
the 
bet 
you 
gain 
$(W 
- 
B). 
If 
you 
lose 
the 
bet 
you 
lose 
$(B 
- 
L). 
Compute 
the 
expected 
amount 
of 
money 
you 
gain 
if 
you 
bet 
Ugain(bet) 
and 
if 
you 
don't 
bet 
Ugain(no 
bet). 
3. 
Show 
that 
U(bet) 
- 
U(no 
bet)= 
Ugain(bet) 
- 
Ugain(no 
bet). 
Exercise 
74. 
Consider 
the 
Party-Friend 
scenario, 
example(30). 
An 
alternative 
is 
to 
replace 
the 
link 
from 
P 
arty 
to 
Uvisit 
by 
an 
information 
link 
from 
P 
arty 
to 
V 
isit 
with 
the 
constraint 
that 
V 
isit 
can 
be 
in 
state 
yes 
only 
if 
P 
arty 
is 
in 
state 
no. 


1. 
Explain 
how 
this 
constraint 
can 
be 
achieved 
by 
including 
an 
additional 
additive 
term 
to 
the 
utilities 
and 
modify 
demoDecPartyFriend.m 
accordingly 
to 
demonstrate 
this. 
2. 
For 
the 
case 
in 
which 
utilities 
are 
all 
positive, 
explain 
how 
the 
same 
constraint 
can 
be 
achieved 
using 
a 
multiplicative 
factor. 
Exercise 
75. 
Consider 
an 
objective 


X

F 
()=U(x)p(xj) 
(7.10.9) 


x 


for 
a 
positive 
function 
U(x) 
and 
that 
our 
task 
is 
to 
maximise 
F 
with 
respect 
to 
. 
An 
Expectation-
Maximisation 
style 
bounding 
approach 
(see 
section(11.2)) 
can 
be 
derived 
by 
dening 
the 
auxiliary 
distrib
ution 


U(x)p(xj) 


p~(xj) 
= 
(7.10.10)

F 
() 
so 
that 
by 
considering 
KL(q(x)jp~(x)) 
for 
some 
variational 
distribution 
q(x) 
we 
obtain 
the 
bound 
log 
F 
() 
..hlog 
q(x)i+ 
hlog 
U(x)i+ 
hlog 
p(xj)i(7.10.11)

q(x) 
q(x) 
q(x) 


The 
M-step 
states 
that 
the 
optimal 
q 
distribution 
is 
given 
by 


q(x)= 
p~(xjold) 
(7.10.12) 
At 
the 
E-step 
of 
the 
algorithm 
the 
new 
parameters 
new 
are 
given 
by 
maximising 
the 
`energy’ 
term 


new 
= 
argmax 
hlog 
p(xj)i~(7.10.13)

p(xjold)

. 


Show 
that 
for 
a 
deterministic 
distribution 


p(xj)= 
d 
(x, 
f()) 
(7.10.14) 
the 
E-step 
fails, 
giving 
new 
= 
old. 


DRAFT 
March 
9, 
2010 



Exercises 


Exercise 
76. 
Consider 
an 
objective 


X

F 
(. 
)=U 
(x 
)p 
(x 
j. 
) 
(7.10.15) 
x 


for 
a 
positive 
function 
U 
(x 
) 
and 


p 
(x 
j. 
) 
= 
(1 
- 
E 
)d 
(x, 
f 
(. 
)) 
+ 
n 
(x 
), 
0 
= 
E 
= 
1 
(7.10.16) 
and 
an 
arbitrary 
distribution 
n 
(x 
). 
Our 
task 
is 
to 
maximise 
F 
with 
respect 
to 
. 
. 
As 
the 
previous 
exercise 
showed, 
if 
we 
attempt 
an 
EM 
algorithm 
in 
the 
limit 
of 
a 
deterministic 
model 
E 
=0, 
then 
no-updating 


occurs 
and 
the 
EM 
algorithm 
fails 
to 
nd 
. 
that 
optimises 
F 
0(. 
). 
Show 
that 
XF(. 
) 
= 
(1 
- 
E 
)F 
0(. 
) 
+ 
n 
(x 
)U 
(x 
) 
(7.10.17) 
x 
and 
hence 
F 
(. 
new) 
- 
F 
(. 
old) 
= 
(1 
- 
E 
) 
[F 
0(. 
new) 
- 
F 
0(. 
old] 
(7.10.18) 
Show 
that 
if 
for 
E 
> 
0 
we 
can 
nd 
a 
. 
new 
such 
that 
F 
(. 
new) 
> 
F 
(. 
old), 
then 
necessarily 
F 
0(. 
new) 
> 


F 
0(. 
old). 


Using 
this 
result, 
derive 
an 
EM-style 
algorithm 
that 
guarantees 
to 
increase 
F 
(. 
) 
(unless 
we 
are 
already 
at 
the 
optimum) 
for 
> 
0 
and 
therefore 
guarantees 
to 
increase 
F 
0(. 
). 
Hint: 
use 


U 
(x 
)p 
(x 
j. 
) 


p 
~(x 
j. 
) 
= 
(7.10.19)

F 
(. 
) 


and 
consider 


KL(q 
(x 
)jp 
~(x 
)) 
(7.10.20) 


for 
some 
variational 
distribution 
q 
(x 
). 


Exercise 
77. 
The 
le 
IDjensen.mat 
contains 
probability 
and 
utility 
tables 
for 
the 
inuence 
diagram 
of 
g(7.8a). 
Using 
BRMLtoolbox, 
write 
a 
program 
that 
returns 
the 
maximal 
expected 
utility 
for 
this 
ID 
using 
a 
strong 
junction 
tree 
approach, 
and 
check 
the 
result 
by 
explicit 
summation 
and 
maximisation. 
Similarly, 
your 
program 
should 
output 
the 
maximal 
expected 
utility 
for 
both 
states 
of 
d 
1, 
and 
check 
that 
the 
computation 
using 
the 
strong 
junction 
tree 
agrees 
with 
the 
result 
from 
explicit 
elimination 
summation 
and 
maximisation. 


Exercise 
78. 
For 
a 
POMDP, 
explain 
the 
structure 
of 
the 
strong 
junction 
tree, 
and 
relate 
this 
to 
the 
complexity 
of 
inference 
in 
the 
POMDP. 


Exercise 
79. 


(i) 
Dene 
a 
partial 
order 
for 
the 
ID 
diagram 
depicted. 
(ii) 
Draw 
a 
(strong) 
junction 
tree 
for 
this 
ID. 
c
ab
d
ef
gh
i
u1
u2
DRAFT 
March 
9, 
2010 



Part 
II 
Learning 
in 
Probabilistic 
Models 


137 



CHAPTER 
8 


Statistics 
for 
Machine 
Learning: 


8.1 
Distributions 
Denition 
54 
(Cumulative 
distribution 
function). 
For 
a 
univariate 
distribution 
p(x), 
the 
CDF 
is 
dened 
as 


cdf(y) 
= 
p(x 
= 
y)= 
hI[x 
= 
y]ip(x) 
(8.1.1) 
For 
an 
unbounded 
domain, 
cdf(..1)=0 
and 
cdf(1) 
= 
1. 


8.2 
Summarising 
distributions 
Denition 
55 
(Mode). 
The 
mode 
x* 
of 
a 
distribution 
p(x) 
is 
the 
state 
of 
x 
at 
which 
the 
distribution 
takes 
its 
highest 
value, 
x* 
= 
argmax 
p(x). 
A 
distribution 
could 
have 
more 
than 
one 
node 
(be 
multi-modal). 
A 


x 


widespread 
abuse 
of 
terminology 
is 
to 
refer 
to 
any 
isolated 
local 
maximum 
of 
p(x) 
to 
be 
a 
mode. 


Denition 
56 
(Averages 
and 
Expectation). 


hf(x)ip(x) 
(8.2.1) 
denotes 
the 
average 
or 
expectation 
of 
f(x) 
with 
respect 
to 
the 
distribution 
p(x). 
A 
common 
alternative 
notation 
is 


E(x) 
(8.2.2) 
When 
the 
context 
is 
clear, 
one 
may 
drop 
the 
notational 
dependency 
on 
p(x). 
The 
notation 


hf(x)jy) 
(8.2.3) 
is 
shorthand 
for 
the 
average 
of 
f(x) 
conditioned 
on 
knowing 
the 
state 
of 
variable 
y, 
i.e. 
the 
average 
of 
f(x) 
with 
respect 
to 
the 
distribution 
p(xjy). 


139 



Summarising 
distributions 


An 
advantage 
of 
the 
expectation 
notations 
is 
that 
they 
hold 
whether 
the 
distribution 
is 
over 
continuous 


or 
discrete 
variables. 
In 
the 
discrete 
case 
hf(x)) 
Xf(x 
= 
x)p(x 
= 
x) 
(8.2.4) 
x 
and 
for 
continuous 
variables, 
hf(x)) 
= 
ZX8 
..8 
f(x)p(x)dx 
(8.2.5) 
The 
reader 
might 
wonder 
what 
hx) 
means 
when 
x 
is 
discrete. 
For 
example, 
if 
dom(x) 
= 


fapple, 
orange, 
pearg, 
with 
associated 
probabilities 
p(x) 
for 
each 
of 
the 
states, 
what 
does 
hx) 
refer 
to? 
Clearly, 
hf(x)) 
makes 
sense 
if 
f(x 
= 
x) 
maps 
the 
state 
x 
to 
a 
numerical 
value. 
For 
example 
f(x 
= 
apple) 
= 
1, 
f(x 
= 
orange) 
= 
2, 
f(x 
= 
pear) 
= 
3 
for 
which 
hf(x)) 
is 
meaningful. 
Unless 
the 
states 
of 
the 
discrete 
variable 
are 
associated 
with 
a 
numerical 
value, 
then 
hx) 
has 
no 
meaning. 


k

Denition 
57 
(Moments). 
The 
kth 
moment 
of 
a 
distribution 
is 
given 
by 
the 
average 
of 
xunder 
the 
distribution:

DE

k

x 
(8.2.6) 


p(x) 


For 
k 
= 
1, 
we 
have 
the 
mean, 
typically 
denoted 
by 
, 
µ 
hx) 
(8.2.7) 


Denition 
58 
(Variance 
and 
Correlation). 


DE

2 
(x 
..hxi)2(8.2.8) 


p(x) 


The 
square 
root 
of 
the 
variance, 
s 
is 
called 
the 
standard 
deviation. 
The 
notation 
var(x) 
is 
also 
used 
to 
emphasise 
for 
which 
variable 
the 
variance 
is 
computed. 
The 
reader 
may 
show 
that 
an 
equivalent 
expression 
is 





2

2 
x 
..hxi2 
(8.2.9) 
For 
a 
multivariate 
distribution 
the 
matrix 
with 
elements 


ij 
= 
h(xi 
- 
i)(xj 
- 
j)) 
(8.2.10) 
where 
i 
= 
hxi) 
is 
called 
the 
covariance 
matrix. 
The 
diagonal 
entries 
of 
the 
covariance 
matrix 
contain 
the 
variance 
of 
each 
variable. 
An 
equivalent 
expression 
is 


ij 
= 
hxixji..hxiihxj) 
(8.2.11) 
The 
correlation 
matrix 
has 
elements 




(xi 
- 
i)(xj 
- 
j)

ij 
=(8.2.12)

i 
j

where 
i 
is 
the 
deviation 
of 
variable 
xi. 
The 
correlation 
is 
a 
normalised 
form 
of 
the 
covariance 
so 
that 
each 
element 
is 
bounded 
..1 
= 
ij 
= 
1. 


DRAFT 
March 
9, 
2010 



Summarising 
distributions 


For 
independent 
variables 
xi 
and 
xj, 
xi 
.
??xjjØ 
the 
covariance 
ij 
is 
zero. 
Similarly 
independent 
variables 
have 
zero 
correlation 
– 
they 
are 
`uncorrelated'. 
Note 
however 
that 
the 
converse 
is 
not 
generally 
true 
– 
two 
variables 
can 
be 
uncorrelated 
but 
dependent. 
A 
special 
case 
is 
for 
when 
xi 
and 
xj 
are 
Gaussian 
distributed 
then 
independence 
is 
equivalent 
to 
being 
uncorrelated, 
see 
exercise(81). 


Denition 
59 
(Skewness 
and 
Kurtosis). 
The 
skewness 
is 
a 
measure 
of 
the 
asymmetry 
of 
a 
distribution: 


DE

(x 
..hxi)3
1 
p(x) 
(8.2.13)

3 


where 
2 
is 
the 
variance 
of 
x 
with 
respect 
to 
p(x). 
A 
positive 
skewness 
means 
the 
distribution 
has 
a 
heavy 
tail 
to 
the 
right. 
Similarly, 
a 
negative 
skewness 
means 
the 
distribution 
has 
a 
heavy 
tail 
to 
the 
left. 


The 
kurtosis 
is 
a 
measure 
of 
how 
peaked 
around 
the 
mean 
a 
distribution 
is: 


DE

(x 
..hxi)4
2 
4 
p(x) 
- 
3 
(8.2.14) 


A 
distribution 
with 
positive 
kurtosis 
has 
more 
mass 
around 
its 
mean 
than 
would 
a 
Gaussian 
with 
the 
same 
mean 
and 
variance. 
These 
are 
also 
called 
super 
Gaussian. 
Similarly 
a 
negative 
kurtosis 
(sub 
Gaussian) 
distribution 
has 
less 
mass 
around 
its 
mean 
than 
the 
corresponding 
Gaussian. 
The 
kurtosis 
is 
dened 
such 
that 
a 
Gaussian 
has 
zero 
kurtosis 
(which 
accounts 
for 
the 
-3 
term 
in 
the 
denition). 


1

Denition 
60 
(Empirical 
Distribution). 
For 
a 
set 
of 
datapoints 
x;:::;xN 
, 
which 
are 
states 
of 
a 
random 
variable 
x, 
the 
empirical 
distribution 
has 
probability 
mass 
distributed 
evenly 
over 
the 
datapoints, 
and 


zero 
elsewhere. 
For 
a 
discrete 
variable 
x 
the 
empirical 
distribution 
is 
p(x) 
= 
1 
N 
NXDI[x 
= 
x 
n] 
(8.2.15) 
n=1 
where 
N 
is 
the 
number 
of 
datapoints. 
For 
a 
continuous 
distribution 
we 
have 
p(x) 
= 
1 
N 
NXDd 
(x 
- 
x 
n) 
(8.2.16) 
n=1 
where 
d 
(x) 
is 
the 
Dirac 
Delta 
function. 
The 
sample 
mean 
of 
the 
datapoints 
is 
given 
by 
the 


X

1 
N

n 


^= 
x 
(8.2.17)

N 


n=1 


and 
the 
sample 
variance 
is 
given 
by 
the 


X

1 
N

^2 
=(x 
n 
- 
^)2 
(8.2.18)

N 


n=1 


DRAFT 
March 
9, 
2010 



Summarising 
distributions 


For 
vectors 
the 
sample 
mean 
vector 
has 
elements 


X

1 
N

n 


^i 
= 
x 
(8.2.19)

i

N 


n=1 


and 
sample 
covariance 
matrix 
has 
elements 


X

1 
N..

^
ij 
=(x 
n
i 
- 
^i)x 
n
j 
- 
^j(8.2.20)

N 


n=1 


Denition 
61 
(Delta 
function). 
For 
continuous 
x, 
we 
dene 
the 
Dirac 
delta 
function 


(x 
- 
x0) 
(8.2.21) 
which 
is 
zero 
everywhere 
expect 
at 
x0, 
where 
there 
is 
a 
spike.R8 
..8 
(x 
- 
x0)dx 
= 
1 
and
Z8 
(x 
- 
x0)f(x)dx 
= 
f(x0) 
(8.2.22) 
..8 
..

One 
can 
view 
the 
Dirac 
delta 
function 
as 
an 
innitely 
narrow 
Gaussian: 
(x 
- 
x0) 
= 
lim!0 
Nx 


x0;2. 


The 
Kronecker 
delta, 


x;x0 
(8.2.23) 


is 
similarly 
zero 
everywhere, 
except 
for 
x0;x0 
= 
1. 
The 
Kronecker 
delta 
is 
equivalent 
to 
x;x0 
= 
I[x 
= 
x0]. 
We 
use 
the 
expression 
d 
(x, 
x0) 
to 
denote 
either 
the 
Dirac 
or 
Kronecker 
delta, 
depending 
on 
the 
context. 


8.2.1 
Estimator 
bias 
1

Denition 
62 
(Unbiased 
estimator). 
Given 
data 
X 
= 
x;:::;xN 
, 
from 
a 
distribution 
p(xj) 
we 
can 
use 
the 
data 
X 
to 
estimate 
the 
parameter 
. 
that 
was 
used 
to 
generate 
the 
data. 
The 
estimator 
is 
a 
function 
of 
the 
data, 
which 
we 
write 
^(X 
). 
For 
an 
unbiased 
estimator 


DE
^

(X 
)= 
. 
(8.2.24) 


p(Xj) 


More 
generally, 
one 
can 
consider 
any 
estimating 
function 
 ^(X 
) 
of 
data. 
This 
is 
an 
unbiased 
estimator 
of 


DE
^

a 
quantity 
. 
if 
 (X 
)= 
 . 


p(x) 



Figure 
8.1: 
Empirical 
distribution 
over 
a 
discrete 
variable 
with 
4 
states. 
The 
empirical 
samples 
consist 
of 
n 
samples 
at 
each 
of 
states 
1, 
2, 
4 
and 
2n 
samples 
at 
state 
3 
where 
n> 
0. 
On 
1234 


normalising 
this 
gives 
a 
distribution 
with 
values 
0:2, 
0:2, 
0:4, 
0:2 
over 
the 
4 
states. 


DRAFT 
March 
9, 
2010 



Discrete 
Distributions 


A 
classical 
example 
for 
estimator 
bias 
are 
those 
of 
the 
mean 
and 
variance. 
Let 


X

1 
N

n 


^(X 
)= 
x 
(8.2.25)

N 


n=1 


This 
is 
an 
unbiased 
estimator 
of 
the 
mean 
hxip(x) 
since 


X

1 
N1

h^(X 
)i= 
hx 
ni= 
N 
hxi= 
hxi(8.2.26)

p(x) 
p(x) 
p(x) 
p(x)

NN 


n=1 


On 
the 
other 
hand, 
consider 
the 
estimator 
of 
the 
variance, 


N

X

^2(X 
)= 
1 
(x 
n 
- 
^(X 
))2 
(8.2.27)

N 


n=1 


This 
is 
biased 
since 
(omitting 
a 
few 
lines 
of 
algebra)

NDE


1 
XN 
- 
1 


^2(X 
)p(x) 
=(x 
n 
- 
^(X 
))2 
= 
2 
(8.2.28)

NN 


n=1 


8.3 
Discrete 
Distributions 
Denition 
63 
(Bernoulli 
Distribution). 
The 
Bernoulli 
distribution 
concerns 
a 
discrete 
binary 
variable 
x, 
with 
dom(x)= 
f0, 
1g. 
The 
states 
are 
not 
merely 
symbolic, 
but 
real 
values 
0 
and 
1. 


p(x 
= 
1) 
= 
. 
(8.3.1) 


From 
normalisation, 
it 
follows 
that 
p(x 
= 
0) 
= 
1 
- 
. 
From 
this 


hx) 
=0 
× 
p(x 
= 
0)+1 
× 
p(x 
= 
1) 
= 
. 
(8.3.2) 
The 
variance 
is 
given 
by 
var(x)= 
. 
(1 
- 
). 


Denition 
64 
(Categorical 
Distribution). 
The 
categorical 
distribution 
generalises 
the 
Bernoulli 
distribution 
to 
more 
than 
two 
(symbolic) 
states. 
For 
a 
discrete 
variable 
x, 
with 
symbolic 
states 
dom(x)= 
f1;:::;Cg, 


X

p(x 
= 
c)= 
c;c 
= 
1 
(8.3.3) 
c 


The 
Dirichlet 
is 
conjugate 
to 
the 
categorical 
distribution. 


Denition 
65 
(Binomial 
Distribution). 
The 
Binomial 
describes 
the 
distribution 
of 
a 
discrete 
two-state 
variable 
x, 
with 
dom(x)= 
f1, 
0} 
where 
the 
states 
are 
symbolic. 
The 
probability 
that 
in 
n 
Bernoulli 
Trials 
(independent 
samples), 
k 
`success’ 
states 
1 
will 
be 
observed 
is 




n 


p(y 
= 
kj)=k 
(1 
- 
)n..k 
(8.3.4)

k

DRAFT 
March 
9, 
2010 
143 



Continuous 
Distributions 


..

n

wherekis 
the 
binomial 
coecient. 
The 
mean 
and 
variance 
are 


hyi6= 
n, 
var(x)= 
n. 
(1 
..6) 
(8.3.5) 
The 
Beta 
distribution 
is 
the 
conjugate 
prior 
for 
the 
Binomial 
distribution. 


Denition 
66 
(Multinomial 
Distribution). 
Consider 
a 
multi-state 
variable 
x, 
with 
dom(x)= 
f1;:::, 
Kg, 
with 
corresponding 
state 
probabilities 
1;:::;K 
. 
We 
then 
draw 
n 
samples 
from 
this 
distribution. 
The 
probability 
of 
observing 
the 
state 
1 
y1 
times, 
state 
2 
y2 
times, 
. 
. 
. 
, 
state 
K 
yK 
times 
in 
the 
n 
samples 
is 


n

Y

n! 


yi

p(yj) 
= 
(8.3.6)

i 


y1! 
:::;yK! 


i=1 


n

where 
n 
= 


i=1 
yi. 


hyii6= 
ni, 
var(yi)= 
ni 
(1 
..6i) 
, 
hyiyji..hyiihyji6= 
..nij 
(i6
= 
j) 
(8.3.7) 


The 
Dirichlet 
distribution 
is 
the 
conjugate 
prior 
for 
the 
multinomial 
distribution. 


Denition 
67 
(Poisson 
Distribution). 
The 
Poisson 
distribution 
can 
be 
used 
to 
model 
situations 
in 
which 
the 
expected 
number 
of 
events 
scales 
with 
the 
length 
of 
the 
interval 
within 
which 
the 
events 
can 
occur. 
If 
. 
is 
the 
expected 
number 
of 
events 
per 
unit 
interval, 
then 
the 
distribution 
of 
the 
number 
of 
events 
x 
within 
an 
interval 
t. 
is 


1 


p(x 
= 
kj)= 
e 
..t 
(t)k 
;k 
=0, 
1, 
2;::. 
(8.3.8)

k!

For 
a 
unit 
length 
interval 
(t 
= 
1), 


hxi6= 
, 
var(x)= 
. 
(8.3.9) 
The 
Poisson 
distribution 
can 
be 
derived 
as 
a 
limiting 
case 
of 
a 
Binomial 
distribution 
in 
which 
the 
success 
probability 
scales 
as 
. 
= 
=n, 
in 
the 
limit 
n 
!1. 


8.4 
Continuous 
Distributions 
8.4.1 
Bounded 
distributions 
Denition 
68 
(Uniform 
distribution). 
For 
a 
variable 
x, 
the 
distribution 
is 
uniform 
if 
p(x)= 
const. 
over 
the 
domain 
of 
the 
variable. 


Denition 
69 
(Exponential 
Distribution). 
For 
x 
60, 


p(xj) 
6e..x 
(8.4.1) 


One 
can 
show 
that 
for 
rate 
. 


hxi6= 
1 

, 
var(x) 
= 
1 
2 
(8.4.2) 
144 
DRAFT 
March 
9, 
2010 



Continuous 
Distributions 


-101234500.511.5 
l=0.2
l=0.5
l=1
l=1.5
-50500.20.40.60.81 
l=0.2
l=0.5
l=1
l=1.5
Figure8.2:(a):Exponentialdistribution.(b):Laplace(doubleexponential)distribu-
tion.
(a) 
(b) 
The 
alternative 
parameterisation 
b 
=1=. 
is 
called 
the 
scale. 


Denition 
70 
(Gamma 
Distribution). 


..1

1 
x 
- 
x 


Gam 
(xj, 
)= 
e, 
x 
= 
0;a 
> 
0;ß 
> 
0 
(8.4.3)

..() 
ß 
ß 


a 
is 
called 
the 
shape 
parameter, 
ß 
is 
the 
scale 
parameter 
and 


Z8 


a..1

..(a)= 
te 
..tdt 
(8.4.4) 


0 


The 
parameters 
are 
related 
to 
the 
mean 
and 
variance 
through 


22 


a 
=
, 
ß 
= 
s(8.4.5) 


sµ 


where 
µ 
is 
the 
mean 
of 
the 
distribution 
and 
s 
is 
the 
standard 
deviation. 
The 
mode 
is 
given 
by 
(a 
- 
1) 
, 
for 
a 
= 
1. 


An 
alternative 
parameterisation 
uses 
the 
inverse 
scale 


..1 
..bx

Gamis 
(xj, 
b)= 
Gam 
(xj, 
1=b) 
. 
xe 
(8.4.6) 


Denition 
71 
(Inverse 
Gamma 
distribution). 


a 
1 


..=x 


InvGam 
(xj, 
)= 
e 
(8.4.7)

+1

..() 
x

This 
has 
mean 
=(a 
- 
1) 
for 
> 
1 
and 
variance 
2 
for 
> 
2. 


(..1)2(..2) 


Denition 
72 
(Beta 
Distribution). 
1 


p(xj, 
)= 
B 
(xj, 
)= 
x 
..1 
(1 
- 
x)..1 
, 
0 
= 
x 
= 
1 
(8.4.8)

B(, 
)
where 
the 
beta 
function 
is 
dened 
as 
..()..()

B(, 
) 
= 
(8.4.9)

..(a 
+ 
) 


DRAFT 
March 
9, 
2010 



Continuous 
Distributions 


and 
..(x) 
is 
the 
gamma 
function. 
Note 
that 
the 
distribution 
can 
be 
ipped 
by 
interchanging 
x 
for 
1 
- 
x, 
which 
is 
equivalent 
to 
interchanging 
a 
and 
. 


The 
mean 
is 
given 
by 


hx) 
= 

, 
var(x)= 
ß 
(8.4.10)
a 
+ 
ß 
(a 
+ 
)2 
(a 
+ 
ß 
+ 
1) 


8.4.2 
Unbounded 
distributions 
Denition 
73 
(Laplace 
(Double 
Exponential) 
Distribution). 


p(xj) 
= 
e- 
1 
b 
jx..| 
(8.4.11) 
For 
scale 
b 


hx) 
= 
, 
var(x)=2b2 
(8.4.12) 
Univariate 
Gaussian 
distribution 


The 
Gaussian 
distribution 
is 
an 
important 
distribution 
in 
science. 
It's 
technical 
description 
is 
given 
in 
denition(74). 


Denition 
74 
(Univariate 
Gaussian 
Distribution). 


..

1 
1

- 
(x..)2 


p(xj, 
2)= 
Nx 
, 
2v 
e 
22 
(8.4.13)
22 


where 
µ 
is 
the 
mean 
of 
the 
distribution, 
and 
2 
the 
variance. 
This 
is 
also 
called 
the 
normal 
distribution. 
One 
can 
show 
that 
the 
parameters 
indeed 
correspond 
to 


2 
=(x 
- 
)2 
(8.4.14)

µ 
= 
hxiN 
(x 


;2) 
, 


N 
(x 


;2) 


For 
µ 
= 
0 
and 
s 
= 
1, 
the 
Gaussian 
is 
called 
the 
standard 
normal 
distribution. 


Denition 
75 
(Student's 
t-distribution). 


"#- 
+1

1
2

..(+1 
) 
. 
2. 
(x 
- 
)2 


2 


p(xj, 
, 
) 
= 
1 
+ 
(8.4.15)

..(. 
)

2 


012345012345 
a=1 b=0.2
a=2 b=0.2
a=5 b=0.2
a=10 b=0.2
012345012345 
a=2 b=0.1
a=2 b=0.5
a=2 b=1
a=2 b=2
Figure 
8.3: 
(a): 
Gamma 
distribution 
with 
varying 
ß 
for 
xed 
. 
(b): 
Gamma 
distribution 
with 
varying 
a 
for 
xed 
. 


(a) 
(b) 
DRAFT 
March 
9, 
2010 



Multivariate 
Distributions 


-5051015-50510150102030-505101500.10.2
1200

Figure 
8.4: 
Top: 
200 
datapoints 
x;:::;xdrawn 
from 
a 
Gaussian 
distribution. 
Each 
vertical 
line 
denotes 
a 
datapoint 
at 
the 
corresponding 
x 
value 
on 
the 
horizontal 
axis. 
Middle: 
Histogram 
using 
10 
equally 
spaced 
bins 
of 
the 
datapoints. 
Bottom: 
Gaussian 
distribution 
N 
(x 


µ 
=5;s 
= 
3) 
from 
which 
the 
datapoints 
were 
drawn. 
In 
the 
limit 
of 
an 
innite 
amount 
of 
data, 
and 
limitingly 
small 
bin 
size, 
the 
normalised 
histogram 
tends 
to 
the 
Gaussian 
probability 
density 
function. 


where 
µ 
is 
the 
mean, 
. 
the 
degrees 
of 
freedom, 
and 
. 
scales 
the 
distribution. 
The 
variance 
is 
given 
by 


var(x) 
= 
. 
(. 
- 
2), 
for 
. 
> 
2 
(8.4.16) 
For 
. 
. 
8 
the 
distribution 
tends 
to 
a 
Gaussian 
with 
mean 
µ 
and 
variance 
1=. 
As 
. 
decreases 
the 
tails 
of 
the 
distribution 
become 
fatter. 
The 
t-distribution 
can 
be 
derived 
from 
a 
scaled 
mixture 
p(xja, 
b) 
=Z8 
=0 
N
1 
..x 
, 
..1Gamis 
(ja, 
b) 
dt 
Z8 
(8.4.17) 


t


2

(x..)2 
..bt 
a..1 
1 


ba

2

- 
t 


= 
e 


(8.4.18)
dt


e


2p


..(a)


=0 


ba 
..(a 
+ 
1 
)

2 
1 


(8.4.19)
v 


a+

..(a)2p 


b 
+ 
1 
(x 
- 
)2

2 


= 


1 


2 


It 
is 
conventional 
to 
reparameterise 
using 
. 
=2a 
and 
. 
= 
a=b. 


8.5 
Multivariate 
Distributions 
Denition 
76 
(Dirichlet 
Distribution). 
The 
Dirichlet 
distribution 
is 
a 
distribution 
on 
probability 
distributions: 


QQ

1 
XYuq..1 


p()= 
i 
- 
1 
q 
(8.5.1)

Z(u)

i=1 
q=1 


where 


QQ 


q=1 
..(uq) 
Z(u)=(8.5.2)

PQ

G 


q=1 
uq

It 
is 
conventional 
to 
denote 
the 
distribution 
as 


Dirichlet 
(ju) 
(8.5.3) 


DRAFT 
March 
9, 
2010 
147 



Multivariate 
Gaussian 


00.5100.5100.51x3x2x1
00.5100.5100.51x3x2x1
00.5100.5100.51x3x2x1
00.5100.5100.51x3x2x1
(a) 
(b) 
(c) 
(d) 
Figure 
8.5: 
Dirichlet 
distribution 
with 
parameter 
(u1;u2;u3) 
displayed 
on 
the 
simplex 
x1;x2;x3 
h0;x1 
+ 
x2 
+ 
x3 
= 
1. 
Black 
denotes 
low 
probability 
and 
white 
high 
probability. 
(a): 
(3, 
3, 
3) 
(b): 
(0:1, 
1, 
1). 
(c): 
(4, 
3, 
2). 
(d): 
(0:05, 
0:05, 
0:05). 


The 
parameter 
u 
controls 
how 
strongly 
the 
mass 
of 
the 
distribution 
is 
pushed 
to 
the 
corners 
of 
the 
simplex. 
Setting 
uq 
= 
1 
for 
all 
q 
corresponds 
to 
a 
uniform 
distribution, 
g(8.5). 
In 
the 
binary 
case 
Q 
= 
2, 
this 
is 
equivalent 
to 
a 
Beta 
distribution. 


The 
product 
of 
two 
Dirichlet 
distributions 
is 


Dirichlet 
(ju1) 
Dirichlet 
(ju2) 
= 
Dirichlet 
(ju1 
+ 
u2) 
(8.5.4) 


Marginal 
of 
a 
Dirichlet: 


..

Dirichlet 
(ju) 
= 
Dirichletnjjunj(8.5.5) 
j 


The 
marginal 
of 
a 
single 
component 
i 
is 
a 
Beta 
distribution: 


0. 


A

p(i)= 
B 
@ijui;uj 
(8.5.6) 
j6

=i 


8.6 
Multivariate 
Gaussian 
The 
multivariate 
Gaussian 
plays 
a 
central 
role 
throughout 
this 
book 
and 
as 
such 
we 
discuss 
its 
properties 
in 
some 
detail. 


Denition 
77 
(Multivariate 
Gaussian 
Distribution). 


1- 
1 
(x..)T..1(x..)

p(xj, 
)= 
Nh(x 


, 
) 
hpe 
2 
(8.6.1)
det 
(2) 


where 
µ 
is 
the 
mean 
vector 
of 
the 
distribution, 
and 
S 
the 
covariance 
matrix. 
The 
inverse 
covariance 
..1 
is 
called 
the 
precision. 


One 
may 
show 


DE

;) 
, 
S 
=(x 
..h)(x 
..h)T

(8.6.2)
µ 
=hxiN 
(x 


N 
(x 


;) 


DRAFT 
March 
9, 
2010 



Multivariate 
Gaussian 


-4-2024-4-202400.020.040.060.080.10.120.14
-4-3-2-101234-4-3-2-101234 
0.020.040.060.080.10.12
(a) 
(b) 
Figure 
8.6: 
(a): 
Bivariate 
Gaussian 
with 
mean 
(0, 
0) 
and 
covariance 
[1, 
0:5; 
0:5, 
1:75]. 
Plotted 
on 
the 
vertical 
axis 
is 
the 
probability 
density 
value 
p(x). 
(b): 
Probability 
density 
contours 
for 
the 
same 


v 


bivariate 
Gaussian. 
Plotted 
are 
the 
unit 
eigenvectors 
scaled 
by 
the 
square 
root 
of 
their 
eigenvalues, 
i. 


The 
multivariate 
Gaussian 
is 
given 
in 
denition(77). 
Note 
that 
det 
(M)= 
Ddet 
(M), 
where 
M 
is 
a 
DD 
matrix, 
which 
explains 
the 
dimension 
independent 
notation 
in 
the 
normalisation 
constant 
of 
denition(77). 


The 
moment 
representation 
uses 
µ 
and 
S 
to 
parameterise 
the 
Gaussian. 
The 
alternative 
canonical 
repres
entation 


..

p(xjb, 
M;c)= 
ce 


x2

1

TMx+xTb 


(8.6.3) 
is 
related 
to 
the 
moment 
representation 
via 


1

1 


2

bTM..1b

S 
= 
M..1 
, 
µ 
= 
M..1b, 


p= 
ce 


(8.6.4)
det 
(2) 


The 
multivariate 
Gaussian 
is 
widely 
used 
and 
it 
is 
instructive 
to 
understand 
the 
geometric 
picture. 
This 
can 
be 
obtained 
by 
view 
the 
distribution 
in 
a 
dierent 
co-ordinate 
system. 
First 
we 
use 
that 
every 
real 
symmetric 
matrix 
D 
× 
D 
has 
an 
eigen-decomposition 


S 
= 
EET 
(8.6.5) 


where 
ETE 
= 
I 
and 
. 
= 
diag 
(1;:::;D). 
In 
the 
case 
of 
a 
covariance 
matrix, 
all 
the 
eigenvalues 
i 
are 
positive. 
This 
means 
that 
one 
can 
use 
the 
transformation 


y 
= 

so 
that 


1 


2

ET 
(x 
- 
) 
(8.6.6) 


T

(x 
- 
)T 
S 
(x 
- 
)=(x 
- 
)T 
EET 
(x 
- 
)= 
yy 
(8.6.7) 


Under 
this 
transformation, 
the 
multivariate 
Gaussian 
reduces 
to 
a 
product 
of 
D 
univariate 
zero-mean 
unit 
variance 
Gaussians 
(since 
the 
Jacobian 
of 
the 
transformation 
is 
a 
constant). 
This 
means 
that 
we 
can 
view 
a 
multivariate 
Gaussian 
as 
a 
shifted, 
scaled 
and 
rotated 
version 
of 
an 
isotropic 
Gaussian 
in 
which 
the 
centre 
is 
given 
by 
the 
mean, 
the 
rotation 
by 
the 
eigenvectors, 
and 
the 
scaling 
by 
the 
square 
root 
of 
the 
eigenvalues, 
as 
depicted 
in 
g(8.6b). 


Isotropic 
means 
`same 
under 
rotation'. 
For 
any 
isotropic 
distribution, 
contours 
of 
equal 
probability 
are 
spherical 
around 
the 
origin. 


Some 
useful 
properties 
of 
the 
Gaussian 
are 
as 
follows: 


DRAFT 
March 
9, 
2010 



Multivariate 
Gaussian 


Denition 
78 
(Partitioned 
Gaussian). 
For 
a 
distribution 
N 
(z 


, 
) 
dened 
jointly 
over 
two 
vectors 
x 
and 
y 
of 
potentially 
diering 
dimensions, 




x 


z 
=(8.6.8)

y

with 
corresponding 
mean 
and 
partitioned 
covariance 




x 
xx 
xy

µ 
=S 
=(8.6.9)
yyx 
yy

where 
yx 
= 
T 
. 
The 
marginal 
distribution 
is 
given 
by 


xy

p(x)= 
N 
(x 


, 
xx) 
(8.6.10)

x

and 
conditional 


..

..

..1 
..1 


p(xjy)= 
Nx 


x 
+ 
xyyyy 
- 
y, 
xx 
- 
xyyy 
yx(8.6.11) 


Denition 
79 
(Product 
of 
two 
Gaussians). 
The 
product 
of 
two 
Gaussians 
is 
another 
Gaussian, 
with 
a 
multiplicative 
factor, 
exercise(114): 




exp..1 
(1 
- 
2)T 
S..1 
(1 
- 
2)

2 


N 
(x 


2, 
2)= 
N 
(x 


, 
) 
p(8.6.12)

1, 
1) 
N 
(x 


det 
(2S) 


where 
S 
= 
1 
+ 
2 
and 
the 
mean 
and 
covariance 
are 
given 
by 


µ 
= 
1S..1 
2 
+ 
2S..1 
1 
S 
= 
1S..12 
(8.6.13) 


Denition 
80 
(Linear 
Transform 
of 
a 
Gaussian). 
For 
the 
linear 
transformation 
y 
= 
Ax 
+ 
b 
(8.6.14) 
where 
x 
~ 
N 
(x 
, 
), 
y 
~ 
Ny 
Aµ 
+ 
b, 
AAT(8.6.15) 


Denition 
81 
(Entropy 
of 
a 
Gaussian). 
The 
dierential 
entropy 
of 
a 
multivariate 
Gaussian 
p(x) 
= 
N 
(x 
, 
) 
is 
1 
D 


H(x) 
..hlog 
p(x)ip(x) 
= 
log 
det 
(2) 
+ 
(8.6.16)

22 
where 
D 
= 
dim 
x. 
Note 
that 
the 
entropy 
is 
independent 
of 
the 
mean 
. 


DRAFT 
March 
9, 
2010 



Multivariate 
Gaussian 


(a) 
(b) 
Figure 
8.7: 
Beta 
distribution. 
The 
parameters 
a 
and 
ß 
can 
also 
be 
witting 
in 
terms 
of 
the 
mean 
and 
variance, 
leading 
to 
an 
alternative 
parameterisation, 
see 
exercise(94). 
8.6.1 
Conditioning 
as 
system 
reversal 


For 
a 
joint 
distribution 
p(x, 
y), 
consider 
the 
conditional 
p(xjy). 
The 
statistics 
of 
p(xjy) 
can 
be 
obtained 
using 
a 
linear 
system 
of 
the 
form 


 

..

..

Ay 
+ 
 

(8.6.17)
x 
= 
where 


..





 

..

  

..

 

- 






(8.6.18)
N


µ


, 


and 
this 
reversed 
noise 
is 
uncorrelated 
with 
y. 


To 
show 
this, 
we 
need 
to 
make 
the 
statistics 
of 
x 
under 
this 
linear 
system 
match 
those 
given 
by 
the 
conditioning 
operation, 
(8.6.11). 
The 
mean 
of 
the 
linear 
system 
is 
given 
by 


 + 
 

..



- 


(8.6.19)
= 
A

xy

µ


and 
the 
covariances 
by 
(note 
that 
covariance 
of 
y 
remains 
unaected 
by 
the 
system 
reversal) 


 ..  
xx 
= 
Ayy 
A..T 
+ 
..(8.6.20) 


 

..

xy 
= 
Ayy 
(8.6.21) 
From 
equation 
(8.6.21) 
we 
have 


 

..1

A..= 
xyyy 
(8.6.22) 
which, 
using 
in 
equation 
(8.6.20), 
gives 


 ..  
....A..T 
..1 
(8.6.23)

S 
= 
xx 
Ayy 
= 
xx 
- 
xyyy 
yx 
Using 
equation 
(8.6.19) 
we 
similarly 
obtain 


..

  ..1

- 
= 
- 
A- 
xyyy(8.6.24)

= 
µ


µ


µ


xy 


x 


y 


This 
means 
that 
we 
can 
write 
an 
explicit 
linear 
system 
of 
the 
form 
equation 
(8.6.17) 
where 
the 
parameters 
are 
given 
in 
terms 
of 
the 
statistics 
of 
the 
original 
system. 
These 
results 
are 
just 
a 
restatement 
of 
the 
conditioning 
results 
but 
shows 
how 
it 
may 
be 
interpreted 
as 
a 
linear 
system. 
This 
is 
useful 
in 
deriving 
results 
in 
inference 
with 
Linear 
Dynamical 
Systems. 


8.6.2 
Completing 
the 
square 
A 
useful 
technique 
in 
manipulating 
Gaussians 
is 
completing 
the 
square. 
For 
example, 
the 
expression 


e 


..

1 


2

x

TAx+bT

x 


(8.6.25) 
can 
be 
transformed 
as 
follows. 
First 
we 
complete 
the 
square: 


11 
..T 
..1 


xTAx 
- 
bTx 
= 
x 
- 
A..1bAx 
- 
A..1b- 
bTA..1b 
(8.6.26)

222

Hence 


..

q..

1

1

bTA..1b

x

TAx..bT

..

A..1b, 
A..1

2A..1

det


(8.6.27)
x 


= 
Nx

2

2

e 


e 


From 
this 
one 
can 
derive

Z

q

.. 


1

bTA..1b

x

TAx+bT

..

2A..1

det


(8.6.28)
xdx 
=

2

e 


e 


DRAFT 
March 
9, 
2010 



Multivariate 
Gaussian 


8.6.3 
Gaussian 
propagation 
Let 
y 
be 
linearly 
related 
to 
x 
through 


y 
= 
Mx 
+ 
. 
(8.6.29) 
where 
. 
N 
(, 
), 
and 
x 
N 
(, 
x).

x

R

Then 
the 
marginal 
p(y)=p(yjx)p(x) 
is 
a 
Gaussian 


x 


p(y)= 
N 
y 


M+ 
, 
MxMT 
+ 
S 
(8.6.30)

x 


8.6.4 
Whitening 
and 
centering 
For 
a 
set 
of 
data 
x1 
;:::, 
xN 
, 
with 
dim 
xn 
= 
D, 
we 
can 
transform 
this 
data 
to 
y1 
;:::, 
yN 
with 
zero 
mean 
using 
centering: 


yn 
= 
xn 
- 
m 
(8.6.31) 
where 
the 
mean 
m 
of 
the 
data 
is 
given 
by 


X

1 
N

m 
= 
xn 
(8.6.32)

N 


n=1 


Furthermore, 
we 
can 
transform 
to 
a 
values 
z1 
;:::, 
zN 
that 
have 
zero 
mean 
and 
unit 
covariance 
using 


whitening 


zn 
= 
S- 
1
2 
(xn 
- 
m) 
(8.6.33) 


where 
the 
covariance 
S 
of 
the 
data 
is 
given 
by 


X

1 
N

S 
=(xn 
- 
m)(xn 
- 
m)T 
(8.6.34)

N 


n=1 


An 
equivalent 
approach 
is 
to 
compute 
the 
SVD 
decomposition 
of 
the 
matrix 
of 
centered 
datapoints 




1 
N

USVT 
= 
Y, 
Y 
=y;:::, 
y(8.6.35) 


then 


v 


Z 
= 
Ndiag 
(1=S1;1;:::, 
1=SD;D) 
UTY 
(8.6.36) 


has 
zero 
mean 
and 
unit 
covariance, 
see 
exercise(111). 


8.6.5 
Maximum 
likelihood 
training 
	

Given 
a 
set 
of 
training 
data 
X 
=x1 
;:::, 
xN, 
drawn 
from 
a 
Gaussian 
N 
(x 


, 
) 
with 
unknown 
mean 
µ 
and 
covariance 
, 
how 
can 
we 
nd 
these 
parameters? 
Assuming 
the 
data 
are 
drawn 
i.i.d. 
the 
log 
likelihood 
is 


XX

N1 
N
N 


L(, 
) 
= 
log 
p(xj, 
)= 
- 
(xn 
- 
)T 
..1 
(xn 
- 
) 
- 
log 
det 
(2) 
(8.6.37)

22 


n=1 
n=1 


DRAFT 
March 
9, 
2010 



Multivariate 
Gaussian 


Optimal 
µ 
Taking 
the 
partial 
derivative 
with 
respect 
to 
the 
vector 
µ 
we 
obtain 
the 
vector 
derivative 


N

rL(, 
)= 
..1 
(xn 
- 
) 
(8.6.38) 


n=1 


Equating 
to 
zero 
gives 
that 
at 
the 
optimum 
of 
the 
log 
likelihood, 


Xp

N

..1xn 


= 
N..1 
(8.6.39) 


n=1 


and 
therefore, 
optimally 


Xp

N

µ 
= 


N 


n=1 


Optimal 
S 


X

1 


xn 
(8.6.40) 


The 
derivative 
of 
L 
with 
respect 
to 
the 
matrix 
S 
requires 
more 
work. 
It 
is 
convenient 
to 
isolate 
the 
dependence 
on 
the 
covariance, 
and 
also 
parameterise 
using 
the 
inverse 
covariance, 
..1 
, 


1

0p

1 


L 
= 
..

2

trace 


BBBB
. 


CCCC
. 


+ 


N

..1 


(xn 
- 
)(xn 
- 
)T 


n=1 


X
|

 


 


N 


2..1

log 
det


(8.6.41)
2 


{z

M 


Using 
M 
= 
MT, 
we 
obtain 
. 
1 
N 


L 
= 
- 
M 
+ 
S 
(8.6.42)

@..1 
22 
Equating 
the 
derivative 
to 
the 
zero 
matrix 
and 
solving 
for 
S 
gives 


X

N

N 


n=1 


1 


(xn 
- 
)(xn 
- 
)T 
(8.6.43)

S 
= 


Equations 
(8.6.40) 
and 
(8.6.43) 
dene 
the 
Maximum 
Likelihood 
solution 
mean 
and 
covariance 
for 
training 
data 
X 
. 
Consistent 
with 
our 
previous 
results, 
in 
fact 
these 
equations 
simply 
set 
the 
parameters 
to 
their 
sample 
statistics 
of 
the 
empirical 
distribution. 
That 
is, 
the 
mean 
is 
set 
to 
the 
sample 
mean 
of 
the 
data 
and 
the 
covariance 
to 
the 
sample 
covariance. 


8.6.6 
Bayesian 
Inference 
of 
the 
mean 
and 
variance 
For 
simplicity 
here 
we 
deal 
with 
the 
univariate 
case. 
Assuming 
i.i.d. 
data 
the 
likelihood 
is 


1 
- 
1 
PN 
(xn..)2 


22n=1

p(Xj, 
2)= 
e 
(8.6.44)

(22)N=2 


For 
a 
Bayesian 
treatment, 
we 
require 
the 
posterior 
of 
the 
parameters 


p(, 
2jX 
) 
. 
p(Xj, 
2)p(, 
2)= 
p(Xj, 
2)p(j2)p(2) 
(8.6.45) 
Our 
aim 
is 
to 
nd 
conjugate 
priors 
for 
the 
mean 
and 
variance. 
A 
convenient 
choice 
for 
a 
prior 
on 
the 
mean 
µ 
is 
that 
it 
is 
a 
Gaussian 
centred 
on 
0: 


1

1- 
22 
(0..)2 


0

p(j0;2)= 


0

 


(8.6.46)
e 


202 


DRAFT 
March 
9, 
2010 



Multivariate 
Gaussian 


The 
posterior 
is 
then 


11

11 
- 
(0..)2- 
n(xn..)2 


2222 


p(, 
2jX 
) 
. 
p2 
(2)N=2 
e 
0 
p(2) 
(8.6.47) 
0 


It 
is 
convenient 
to 
write 
this 
in 
the 
form 
p(, 
2jX 
)= 
p(j2 
, 
X 
)p(2jX 
) 
(8.6.48) 
Since 
equation 
(8.6.47) 
has 
quadratic 
contributions 
in 
µ 
in 
the 
exponent, 
the 
conditional 
posterior 
p(j2 
, 
X 
) 
is 
Gaussian. 
To 
identify 
this 
Gaussian 
we 
multiply 
out 
the 
terms 
in 
the 
exponent 
to 
arrive 
at 
1 
..

exp 
- 
aµ 
2 
- 
2bµ 
+ 
c(8.6.49)

2with 


Ppn 
2 
Xn)2
1 
N0 
x(x
n 
0 


a 
=+ 
;b 
=+ 
;c 
= 
+(8.6.50)

2 
2 
2 
2 
2 
2 


00 
0 
n 


Using 
the 
identity 


2 


bb2 


aµ 
2 
- 
2bµ 
+ 
c 
= 
aµ 
- 
+c 
- 
(8.6.51) 


aa

we 
can 
write 


v 
- 
1 
a(- 
b 
)21 
- 
1 
c- 
b2 
11 


2 
a

p(, 
2jX 
) 
. 
ae 
2 
a 
v 
e 
pp(2)(8.6.52)

|{z}a2 
(2)N=2 


0 


p(jX 
;2) 
|{z}p(2jX 
) 


We 
encounter 
a 
diculty 
in 
attempting 
to 
nd 
a 
conjugate 
prior 
for 
2 
because 
the 
term 
b2=a 
is 
not 
a 
simple 
expression 
of 
2 
. 
For 
this 
reason 
we 
constrain 


02 
= 
2 
(8.6.53) 
for 
some 
xed 
hyperparameter 
. 
Dening 
the 
constants 


a~= 
1+ 
N, 
~b 
= 
0 
+Xx 
n 
;c~= 
20 
+X(x 
n)2 
(8.6.54)

. 
. 


nn 


we 
have 


 !

~

b2 
1 
b2 


c 
- 
= 
c~- 
(8.6.55) 


a2a~
Using 
this 
expression 
in 
equation 
(8.6.52) 
we 
obtain 


..1~b
2

..N=2 
- 
c~- 


222 
a~

p(2jX 
) 
/ep(2) 
(8.6.56) 


If 
we 
therefore 
use 
an 
inverse 
gamma 
distribution 
we 
will 
have 
a 
conjugate 
prior 
for 
2 
. 
For 
a 
GaussInverse-
Gamma 
prior: 


..

..

p(, 
2)= 
Nµ 


0, 
2InvGam2j, 
(8.6.57) 
the 
posterior 
is 
also 
Gauss-Inverse-Gamma 
with 


 !  !!

~2 
~b
2

bN 
1 


p(, 
2jX 
)= 
Nµ 


, 
InvGam2ja 
+ 
;ß 
+ 
c~- 
(8.6.58) 


a~a~22a~

DRAFT 
March 
9, 
2010 



Exponential 
Family 


8.6.7 
Gauss-Gamma 
distribution 
It 
is 
common 
to 
to 
use 
a 
prior 
on 
the 
precision, 
dened 
as 
the 
inverse 
variance 


1 


. 
= 
(8.6.59)

2 


If 
we 
then 
use 
a 
Gamma 
prior 


1 


p(j, 
)= 
Gam 
(j, 
)= 
..1 
e 
..=ß 
(8.6.60)

..()

The 
posterior 
will 
be 




~

p(jX 
, 
, 
)= 
Gamja 
+ 
N=2;(8.6.61) 


where 


 !

~

1 
11 
b2 


=+ 
c~- 
(8.6.62) 


~ß 
2a~

The 
Gauss-Gamma 
prior 
distribution 


..



p(, 
j0, 
, 
, 
)= 
Nµ 


0, 
..1Gam 
(j, 
) 
(8.6.63) 


is 
the 
conjugate 
prior 
for 
a 
Gaussian 
with 
unknown 
mean 
µ 
and 
precision 
. 


The 
posterior 
for 
this 
prior 
is 
a 
Gauss-Gamma 
distribution 
with 
parameters 


 !

~

b 
1 


~

p(, 
jX 
;0, 
, 
, 
)= 
Nµ 


, 
Gamja 
+ 
N=2;(8.6.64) 


a~a

~

The 
marginal 
p(jX 
;0, 
, 
, 
) 
is 
a 
Student's 
t-distribution. 
An 
example 
of 
a 
Gauss-Gamma 
prior/posterior 
is 
given 
in 
g(8.8). 


The 
Maximum 
Likelihood 
solution 
is 
recovered 
in 
the 
limit 
of 
a 
`at’ 
(improper) 
prior 
0 
=0;. 
!1;a 
= 
1=2;ß 
!1, 
see 
exercise(102). 
The 
unbiased 
estimators 
for 
the 
mean 
and 
variance 
are 
given 
using 
the 
proper 
prior 
0 
=0;. 
!1;a 
=1;ß 
!1, 
exercise(103). 


For 
the 
multivariate 
case, 
the 
extension 
of 
these 
techniques 
uses 
a 
multivariate 
Gaussian 
distribution 
for 
the 
conjugate 
prior 
on 
the 
mean, 
and 
an 
Inverse 
Wishart 
distribution 
for 
the 
conjugate 
prior 
on 
the 
covariance[124]. 


8.7 
Exponential 
Family 
A 
theoretically 
convenient 
class 
of 
distributions 
are 
the 
exponential 
family, 
which 
contains 
many 
standard 
distributions, 
including 
the 
Gaussian, 
Gamma, 
Poisson, 
Dirichlet, 
Wishart, 
Multinomial, 
Markov 
Random 
Field. 


Denition 
82 
(Exponential 
Family). 
For 
a 
distribution 
on 
a 
(possibly 
multidimensional) 
variable 
x 
(continuous 
or 
discrete) 
an 
exponential 
family 
model 
is 
of 
the 
form 


P

i 
i()Ti(x).. ()

p(xj)= 
h(x)e(8.7.1) 


. 
are 
the 
parameters, 
Ti(x) 
the 
test 
statistics, 
and 
. 
() 
is 
the 
log 
partition 
function 
that 
ensure 
normalisation 


P

. 
() 
= 
log 
h(x)ei 
i()Ti(x) 
(8.7.2) 


x 


DRAFT 
March 
9, 
2010 



Exponential 
Family 



(a) 
Prior 
(b) 
Posterior 
Figure 
8.8: 
Bayesian 
approach 
to 
inferring 
the 
mean 
and 
precision 
(inverse 
variance) 
of 
a 
Gaussian 
based 
on 
N 
= 
10 
randomly 
drawn 
datapoints. 
(a): 
A 
Gauss-Gamma 
prior 
with 
0 
= 
0, 
a 
= 
2, 
ß 
= 
1, 
. 
= 
1. 
(b): 
Gauss-Gamma 
posterior 
conditional 
on 
the 
data. 
For 
comparison, 
the 
sample 
mean 
of 
the 
data 
is 


1.87 
and 
Maximum 
Likelihood 
optimal 
variance 
is 
1:16 
(computed 
using 
the 
N 
normalisation). 
The 
10 
datapoints 
were 
drawn 
from 
a 
Gaussian 
with 
mean 
2 
and 
variance 
1. 
See 
demoGaussBayes.m. 
One 
can 
always 
transform 
the 
parameters 
to 
the 
form 
. 
()= 
. 
in 
which 
case 
the 
distribution 
is 
in 
canonical 
form: 


TT(x).. ()

p(xj)= 
h(x)e 
(8.7.3) 


For 
example 
the 
univariate 
Gaussian 
can 
be 
written 


2

11 


1 
- 
(x..)2 
- 
2+ 
2 
x- 
µ 
- 
1 
log 
2 


v 
e 
22 
= 
e 
22 
x
222 
(8.7.4)
22 


Dening 
t1(x)= 
x, 
t2(x)= 
..x2=2 
and 
, 
1 
= 
, 
2 
= 
2 
, 
h(x) 
= 
1, 
then 


1 
11 
2 


1

1()= 
;2()= 
; ()= 
+log 
2 
(8.7.5)

2 
2 
2 
2 


Note 
that 
the 
parameterisation 
is 
not 
necessarily 
unique 
– 
we 
can 
for 
example 
rescale 
the 
functions 
Ti(x) 
and 
inversely 
scale 
i 
by 
the 
same 
amount 
to 
arrive 
at 
an 
equivalent 
representation. 


8.7.1 
Conjugate 
priors 
In 
principle, 
Bayesian 
learning 
for 
the 
exponential 
family 
is 
straightforward. 
In 
canonical 
form 


TT(x).. ()

p(xj)= 
h(x)e 
(8.7.6) 


For 
a 
prior 
with 
hyperparameters 
;, 


T.. ()

p(j;) 
. 
e 
(8.7.7) 


the 
posterior 
is 


p(jx) 
. 
p(xj)p() 
(8.7.8) 


TT(x).. ()T.. ()

. 
h(x)ee 
(8.7.9) 


T[T(x)+]..[+1] ()

. 
e 
(8.7.10) 


so 
that 
the 
prior 
equation 
(8.7.7) 
is 
conjugate 
for 
the 
exponential 
family 
likelihood 
equation 
(8.7.6). 
Whilst 
the 
likelihood 
is 
in 
the 
exponential 
family, 
the 
conjugate 
prior 
is 
not 
necessarily 
in 
the 
exponential 
family. 


156 
DRAFT 
March 
9, 
2010 



The 
Kullback-Leibler 
Divergence 
KL(qjp) 


8.8 
The 
Kullback-Leibler 
Divergence 
KL(qjp) 
The 
Kullback-Leibler 
divergence 
KL(qjp) 
measures 
the 
`dierence’ 
between 
distributions 
q 
and 
p[68]. 


Denition 
83. 
KL 
divergence 
For 
two 
distributions 
q(x) 
and 
p(x) 
KL(qjp) 
hlog 
q(x) 
- 
log 
p(x)i= 
0 
(8.8.1)

q(x) 
where 
hf(x)ir(x) 
denotes 
average 
of 
the 
function 
f(x) 
with 
respect 
to 
the 
distribution 
r(x). 


The 
KL 
divergence 
is 
= 
0 
The 
KL 
divergence 
is 
widely 
used 
and 
it 
is 
therefore 
important 
to 
understand 
why 
the 
divergence 
is 
positive. 


To 
see 
this, 
consider 
the 
following 
linear 
bound 
on 
the 
function 
log(x) 
log(x) 
= 
x 
- 
1 
(8.8.2) 


as 
plotted 
in 
the 
gure 
on 
the 
right. 
Replacing 
x 
by 
p(x)=q(x) 
in 
the 
above 
bound 
p(x) 
p(x)

- 
1 
= 
log 
(8.8.3) 


q(x) 
q(x) 


Since 
probabilities 
are 
non-negative, 
we 
can 
multiply 
both 
sides 
by 
q(x) 
to 
obtain 
p(x) 
- 
q(x) 
= 
q(x) 
log 
p(x) 
- 
q(x) 
log 
q(x) 
(8.8.4) 


R

We 
now 
integrate 
(or 
sum 
in 
the 
case 
of 
discrete 
variables) 
both 
sides. 
Usingp(x)dx 
= 
1;q(x)dx 
= 
1, 
1 
- 
1 
= 
hlog 
p(x) 
- 
log 
q(x)iq(x) 
(8.8.5) 
Rearranging 
gives 
hlog 
q(x) 
- 
log 
p(x)iq(x) 
= 
KL(qjp) 
= 
0 
(8.8.6) 


The 
KL 
divergence 
is 
zero 
if 
and 
only 
if 
the 
two 
distributions 
are 
exactly 
the 
same. 


8.8.1 
Entropy 
For 
both 
discrete 
and 
continuous 
variables, 
the 
entropy 
is 
dened 
as 


H(p) 
..hlog 
p(x)ip(x) 
(8.8.7) 


For 
continuous 
variables, 
this 
is 
also 
called 
the 
dierential 
entropy, 
see 
also 
exercise(113). 
The 
entropy 
is 
a 
measure 
of 
the 
uncertainty 
in 
a 
distribution. 
One 
way 
to 
see 
this 
is 
that 


H(p)= 
..KL(pju)+ 
const. 
(8.8.8) 


where 
u 
is 
a 
uniform 
distribution. 
Since 
the 
KL(pju) 
= 
0, 
the 
less 
like 
a 
uniform 
distribution 
p 
is, 
the 
smaller 
will 
be 
the 
entropy. 
Or, 
vice 
versa, 
the 
more 
similar 
p 
is 
to 
a 
uniform 
distribution, 
the 
greater 
will 
be 
the 
entropy. 
Since 
the 
uniform 
distribution 
contains 
the 
least 
information 
a 
prior 
about 
which 
state 
p(x) 
is 
in, 
the 
entropy 
is 
therefore 
a 
measure 
of 
the 
a 
priori 
uncertainty 
in 
the 
state 
occupancy. 
For 
a 
discrete 
distribution 
we 
can 
permute 
the 
state 
labels 
without 
changing 
the 
entropy. 
For 
a 
discrete 
distribution 
the 
entropy 
is 
positive, 
whereas 
the 
dierential 
entropy 
can 
be 
negative. 


DRAFT 
March 
9, 
2010 
157 


1234-4-3-2-10123
R



Exercises 


8.9 
Code 
demoGaussBayes.m: 
Bayesian 
tting 
of 
a 
univariate 
Gaussian 
logGaussGamma.m: 
Plotting 
routine 
for 
a 
Gauss-Gamma 
distribution 


8.10 
Exercises 
Exercise 
80. 
In 
a 
public 
lecture, 
the 
following 
phrase 
was 
uttered 
by 
a 
Professor 
of 
Experimental 
Psyc
hology: 
`In 
a 
recent 
data 
survey, 
90% 
of 
people 
claim 
to 
have 
above 
average 
intelligence, 
which 
is 
clearly 
nonsense!’ 
[Audience 
Laughs]. 
Is 
it 
theoretically 
possible 
for 
90% 
of 
people 
to 
have 
above 
average 
intellig
ence? 
If 
so, 
give 
an 
example, 
otherwise 
explain 
why 
not. 
What 
about 
above 
median 
intelligence? 


Exercise 
81. 
Consider 
the 
distribution 
dened 
on 
real 
variables 
x, 
y: 


..x2..y

p(x, 
y) 
. 
(x 
2 
+ 
y 
2)2 
e 
2 
, 
dom(x)= 
dom(y)= 
f..8 
::. 
1} 
(8.10.1) 


Show 
that 
hx) 
= 
hy) 
=0. 
Furthermore 
show 
that 
x 
and 
y 
are 
uncorrelated, 
hxy) 
= 
hxihyi. 
Whilst 
x 
and 
y 
are 
uncorrelated, 
show 
that 
they 
are 
nevertheless 
dependent. 


Exercise 
82. 
For 
a 
variable 
x 
with 
dom(x)= 
f0, 
1g, 
and 
p(x 
= 
1)= 
, 
show 
that 
in 
n 
independent 
draws 
x1;:::;xn 
from 
this 
distribution, 
the 
probability 
of 
observing 
k 
states 
1 
is 
the 
Binomial 
distribution 


n 


k 
(1 
- 
)n..k 
(8.10.2)

k 


Exercise 
83 
(Normalisation 
constant 
of 
a 
Gaussian). 
The 
normalisation 
constant 
of 
a 
Gaussian 
distrib
ution 
is 
related 
to 
the 
integral 


ZD8 


- 
12

x

I 
= 
e 
2 
dx 
(8.10.3) 


..8 


By 
considering 


ZD8 
ZD8 
ZD8 
ZD8 


- 
12 
- 
12 
- 
122 


I2 
xy 
x+y

= 
e 
2 
dx 
e 
2 
dy 
= 
e 
2 
dxdy 
(8.10.4) 


..8 
..8 
..1..8 


and 
transforming 
to 
polar 
coordinates, 
show 
that 


v 


1. 
I 
=2p 
R8 
1 
p

- 
22 
(x..)2 


2.
..8 
e 
dx 
=22 


Exercise 
84. 
For 
a 
univariate 
Gaussian 
distribution, 
show 
that 


1. 
µ 
= 
hxiN 
(x 
;2) 


DE

2. 
2 
=(x 
- 
)2
N 
(x 
;2) 


Exercise 
85. 
Show 
that 
the 
marginal 
of 
a 
Dirichlet 
distribution 
is 
another 
Dirichlet 
distribution: 


..

Dirichlet 
(ju) 
= 
Dirichletnjjunj(8.10.5) 
j 


Exercise 
86. 
For 
a 
Beta 
distribution, 
show 
that

DE

B(a 
+ 
k, 
)

k

x 
= 
(8.10.6)

B(, 
) 


and, 
using 
..(x 
+1) 
= 
x..(x), 
derive 
an 
explicit 
expression 
for 
the 
kth 
moment 
of 
a 
Beta 
distribution. 


158 
DRAFT 
March 
9, 
2010 



Exercises 


Exercise 
87. 
Dene 
the 
moment 
generating 
function 
as 





tx

g(t) 
e 
p(x) 
(8.10.7) 


Show 
that 


DE

dk 


k

lim 
g(t)=x 
(8.10.8) 


t!0 
dtkp(x) 


Exercise 
88 
(Change 
of 
variables). 
Consider 
a 
one 
dimensional 
continuous 
random 
variable 
x 
with 
corresponding 
p(x). 
For 
a 
variable 
y 
= 
f(x), 
where 
f(x) 
is 
a 
monotonic 
function, 
show 
that 
the 
distribution 
of 
y 
is 


p(y)= 
p(x)df 
..1 
;x 
= 
f..1(y) 
(8.10.9)

dx

Exercise 
89 
(Normalisation 
of 
a 
Multivariate 
Gaussian). 
Consider 


Z8 


- 
1 
(x..)T

I 
= 
e 
2 
..1(x..)dx 
(8.10.10) 


..8 


By 
using 
the 
transformation 


z 
= 
- 
1
2 
(x 
- 
) 
(8.10.11) 


show 
that 


p

I 
=det 
(2) 
(8.10.12) 
Exercise 
90. 
Consider 
the 
partitioned 
matrix 




AB

M 
=(8.10.13)

CD

for 
which 
we 
wish 
to 
nd 
the 
inverse 
M..1 
. 
We 
assume 
that 
A 
is 
m 
× 
m 
and 
invertible, 
and 
D 
is 
n 
× 
n 
and 
invertible. 
By 
denition, 
the 
partitioned 
inverse 


M..1 
=
P 
R 
Q 
S
(8.10.14) 
must 
satisfy
A 
C 
B 
D
P 
R 
Q 
S
=
Im 
0 
0 
In
(8.10.15) 


where 
in 
the 
above 
Im 
is 
the 
m 
× 
m 
identity 
matrix 
of 
the 
same 
dimension 
as 
A, 
and 
0 
the 
zero 
matrix 
of 
the 
same 
dimension 
as 
D. 
Using 
the 
above, 
derive 
the 
results 


....1

P 
=A 
- 
BD..1C

....1Q 
= 
..A..1BD 
- 
CA..1B

....1

R 
= 
..D..1CA 
- 
BD..1C

....1

S 
=D 
- 
CA..1B

..



Exercise 
91. 
Show 
that 
for 
Gaussian 
distribution 
p(x)= 
Nx 


, 
2the 
skewness 
and 
kurtosis 
are 
both 
zero. 


Exercise 
92. 
Consider 
a 
small 
interval 
of 
time 
t 
and 
let 
the 
probability 
of 
an 
event 
occurring 
in 
this 
small 
interval 
be 
t. 
Derive 
a 
distribution 
that 
expresses 
the 
probability 
of 
at 
least 
one 
event 
in 
an 
interval 
from 
0 
to 
t. 


DRAFT 
March 
9, 
2010 



Exercises 


Exercise 
93. 
Consider 
a 
vector 
variable 
x 
=(x1;:::;xn) 
and 
set 
of 
functions 
dened 
on 
each 
component 
of 
x, 
i(xi). 
For 
example 
for 
x 
=(x1;x2) 
we 
might 
have 


2

1(x1)= 
..jx1j;2(x2)= 
..x 
(8.10.16)

2 


Consider 
the 
distribution 


1 


T(x)

p(xj)= 
e 
(8.10.17)

Z 
where 
(x) 
is 
a 
vector 
function 
with 
ith 
component 
i(xi), 
and 
. 
is 
a 
parameter 
vector. 
Each 
component 
is 
tractably 
integrable 
in 
the 
sense 
that

Z8 


e 
ii(xi)dxi 
(8.10.18) 


..8 


can 
be 
computed 
either 
analytically 
or 
to 
an 
acceptable 
numerical 
accuracy. 
Show 
that 


1. 
xi 
.
??xjj;. 
2. 
The 
normalisation 
constant 
Z 
can 
be 
tractably 
computed. 
3. 
Consider 
the 
transformation 
x 
= 
My 
(8.10.19) 


for 
an 
invertible 
matrix 
M. 
Show 
that 
the 
distribution 
p(yjM;) 
is 
tractable 
(its 
normalisation 
constant 
is 
known), 
and 
that, 
in 
general, 
yiT
>>yj 
j;. 
Explain 
the 
signicance 
of 
this 
is 
deriving 
tractable 
multivariate 
distributions. 


Exercise 
94. 
Show 
that 
we 
may 
reparameterise 
the 
Beta 
distribution, 
denition(72) 
by 
writing 
the 
par
ameters 
a 
and 
ß 
as 
functions 
of 
the 
mean 
m 
and 
variance 
s 
using 


a 
= 
, 
. 
= 
m=(1 
- 
m) 
(8.10.20) 


ß 
= 
1 
1 
+ 
. 
s 
(1 
+ 
)2 
- 
1
Exercise 
95. 
Consider 
the 
function 
(8.10.21) 
f(. 
+ 
, 
, 
) 
= 
+..1 
(1 
- 
)..1 
(8.10.22) 
show 
that 
lim 
!0 
. 
@. 
f(. 
+ 
, 
, 
) 
= 
..1 
(1 
- 
)..1 
log 
. 
(8.10.23) 
and 
hence 
that
Z..1 
(1 
- 
)..1 
log 
d. 
= 
lim 
!0 
. 
@Zf(. 
+ 
, 
, 
)d. 
= 
. 
@Zf(, 
, 
)d. 
(8.10.24) 


Using 
this 
result, 
show 
therefore 
that 


@

hlog 
iB(j;) 
= 
log 
B(, 
) 
(8.10.25)

@a 
where 
B(, 
) 
is 
the 
Beta 
function. 
Show 
additionally 
that 
@

hlog 
(1 
- 
)iB(j;) 
= 
log 
B(, 
) 
(8.10.26)

@ß 


Using 
the 
fact 
that 


..()..()

B(, 
) 
= 
(8.10.27)

..(a 
+ 
) 
where 
..(x) 
is 
the 
gamma 
function, 
relate 
the 
above 
averages 
to 
the 
digamma 
function, 
dened 
as 
d 


 (x) 
= 
log 
..(x) 
(8.10.28)

dx 


DRAFT 
March 
9, 
2010 



Exercises 


Exercise 
96. 
Using 
a 
similar 
`generating 
function’ 
approach 
as 
in 
exercise(95), 
explain 
how 
to 
compute 


hlog 
iiDirichlet(ju) 
(8.10.29) 


Exercise 
97. 
Consider 
the 
function 


ZY8 
n

XY

ui..1

f(x)= 
i 
- 
x 
d1 
. 
. 
. 
dn 
(8.10.30)

i 
i=1 
i 


R1

Show 
that 
the 
Laplace 
transform 
of 
f(x), 
f~(s) 
e..sxf(x)dx 
is

0 


nZY8 
Yn

YY

..si 
ui..1

f~(s)= 
e 
di 
= 
P
1 
..(ui) 
(8.10.31)

i 


i 
ui

s

0

i=1 
i=1 


By 
taking 
the 
inverse 
Laplace 
transform, 
show 
that 


Q

n 
P

..(ui)

i=1 
i 
ui..1

f(x)=Px(8.10.32)

..(i 
ui 
- 
1)

Hence 
show 
that 
the 
normalisation 
constant 
of 
a 
Dirichlet 
distribution 
with 
parameters 
u 
is 
given 
by

Q

n 


..(ui)

i=1

P(8.10.33)
..(i 
ui) 


Exercise 
98. 
By 
using 
the 
Laplace 
transform, 
as 
in 
exercise(97), 
show 
that 
the 
marginal 
of 
a 
Dirichlet 
distribution 
is 
a 
Dirichlet 
distribution. 


Exercise 
99. 
Derive 
the 
formula 
for 
the 
dierential 
entropy 
of 
a 
multi-variate 
Gaussian. 


Exercise 
100. 
Show 
that 
for 
a 
gamma 
distribution 
Gam 
(xj, 
) 
the 
mode 
is 
given 
by 


x 
* 
=(a 
- 
1) 
ß 
(8.10.34) 


provided 
that 
a 
= 
1. 


Exercise 
101. 
Consider 
a 
distribution 
p(xj) 
and 
a 
distribution 
with 
. 
changed 
by 
a 
small 
amount, 
. 
Take 
the 
Taylor 
expansion 
of 


KL(p(xj)jp(xj. 
+ 
)) 
(8.10.35) 


for 
small 
d 
and 
show 
that 
this 
is 
equal 
to 




2 
@2 
- 
log 
p(xj)(8.10.36)

2@2 


p() 


More 
generally 
for 
a 
distribution 
parameterised 
by 
a 
vector 
i 
+ 
i, 
show 
that 
a 
small 
change 
in 
the 
parameter 
results 
in

X

ij 


Fij 
(8.10.37)

2 


i;j 


where 
the 
Fisher 
Information 
matrix 
is 
dened 
as 




@2 
Fij 
= 
..log 
p(xj)(8.10.38)

@i@j 


p() 


Show 
that 
the 
Fisher 
information 
matrix 
is 
positive 
(semi) 
denite 
by 
expressing 
it 
equivalently 
as 




@. 


Fij 
=log 
p(xj) 
log 
p(xj)(8.10.39)

@i 
@j 


p() 


DRAFT 
March 
9, 
2010 



Exercises 


Exercise 
102. 
Consider 
the 
joint 
prior 
distribution 


..

X

p(, 
j0, 
, 
, 
)= 
Nµ 


0, 
..1Gam 
(j, 
) 
(8.10.40) 


Show 
that 
for 
0 
=0, 
. 
!1, 
ß 
!1, 
then 
the 
prior 
distribution 
becomes 
`at’ 
(independent 
of 
µ 
and 
) 
for 
a 
=1=2. 
Show 
that 
for 
these 
settings 
the 
mean 
and 
variance 
that 
jointly 
maximise 
the 
posterior 
equation 
(8.6.64) 
are 
given 
by 
the 
standard 
Maximum 
Likelihood 
settings 


XX

n 


* 
=
1 
x, 
2 
=
1 
(x 
n 
- 
)2 
(8.10.41)

* 


NN

nn 


Exercise 
103. 
Show 
that 
in 
the 
limit 
0 
=0;. 
!1;a 
=1;ß 
!1, 
the 
jointly 
optimal 
mean 
and 
variance 
obtained 
from 


argmax 
p(, 
jX 
, 
, 
, 
) 
(8.10.42) 


;. 


is 
given 
by 


XX

n 


* 
=
1 
x, 
2 
=
1 
(x 
n 
- 
)2 
(8.10.43)

* 


NN 
+1

nn 


where 
2 
=1=. 
Note 
that 
these 
correspond 
to 
the 
standard 
`unbiased’ 
estimators 
of 
the 
mean 
and 


* 


variance. 


Exercise 
104. 
For 
the 
Gauss-Gamma 
posterior 
p(, 
j0, 
, 
, 
X 
) 
given 
in 
equation 
(8.6.64) 
compute 
the 
marginal 
posterior 
p(j0, 
, 
, 
X 
). 
What 
is 
the 
mean 
of 
this 
distribution? 
Exercise 
105. 
Derive 
equation 
(8.6.30). 
Exercise 
106. 
Consider 
the 
multivariate 
Gaussian 
distribution 
p(x) 
N 
(x 


, 
) 
on 
the 
vector 
x 
with 
components 
x1;:::;xn: 


1- 
1 
(x..)T..1(x..)

p(x)= 
pe 
2 
(8.10.44)
det 
(2) 
Calculate 
p(xijx1;:::;xi..1;xi+1;:::;xn). 
Exercise 
107. 
Observations 
y0;:::;yn..1 
are 
noisy 
i.i.d. 
measurements 
of 
an 
underlying 
variable 
x 
with

..

X..

X

p(x) 
Nx 


0;2 
and 
p(yijx) 
Nyi 


x, 
2 
for 
i 
=0;:::;n..1. 
Show 
that 
p(xjy0;:::;yn..1) 
is 
Gaussian 


0 


with 
mean 


n02 


µ 
= 
y 
(8.10.45)

n02 
+ 
2 
where 
y 
=(y0 
+ 
y1 
+ 
::. 
+ 
yn..1)=n 
and 
variance 
2 
such 
that 


n 


1 
n 
1 


=+ 
. 
(8.10.46)
n 
2 
2 
02 


1N

Exercise 
108. 
Consider 
a 
set 
of 
data 
x;:::;xdrawn 
from 
a 
Gaussian 
with 
know 
mean 
µ 
and 
unknown 
variance 
2 
. 
Assume 
a 
gamma 
distribution 
prior 
on 
t 
=1=2 
, 


p(t 
)= 
Gamis 
(ja, 
b) 
(8.10.47) 


1. 
Show 
that 
the 
posterior 
distribution 
is 
X

N 
1 
N

p(jX 
)= 
Gamis 
ja 
+ 
;b 
+(x 
n 
- 
)2 
(8.10.48)

22 


n=1 


DRAFT 
March 
9, 
2010 



Exercises 


2. 
Show 
that 
the 
distribution 
for 
x 
is 
Z



a 


p(xjXh)=p(xj)p(t 
jXh)dt 
= 
Studentxj, 
. 
= 
;. 
=2a(8.10.49)

b 


Exercise 
109. 
The 
Poisson 
distribution 
is 
a 
discrete 
distribution 
on 
the 
non-negative 
integers, 
with 


..x

e

P 
(x)= 
x 
=0, 
1, 
2;::. 
(8.10.50) 


x! 


You 
are 
given 
a 
sample 
of 
n 
observations 
x1;:::;xn 
drawn 
from 
this 
distribution. 
Determine 
the 
maximum 
likelihood 
estimator 
of 
the 
Poisson 
parameter 
. 


Exercise 
110. 
For 
a 
Gaussian 
mixture 
model 


XX

p(x)=piNh(x 


i, 
i) 
;pi 
> 
0;pi 
= 
1 
(8.10.51) 
ii 


show 
that 
p(x) 
has 
mean

X

hxi=pii 
(8.10.52) 
i 


and 
covariance



XXX

TT 


pii 
+ 
ii..piipjj 
(8.10.53) 
i 
ij 


Exercise 
111. 
Show 
that 
for 
the 
whitened 
data 
matrix, 
given 
in 
equation 
(8.6.36), 
ZZT 
= 
NI. 


Exercise 
112. 
Consider 
a 
uniform 
distribution 
pi 
=1=N 
dened 
on 
states 
i 
=1;:::;N. 
Show 
that 
the 
entropy 
of 
this 
distribution 
is 


N

X

H 
= 
..hpi 
log 
pi 
= 
log 
N 
(8.10.54) 
i=1 


and 
that 
there 
for 
as 
the 
number 
of 
states 
N 
increases 
to 
innity, 
the 
entropy 
diverges 
to 
innity. 


Exercise 
113. 
Consider 
a 
continuous 
distribution 
p(x), 
x 
2h[0, 
1]. 
We 
can 
form 
a 
discrete 
approximation 
with 
probabilities 
pi 
to 
this 
continuous 
distribution 
by 
identifying 
a 
continuous 
value 
i=N 
for 
each 
state 
i 
=1;:::;N. 
With 
this 


p(i=N)

pi 
= 
P(8.10.55) 


i 
p(i=N) 


P

show 
that 
the 
entropy 
H 
= 
..i 
pi 
log 
pi 
is 
given 
by 


XX

1

H 
= 
..Pp(i=N) 
log 
p(i=N) 
+ 
logp(i=N) 
(8.10.56) 


i 
p(i=N)

ii 


Since 
for 
a 
continuous 
distribution

Z1 


p(x)dx 
= 
1 
(8.10.57) 


0 


a 
discrete 
approximation 
of 
this 
integral 
into 
bins 
of 
size 
1=N 
gives 


X

1 
N

p(i=N) 
= 
1 
(8.10.58)

N 


i=1 


DRAFT 
March 
9, 
2010 



Exercises 


Hence 
show 
that 
for 
large 
N, 


H 
h..Z1 
0 
p(x) 
log 
p(x)dx 
+ 
const. 
(8.10.59) 
where 
the 
constant 
tends 
to 
innity 
as 
N 
!h1. 
Note 
that 
this 
result 
says 
that 
as 
a 
continuous 
distribu


tion 
has 
essentially 
an 
innite 
number 
of 
states, 
the 
amount 
of 
uncertainty 
in 
the 
distribution 
is 
innite 
(alternatively, 
we 
would 
need 
an 
innite 
number 
of 
bits 
to 
specify 
a 
continuous 
value). 
This 
motivates 
the 
denition 
of 
the 
dierential 
entropy, 
which 
neglects 
the 
innite 
constant 
of 
the 
limiting 
case 
of 
the 
discrete 
entropy. 


Exercise 
114. 
Consider 
two 
multivariate 
Gaussians 
Nh(x 


1, 
1) 
and 
Nh(x 


2, 
2). 


1. 
Show 
that 
the 
log 
product 
of 
the 
two 
Gaussians 
is 
given 
by 


1 


1 


1


..1 
+ 
..1 


12 


..1 
1 
+ 
..1 


12 


..1 


1 


..1 


2 


T

T

T

T

log 
det 
(21) 
det 
(22)

x+x


1 
+ 


- 


2 


- 


2

-


x


µ


1 


2

2


2


2 


2. 
Dening 
A 
= 
..1 
+ 
..1 
and 
b 
= 
..1 
1 
+ 
..1 
2 
we 
can 
write 
the 
above 
as 
12 
12 




1 


11 


1


T 


..1 


1 


..1 


2

x 
..hA..1b 


x 
..hA..1b 


bTA..1b..

T 
1 


T

log 
det 
(21) 
det 
(22)

A 


+ 


1 
+ 


- 


2

-


µ 


2

2 


2


2


2 


Writing 
S 
= 
A..1 
and 
µ 
= 
A..1b 
show 
that 
the 
product 
of 
Gaussians 
is 
a 
Gaussian 
with 
covariance 


S 
= 
1 
(1 
+ 
2)..1 
2 
(8.10.60) 
mean 


µ 
= 
1 
(1 
+ 
2)..1 
2 
+ 
2 
(1 
+ 
2)..1 
1 
(8.10.61) 


and 
log 
prefactor 




11 


11


..1 


1 


1 
+ 
µ 


T 


2 


..1 


2 


2

-


bTA..1b 
..

T

log 
det 
(21) 
det 
(22)+ 


log 
det 
(2)


µ


1

2


2


2 


2 


3. 
Show 
that 
this 
can 
be 
written 
as 


exp..1 
(1 
..h2)T 
S..1 
(1 
..h2)

2 


Nh(x 


2, 
2)= 
Nh(x 


, 
) 
p(8.10.62)

1, 
1) 
Nh(x 


det 
(2S) 


Exercise 
115. 
Show 
that 


. 


hlog 
p(xj)ip(xj0) 
j=0 
= 
0 
(8.10.63)

@

DRAFT 
March 
9, 
2010 



CHAPTER 
9 


Learning 
as 
Inference 


9.1 
Learning 
as 
Inference 
In 
previous 
chapters 
we 
largely 
assumed 
that 
all 
distributions 
are 
fully 
specied 
for 
the 
inference 
tasks. 
In 
Machine 
Learning 
and 
related 
elds, 
however, 
the 
distributions 
need 
to 
be 
learned 
on 
the 
basis 
of 
data. 
Learning 
is 
then 
the 
problem 
of 
integrating 
data 
with 
domain 
knowledge 
of 
the 
model 
environment. 


Denition 
84 
(Priors 
and 
Posteriors). 
Priors 
and 
posteriors 
typically 
refer 
to 
the 
parameter 
distributions 
before 
(prior 
to) 
and 
after 
(posterior 
to) 
seeing 
the 
data. 
Formally, 
Bayes’ 
rule 
relates 
these 
via 


p(Vj)p() 


p(jV) 
= 
(9.1.1) 


p(V) 


where 
. 
is 
the 
parameter 
of 
interest 
and 
V 
represents 
the 
observed 
(visible) 
data. 


9.1.1 
Learning 
the 
bias 
of 
a 
coin 
n

Consider 
data 
expressing 
the 
results 
of 
tossing 
a 
coin. 
We 
write 
v= 
1 
if 
on 
toss 
n 
the 
coin 
comes 


n

up 
heads, 
and 
v= 
0 
if 
it 
is 
tails. 
Our 
aim 
is 
to 
estimate 
the 
probability 
. 
that 
the 
coin 
will 
be 
a 


n

head, 
p(v=1j)= 
. 
– 
called 
the 
`bias’ 
of 
the 
coin. 
For 
a 
fair 
coin, 
. 
=0:5. 
The 
variables 
in 
this 


1N

environment 
are 
v;:::;vand 
. 
and 
we 
require 
a 
model 
of 
the 
probabilistic 
interaction 
of 
the 
variables, 


1

p(v;:::;vN 
;). 
Assuming 
there 
is 
no 
dependence 
between 
the 
observed 
tosses, 
except 
through 
, 
we 
have 
the 
Belief 
Network 


N

N 


1 


p(v 
;:::;v 
N 
;)= 
p() 
p(v 
nj) 
(9.1.2) 
n=1 


which 
is 
depicted 
in 
g(9.1). 
The 
assumption 
that 
each 
observation 
is 
identically 
and 
independently 
dist
ributed 
is 
called 
the 
i.i.d. 
assumption. 


1N

Learning 
refers 
to 
using 
the 
observations 
v;:::;vto 
infer 
. 
In 
this 
context, 
our 
interest 
is 


11

p(v;:::;vN 
;) 
p(v;:::;vN 
j)p()

1 


p(jv 
;:::;v 
N 
) 
= 
= 
(9.1.3) 


p(v1;:::;vN 
) 
p(v1;:::;vN 
) 


We 
still 
need 
to 
fully 
specify 
the 
prior 
p(). 
To 
avoid 
complexities 
resulting 
from 
continuous 
variables, 
we'll 
consider 
a 
discrete 
. 
with 
only 
three 
possible 
states, 
. 
2f0:1, 
0:5, 
0:8g. 
Specically, 
we 
assume 


p(. 
=0:1) 
= 
0:15;p(. 
=0:5) 
= 
0:8;p(. 
=0:8) 
= 
0:05 
(9.1.4) 


165 



Learning 
as 
Inference 



v1v2v3vN

vn
N
Figure 
9.1: 
(a): 
Belief 
Network 
for 
coin 
tossing 
model. 
(b): 
Plate 
notation 
equivalent 
of 
(a). 
A 
plate 
replicates 
the 
quantities 
inside 
the 
plate 
a 
number 
of 
times 
as 
specied 
in 
the 
plate. 


(a) 
(b) 
as 
shown 
in 
g(9.2a). 
This 
prior 
expresses 
that 
we 
have 
80% 
belief 
that 
the 
coin 
is 
`fair', 
5% 
belief 
the 
coin 
is 
biased 
to 
land 
heads 
(with 
. 
=0:8), 
and 
15% 
belief 
the 
coin 
is 
biased 
to 
land 
tails 
(with 
. 
=0:1). 
The 
distribution 
of 
. 
given 
the 
data 
and 
our 
beliefs 
is 


NN

YYP

1 
n=1] 
(1 
- 
)I[vn=0]

p(jv 
;:::;v 
N 
) 
. 
p() 
p(v 
nj)= 
p() 
I[v(9.1.5) 
n=1 
n=1 
PN 
I[vn=1] 
(1 
- 
)PN 
I[vn=0]

n=1 
n=1

. 
p()(9.1.6) 


PN

In 
the 
aboveI[vn 
= 
1] 
is 
the 
number 
of 
occurrences 
of 
heads, 
which 
we 
more 
conveniently 
denote 


n=1

PN

as 
NH 
. 
Likewise,I[vn 
= 
0] 
is 
the 
number 
of 
tails, 
NT 
. 
Hence 


n=1 


1 


p(jv 
;:::;v 
N 
) 
. 
p()NH 
(1 
- 
)NT 
(9.1.7) 


For 
an 
experiment 
with 
NH 
= 
2, 
NT 
= 
8, 
the 
posterior 
distribution 
is 


p(. 
=0:1jV)= 
k 
× 
0:15 
× 
0:12 
× 
0:98 
= 
k 
× 
6:46 
× 
10..4 
(9.1.8) 


p(. 
=0:5jV)= 
k 
× 
0:8 
× 
0:52 
× 
0:58 
= 
k 
× 
7:81 
× 
10..4 
(9.1.9) 


p(. 
=0:8jV)= 
k 
× 
0:05 
× 
0:82 
× 
0:28 
= 
k 
× 
8:19 
× 
10..8 
(9.1.10) 


1N

where 
V 
is 
shorthand 
for 
v;:::;v. 
From 
the 
normalisation 
requirement 
we 
have 
1=k 
=6:46 
× 
10..4 
+ 


7:81 
× 
10..4 
+8:19 
× 
10..8 
=0:0014, 
so 
that 
p(. 
=0:1jV)=0:4525;p(. 
=0:5jV)=0:5475;p(. 
=0:8jV)=0:0001 
(9.1.11) 


as 
shown 
in 
g(9.2b). 
These 
are 
the 
`posterior’ 
parameter 
beliefs. 
In 
this 
case, 
if 
we 
were 
asked 
to 
choose 
a 
single 
a 
posteriori 
most 
likely 
value 
for 
, 
it 
would 
be 
. 
=0:5, 
although 
our 
condence 
in 
this 
is 
low 
since 
the 
posterior 
belief 
that 
. 
=0:1 
is 
also 
appreciable. 
This 
result 
is 
intuitive 
since, 
even 
though 
we 
observed 
more 
Tails 
than 
Heads, 
our 
prior 
belief 
was 
that 
it 
was 
more 
likely 
the 
coin 
is 
fair. 


Repeating 
the 
above 
with 
NH 
= 
20, 
NT 
= 
80, 
the 
posterior 
changes 
to 


p(. 
=0:1jV)=1..1:9310..6 
;p(. 
=0:5jV)=1:9310..6 
;p(. 
=0:8jV)=2:1310..35 
(9.1.12) 


g(9.1c), 
so 
that 
the 
posterior 
belief 
in 
. 
=0:1 
dominates. 
This 
is 
reasonable 
since 
in 
this 
situation, 
there 
are 
so 
many 
more 
tails 
than 
heads 
that 
this 
is 
unlikely 
to 
occur 
from 
a 
fair 
coin. 
Even 
though 
we 
a 
priori 
thought 
that 
the 
coin 
was 
fair, 
a 
posteriori 
we 
have 
enough 
evidence 
to 
change 
our 
minds. 


0:10:50:8
0:10:50:8
0:10:50:8
Figure 
9.2: 
(a): 
Prior 
encoding 
our 
beliefs 
about 
the 
amount 
the 
coin 
is 
biased 
to 
heads. 
(b): 
Posterior 
having 
seen 
2 
heads 
and 
8 
tails. 
(c): 
Posterior 
having 
seen 
20 
heads 
and 
80 
tails. 


. 


(a) 
(b) 
(c) 
DRAFT 
March 
9, 
2010 



Learning 
as 
Inference 


9.1.2 
Making 
decisions 
In 
itself, 
the 
Bayesian 
posterior 
merely 
represents 
our 
beliefs 
and 
says 
nothing 
about 
how 
best 
to 
summarise 
these 
beliefs. 
In 
situations 
in 
which 
decisions 
need 
to 
be 
taken 
under 
uncertainty 
we 
need 
to 
additionally 
specify 
what 
the 
utility 
of 
any 
decision 
is, 
as 
in 
chapter(7). 


In 
the 
coin 
tossing 
scenario 
where 
. 
is 
assumed 
to 
be 
either 
0:1, 
0:5 
or 
0:8, 
we 
setup 
a 
decision 
problem 
as 
follows: 
If 
we 
correctly 
state 
the 
bias 
of 
the 
coin 
we 
gain 
10 
points; 
being 
incorrect, 
however, 
loses 
20 
points. 
We 
can 
write 
this 
using 




U(, 
0) 
= 
10I. 
= 
0..620I6
= 
0(9.1.13) 


where 
0 
is 
the 
true 
value 
for 
the 
bias. 
The 
expected 
utility 
of 
the 
decision 
that 
the 
coin 
is 
. 
=0:1 
is 


U(. 
=0:1) 
= 
U(. 
=0:1;0 
=0:1)p(0 
=0:1jV) 


+ 
U(. 
=0:1;0 
=0:5)p(0 
=0:5jV)+ 
U(. 
=0:1;0 
=0:8)p(0 
=0:8jV) 
(9.1.14) 
Plugging 
in 
the 
numbers 
from 
equation 
(9.1.11), 
we 
obtain 


U(. 
=0:1) 
= 
10 
60:4525 
..620 
60:5475 
..620 
60:0001 
= 
..6:4270 
(9.1.15) 


Similarly 


U(. 
=0:5) 
= 
10 
60:5475 
..620 
60:4525 
..620 
60:0001 
= 
..3:5770 
(9.1.16) 


and 


U(. 
=0:8) 
= 
10 
60:0001 
..620 
60:4525 
..620 
60:5475 
= 
..19:999 
(9.1.17) 


So 
that 
the 
best 
decision 
is 
to 
say 
that 
the 
coin 
is 
unbiased, 
. 
=0:5. 


Repeating 
the 
above 
calculations 
for 
NH 
= 
20;NT 
= 
80, 
we 
arrive 
at 


..

U(. 
=0:1) 
= 
10 
6(1 
..61:93 
610..6) 
..6201:93 
610..6 
+2:13 
610..35=9:9999 
(9.1.18) 


..

U(. 
=0:5) 
= 
10 
61:93 
610..6 
..6201 
..61:93 
610..6 
+2:13 
610..35..20:0 
(9.1.19) 


..

U(. 
=0:8) 
= 
10 
62:13 
610..35 
..6201 
..61:93 
610..6 
+1:93 
610..6..20:0 
(9.1.20) 


so 
that 
the 
best 
decision 
in 
this 
case 
is 
to 
choose 
. 
=0:1. 


As 
more 
information 
about 
the 
distribution 
p(v, 
) 
becomes 
available 
the 
posterior 
p(jV) 
becomes 
increasingly 
peaked, 
aiding 
our 
decision 
making 
process. 


9.1.3 
A 
continuum 
of 
parameters 
In 
section(9.1.1) 
we 
considered 
only 
three 
possible 
values 
for 
. 
Here 
we 
discuss 
a 
continuum 
of 
parameters. 


Using 
a 
at 
prior 


We 
rst 
examine 
the 
case 
of 
a 
`at’ 
or 
uniform 
prior 
p()= 
k 
for 
some 
constant 
k. 
For 
continuous 
variables, 
normalisation 
requires

Z

p()d. 
= 
1 
(9.1.21) 


Since 
0 
6. 
61,

Z1 


p()d. 
= 
k 
= 
1 
(9.1.22) 


0 


DRAFT 
March 
9, 
2010 



Learning 
as 
Inference 


00.20.40.60.81
0
5
10
q
Figure 
9.3: 
Posterior 
p(jV) 
assuming 
a 
at 
prior 
on 
. 
(red) 
NH 
= 
2, 
NT 
= 
8 
and 
(blue) 
NH 
= 
20, 
NT 
= 
80. 
In 
both 
cases, 
the 
most 
probable 
state 
of 
the 
posterior 
is 
0.2, 
which 
makes 
intuitive 
sense, 
since 
the 
fraction 
of 
Heads 
to 
Tails 
in 
both 
cases 
is 
0.2. 
Where 
there 
is 
more 
data, 
the 
posterior 
is 
more 
certain 
and 
sharpens 
around 
the 
most 
probable 
value. 
The 
Maximum 
A 
Posteriori 
setting 
is 
. 
=0:2 
in 
both 
cases, 
this 
being 
the 
value 


of 
. 
for 
which 
the 
posterior 
attains 
its 
highest 
value. 
Repeating 
the 
previous 
calculations 
with 
this 
at 
continuous 
prior, 
we 
have 
p(jV) 
= 
1 
c 
NH 
(1 
- 
)NT 
(9.1.23) 
where 
c 
is 
a 
constant 
to 
be 
determined 
by 
normalisation, 
Z1 
c 
=NH 
(1 
- 
)NT 
d. 
= 
B(NH 
+ 
1, 
NT 
+ 
1) 
(9.1.24) 
0 
where 
B(, 
) 
is 
the 
Beta 
function. 


Denition 
85 
(conjugacy). 
If 
the 
posterior 
is 
of 
the 
same 
parametric 
form 
as 
the 
prior, 
then 
we 
call 
the 
prior 
the 
conjugate 
distribution 
for 
the 
likelihood 
distribution. 


Using 
a 
conjugate 
prior 


Determining 
the 
normalisation 
constant 
of 
a 
continuous 
distribution 
requires 
that 
the 
integral 
of 
the 
unnormalised 
posterior 
can 
be 
carried 
out. 
For 
the 
coin 
tossing 
case, 
it 
is 
clear 
that 
if 
the 
prior 
is 
of 
the 
form 
of 
a 
Beta 
distribution, 
then 
the 
posterior 
will 
be 
of 
the 
same 
parametric 
form: 


p()= 
1 
..1 
(1 
- 
)..1 
(9.1.25)

B(, 
)

the 
posterior 
is 


p(jV) 
. 
..1 
(1 
- 
)..1 
NH 
(1 
- 
)NT 
(9.1.26) 


so 
that 
1 


p(jV)= 
+NH 
..1 
(1 
- 
)+NT 
..1 
= 
B(ja 
+ 
NH 
;ß 
+ 
NT 
) 
(9.1.27)

B(a 
+ 
NH 
;ß 
+ 
NT 
)

The 
prior 
and 
posterior 
are 
of 
the 
same 
form 
(both 
Beta 
distributions) 
but 
simply 
with 
dierent 
parameters. 
Hence 
the 
Beta 
distribution 
is 
`conjugate’ 
to 
the 
Binomial 
distribution. 


9.1.4 
Decisions 
based 
on 
continuous 
intervals 
The 
result 
of 
a 
coin 
tossing 
experiment 
is 
NH 
= 
2 
heads 
and 
NT 
= 
8 
tails. 
You 
now 
need 
to 
make 
a 
decision 
: 
you 
win 
10 
dollars 
if 
your 
guess 
that 
the 
coin 
is 
more 
likely 
to 
come 
up 
heads 
than 
tails 
is 
correct. 
If 
your 
guess 
is 
incorrect, 
you 
lose 
a 
million 
dollars. 
What 
is 
your 
decision? 
(Assume 
an 
uninformative 
prior). 


We 
need 
two 
quantities, 
. 
for 
our 
guess 
and 
0 
for 
the 
truth. 
Then 
the 
utility 
of 
saying 
Heads 
is 


U(> 
0:5;0 
> 
0:5)p(0 
> 
0:5jV)+ 
U(> 
0:5;0 
< 
0:5)p(0 
< 
0:5jV) 
(9.1.28) 


168 
DRAFT 
March 
9, 
2010 



Maximum 
A 
Posteriori 
and 
Maximum 
Likelihood 


In 
the 
above, 


Z0:5 
p(0 
< 
0:5jV)=p(0jV)d0 
(9.1.29) 


0 


Z0:5 
=
1 
+NH 
..1 
(1 
- 
)+NT 
..1 
d. 
(9.1.30)
B(a 
+ 
NH 
;ß 
+ 
NT 
)

0 


= 
I0:5(a 
+ 
NH 
;ß 
+ 
NT 
) 
(9.1.31) 
where 
Ix(a, 
b) 
is 
the 
regularised 
incomplete 
Beta 
function. 
For 
the 
former 
case 
of 
NH 
=2;NT 
= 
8, 
under 
a 
at 
prior, 
p(0 
< 
0:5jV)= 
I0:5(NH 
+1;NT 
+1) 
= 
0:9673 
(9.1.32) 
Since 
the 
events 
are 
exclusive, 
p(0 
= 
0:5jV)=1 
- 
0:9673 
= 
0:0327. 
Hence 
the 
expected 
utility 
of 
saying 
heads 
is 
more 
likely 
is 
10 
× 
0:0327 
- 
1000000 
× 
0:9673 
= 
..9:673 
× 
105 
. 
(9.1.33) 
Similarly, 
the 
utility 
of 
saying 
tails 
is 
more 
likely 
is 
10 
× 
0:9673 
- 
1000000 
× 
0:0327 
= 
..3:269 
× 
104 
. 
(9.1.34) 
So 
we 
are 
better 
off 
taking 
the 
decision 
that 
the 
coin 
is 
more 
likely 
to 
come 
up 
tails. 
If 
we 
modify 
the 
above 
so 
that 
we 
lose 
100 
million 
dollars 
if 
we 
guess 
tails 
when 
in 
fact 
it 
as 
heads, 
the 
expected 
utility 
of 
saying 
tails 
would 
be 
..3:27 
× 
106 
in 
which 
case 
we 
would 
be 
better 
of 
saying 
heads. 
In 
this 
case, 
even 
though 
we 
are 
more 
condent 
that 
the 
coin 
is 
likely 
to 
come 
up 
tails, 
we 
would 
pay 
such 
a 
penalty 
of 
making 
a 
mistake 
in 
saying 
tails, 
that 
it 
is 
fact 
better 
to 
say 
heads. 


9.2 
Maximum 
A 
Posteriori 
and 
Maximum 
Likelihood 
9.2.1 
Summarising 
the 
posterior 
Denition 
86 
(Maximum 
Likelihood 
and 
Maximum 
a 
Posteriori). 
Maximum 
Likelihood 
sets 
parameter 
, 
given 
data 
V, 
using 


ML 
= 
argmax 
p(Vj) 
(9.2.1) 


. 


Maximum 
A 
Posteriori 
uses 
that 
setting 
. 
that 
maximises 
the 
posterior 
distribution 
of 
the 
parameter, 
MAP 
= 
argmax 
p(Vj)p() 
(9.2.2) 


. 


where 
p() 
is 
the 
prior 
distribution. 


A 
crude 
summary 
of 
the 
posterior 
is 
given 
by 
a 
distribution 
with 
all 
its 
mass 
in 
a 
single 
most 
likely 
state, 


..

;MAP. 
In 
making 
such 
an 
approximation, 
potentially 
useful 
information 
concerning 
the 
reliability 
of 
the 
parameter 
estimate 
is 
lost. 
In 
contrast 
the 
full 
posterior 
reects 
our 
beliefs 
about 
the 
range 
of 
possibilities 
and 
their 
associated 
credibilities. 


One 
can 
motivate 
MAP 
from 
a 
decision 
theoretic 
perspective. 
If 
we 
assume 
a 
utility 
that 
is 
zero 
for 
all 
but 
the 
correct 
, 


U(true;)= 
I[true 
= 
] 
(9.2.3) 


DRAFT 
March 
9, 
2010 
169 



Maximum 
A 
Posteriori 
and 
Maximum 
Likelihood 


a 
s 


c
as
cn
ansn
c
n=1:N
Figure 
9.4: 
(a): 
A 
model 
for 
the 
relationship 
between 
lung 
Cancer, 
Asbestos 
exposure 
and 
Smoking. 
(b): 
Plate 
notation 
replicating 
the 
observed 
n 
datapoints 
and 
placing 
priors 
over 
the 
CPTs, 
tied 
across 
all 
datapoints. 


(a) 
(b) 
then 
the 
expected 
utility 
of 
. 
is 


X

U()= 
I[true 
= 
] 
p(truejV)= 
p(jV) 
(9.2.4) 


true 


This 
means 
that 
the 
maximum 
utility 
decision 
is 
to 
return 
that 
. 
with 
the 
highest 
posterior 
value. 


When 
a 
`at’ 
prior 
p()= 
const. 
is 
used 
the 
MAP 
parameter 
assignment 
is 
equivalent 
to 
the 
Maximum 
Likelihood 
setting 


ML 
= 
argmax 
p(Vj) 
(9.2.5) 


. 


The 
term 
Maximum 
Likelihood 
refers 
to 
the 
parameter 
. 
for 
which 
the 
observed 
data 
is 
most 
likely 
to 
be 
generated 
by 
the 
model. 


Since 
the 
logarithm 
is 
a 
strictly 
increasing 
function, 
then 
for 
a 
positive 
function 
f() 


opt 
= 
argmax 
f() 
. 
opt 
= 
argmax 
log 
f() 
(9.2.6) 
. 


so 
that 
the 
MAP 
parameters 
can 
be 
found 
either 
by 
optimising 
the 
MAP 
objective 
or, 
equivalently, 
its 
logarithm, 


log 
p(jV) 
= 
log 
p(Vj) 
+ 
log 
p() 
- 
log 
p(V) 
(9.2.7) 


where 
the 
normalisation 
constant, 
p(V), 
is 
not 
a 
function 
of 
. 


The 
log 
likelihood 
is 
convenient 
since 
under 
the 
i.i.d. 
assumption 
it 
is 
a 
summation 
of 
data 
terms, 


X

log 
p(jV) 
= 
log 
p(v 
nj) 
+ 
log 
p() 
- 
log 
p(V) 
(9.2.8) 


n 


so 
that 
quantities 
such 
as 
derivatives 
of 
the 
log-likelihood 
w.r.t. 
. 
are 
straightforward 
to 
compute. 


Example 
36. 
In 
the 
coin-tossing 
experiment 
of 
section(9.1.1) 
the 
ML 
setting 
is 
. 
=0:2 
in 
both 
NH 
= 
2;NT 
= 
8 
and 
NH 
= 
20;NT 
= 
80. 


9.2.2 
Maximum 
likelihood 
and 
the 
empirical 
distribution 
	

1N

Given 
a 
dataset 
of 
discrete 
variables 
X 
=x;:::;xwe 
dene 
the 
empirical 
distribution 
as 


X

1 
N

q(x)= 
I[x 
= 
x 
n] 
(9.2.9)

N 


n=1 


DRAFT 
March 
9, 
2010 



Maximum 
A 
Posteriori 
and 
Maximum 
Likelihood 


a 
s 
c 
1 
1 
1 
1 
0 
0 
0 
1 
1 
0 
1 
0 
1 
1 
1 
0 
0 
0 
1 
0 
1 


Figure 
9.5: 
A 
database 
containing 
information 
about 
the 
Asbestos 
exposure 
(1 
signies 
exposure), 
being 
a 
Smoker 
(1 
signies 
the 
individual 
is 
a 
smoker), 
and 
lung 
Cancer 
(1 
signies 
the 
individual 
has 
lung 
Cancer). 
Each 
row 
contains 
the 
information 
for 
an 
individual, 
so 
that 
there 
are 
7 
individuals 
in 
the 
database. 


in 
the 
case 
that 
x 
is 
a 
vector 
of 
variables, 


YP

n

I[x 
= 
x 
n]= 
I[xi 
= 
x 
] 
(9.2.10)

i 
i 


The 
Kullback-Leibler 
divergence 
between 
the 
empirical 
distribution 
q(x) 
and 
a 
distribution 
p(x) 
is 


KL(qjp)= 
hlog 
q(x)i..hlog 
p(x)i(9.2.11)

q(x) 
q(x) 


Our 
interest 
is 
the 
functional 
dependence 
of 
KL(qjp) 
on 
p. 
Since 
the 
entropic 
term 
hlog 
q(x)iis 
inde


q(x) 


pendent 
of 
p(x) 
we 
may 
consider 
this 
constant 
and 
focus 
on 
the 
second 
term 
alone. 
Hence 


X

1 
N

KL(qjp)= 
..hlog 
p(x)i+ 
const. 
= 
- 
log 
p(x 
n)+ 
const. 
(9.2.12)

q(x) 


N 


n=1 


PN

We 
recogniselog 
p(xn) 
as 
the 
log 
likelihood 
under 
the 
model 
p(x), 
assuming 
that 
the 
data 
is 
i.i.d. 


n=1 


This 
means 
that 
setting 
parameters 
by 
maximum 
likelihood 
is 
equivalent 
to 
setting 
parameters 
by 
minimising 
the 
Kullback-Leibler 
divergence 
between 
the 
empirical 
distribution 
and 
the 
parameterised 
distribution. 
In 
the 
case 
that 
p(x) 
is 
unconstrained, 
the 
optimal 
choice 
is 
to 
set 
p(x)= 
q(x), 
namely 
the 
maximum 
likelihood 
optimal 
distribution 
corresponds 
to 
the 
empirical 
distribution. 


9.2.3 
Maximum 
likelihood 
training 
of 
belief 
networks 
Consider 
the 
following 
model 
of 
the 
relationship 
between 
exposure 
to 
asbestos 
(a), 
being 
a 
smoker 
(s) 
and 
the 
incidence 
of 
lung 
cancer 
(c) 


p(a, 
s, 
c)= 
p(cja, 
s)p(a)p(s) 
(9.2.13) 


which 
is 
depicted 
in 
g(9.4a). 
Each 
variable 
is 
binary, 
dom(a)= 
f0, 
1g, 
dom(s)= 
f0, 
1g, 
dom(c)= 
f0, 
1g. 
We 
assume 
that 
there 
is 
no 
direct 
relationship 
between 
Smoking 
and 
exposure 
to 
Asbestos. 
This 
is 
the 
kind 
of 
assumption 
that 
we 
may 
be 
able 
to 
elicit 
from 
medical 
experts. 
Furthermore, 
we 
assume 
that 
we 
have 
a 
list 
of 
patient 
records, 
g(9.5), 
where 
each 
row 
represents 
a 
patient's 
data. 
To 
learn 
the 
table 
entries 
p(cja, 
s) 
we 
can 
do 
so 
by 
counting 
the 
number 
of 
c 
is 
in 
state 
1 
for 
each 
of 
the 
4 
parental 
states 
of 
a 
and 
s: 


p(c 
=1ja 
=0;s 
= 
0) 
= 
0;p(c 
=1ja 
=0;s 
= 
1) 
= 
0:5 


(9.2.14)
p(c 
=1ja 
=1;s 
= 
0) 
= 
0:5 
p(c 
=1ja 
=1;s 
= 
1) 
= 
1 


Similarly, 
based 
on 
counting, 
p(a 
= 
1) 
= 
4=7, 
and 
p(s 
=1) 
=4=7. 
These 
three 
CPTs 
then 
complete 
the 
full 
distribution 
specication. 


Setting 
the 
CPT 
entries 
in 
this 
way 
by 
counting 
the 
relative 
number 
of 
occurrences 
corresponds 
mathematically 
to 
maximum 
likelihood 
learning 
under 
the 
i.i.d. 
assumption, 
as 
we 
show 
below. 


Maximum 
likelihood 
corresponds 
to 
counting 


For 
a 
BN 
there 
is 
a 
constraint 
on 
the 
form 
of 
p(x), 
namely 


KYPp(x) 
= 
i=1 
p(xijpa 
(xi)) 
(9.2.15) 
DRAFT 
March 
9, 
2010 
171 



Maximum 
A 
Posteriori 
and 
Maximum 
Likelihood 


To 
compute 
the 
Maximum 
Likelihood 
setting 
of 
each 
term 
p(xijpa 
(xi)), 
as 
shown 
in 
section(9.2.2), 
we 
can 
equivalently 
minimise 
the 
Kullback-Leibler 
divergence 
between 
the 
empirical 
distribution 
q(x) 
and 
p(x). 
For 
the 
BN 
p(x), 
and 
empirical 
distribution 
q(x) 
we 
have 


*+

KK

XX

KL(qjp)= 
..log 
p](xijpa 
(xi))+ 
const. 
= 
..hlog 
p](xijpa 
(xi))i+ 
const. 


q(xi;pa(xi)) 
i=1 
q(x) 
i=1 


(9.2.16) 
This 
follows 
using 
the 
general 
result 


hf(Xi)) 
= 
hf(Xi)) 
(9.2.17)

q(X 
) 
q(Xi) 


which 
says 
that 
if 
the 
function 
f]only 
depends 
on 
a 
subset 
of 
the 
variables, 
we 
only 
need 
to 
know 
the 
marginal 
distribution 
of 
this 
subset 
of 
variables 
in 
order 
to 
carry 
out 
the 
average. 


Since 
q(x) 
is 
xed, 
we 
can 
add 
on 
entropic 
terms 
in 
q]and 
equivalently 
mimimize 


Khi

XY

KL(qjp)= 
hlog 
q(xijpa 
(xi))i..hlog 
p](xijpa 
(xi))i(9.2.18)

q(xi;pa(xi)) 
q(xi;pa(xi))
i=1
K

XY

= 
hKL(q(xijpa 
(xi))jp(xijpa 
(xi)))i(9.2.19)

q(pa(xi)) 
i=1 


The 
nal 
line 
is 
a 
positive 
weighted 
sum 
of 
individual 
Kullback-Leibler 
divergences. 
The 
minimal 
Kullback-
Leibler 
setting, 
and 
that 
which 
corresponds 
to 
Maximum 
Likelihood, 
is 
therefore 


p(xijpa 
(xi)) 
= 
q(xijpa 
(xi)) 
(9.2.20) 


In 
terms 
of 
the 
original 
data, 
this 
is 


N

XYY

n 
nj

p(xi 
= 
sjpa 
(xi)= 
t) 
. 
I[x]= 
s]Ix]= 
t(9.2.21)

ij 
n=1 
xj 
2pa(xi) 


This 
expression 
corresponds 
to 
the 
intuition 
that 
the 
table 
entry 
p(xijpa 
(xi)) 
can 
be 
set 
by 
counting 
the 
number 
of 
times 
the 
state 
fxi 
= 
s;]pa 
(xi)= 
t} 
occurs 
in 
the 
dataset 
(where 
t 
is 
a 
vector 
of 
parental 
states). 
The 
table 
is 
then 
given 
by 
the 
relative 
number 
of 
counts 
of 
being 
in 
state 
s 
compared 
to 
the 
other 
states 
s0, 
for 
xed 
joint 
parental 
state 
t. 


An 
alternative 
method 
to 
derive 
this 
intuitive 
result 
is 
to 
use 
Lagrange 
multipliers, 
see 
exercise(120). 
For 
reader 
less 
comfortable 
with 
the 
above 
Kullback-Leibler 
derivation, 
a 
more 
direct 
example 
is 
given 
below 
which 
makes 
use 
of 
the 
notation

](x1 
= 
s1;x2 
= 
s2;x3 
= 
s3;:::) 
(9.2.22) 


to 
denote 
the 
number 
of 
times 
that 
states 
x1 
= 
s1;x2 
= 
s2;x3 
= 
s3;:::]occur 
together 
in 
the 
training 
data. 


Example 
37. 
We 
wish 
to 
learn 
the 
table 
entries 
of 
the 
distribution 
p(x1;x2;x3)= 
p(x1jx2;x3)p(x2)p(x3). 
We 
address 
here 
how 
to 
nd 
the 
CPT 
entry 
p(x1 
=1jx2 
=1;x3 
= 
0) 
using 
Maximum 
Likelihood. 
For 


i.i.d. 
data, 
the 
contribution 
from 
p(x1jx2;x3) 
to 
the 
log 
likelihood 
is 
XY

nn 
n

log 
p(x1 
jx2 
;x])

3 


n 


The 
number 
of 
times 
p(x1 
=1jx2 
=1;x3 
= 
0) 
occurs 
in 
the 
log 
likelihood 
is](x1 
=1;x2 
=1;x3 
= 
0), 
the 
number 
of 
such 
occurrences 
in 
the 
training 
set. 
Since 
(by 
the 
normalisation 
constraint) 
p(x1 
=0jx2 
= 


DRAFT 
March 
9, 
2010 



Maximum 
A 
Posteriori 
and 
Maximum 
Likelihood 


x1x2xn..1xn
y]
Figure 
9.6: 
A 
variable 
y]with 
a 
large 
number 
of 
parents 
x1;:::;xn 
requires 
the 
specication 
of 
an 
exponentially 
large 
number 
of 
entries 
in 
the 
conditional 
probability 
p(yjx1;:::;xn). 
One 
solution 
to 
this 
diculty 
is 
to 
parameterise 
the 
conditional, 
p(yjx1;:::;xn;). 


1;x3 
= 
0) 
= 
1 
- 
p(x1 
=1jx2 
=1;x3 
= 
0), 
the 
total 
contribution 
of 
p(x1 
=1jx2 
=1;x3 
= 
0) 
to 
the 
log 
likelihood 
is

](x1 
=1;x2 
=1;x3 
= 
0) 
log 
p(x1 
=1jx2 
=1;x3 
= 
0) 


+](x1 
=0;x2 
=1;x3 
= 
0) 
log 
(1 
- 
p(x1 
=1jx2 
=1;x3 
= 
0)) 
(9.2.23) 
Using 
]= 
p(x1 
=1jx2 
=1;x3 
= 
0) 
we 
have

](x1 
=1;x2 
=1;x3 
= 
0) 
log 
]+](x1 
=0;x2 
=1;x3 
= 
0) 
log 
(1 
- 
) 
(9.2.24) 


Dierentiating 
the 
above 
expression 
w.r.t. 
]and 
equating 
to 
zero 
gives

](x1 
=1;x2 
=1;x3 
= 
0) 
](x1 
=0;x2 
=1;x3 
= 
0) 


..= 
0 
(9.2.25)

]1 
- 
]

The 
solution 
for 
optimal 
]is 
then 


](x1 
=1;x2 
=1;x3 
= 
0)

p(x1 
=1jx2 
=1;x3 
= 
0) 
=;](9.2.26)

](x1 
=1;x2 
=1;x3 
= 
0)+](x1 
=0;x2 
=1;x3 
= 
0)

corresponding 
to 
the 
intuitive 
counting 
procedure. 


Conditional 
probability 
functions 


Consider 
a 
binary 
variable 
y]with 
n]binary 
parental 
variables, 
x 
=(x1;:::;xn). 
There 
are 
2n 
entries 
in 
the 
CPT 
of 
p(yjx) 
so 
that 
it 
is 
infeasible 
to 
explicitly 
store 
these 
entries 
for 
even 
moderate 
values 
of 
n. 
To 
reduce 
the 
complexity 
of 
this 
CPT 
we 
may 
constrain 
the 
form 
of 
the 
table. 
For 
example, 
one 
could 
use 
a 
function 


1 


p(y]=1jx;]w) 
= 
(9.2.27)

x

1+ 
e..wT
where 
we 
only 
need 
to 
specify 
the 
n-dimensional 
parameter 
vector 
w. 


In 
this 
case, 
rather 
than 
using 
Maximum 
Likelihood 
to 
learn 
the 
entries 
of 
the 
CPTs 
directly, 
we 
instead 
learn 
the 
value 
of 
the 
parameter 
w. 
Since 
the 
number 
of 
parameters 
in 
w 
is 
small 
(n, 
compared 
with 
2n 
in 
the 
unconstrained 
case), 
we 
also 
have 
some 
hope 
that 
with 
a 
small 
number 
of 
training 
examples 
we 
can 
learn 
a 
reliable 
value 
for 
w. 


Example 
38. 
Consider 
the 
following 
3 
variable 
model 
p(x1;x2;x3)= 
p(x1jx2;x3)p(x2)p(x3), 
where 
xi 
. 
f0;]1} 
;i]=1;]2;]3. 
We 
assume 
that 
the 
CPT 
is 
parameterised 
using 


..2..2(x2..x3)2 


12

p(x1 
=1jx2;x3;) 
= 
e](9.2.28) 


One 
may 
verify 
that 
the 
above 
probability 
is 
always 
positive 
and 
lies 
between 
0 
and 
1. 
Due 
to 
normalisation, 
we 
must 
have 


p(x1 
=0jx2;x3)=1 
- 
p(x1 
=1jx2;x3) 
(9.2.29) 


DRAFT 
March 
9, 
2010 



Bayesian 
Belief 
Network 
Training 


For 
unrestricted 
p(x2) 
and 
p(x3), 
the 
Maximum 
Likelihood 
setting 
is 
p(x2 
= 
1) 
. 
. 
(x2 
= 
1), 
and 
p(x3 
= 
1) 
. 
. 
(x3 
= 
1). 
The 
contribution 
to 
the 
log 
likelihood 
from 
the 
term 
p(x1jx2;x3;), 
assuming 
i.i.d. 
data, 
is 


N

X..

 


..

..

(x

..x

)2

n

L(1;2)= 
I[x 
= 
1]

1 


nn

..12 
- 
2(x2 
- 
x 
)2

23 


n

+ 
I[x 
= 
0] 
log1 
- 
e
1 


2
1

2
2

n 
2

n 
3

(9.2.30) 
n=1 


This 
objective 
function 
needs 
to 
be 
optimised 
numerically 
to 
nd 
the 
best 
1 
and 
2. 
The 
gradient 
is 


N

X

2
1

2
2

)2

n 
2

n 
3

..

..

..x

(x

dL 


1e

nn

..2I[x 
= 
1] 
1 
+2I[x 
= 
0] 


11 


(9.2.31)
= 


2
1

2
2

n 
3

)2

n 
2

..

..

..x

(x

1 
- 
e


d1 


n=1 
N

X

n 
3

n 
2
2
2

nn)2....(x..x)2

2
1

(x2 
- 
xe

3 


The 
gradient 
can 
be 
used 
as 
part 
of 
a 
standard 
optimisation 
procedure 
(such 
as 
conjugate 
gradients, 
see 
Appendix 
(A)) 
to 
aid 
nding 
the 
Maximum 
Likelihood 
parameters 
1, 
2. 


9.3 
Bayesian 
Belief 
Network 
Training 
An 
alternative 
to 
Maximum 
Likelihood 
training 
of 
a 
BN 
is 
to 
use 
a 
Bayesian 
approach 
in 
which 
we 
maintain 
a 
distribution 
over 
parameters. 
We 
continue 
with 
the 
Asbestos, 
Smoking, 
Cancer 
scenario, 


p(a, 
c, 
s)= 
p(cja, 
s)p(a)p(s) 
(9.3.1) 


which 
can 
be 
represented 
as 
a 
Belief 
Network, 
g(9.4a). 
So 
far 
we've 
only 
specied 
the 
independence 
structure, 
but 
not 
the 
entries 
of 
the 
tables 
p(cja, 
s), 
p(a), 
p(s). 
Given 
a 
set 
of 
visible 
observations, 


nn

V 
= 
f(a;s;cn) 
;n 
=1;:::;Ng, 
we 
would 
like 
to 
learn 
appropriate 
distributions 
for 
the 
table 
entries. 


To 
begin 
we 
need 
a 
notation 
for 
the 
table 
entries. 
With 
all 
variables 
binary 
we 
have 
parameters 
such 
as 
p(a 
=1ja)= 
a;p(c 
=1ja 
=0;s 
=1;c)= 
0;1 
(9.3.2)

c 
1;10;01;0

and 
similarly 
for 
the 
remaining 
parameters 
c 
;c 
;c 
. 
For 
our 
example, 
the 
parameters 
are 


a;s;c 
0;0;c 
0;1;c 
1;0;c
1;1 
(9.3.3)

|{z}

c 


9.3.1 
Global 
and 
local 
parameter 
independence 
In 
Bayesian 
learning 
of 
BNs, 
we 
need 
to 
specify 
a 
prior 
on 
the 
joint 
table 
entries. 
Since 
in 
general 
dealing 
with 
multi-dimensional 
continuous 
distributions 
is 
computationally 
problematic, 
it 
is 
useful 
to 
specify 
only 
uni-variate 
distributions 
in 
the 
prior. 
As 
we 
show 
below, 
this 
has 
a 
pleasing 
consequence 
that 
for 
i.i.d. 
data 
the 
posterior 
also 
factorises 
into 
uni-variate 
distributions. 


Global 
parameter 
independence 


A 
convenient 
assumption 
is 
that 
the 
prior 
factorises 
over 
parameters. 
For 
our 
Asbestos, 
Smoking, 
Cancer 
example, 
we 
assume 


p(a;s;c)= 
p(a)p(s)p(c) 
(9.3.4) 
Assuming 
the 
data 
is 
i.i.d., 
we 
then 
have 
the 
joint 
model 


Y

n 


p(a;s;c, 
V)= 
p(a)p(s)p(c)p(a 
nja)p(s 
njs)p(c 
njs 
;a 
n;c) 
(9.3.5) 
n 


dL 


)2 
+22I[x 


n

..2I[x 
= 
1] 
2 
(x

1 


= 
0] 


(9.2.32)
nn 


2 
- 
x 


n

= 


3 


1

2
2

2..1

)2

n 
2

n 
3

..

..x

(x

1 
- 
e


d2 


n=1 


DRAFT 
March 
9, 
2010 



Bayesian 
Belief 
Network 
Training 


cn
ansn
a;s
c
as
n=1:N
(a;s)2P
Figure 
9.7: 
A 
Bayesian 
parameter 
model 
for 
the 
relationship 
between 
lung 
Cancer, 
Asbestos 
exposure 
and 
Smoking 
with 
factorised 
parameter 
priors. 
The 
global 
parameter 
independence 
assumption 
means 
that 
the 
prior 
over 
tables 
factorises 
into 
priors 
over 
each 
conditional 
probability 
table. 
The 
local 
independence 
assumption, 
which 
in 
this 
case 
comes 
into 
ef


QY

fect 
only 
for 
p(cja, 
s), 
means 
that 
p(c) 
factorises 
in 
a;s2P 
p(c
a;s), 
where 
P 
= 
f(0, 
0), 
(0, 
1), 
(1, 
0), 
(1, 
1)g. 


the 
Belief 
Network 
for 
which 
is 
given 
in 
g(9.7.) 
Learning 
then 
corresponds 
to 
inference 
of 


p(Vja;s;c)p(a;s;c) 
p(Vja;s;c)p(a)p(s)p(c) 


p(a;s;cjV) 
= 
= 
(9.3.6) 


p(V) 
p(V) 


A 
convenience 
of 
the 
factorised 
prior 
for 
a 
BN 
is 
that 
the 
posterior 
also 
factorises, 
since 


p(a;s;cjV) 
. 
p(a;s;c, 
V)

()()()

YYY

n

/p(a)p(a 
nja)p(s)p(s 
njs)p(c)p(c 
njs 
;a 
n;c)
n 
nn 


= 
p(ajVa)p(sjVs)p(cjV) 
(9.3.7) 


so 
that 
one 
can 
consider 
each 
parameter 
posterior 
separately. 
In 
this 
case, 
`learning’ 
involves 
computing 
the 
posterior 
distributions 
p(ijVi) 
where 
Vi 
is 
the 
set 
of 
training 
data 
restricted 
to 
the 
family 
of 
variable 


i. 
The 
global 
independence 
assumption 
conveniently 
results 
in 
a 
posterior 
distribution 
that 
factorises 
over 
the 
conditional 
tables. 
However, 
the 
parameter 
c 
is 
itself 
4 
dimensional. 
To 
simplify 
this 
we 
need 
to 
make 
a 
further 
assumption 
as 
to 
the 
structure 
of 
each 
local 
table. 


Local 
parameter 
independence 


If 
we 
further 
assume 
that 
the 
prior 
for 
the 
table 
factorises 
over 
all 
states 
a, 
c: 


p(c)= 
p(0;0)p(1;0)p(0;1)p(1;1) 
(9.3.8)

cccc 


then 
the 
posterior 


p(cjV) 
. 
p(Vjc)p(0;0)p(1;0)p(0;1)p(1;1) 
(9.3.9)

cccc 
](a=0;s=0) 
](a=0;s=1) 
](a=1;s=0) 
](a=1;s=1)

0;0 
p(0;0 
0;1 
p(0;1 
1;0 
p(1;0 
1;1 
p(1;

= 
/)/)/)/

cccccccc

|{z}|{z}|{z}|{z

0;00;11;01;1 


p(c 
jV) 
p(c 
jV) 
p(c 
jV) 
p(c 
jV) 


(9.3.10) 
so 
that 
the 
posterior 
also 
factorises 
over 
the 
parental 
states 
of 
the 
local 
conditional 
table. 


Posterior 
marginal 
table 


A 
marginal 
probability 
table 
is 
given 
by, 
for 
example, 


Z

p(c 
=1ja 
=1;s 
=0, 
V)=p(c 
=1ja 
=1;s 
=0;1;0)p(cjV) 
(9.3.11)

c 


c 


DRAFT 
March 
9, 
2010 



Bayesian 
Belief 
Network 
Training 


The 
integral 
over 
all 
the 
other 
tables 
in 
equation 
(9.3.11) 
is 
unity, 
and 
we 
are 
left 
with 


Z

p(c 
=1ja 
=1;s 
=0, 
V)=p(c 
=1ja 
=1;s 
=0;1;0)p(1;0jV) 
(9.3.12)

cc

1;0

c 


9.3.2 
Learning 
binary 
variable 
tables 
using 
a 
Beta 
prior 
We 
continue 
the 
example 
of 
section(9.3.1) 
where 
all 
variables 
are 
binary, 
but 
using 
a 
continuous 
valued 
table 
prior. 
The 
simplest 
case 
is 
to 
start 
with 
p(aja) 
since 
this 
requires 
only 
a 
univariate 
prior 
distribution 
p(a). 
The 
likelihood 
depends 
on 
the 
table 
variable 
via 


p(a 
=1ja)= 
a](9.3.13) 
so 
that 
the 
total 
likelihood 
term 
is 
](a=1) 
)](a=0)

(1 
- 
a(9.3.14)

a]

The 
posterior 
is 
therefore 
)](a=1) 
)](a=0)

p(ajVa) 
. 
p(a(1 
- 
a(9.3.15)

a]
This 
means 
that 
if 
the 
prior 
is 
also 
of 
the 
form 
](1 
- 
a)]then 
conjugacy 
will 
hold, 
and 
the 
mathematics 


a]

of 
integration 
will 
be 
straightforward. 
This 
suggests 
that 
the 
most 
convenient 
choice 
is 
a 
Beta 
distribution, 
1 
a..1 
)a..1 


p(a)= 
B 
(aja;a)= 
(1 
- 
a(9.3.16)

a

B(a;a)
for 
which 
the 
posterior 
is 
also 
a 
Beta 
distribution: 
p(ajVa)= 
B 
(aja]+ 
. 
(a 
= 
1) 
;a]+ 
. 
(a 
= 
0)) 
(9.3.17) 
The 
marginal 
table 
is 
given 
by 


Z

a]+ 
. 
(a 
= 
1) 


p(a 
=1jVa)=p(ajVa)a]= 
(9.3.18) 


a 
a]+ 
. 
(a 
= 
1)+ 
a]+ 
. 
(a 
= 
0) 


using 
the 
result 
for 
the 
mean 
of 
a 
Beta 
distribution, 
denition(72). 


The 
situation 
for 
the 
table 
p(cja, 
s) 
is 
slightly 
more 
complex 
since 
we 
need 
to 
specify 
a 
prior 
for 
each 
of 
the 
parental 
tables. 
As 
above, 
this 
is 
most 
convenient 
if 
we 
specify 
a 
Beta 
prior, 
one 
for 
each 
of 
the 
(four) 
parental 
states. 
Let's 
look 
at 
a 
specic 
table 


p(c 
=1ja 
=1;s 
= 
0) 
(9.3.19) 


1;0

Assuming 
the 
local 
independence 
property, 
we 
have 
p(c]jVc) 
given 
by 


..

B1;0jc(a 
=1;s 
= 
0)+ 
. 
(c 
=1;a 
=1;s 
= 
0) 
;c(a 
=1;s 
= 
0)+ 
. 
(c 
=0;a 
=1;s 
= 
0)(9.3.20)

c]

As 
before, 
the 
marginal 
probability 
table 
is 
then 
given 
by 
c(a 
=1;s 
= 
0)+ 
. 
(c 
=1;a 
=1;s 
= 
0) 


p(c 
=1ja 
=1;s 
=0, 
Vc) 
= 
(9.3.21)

c(a 
=1;s 
= 
0)+ 
c(a 
=1;s 
= 
0)+ 
. 
(a 
=1;s 
= 
0) 
since 
. 
(a 
=1;s 
= 
0) 
= 
. 
(c 
=0;a 
=1;s 
= 
0)+ 
. 
(c 
=1;a 
=1;s 
= 
0). 


The 
prior 
parameters 
c(a, 
s) 
are 
called 
hyperparameters. 
If 
one 
had 
no 
preference, 
one 
could 
set 
all 
of 
the 
c(a, 
s) 
to 
be 
equal 
to 
the 
same 
value 
a 
and 
similarly 
for 
. 
A 
complete 
ignorance 
prior 
would 
correspond 
to 
setting 
a 
= 
ß 
= 
1, 
see 
g(8.7). 


DRAFT 
March 
9, 
2010 



Bayesian 
Belief 
Network 
Training 


No 
data 
limit 
N 
. 
0 
In 
the 
limit 
of 
no 
data, 
the 
marginal 
probability 
table 
corresponds 
to 
the 
prior, 
which 
is 
given 
in 
this 
case 
by 


c(a 
=1;s 
= 
0) 


p(c 
=1ja 
=1;s 
= 
0) 
= 
(9.3.22)

c(a 
=1;s 
= 
0)+ 
c(a 
=1;s 
= 
0) 


For 
a 
at 
prior 
a 
= 
ß 
= 
1 
for 
all 
states 
a, 
c, 
this 
would 
give 
a 
prior 
probability 
of 
p(c 
=1ja 
=1;s 
= 
0) 
= 
0:5. 


Innite 
data 
limit 
N 
!8 
In 
this 
limit 
the 
marginal 
probability 
tables 
are 
dominated 
by 
the 
data 
counts, 
since 
these 
will 
typically 
grow 
in 
proportion 
to 
the 
size 
of 
the 
dataset. 
This 
means 
that 
in 
the 
innite 
(or 
very 
large) 
data 
limit, 


. 
(c 
=1;a 
=1;s 
= 
0) 
p(c 
=1ja 
=1;s 
=0, 
V) 
. 
(9.3.23)

. 
(c 
=1;a 
=1;s 
= 
0)+ 
. 
(c 
=0;a 
=1;s 
= 
0) 


which 
corresponds 
to 
the 
Maximum 
Likelihood 
solution. 


This 
eect 
that 
the 
large 
data 
limit 
of 
a 
Bayesian 
procedure 
corresponds 
to 
the 
Maximum 
Likelihood 
solution 
is 
general 
unless 
the 
prior 
has 
a 
pathologically 
strong 
eect. 


Example 
39 
(Asbestos-Smoking-Cancer). 


Consider 
the 
binary 
variable 
network 


p(c, 
a, 
s)= 
p(cja, 
s)p(a)p(s) 
(9.3.24) 


The 
data 
V 
is 
given 
in 
g(9.5). 
Using 
a 
at 
Beta 
prior 
a 
= 
ß 
= 
1 
for 
all 
conditional 
probability 
tables, 
the 
marginal 
posterior 
tables 
are 
given 
by 


1+ 
. 
(a 
=1) 
1+4 
5 


p(a 
=1jV)= 
== 
˜ 
0:556 
(9.3.25)

2+ 
N 
2+7 
9 


By 
comparison, 
the 
Maximum 
Likelihood 
setting 
is 
4=7=0:571. 
The 
Bayesian 
result 
is 
a 
little 
more 
cautious 
than 
the 
Maximum 
Likelihood, 
which 
squares 
with 
our 
prior 
belief 
that 
any 
setting 
of 
the 
probability 
is 
equally 
likely, 
pulling 
the 
posterior 
towards 
0.5. 


Similarly, 


1+ 
. 
(s 
=1) 
1+4 
5 


p(s 
=1jV)= 
== 
˜ 
0:556 
(9.3.26)

2+ 
N 
2+7 
9 


and 


1+ 
. 
(c 
=1;a 
=1;s 
=1) 
1+2 
3 


p(c 
=1ja 
=1;s 
=1, 
V) 
= 
= 
= 
(9.3.27)

2+ 
. 
(c 
=1;a 
=1;s 
= 
1)+ 
. 
(c 
=0;a 
=1;s 
=1) 
2+2 
4 


1+ 
. 
(c 
=1;a 
=1;s 
=0) 
1+1 
2 


p(c 
=1ja 
=1;s 
=0, 
V) 
= 
= 
= 
(9.3.28)

2+ 
. 
(c 
=1;a 
=1;s 
= 
0)+ 
. 
(c 
=0;a 
=1;s 
=0) 
2+1 
3 
1+ 
. 
(c 
=1;a 
=0;s 
=1) 
1+1 
1 


p(c 
=1ja 
=0;s 
=1, 
V) 
= 
= 
= 
(9.3.29)

2+ 
. 
(c 
=1;a 
=0;s 
= 
1)+ 
. 
(c 
=0;a 
=0;s 
=1) 
2+2 
2 
1+ 
. 
(c 
=1;a 
=0;s 
=0) 
1+0 
1 


p(c 
=1ja 
=0;s 
=0, 
V) 
= 
= 
= 
(9.3.30)

2+ 
. 
(c 
=1;a 
=0;s 
= 
0)+ 
. 
(c 
=0;a 
=0;s 
=0) 
2+1 
3 


DRAFT 
March 
9, 
2010 



Bayesian 
Belief 
Network 
Training 


9.3.3 
Learning 
multivariate 
discrete 
tables 
using 
a 
Dirichlet 
prior 
The 
natural 
generalisation 
to 
more 
than 
two-state 
variables 
is 
given 
by 
using 
a 
Dirichlet 
prior, 
again 
assuming 
i.i.d. 
data 
and 
the 
local 
and 
global 
parameter 
prior 
independencies. 
Since 
under 
the 
global 
parameter 
independence 
assumption 
the 
posterior 
factorises 
over 
variables 
(as 
in 
equation 
(9.3.7)), 
we 
can 
concentrate 
on 
the 
posterior 
of 
a 
single 
variable. 


No 
parents 


Let's 
consider 
the 
contribution 
of 
a 
variable 
v 
with 
dom(v)= 
f1;:::;Ig. 
The 
contribution 
to 
the 
posterior 


n

from 
a 
datapoint 
vis 


II

YnX

I[v=i]

p(v 
nj)= 
, 
i 
= 
1 
(9.3.31)

i 
i=1 
i=1 


so 
that 
the 
posterior 
is 
proportional 
to 


NII

YYnYPNn

I[v=i] 
I[v=i]

n=1

p() 
= 
p() 
(9.3.32)

ii 
n=1 
i=1 
i=1 


For 
a 
Dirichlet 
prior 
distribution 
with 
hyperparameters 
u 


I

Y

ui..1 


p() 
. 
(9.3.33)

i 
i=1 


Using 
this 
prior 
the 
posterior 
becomes 


III

YYPNnYPNn

I[v=i] 
ui..1+I[v=i] 


p(jV) 
. 
ui..1 
n=1 
= 
n=1 
(9.3.34)

ii 
i 
i=1 
i=1 
i=1 


which 
means 
that 
the 
posterior 
is 
given 
by 


p(jV) 
= 
Dirichlet 
(ju 
+ 
c) 
(9.3.35) 


where 
c 
is 
a 
count 
vector 
with 
components 


N

XY

n 


ci 
= 
I[v 
= 
i] 
(9.3.36) 
n=1 


being 
the 
number 
of 
times 
state 
i 
was 
observed 
in 
the 
training 
data. 


The 
marginal 
table 
is 
given 
by 
integrating 


p(v 
= 
ijV)= 
p(v 
= 
ij)p(jV)= 
ip(ijV) 
(9.3.37) 


i 


Since 
the 
single-variable 
marginal 
distribution 
of 
a 
Dirichlet 
is 
a 
Beta 
distribution, 
section(8.5), 
the 
marginal 
table 
is 
the 
mean 
of 
a 
Beta 
distribution. 
Given 
that 
the 
marginal 
p(jV) 
is 
Beta 
distribution 


PY

with 
parameters 
a 
= 
ui 
+ 
ci, 
ß 
= 
j=i 
uj 
+ 
cj, 
the 
marginal 
table 
is 
given 
by 


6

ui 
+ 
ci 


p(v 
= 
ijV)= 
PY(9.3.38) 
j 
uj 
+ 
cj 


which 
generalises 
the 
binary 
state 
formula 
equation 
(9.3.18). 


178 
DRAFT 
March 
9, 
2010 



Bayesian 
Belief 
Network 
Training 


9.3.4 
Parents 
To 
deal 
with 
the 
general 
case 
of 
a 
variable 
v 
with 
parents 
pa 
(v) 
we 
denote 
the 
probability 
of 
v 
being 
in 
state 
i, 
conditioned 
on 
the 
parents 
being 
in 
state 
j 
as 


p(v 
= 
ijpa 
(v)= 
j;)= 
i(v; 
j) 
(9.3.39) 


P

wherei(v; 
j) 
= 
1. 
This 
forms 
the 
components 
of 
a 
vector 
(v; 
j). 
Note 
that 
if 
v 
has 
K 
parents 
then 


i 


the 
number 
of 
states 
j 
will 
be 
exponential 
in 
K. 


Local 
(state) 
independence 
means 


Y

p((v)) 
=p((v; 
j)) 
(9.3.40) 


j 


And 
global 
independence 
means 


Y

p()=p((v)) 
(9.3.41) 


v 


where 
. 
=((v);v 
=1;:::;V 
) 
represents 
the 
combined 
table 
of 
all 
the 
variables. 
We 
drop 
the 
explicit 
sans-serif 
font 
on 
the 
states 
from 
here 
on 
in. 


Parameter 
posterior 


Thanks 
to 
the 
global 
parameter 
independence 
the 
posterior 
distribution 
over 
the 
tables 
. 
factorises, 
with 
one 
posterior 
table 
per 
variable. 
Each 
posterior 
table 
for 
a 
variable 
v 
depends 
only 
on 
the 
information 
local 
to 
the 
family 
of 
each 
variable 
D(v). 
Assuming 
a 
Dirichlet 
distribution 
prior 


p((v; 
j)) 
= 
Dirichlet 
((v; 
j)ju(v; 
j)) 
(9.3.42) 


the 
posterior 
is 
proportional 
to 
the 
joint 
distribution 


p((v), 
D(v)) 
= 
p((v))p(D(v)j(v)) 
(9.3.43) 


YYYYY

=i;pa(vn)=j]

=
1 
i(v; 
j)ui(v;j)..1i(v; 
j)I[vn(9.3.44)

Z(u(v; 
j))

j 
i 
nji 


YY

=
1 
i(v; 
j)ui(v;j)..1+](v=i;pa(v)=j) 
(9.3.45) 
j 
Z(u(v; 
j))
i 


where 
Z(u) 
is 
the 
normalisation 
constant 
of 
a 
Dirichlet 
distribution. 
Hence 
the 
posterior 
is 


Y..

p((v)jD(v)) 
=Dirichlet(v; 
j)ju0(v; 
j)(9.3.46) 


j 


where 
the 
hyperparameter 
prior 
term 
is 
updated 
by 
the 
observed 
counts, 


0

u(v; 
j) 
= 
ui(v; 
j)+ 
. 
(v 
= 
i, 
pa 
(v)= 
j) 
(9.3.47)

i

By 
analogy 
with 
the 
no-parents 
case, 
the 
marginal 
table 
is 
given 
by 
(writing 
the 
states 
explicitly) 


p(v 
= 
ijpa 
(v)= 
j, 
D(v)) 
. 
u0(v; 
j) 
(9.3.48)

i

DRAFT 
March 
9, 
2010 
179 



Bayesian 
Belief 
Network 
Training 


a 
s 
c 
1 
1 
2 
1 
0 
0 
0 
1 
1 
0 
1 
0 
1 
1 
2 
0 
0 
0 
1 
0 
1 


Figure 
9.8: 
A 
database 
of 
patient 
records 
about 
the 
Asbestos 
exposure 
(1 
signies 
exposure), 
being 
a 
Smoker 
(1 
signies 
the 
individual 
is 
a 
smoker), 
and 
lung 
Cancer 
(0 
signies 
no 
cancer, 
1 
signies 
early 
stage 
cancer, 
2 
signies 
late 
state 
cancer). 
Each 
row 
contains 
the 
information 
for 
an 
individual, 
so 
that 
there 
are 
7 
individuals 
in 
the 
database. 


Example 
40. 
Consider 
the 
p(cja, 
s)p(s)p(a) 
asbestos 
example 
with 
dom(a) 
= 
dom(s)= 
f0, 
1g, 
except 
now 
with 
the 
variable 
c 
taking 
three 
states, 
dom(c)= 
f0, 
1, 
2g, 
accounting 
for 
dierent 
kinds 
of 
cancer. 
The 
marginal 
table 
under 
a 
Dirichlet 
prior 
is 
then 
given 
by 


u0(a 
=1;s 
= 
1)+ 
. 
(c 
=0;a 
=1;s 
= 
1) 


p(c 
=0ja 
=1;s 
=1, 
V)= 
P(9.3.49) 


i2f0;1;2} 
ui(a 
=1;s 
= 
1)+ 
. 
(c 
= 
i, 
a 
=1;s 
= 
1) 


Assuming 
a 
at 
Dirichlet 
prior, 
which 
corresponds 
to 
setting 
all 
components 
of 
u 
to 
1, 
this 
gives 


1+0 
1 


p(c 
=0ja 
=1;s 
=1, 
V) 
= 
= 
(9.3.50)

3+2 
5 


1+0 
1 


p(c 
=1ja 
=1;s 
=1, 
V) 
= 
= 
(9.3.51)

3+2 
5 
1+2 
3 


p(c 
=2ja 
=1;s 
=1, 
V) 
= 
= 
(9.3.52)

3+2 
5 


and 
similarly 
for 
the 
other 
three 
tables 
p(cja 
=1;s 
= 
0);p(cja 
=0;s 
= 
1);p(cja 
=1;s 
= 
1). 


Model 
likelihood 


For 
a 
Belief 
Network 
M, 
the 
joint 
probability 
of 
all 
variables 
factorises 
into 
the 
local 
probabilities 
of 
each 
variable 
conditioned 
on 
its 
parents: 


p(VjM)= 
p(vjpa 
(v) 
;M) 
(9.3.53) 


v 


For 
i.i.d. 
data 
D, 
the 
likelihood 
under 
the 
network 
M 
is 


Z(u0(v; 
j)) 


p(DjM)= 
p(v 
njpa 
(v 
n) 
;M) 
= 
(9.3.54)

Z(u(v; 
j))

vn 
vj 


where 
u 
are 
the 
Dirichlet 
hyperparameters 
and 
u' 
is 
given 
by 
equation 
(9.3.47). 
Expression 
(9.3.54) 
can 
be 
written 
explicitly 
in 
terms 
of 
Gamma 
functions, 
see 
exercise(125). 
In 
the 
above 
expression 
in 
general 
the 
number 
of 
parental 
states 
diers 
for 
each 
variable 
v, 
so 
that 
implicit 
in 
the 
above 
formula 
is 
that 
the 
state 
product 
over 
j 
goes 
from 
1 
to 
the 
number 
of 
parental 
states 
of 
variable 
v. 
Due 
to 
the 
local 
and 
global 
parameter 
independence 
assumptions, 
the 
logarithm 
of 
the 
model 
likelihood 
splits 
into 
terms, 
one 
for 
each 
variable 
v 
and 
parental 
conguration. 
This 
is 
called 
the 
likelihood 
decomposable 
property. 


9.3.5 
Structure 
learning 
Up 
to 
this 
point 
we 
have 
assumed 
that 
we 
are 
given 
both 
the 
structure 
of 
the 
distribution 
and 
a 
dataset‘ 


D. 
A 
more 
complex 
task 
is 
when 
we 
need 
to 
learn 
the 
structure 
of 
the 
network 
as 
well. 
We'll 
consider 
the 
case 
in 
which 
the 
data 
is 
complete 
(i.e. 
there 
are 
no 
missing 
observations). 
Since 
for 
D 
variables, 
there 
is 
an 
exponentially 
large 
number 
(in 
D) 
of 
BN 
structures, 
it's 
clear 
that 
we 
cannot 
search 
over 
all 
possible 
structures. 
For 
this 
reason 
structure 
learning 
is 
a 
computationally 
challenging 
problem 
and 
we 
must 
rely 
on 
constraints 
and 
heuristics 
to 
help 
guide 
the 
search. 
Furthermore, 
for 
all 
but 
the 
sparsest 
180 
DRAFT 
March 
9, 
2010 



Bayesian 
Belief 
Network 
Training 


Algorithm 
3 
PC 
algorithm 
for 
skeleton 
learning. 


1: 
Start 
with 
a 
complete 
undirected 
graph 
G 
on 
the 
set 
V6of 
all 
vertices. 
2: 
i 
=0 
3: 
repeat 
4: 
for 
x 
2V6do 
5: 
for 
y 
26Adj 
fxg6do 
Determine 
if 
there 
a 
subset 
S6of 
size 
i 
of 
the 
neighbours 
of 
x 
(not 
including 
y) 
for 
which 
6: 
x.
??yjS. 
If 
this 
set 
exists 
remove 
the 
x 
..6y 
link 
from 
the 
graph 
G 
and 
set 
Sxy 
= 
S. 


7: 
end 
for 
8: 
end 
for 
9: 
i 
= 
i 
+1. 
10: 
until 
all 
nodes 
have 
6i 
neighbours. 
networks, 
estimating 
the 
dependencies 
to 
any 
accuracy 
requires 
a 
large 
amount 
of 
data, 
making 
testing 
of 
dependencies 
dicult. 
Indeed, 
for 
a 
nite 
amount 
of 
data, 
two 
variables 
will 
always 
have 
non-zero 
mutual 
information, 
so 
that 
a 
threshold 
needs 
to 
be 
set 
to 
decide 
if 
the 
measured 
dependence 
is 
signicant 
under 
the 
nite 
sample, 
see 
section(9.3.6). 
Other 
complexities 
arise 
from 
the 
concern 
that 
a 
Belief 
or 
Markov 
Network 
on 
the 
visible 
variables 
alone 
may 
not 
be 
a 
parsimonious 
way 
to 
represent 
the 
observed 
data 
if, 
for 
example, 
there 
may 
be 
latent 
variables 
which 
are 
driving 
the 
observed 
dependencies. 
For 
these 
reasons 
we 
will 
not 
discuss 
this 
topic 
in 
detail 
here 
and 
limit 
the 
discussion 
to 
two 
central 
approaches. 


A 
special 
case 
that 
is 
computationally 
tractable 
is 
when 
the 
network 
is 
constrained 
to 
have 
at 
most 
one 
parent. 
We 
defer 
discussion 
of 
this 
to 
section(10.4.1). 


PC 
algorithm 


The 
PC 
algorithm[259] 
rst 
learns 
the 
skeleton 
of 
a 
graph, 
after 
which 
edges 
may 
be 
oriented 
to 
form 
a 
(partially 
oriented) 
DAG. 


The 
PC 
algorithm 
begins 
at 
the 
rst 
round 
with 
a 
complete 
skeleton 
G 
and 
attempts 
to 
remove 
as 
many 
links 
as 
possible. 
At 
the 
rst 
step 
we 
test 
all 
pairs 
x 
. 
??6yj;. 
If 
an 
x 
and 
y 
pair 
are 
deemed 
independent 
then 
the 
link 
x 
..6y 
is 
removed 
from 
the 
complete 
graph. 
One 
repeats 
this 
for 
all 
the 
pairwise 
links. 
In 
the 
second 
round, 
for 
the 
remaining 
graph, 
one 
examines 
each 
x 
..6y 
link 
and 
conditions 
on 
a 
single 
neighbour 
z 
of 
x. 
If 
x 
.
??yjz 
then 
remove 
the 
link 
x 
..6y. 
One 
repeats 
in 
this 
way 
through 
all 
the 
variables. 
At 
each 
round 
the 
number 
of 
neighbours 
in 
the 
conditioning 
set 
is 
increased 
by 
one. 
See 
algorithm(3), 
g(9.9)1 
and 
demoPCoracle.m. 
A 
renement 
of 
this 
algorithm, 
known 
as 
NPC 
for 
necessary 
path 
PC[261] 
attempts 
to 
limit 
the 
number 
of 
independence 
checks 
which 
may 
otherwise 
result 
in 
inconsistencies 
due 
to 
the 
empirical 
estimates 
of 
conditional 
mutual 
information. 
Given 
a 
learned 
skeleton, 
a 
partial 
DAG 
can 
be 
constructed 
using 
algorithm(4). 
Note 
that 
this 
is 
necessary 
since 
the 
undirected 
graph 
G 
is 
a 
skeleton 
– 
not 
a 
Belief 
Network 
of 
the 
independence 
assumptions 
discovered. 
For 
example, 
we 
may 
have 
a 
graph 
G 
with 
x 
..6z 
..6y 
in 
which 
the 
x 
..6y 
link 
was 
removed 
on 
the 
basis 
x 
. 
??6yj;6!6Sxy 
= 
;. 
As 
a 
MN 
the 
graph 
x 
..6z 
..6y 
implies 
xl
>>y, 
although 
this 
is 
inconsistent 
with 
the 
discovery 
in 
the 
rst 
round 
x 
. 
??6y. 
This 
is 
the 
reason 
for 
the 
orientation 
part: 
for 
consistency, 
we 
must 
have 
x 
!6z 
 6y, 
for 
which 
x 
. 
??6y 
and 
xl
>>yj6z. 
Note 
that 
in 
algorithm(4) 
we 
have 
for 
the 
`unmarried 
collider’ 
test, 
z62;, 
which 
in 
this 
case 
is 
true, 
resulting 
in 
a 
collider 
forming. 
See 
also 
g(9.10). 


Example 
41 
(Skeleton 
orienting). 


z
xy
If 
x 
is 
(unconditionally) 
independent 
of 
y, 
it 


z
xy
must 
be 
that 
z 
is 
a 
collider 
since 
otherwise 


x.
??yj;6)6

marginalising 
over 
z 
would 
introduce 
a 
dependence 
between 
x 
and 
y. 


1This 
example 
appears 
in 
[148] 
and 
[208] 
– 
thanks 
also 
to 
Serafn 
Moral 
for 
his 
online 
notes. 


DRAFT 
March 
9, 
2010 



Bayesian 
Belief 
Network 
Training 


t
zw
xy
t
zw
xy
t
zw
xy
t
zw
xy
t
zw
xy
t
zw
xy
t
zw
xy
t
zw
xy
t
zw
xy
(a) 
(b) 
(c) 
(d) 
(e) 
(f) 
(g) 
(h) 
(i) 
t
zw
xy
t
zw
xy
t
zw
xy
t
zw
xy
t
zw
xy
t
zw
xy
t
zw
xy
t
zw
xy
t
zw
xy
(j) 
(k) 
(l) 
(m) 
(n) 
(o) 
(p) 
(q) 
(r) 
Figure 
9.9: 
PC 
algorithm. 
(a): 
The 
BN 
from 
which 
data 
is 
assumed 
generated 
and 
against 
which 
conditional 
independence 
tests 
will 
be 
performed. 
(b): 
The 
initial 
skeleton 
is 
fully 
connected. 
(c-l): 
In 
the 
rst 
round 
(i 
= 
0) 
all 
the 
pairwise 
mutual 
informations 
x 
. 
??yjØ 
are 
checked, 
and 
the 
link 
between 
x 
and 
y 
removed 
if 
deemed 
independent 
(green 
line). 
(m-o): 
i 
= 
1. 
We 
now 
look 
at 
connected 
subsets 
on 
the 
three 
variables 
x, 
y, 
z 
of 
the 
remaining 
graph, 
removing 
the 
link 
x 
- 
y 
if 
x.
??yjz 
is 
true. 
Not 
all 
steps 
are 
shown. 
(p,q): 
i 
= 
2. 
We 
now 
examine 
all 
x 
. 
??yjfa, 
bg. 
The 
algorithm 
terminates 
after 
this 
round 
(when 
i 
gets 
incremented 
to 
3) 
since 
there 
are 
no 
nodes 
with 
3 
or 
more 
neighbours. 
(r): 
Final 
skeleton. 
During 
this 
process 
the 
sets 
Sx;y 
= 
;;Sx;w 
= 
;;Sz;w 
= 
y, 
Sx;t 
= 
fz, 
w} 
;Sy;t 
= 
fz, 
w} 
were 
found. 
See 
also 


demoPCoracle.m 


z
xy
z
xy
If 
x 
is 
independent 
of 
y 
conditioned 
on 
z, 
z 
must 
x.
??yjz 
. 


not 
be 
a 
collider. 
Any 
other 
orientation 
is 
appropriate. 


9.3.6 
Empirical 
independence 
Given 
a 
data 
set 
D, 
containing 
variables 
x, 
y, 
z, 
our 
interest 
is 
to 
measure 
if 
x 
.
??yjz. 
One 
approach 
is 
to 
use 
the 
conditional 
mutual 
information 
which 
is 
the 
average 
of 
conditional 
Kullback-Leibler 
divergences. 


Denition 
87 
(Mutual 
Information). 


MI(x; 
yjz) 
hKL(p(x, 
yjz)jp(xjz)p(yjz))i= 
0 
(9.3.55)

p(z) 


where 
this 
expression 
is 
equally 
valid 
for 
sets 
of 
variables. 
If 
x 
. 
??y| 
z 
is 
true, 
then 
MI(x; 
yjz) 
is 
zero, 
and 
vice 
versa. 
When 
z 
= 
;, 
the 
average 
over 
p(z) 
is 
absent 
and 
one 
writes 
MI(x; 
y). 


Given 
data 
we 
can 
obtain 
an 
estimate 
of 
the 
conditional 
mutual 
information 
by 
using 
the 
empirical 
distribution 
p(x, 
y, 
z) 
estimated 
by 
simply 
counting 
occurrences 
in 
the 
data. 
In 
practice, 
however, 
we 
only 
have 
a 
nite 
amount 
of 
data 
to 
estimate 
the 
empirical 
distribution 
so 
that 
for 
data 
sampled 
from 
distribution 
for 
which 
the 
variables 
truly 
are 
independent, 
the 
empirical 
mutual 
information 
will 
typically 
be 
greater 
than 
zero. 
An 
issue 
therefore 
is 
what 
threshold 
to 
use 
for 
the 
empirical 
conditional 
mutual 
information 
to 
decide 
if 
this 
is 
suciently 
far 
from 
zero 
to 
be 
caused 
by 
dependence. 
A 
frequentist 
approach 
is 
to 
compute 
the 
distribution 
of 
the 
conditional 
mutual 
information 
and 
then 
see 
where 
the 
sample 
value 
is 
compared 
to 
the 
distribution. 
According 
to 
[164] 
2NMI(x; 
yjz) 
is 
Chi-square 
distributed 


182 
DRAFT 
March 
9, 
2010 



Bayesian 
Belief 
Network 
Training 


Algorithm 
4 
Skeleton 
orientation 
algorithm 
(returns 
a 
DAG). 


1: 
Unmarried 
Collider: 
Examine 
all 
undirected 
links 
x]..6z]..6y. 
If 
z62Sxy 
set 
x]!6z] 6y. 
2: 
repeat 
3: 
x]!6z]..6y])6x]!6z]!6y]
4: 
For 
x]..6y, 
if 
there 
is 
a 
directed 
path 
from 
x]to 
y]orient 
x]!6y]
5: 
If 
for 
x]..6z]..6y]there 
is 
a 
w]such 
that 
x]!6w, 
y]!6w, 
z]..6w]then 
orient 
z]!6w]
6: 
until 
No 
more 
edges 
can 
be 
oriented. 
7: 
The 
remaining 
edges 
can 
be 
arbitrarily 
oriented 
provided 
that 
the 
graph 
remains 
a 
DAG 
and 
no 
additional 
colliders 
are 
introduced. 
with 
(X]..1)(Y]..1)Z]degrees 
of 
freedom, 
although 
this 
test 
does 
not 
work 
well 
in 
the 
case 
of 
small 
amounts 
of 
data. 
An 
alternative 
pragmatic 
approach 
is 
to 
estimate 
the 
threshold 
based 
on 
empirical 
samples 
of 
the 
MI 
under 
controlled 
independent/dependent 
conditions 
– 
see 
demoCondindepEmp.m 
for 
a 
comparison 
of 
these 
approaches. 


Bayesian 
conditional 
independence 
test 


A 
Bayesian 
approach 
to 
testing 
for 
independence 
can 
be 
made 
by 
comparing 
the 
likelihood 
of 
the 
data 
under 
the 
independence 
hypothesis, 
versus 
the 
likelihood 
under 
the 
dependent 
hypothesis. 
For 
the 
independence 
hypothesis 
we 
have 
a 
joint 
distribution 
over 
variables 
and 
parameters: 


p(x;]y;]z;]jHindep)= 
p(xjz;]xjz)p(yjz;]yjz)p(zjz)p(xjz)p(yjz)p(z) 
(9.3.56) 


For 
categorical 
distributions, 
it 
is 
convenient 
to 
use 
a 
prior 
Dirichlet 
(ju) 
on 
the 
parameters 
, 
assuming 
also 
local 
as 
well 
as 
global 
parameter 
independence. 
For 
a 
set 
of 
assumed 
i.i.d. 
data 
(X6;]Y;]Z)= 


nn

(x;y;zn) 
;n]=1;:::;N, 
the 
likelihood 
is 
then 
given 
by 
integratingover 
the 
parameters 
: 


nn 


p(X6;]Y;]ZjHindep)= 
p(x];y];z]n;jHindep) 
. 


n 


Thanks 
to 
conjugacy, 
this 
is 
straightforward 
and 
gives 
the 
expression 


Z(uz 
+](z)) 
. 
Z(uxjz 
+](x;]z)) 
Z(uyjz 
+](y;]z)) 


p(X6;]Y;]ZjHindep) 
= 
(9.3.57)

Z(uz) 
Z(uxjz) 
Z(uyjz)

z 


where 
uxjz 
is 
a 
hyperparameter 
matrix 
of 
pseudo 
counts 
for 
each 
state 
of 
x]given 
each 
state 
of 
z. 
Z(v) 
is 
the 
normalisation 
constant 
of 
a 
Dirichlet 
distribution 
with 
vector 
parameter 
v. 


For 
the 
dependent 
hypothesis 
we 
have 


p(x;]y;]z;]jHdep)= 
p(x;]y;]zjx;y;z)p(x;y;z) 
(9.3.58) 


The 
likelihood 
is 
then 


Z(ux;y;z 
+](x;]y;]z)) 


p(X6;]Y;]ZjHdep) 
= 
(9.3.59)
Z(ux;y;z) 


t]
z]w]
x]y]
t]
z]w]
x]y]
t]
z]w]
x]y]
t]
z]w]
x]y]
Figure 
9.10: 
Skeleton 
orientation 
algorithm. 
(a): 
The 
skeleton 
along 
with 
Sx;y 
= 
;;Sx;w 
= 
;;Sz;w 
= 
y;]Sx;t 
= 
fz;]wg6;Sy;t 
= 
fz;]wg. 
(b): 
z626Sx;y, 
so 
form 
collider. 
(c): 
t626Sz;w, 
so 
form 
collider. 
(d): 
Final 
partially 
oriented 
DAG. 
The 
remaining 


edge 
may 
be 
oriented 
as 
desired, 
without 
violating 


(a) 
(b) 
(c) 
(d) 
the 
DAG 
condition. 
See 
also 
demoPCoracle.m. 


DRAFT 
March 
9, 
2010 



zz
Bayesian 
Belief 
Network 
Training 


zn
xnyn
xjzyjz
N
Figure 
9.11: 
Bayesian 
conditional 
independence 
test 
using 
Dirichlet 
priors 
on 
the 
tables. 
(a):A 
model 
Hindep 
for 
conditional 
independence 
x 
. 
??y 
j

z. 
(b): 
A 
model 
Hdep 
for 
conditional 
dependence 
xl
>>y 
| 
z. 
By 
computing 
the 
likelihood 
of 
the 
data 
under 
each 
model, 
a 
numerical 
score 
for 
the 
whether 
the 
data 
is 
more 
consistent 
with 
the 
conditional 
independence 
assumption 
can 
be 
formed. 
See 
demoCondindepEmp.m. 
(a) 
xn;yn;zn
x;y;z
N
(b)
xyz1z2
Figure 
9.12: 
Conditional 
independence 
test 
of 
x 
. 
??y 
| 
z1;z2 
with 
x, 
y, 
z1;z2 
having 
3, 
2, 
4, 
2 
states 
respectively. 
From 
the 
oracle 
Belief 
network 
shown, 
in 
each 
experiment 
the 
tables 
are 
drawn 
at 
random 
and 
20 
examples 
are 
sampled 
to 
form 
a 
dataset. 
For 
each 
dataset 
a 
test 
is 
carried 
out 
to 
determine 
if 
x 
and 
y 
are 
independent 
conditioned 
on 
z1;z2 
(the 
correct 
answer 
being 
that 
they 
are 
independent). 
Over 
500 
experiments, 
the 
Bayesian 
conditional 
independence 
test 
correctly 
states 
that 
the 
variables 
are 
conditionally 
independent 
74% 
of 
the 
time, 
compared 
with 
only 
50% 
accuracy 
using 
the 
chi-square 
mutual 
information 


test. 
See 
demoCondindepEmp.m. 


Assuming 
each 
hypothesis 
is 
equally 
likely, 
for 
a 
Bayes’ 
Factor 


p(X 
, 
Y, 
ZjHindep) 


(9.3.60) 
p(X 
, 
Y, 
ZjHdep) 


greater 
than 
1, 
we 
assume 
that 
conditional 
independence 
holds, 
otherwise 
we 
assume 
the 
variables 
are 
conditionally 
dependent. 
demoCondindepEmp.m 
suggests 
that 
the 
Bayesisan 
hypothesis 
test 
tends 
to 
outperform 
the 
conditional 
mutual 
information 
approach, 
particularly 
in 
the 
small 
sample 
size 
case, 
see 
g(9.12). 


9.3.7 
Network 
scoring 
An 
alternative 
to 
local 
methods 
such 
as 
the 
PC 
algorithm 
is 
to 
evaluate 
the 
whole 
network 
structure. 
In 
a 
probabilistic 
context, 
given 
a 
model 
structure 
M, 
we 
wish 
to 
compute 
p(MjD) 
. 
p(DjM)p(M). 
Some 
care 
is 
needed 
here 
since 
we 
have 
to 
rst 
`t’ 
each 
model 
with 
parameters 
, 
p(Vj, 
M) 
to 
the 
data 
D. 
If 
we 
do 
this 
using 
Maximum 
Likelihood 
alone, 
with 
no 
constraints 
on 
, 
we 
will 
always 
end 
up 
favouring 
that 
model 
M 
with 
the 
most 
complex 
structure 
(assuming 
p(M)= 
const.). 
This 
can 
be 
remedied 
by 
using 
the 
Bayesian 
technique 


p(DjM)= 
p(Dj, 
M)p(jM) 
(9.3.61) 


. 


In 
the 
case 
of 
directed 
networks, 
however, 
as 
we 
saw 
in 
section(9.3), 
the 
assumptions 
of 
local 
and 
global 
parameter 
independence 
make 
the 
integrals 
tractable. 
For 
a 
discrete 
state 
network 
and 
Dirichlet 
priors, 
we 
have 
p(DjM) 
given 
explicitly 
by 
the 
Bayesian 
Dirichlet 
score 
equation 
(9.3.54). 
First 
we 
specify 
the 
hyperparameters 
u(v; 
j), 
and 
then 
search 
over 
structures 
M, 
to 
nd 
the 
one 
with 
the 
best 
score 
p(DjM). 
The 
simplest 
setting 
for 
the 
hyperparameters 
is 
set 
them 
all 
to 
unity[66]. 
Another 
setting 
is 
the 
`uninformative 
prior'[52] 


a 


ui(v; 
j) 
= 
(9.3.62)

dim 
v 
dim 
pa 
(v) 


184 
DRAFT 
March 
9, 
2010 



Maximum 
Likelihood 
for 
Undirected 
models 


12345678
12345678
12345678
(a) 
(b) 
(c) 
Figure 
9.13: 
Learning 
the 
structure 
of 
a 
Bayesian 
network. 
(a): 
The 
correct 
structure 
in 
which 
all 
variables 
are 
binary. 
The 
ancestral 
order 
is 
2, 
1, 
5, 
4, 
3, 
8, 
7, 
6. 
The 
dataset 
is 
formed 
from 
1000 
samples 
from 
this 
network. 
(b): 
The 
learned 
structure 
based 
on 
the 
PC 
algorithm 
using 
the 
Bayesian 
empirical 
conditional 
independence 
test. 
Undirected 
edges 
may 
be 
oriented 
arbitrarily. 
(c): 
The 
learned 
structure 
based 
on 
the 
Bayes 
Dirichlet 
network 
scoring 
method. 
See 
demoPCdata.m 
and 
demoBDscore.m. 


where 
dim 
x 
is 
the 
number 
of 
states 
of 
the 
variable(s) 
x, 
giving 
rise 
to 
the 
BDeu 
score, 
for 
an 
`equivalent 
sample 
size’ 
parameter 
. 
A 
discussion 
of 
these 
settings 
is 
given 
in 
[129] 
under 
the 
concept 
of 
likelihood 
equivalence, 
namely 
that 
two 
networks 
which 
are 
Markov 
equivalent 
should 
have 
the 
same 
score. 
How 
dense 
the 
resulting 
network 
is 
can 
be 
sensitive 
to 
[263, 
250, 
262]. 
Including 
an 
explicit 
prior 
p(M) 
on 
the 
networks 
to 
favour 
those 
with 
sparse 
connections 
is 
also 
a 
sensible 
idea, 
for 
which 
one 
modied 
the 
score 
to 
p(DjM)p(M). 


Searching 
over 
structures 
is 
a 
computationally 
demanding 
task. 
However, 
since 
the 
log-score 
decomposes 
into 
terms 
involving 
each 
family 
of 
v, 
we 
can 
compare 
two 
networks 
diering 
in 
a 
single 
arc 
eciently. 
Search 
heuristics 
based 
on 
local 
addition/removal/reversal 
of 
links 
[66, 
129] 
that 
increase 
the 
score 
are 
popular[129]. 
In 
learnBayesNet.m 
we 
simplify 
the 
problem 
for 
demonstration 
purposes 
in 
which 
we 
assume 
we 
know 
the 
ancestral 
order 
of 
the 
variables, 
and 
also 
the 
maximal 
number 
of 
parents 
of 
each 
variable. 


Example 
42 
(PC 
algorithm 
versus 
network 
scoring). 
In 
g(9.13) 
we 
compare 
the 
PC 
algorithm 
with 
BD 
network 
scoring 
based 
(with 
Dirichlet 
hyperparameters 
set 
to 
unity) 
on 
1000 
samples 
from 
a 
known 
Belief 
Network. 
The 
PC 
algorithm 
conditional 
independence 
test 
is 
based 
on 
the 
Bayesian 
factor 
(9.3.60) 
in 
which 
Dirichlet 
priors 
with 
a 
=0:1 
were 
used 
throughout. 
In 
g(9.13) 
the 
network 
scoring 
technique 
outperforms 
the 
PC 
algorithm. 
This 
is 
partly 
explained 
by 
the 
network 
scoring 
technique 
being 
provided 
with 
the 
correct 
ancestral 
order 
and 
the 
constraint 
that 
each 
variable 
has 
maximally 
two 
parents. 


9.4 
Maximum 
Likelihood 
for 
Undirected 
models 
Consider 
a 
Markov 
network 
distribution 
p(X 
) 
dened 
on 
(not 
necessarily 
maximal) 
cliques 
Xc 
X 


Y

1 


p(Xj)= 
c(Xcjc) 
(9.4.1)

Z()

c 


where 
Z() 
=XXYc 
c(Xcjc) 
(9.4.2) 
DRAFT 
March 
9, 
2010 
185 



Maximum 
Likelihood 
for 
Undirected 
models 


ensures 
normalisation. 
Given 
a 
set 
of 
data, 
Xhn;n=1;:::;N, 
and 
assuming 
i.i.d. 
data, 
the 
log 
likelihood 
is 


XX

L() 
=log 
c(Xhnjc) 
..hNlog 
Z() 
(9.4.3)

c 
nc 


In 
general 
learning 
the 
optimal 
parameters 
c;c=1;:::;Cis 
awkward 
since 
they 
are 
coupled 
via 
Z(). 
Unlike 
the 
BN, 
the 
objective 
function 
does 
not 
split 
into 
a 
set 
of 
isolated 
parameter 
terms 
and 
in 
general 
we 
need 
to 
resort 
to 
numerical 
methods. 
In 
special 
cases, 
however, 
exact 
results 
still 
apply, 
in 
particular 
when 
the 
MN 
is 
decomposable 
and 
no 
constraints 
are 
placed 
on 
the 
form 
of 
the 
clique 
potentials, 
as 
we 
discuss 
in 
section(9.4.2). 
More 
generally, 
however, 
gradient 
based 
techniques 
may 
be 
used 
and 
also 
give 
insight 
into 
properties 
of 
the 
Maximum 
Likelihood 
solution. 


9.4.1 
The 
likelihood 
gradient 


X

@@@

L() 
=log 
c(Xhnjc) 
..hNlog 
c(Xcjc)(9.4.4)

c

@c@c@c

p(XCj)

n

where 
we 
used 
the 
result 




XY

@1 
@@

log 
Z()= 
c(Xcjc)c0(Xc0jc0) 
=log 
c(Xcjc)(9.4.5)

@cZ()@c@c

0p(Xcj)

X6c6

=c

The 
gradient 
can 
then 
be 
used 
as 
part 
of 
a 
standard 
numerical 
optimisation 
package. 


Exponential 
form 
potentials 


A 
common 
form 
of 
parameterisation 
is 
to 
use 
an 
exponential 
form 


T (XC)

c

c(Xc)= 
e(9.4.6) 
where 
. 
are 
the 
parameters 
and 
 (XC) 
is 
a 
xed 
`feature 
function’ 
dened 
on 
the 
variables 
of 
clique 
c.

c

Dierentiating 
with 
respect 
to 
. 
and 
equating 
to 
zero, 
we 
obtain 
that 
the 
Maximum 
Likelihood 
solution 
satises 
that 
the 
empirical 
average 
of 
a 
feature 
function 
matches 
the 
average 
of 
the 
feature 
function 
with 


N

respect 
to 
the 
model:
h c(XC)i(Xc) 
=h (XC)icp(Xc) 
(9.4.7)
(Xc) 
= 
1 
](Xc) 
(9.4.8) 


where](Xc) 
is 
the 
number 
of 
times 
the 
clique 
state 
Xcis 
observed 
in 
the 
dataset. 
An 
intuitive 
interpretation 
is 
to 
sample 
states 
Xhfrom 
the 
trained 
model 
p(Xh) 
and 
use 
these 
to 
compute 
the 
average 
of 
each 
feature 
function. 
In 
the 
limit 
of 
an 
innite 
number 
of 
samples, 
for 
a 
Maximum 
Likelihood 
optimal 
model, 
these 
sample 
averages 
will 
match 
those 
based 
on 
the 
empirical 
average. 


Unconstrained 
potentials 


For 
unconstrained 
potentials 
we 
have 
a 
separate 
table 
for 
each 
of 
the 
states 
dened 
on 
the 
clique. 
This 
means 
we 
may 
write 


Y

)I[Yc 
=X6n]

c(Xc
n)=(Ycc 
(9.4.9) 


Yc 


where 
the 
product 
is 
over 
all 
states 
of 
potential 
c. 
This 
expression 
follows 
since 
the 
indicator 
is 
zero 
for 
all 
but 
the 
single 
observed 
state 
Xhn. 
The 
log 
likelihood 
is 
then 


c

XXX

L()=I[Yc= 
Xhn] 
log 
c(Yc) 
..hNlog 
Z() 
(9.4.10)

c
cYcn

DRAFT 
March 
9, 
2010 



Maximum 
Likelihood 
for 
Undirected 
models 


x1x2
x3
x4
x5x6
 (x1;x2) (x2) (x2;x3;x5) (x5) (x5;x6)
 (x2;x4;x5)
 (x2;x5)x1x2;x3;x5
x4
x6
(a) 
(b) 
(c) 
Figure 
9.14: 
(a): 
A 
decomposable 
Markov 
network. 
(b): 
A 
junction 
tree 
for 
(a). 
(c): 
Set 
chain 
for 


(a) 
formed 
by 
choosing 
clique 
x2;x3;x5 
as 
root 
and 
orienting 
edges 
consistently 
away 
from 
the 
root. 
Each 
separator 
is 
absorbed 
into 
its 
child 
clique 
to 
form 
the 
set 
chain. 
where 


XY

Z()=c(Yc) 
(9.4.11) 
Yc 


Dierentiating 
the 
log 
likelihood 
with 
respect 
to 
a 
specic 
table 
entry 
(Yc) 
we 
obtain

X

1 
p(Yc)

I[Yc 
= 
X 
n] 
- 
N 
(9.4.12)

c 


c(Yc) 
c(Yc)

n 


Equating 
to 
zero, 
the 
Maximum 
Likelihood 
solution 
is 
obtained 
when 


p(Yc) 
= 
(Yc) 
= 
1 
NXI[Yc 
= 
X 
n 
c 
] 
(9.4.13) 
n 


That 
is, 
the 
unconstrained 
optimal 
Maximum 
Likelihood 
solution 
is 
given 
by 
setting 
the 
clique 
potentials 
such 
that 
the 
marginal 
distribution 
on 
each 
clique 
p(Yc) 
matches 
the 
empirical 
distribution 
on 
each 
clique 
(Yc). 


9.4.2 
Decomposable 
Markov 
networks 
In 
the 
case 
that 
there 
is 
no 
constraint 
placed 
on 
the 
form 
of 
the 
factors 
c 
and 
if 
the 
MN 
corresponding 
to 
these 
potentials 
is 
decomposable, 
then 
we 
know 
(from 
the 
junction 
tree 
representation) 
that 
we 
can 
express 
the 
distribution 
in 
the 
form 
of 
a 
product 
of 
local 
marginals 
divided 
by 
the 
separator 
distributions 


Q

p(XC)

c

p(X 
)=Q(9.4.14) 


p(XS 
)

s 


By 
reabsorbing 
the 
separators 
into 
the 
numerator 
terms, 
we 
can 
form 
a 
set 
chain 
distribution, 
section(6.8) 


Y

p(X 
)=p(XCjXS 
) 
(9.4.15) 


c 


Since 
this 
is 
directed, 
the 
Maximum 
Likelihood 
solution 
to 
learning 
the 
tables 
is 
trivial 
since 
we 
assign 
each 
set 
chain 
factor 
p(XCjXS 
) 
by 
counting 
the 
instances 
in 
the 
dataset[168], 
see 
learnMarkovDecom.m. 
The 
procedure 
is 
perhaps 
best 
explained 
by 
an 
example, 
as 
given 
below. 
See 
algorithm(5) 
for 
a 
general 
description. 


Example 
43. 
Given 
a 
dataset 
V 
= 
fX 
n;n 
=1;:::;Ng, 
we 
wish 
to 
t 
by 
Maximum 
Likelihood 
a 
MN 
of 
the 
form 


1 


p(x1;:::;x6)= 
(x1;x2)(x2;x3;x5)(x2;x4;x5)(x5;x6) 
(9.4.16)

Z 


DRAFT 
March 
9, 
2010 



Maximum 
Likelihood 
for 
Undirected 
models 


Algorithm 
5 
Learning 
of 
an 
unconstrained 
decomposable 
Markov 
network 
using 
Maximum 
Likelihood. 
We 
have 
a 
triangulated 
(decomposable) 
Markov 
network 
on 
cliques 
c(Xc), 
c 
=1;:::;C 
and 
the 
empirical 
marginal 
distributions 
on 
all 
cliques 
and 
separators, 
(Xc), 
(Xs) 


1: 
Form 
a 
junction 
tree 
from 
the 
cliques. 
2: 
Initialise 
each 
clique 
 c(Xc) 
to 
(Xc) 
and 
each 
separator 
 s(Xs) 
to 
(Xs). 
3: 
Choose 
a 
root 
clique 
on 
the 
junction 
tree 
and 
orient 
edges 
consistently 
away 
from 
this 
root. 
4: 
For 
this 
oriented 
junction 
tree, 
divide 
each 
clique 
by 
its 
parent 
separator. 
5: 
Return 
the 
new 
potentials 
on 
each 
clique 
as 
the 
Maximum 
Likelihood 
solution. 
where 
the 
potentials 
are 
unconstrained 
tables, 
see 
g(9.14a). 
Since 
the 
graph 
is 
decomposable, 
we 
know 
it 
admits 
a 
factorisation 
of 
clique 
potentials 
divided 
by 
the 
separators: 


p(x1;x2)p(x2;x3;x5)p(x2;x4;x5)p(x5;x6) 


p(x1;:::;x6) 
= 
(9.4.17) 


p(x2)p(x2;x5)p(x5) 


We 
can 
convert 
this 
to 
a 
set 
chain 
by 
reabsorbing 
the 
denominators 
into 
numerator 
terms, 
see 
section(6.8). 
For 
example, 
by 
choosing 
the 
clique 
x2;x3;x5 
as 
root,we 
can 
write 


p(x1;:::;x6)= 
(x1jx2)(x2;x3;x5)(x4jx2;x5)(x6jx5) 
(9.4.18)

p
p|
{z}p 
p
|X{z}p 
p
|X{z}p
p|
{z}X

 (x1;x2) 
 (x2;x3;x5) 
 (x2;x4;x5) 
 (x5;x6) 


where 
we 
identied 
the 
factors 
with 
clique 
potentials, 
and 
the 
normalisation 
constant 
Z 
is 
unity, 
see 
g(9.14b). 
The 
advantage 
is 
that 
in 
this 
representation, 
the 
clique 
potentials 
are 
independent 
since 
the 


n

distribution 
is 
a 
BN 
on 
cluster 
variables. 
The 
log 
likelihood 
for 
an 
i.i.d. 
dataset 
X 
= 
fx;n 
=1;:::;Ngis 


X

nn 
nnn 
nnn 
nn

L 
=log 
p(x1 
jx 
) 
+ 
log 
p(x2 
;x3 
;x 
) 
+ 
log 
p(x4 
jx2 
;x 
) 
+ 
log 
p(x6 
jx 
) 
(9.4.19)

2 
555 


n 


where 
each 
of 
the 
terms 
is 
an 
independent 
parameter 
of 
the 
model. 
The 
Maximum 
Likelihood 
solution 
then 
corresponds 
(as 
for 
the 
BN 
case) 
to 
simply 
setting 
each 
factor 
to 
the 
datacounts. 
For 
example 


. 
(x2;x4;x5) 
. 
(x2;x3;x5)

(x2;x4;x5)= 
p(x4jx2;x5)= 
;(x2;x3;x5)= 
p(x2;x3;x5) 
= 
(9.4.20)

. 
(x2;x5) 
N 
9.4.3 
Non-decomposable 
Markov 
networks 
In 
the 
non-decomposable 
or 
constrained 
case, 
no 
closed 
form 
Maximum 
Likelihood 
solution 
generally 
exists 
and 
one 
needs 
to 
resort 
to 
numerical 
methods. 
According 
to 
equation 
(9.4.13) 
the 
Maximum 
Likelihood 
solution 
is 
such 
that 
the 
clique 
marginals 
match 
the 
empirical 
marginals. 
Assuming 
that 
we 
can 
absorb 
the 
normalisation 
constant 
into 
an 
arbitrarily 
chosen 
clique, 
we 
can 
drop 
explicitly 
representing 
the 
normalisation 
constant. 
For 
a 
clique 
c, 
the 
requirement 
that 
the 
marginal 
of 
p 
matches 
the 
empirical 
marginal 
on 
the 
variables 
in 
the 
clique 
is 


XY

(Xc)(Xd)= 
(Xc) 
(9.4.21) 
Xncd6

=c 


Given 
an 
initial 
setting 
for 
the 
potentials 
we 
can 
then 
update 
(Xc) 
to 
satisfy 
the 
above 
marginal 
requirement, 


(Xc)

new(Xc

)= 
PQ(9.4.22) 
Xncd6(Xd)

=c 


DRAFT 
March 
9, 
2010 



Maximum 
Likelihood 
for 
Undirected 
models 


which 
is 
required 
for 
each 
of 
the 
states 
of 
Xc. 
By 
multiplying 
and 
dividing 
the 
right 
hand 
side 
by 
(Xc) 
this 
is 
equivalent 
to 


new(Xc
(Xc)(Xc)

) 
= 
(9.4.23) 


p(Xc) 


One 
can 
view 
this 
IPF 
update 
as 
coordinate-wise 
optimisation 
of 
the 
log 
likelihood 
in 
which 
the 
coordinate 
corresponds 
to 
c(Xc), 
with 
all 
other 
parameters 
xed. 
In 
this 
case 
this 
conditional 
optimum 
is 
analytically 
given 
by 
the 
above 
setting. 
One 
proceeds 
by 
selecting 
another 
potential 
to 
update. 
Note 
that 
in 
general, 
with 
each 
update, 
the 
marginal 
p(Xc) 
need 
to 
be 
recomputed. 
Computing 
these 
marginals 
may 
be 
expensive 
unless 
the 
width 
of 
the 
junction 
tree 
formed 
from 
the 
graph 
is 
suitably 
limited. 


Example 
44 
(Boltzmann 
Machine 
learning). 
We 
dene 
the 
BM 
as 


X

1 
11 


vTWv 
vTWv 


p(vjW)= 
e2 
;Z(W)= 
e2 
(9.4.24)

Z(W)

v 


	

v1

for 
symmetric 
W 
and 
binary 
variables 
dom(vi)= 
f0;1g. 
Given 
a 
set 
of 
training 
data, 
D6=;:::;vN, 


the 
log 
likelihood 
is 


N

XY

L(W)= 
1 
(vn)T 
Wvn 
..6Nlog 
Z(W) 
(9.4.25)

2 


n=1 


Dierentiating 
w.r.t. 
wij, 
i6

= 
jwe 
have 
the 
gradient 


N

X

@L

nn

= 
vi 
vj 
..hvivji6(9.4.26)

p(vjW)

@wij 


n=1

A 
simple 
algorithm 
to 
optimise 
the 
weight 
matrix 
W 
is 
to 
use 
gradient 
ascent, 


N

XY

new 
old 
nn 


wij 
= 
wij 
+ 
vi 
vj 
..hvivjip(vjW)(9.4.27) 
n=1

for 
a 
learning 
rate 
>0. 
The 
intuitive 
interpretation 
is 
that 
learning 
will 
stop 
(the 
gradient 
is 
zero) 
when 


PY

nn

the 
second 
order 
statistics 
of 
the 
model 
hvivjip(vjW) 
match 
those 
of 
the 
empirical 
distribution, 
n 
vi 
vj 
=N. 
BM 
learning 
however 
is 
dicult 
since 
hvivjiis 
typically 
computationally 
intractable 
for 
an 
arbitrary 


p(vjW)

interaction 
matrix 
W 
and 
therefore 
needs 
to 
be 
approximated. 
Indeed, 
one 
cannot 
compute 
the 
likelihood 
L(W) 
exactly 
so 
that 
monitoring 
performance 
is 
also 
dicult. 


9.4.4 
Constrained 
decomposable 
Markov 
networks 
If 
there 
are 
no 
constraints 
on 
the 
forms 
of 
the 
maximal 
clique 
potentials 
of 
the 
Markov 
network, 
as 
we've 
seen, 
learning 
is 
straightforward. 
Here 
our 
interest 
is 
when 
the 
functional 
form 
of 
the 
maximal 
clique 
is 
constrained 
to 
be 
a 
product 
of 
potentials 
on 
smaller 
cliques2: 


Y

c(Xc)=i 
(X6i) 
(9.4.28)

cc 
i 


with 
no 
constraint 
being 
placed 
on 
the 
non-maximal 
clique 
potentials 
i 
(X6i). 
In 
general, 
in 
this 
case 


cc 


one 
cannot 
write 
down 
directly 
the 
Maximum 
Likelihood 
solution 
for 
the 
non-maximal 
clique 
potentials 
i 
(X6i).

cc 


2A 
Boltzmann 
machine 
is 
of 
this 
form 
since 
any 
unconstrained 
binary 
pairwise 
potentials 
can 
be 
converted 
into 
a 
BM. 
For 
other 
cases 
in 
which 
the 
i
c 
are 
constrained, 
then 
Iterative 
scaling 
may 
be 
used 
in 
place 
of 
IPF. 


DRAFT 
March 
9, 
2010 
189 



Maximum 
Likelihood 
for 
Undirected 
models 


x1
x2
x3
x4x51;41;5x1;41;21;4x2;42;32;43;4
(a) 
(b) 
Figure 
9.15: 
(a): 
Interpreted 
as 
a 
Markov 
network, 
the 
graph 
represents 
the 
distribution 
(x1;x4;x5)(x1;x2;x4)(x2;x4;x3). 
As 
a 
pairwise 
MN, 
the 
graph 
represents 
(x4;x5)(x1;x4)(x4;x5)(x1;x2)(x2;x4)(x2;x3)(x3;x4). 
(b): 
A 
junction 
tree 
for 
the 
pair-
wise 
MN 
in 
(a). 
We 
have 
a 
choice 
were 
to 
place 
the 
pairwise 
cliques, 
and 
this 
is 
one 
valid 
choice, 
using 
the 
shorthand 
a;b 
= 
a;b(xa;xb) 
and 
xa;b 
= 
fxa;xbg. 


Consider 
the 
graph 
in 
g(9.15). 
In 
the 
constrained 
case, 
in 
which 
we 
interpret 
the 
graph 
as 
a 
pairwise 
MN, 
IPF 
may 
be 
used 
to 
learn 
the 
pairwise 
tables. 
Since 
the 
graph 
is 
decomposable, 
there 
are 
however, 
computational 
savings 
that 
can 
be 
made 
in 
this 
case[11]. 
For 
an 
empirical 
distribution 
, 
Maximum 
Likelihood 
requires 
that 
all 
the 
pairwise 
marginals 
of 
the 
MN 
match 
the 
corresponding 
marginals 
obtained 
from 
. 
As 
explained 
in 
g(9.15) 
we 
have 
a 
choice 
as 
to 
which 
junction 
tree 
clique 
each 
potential 
is 
assigned 
to, 
with 
one 
valid 
choice 
being 
given 
in 
g(9.15b). 
Keeping 
the 
potentials 
of 
the 
cliques 
1;41;5 
and 
2;32;43;4 
xed 
we 
can 
update 
the 
potentials 
of 
clique 
1;21;4. 
Using 
a 
bar 
to 
denote 
xed 
potentials, 
we 
the 
marginal 
requirement 
that 
the 
MN 
matches 
the 
empirical 
marginal 
(x1;x2) 
can 
be 
written 
in 


shorthand 
as 
p(x1, 
x2) 
=X¯ 
1;5 
¯ 
4;51;41;2 
¯ 
2;4 
¯ 
2;3 
¯ 
3;4 
= 
(x1, 
x2) 
(9.4.29) 
x3;x4;x5 
which 
can 
be 
expressed 
as
 !X !XXX¯ 
1;5 
¯ 
4;5 
1;41;2 
X¯ 
2;4 
¯ 
2;3 
¯ 
3;4 
= 
(x1, 
x2) 
(9.4.30) 
x4 
|X
x5 
{zX}|X
x3 
{zX}1;4 
2;4 


The 
`messages’ 
1;4 
and 
1;2 
are 
the 
boundary 
separator 
tables 
when 
we 
choose 
the 
central 
clique 
as 
root 
and 
carry 
out 
absorption 
towards 
the 
root. 
Given 
these 
xed 
messages 
we 
can 
then 
perform 
updates 
of 
the 
root 
clique 
using 


(x1;x2)

new 


1;2 
= 
P1;41;42;4 
(9.4.31) 
x4 


After 
making 
this 
update, 
we 
can 
subsequently 
update 
1;4 
similarly 
using 
the 
constraint

XXX

¯ 
¯ 


1;54;5 
1;41;2 
2;42;33;4 
= 
(x1;x4) 
(9.4.32) 


x2 
x5 
x3

|{z}|{z


1;4 
2;4 


so 
that 


(x1;x4)

new 


= 
P(9.4.33)

1;4 
1;41;22;4

x2 


Given 
converged 
updates 
for 
this 
clique, 
we 
can 
choose 
another 
clique 
as 
root, 
propagate 
towards 
the 
root 
and 
compute 
the 
separator 
cliques 
on 
the 
boundary 
of 
the 
root. 
Given 
these 
xed 
boundary 
clique 
potentials 
we 
perform 
IPF 
within 
the 
clique. 


DRAFT 
March 
9, 
2010 



Maximum 
Likelihood 
for 
Undirected 
models 


Algorithm 
6 
Ecient 
Iterative 
Proportional 
Fitting. 
Given 
a 
set 
of 
i, 
i 
=1;:::;I 
and 
a 
corresponding 
set 
of 
reference 
(empirical) 
marginal 
distributions 
on 
the 
variables 
of 
each 
potential, 
i, 
we 
aim 
to 
set 
all 
f 
such 
that 
all 
marginals 
of 
the 
Markov 
network 
match 
the 
given 
empirical 
marginals. 


1: 
Given 
a 
Markov 
network 
on 
potentials 
i, 
i 
=1;:::;I, 
triangulate 
the 
graph 
and 
form 
the 
cliques 
C1;:::, 
CC 
. 
2: 
Assign 
potentials 
to 
cliques. 
Thus 
each 
clique 
has 
a 
set 
of 
associated 
potentials 
Fc 
3: 
Initialise 
all 
potentials 
(for 
example 
to 
unity). 
4: 
repeat 
5: 
Choose 
a 
clique 
c 
as 
root. 
6: 
Propagate 
messages 
towards 
the 
root 
and 
compute 
the 
separators 
on 
the 
boundary 
of 
the 
root. 
7: 
repeat 
8: 
Choose 
a 
potential 
i 
in 
clique 
c, 
i 
2Fc. 
9: 
Perform 
an 
IPF 
update 
for 
i, 
given 
xed 
boundary 
separators 
and 
other 
potentials 
in 
c. 
10: 
until 
Potentials 
in 
clique 
c 
converge. 
11: 
until 
All 
Markov 
network 
marginals 
converge 
to 
the 
reference 
marginals. 
This 
`ecient’ 
IPF 
procedure 
is 
described 
more 
generally 
in 
algorithm(6) 
for 
an 
empirical 
distribution 
. 
More 
generally, 
IPF 
minimises 
the 
Kullback-Leibler 
divergence 
between 
a 
given 
reference 
distribution 
E 
and 
the 
Markov 
network. 
See 
demoIPFeff.m 
and 
IPF.m. 


Example 
45. 
In 
g(9.17) 
36 
examples 
of 
18 
× 
14 
= 
252 
binary 
pixel 
handwritten 
twos 
are 
presented, 
forming 
the 
training 
set 
from 
which 
we 
wish 
to 
t 
a 
Markov 
network. 
First 
all 
pairwise 
empirical 
entropies 
H(xi;xj), 
i, 
j 
=1;:::, 
252 
were 
computed 
and 
used 
to 
rank 
edges, 
with 
highest 
entropy 
edges 
ranked 
rst. 
Edges 
were 
included 
in 
a 
graph 
G, 
highest 
ranked 
rst, 
provided 
the 
triangulated 
G 
had 
all 
cliques 
less 
than 
size 
15. 
This 
resulted 
in 
238 
unique 
cliques 
and 
an 
adjacency 
matrix 
for 
the 
triangulated 
G 
as 
presented 
in 
g(9.16a). 
In 
g(9.16b) 
the 
number 
of 
times 
that 
a 
pixel 
appears 
in 
the 
238 
cliques 
is 
shown, 
and 
indicates 
the 
degree 
of 
importance 
of 
each 
pixel 
in 
distinguishing 
between 
the 
36 
examples. 
Two 
models 
were 
then 
trained 
and 
used 
to 
compute 
the 
most 
likely 
reconstruction 
based 
on 
missing 
data 
p(xmissingjxvisible). 
The 
rst 
model 
was 
a 
Markov 
network 
on 
the 
maximal 
cliques 
of 
the 
graph, 
for 
which 
essentially 
no 
training 
is 
required, 
and 
the 
settings 
for 
each 
clique 
potential 
can 
be 
obtained 
as 
explained 
in 
algorithm(5). 
The 
model 
makes 
3.8% 
errors 
in 
reconstruction 
of 
the 
missing 
pixels. 
Note 
that 
the 
unfortunate 
eect 
of 
reconstructing 
a 
white 
pixel 
surrounded 
by 
black 
pixels 
is 
an 
eect 
of 
the 
limited 
training 
data. 
With 
larger 
amounts 
of 
data 
the 
model 
would 
recognise 
that 
such 
eects 
do 
not 
occur. 
In 
the 
second 
model, 
the 
same 
maximal 
cliques 
were 
used, 
but 
the 
maximal 
clique 
potentials 
restricted 
to 
be 
the 
product 
of 
all 
pairwise 
two-cliques 
within 
the 
maximal 
clique. 
This 
is 
equivalent 
to 
using 
a 
Boltzmann 
machine, 
and 
was 
trained 
using 
the 
ecient 
IPF 
approach 
of 
algorithm(6). 
The 
corresponding 
reconstruction 
error 
is 
20%. 
This 
performance 
is 
worse 
than 
the 
unconstrained 
network 
since 
the 
Boltzmann 
machine 
is 
a 
highly 
constrained 
Markov 
network. 
See 
demoLearnDecMN.m. 


5010015020025050100150200250 
24681012142468101214161820406080100120140160180200220
Figure 
9.16: 
(a): 
Based 
on 
the 
pairwise 
empirical 
entropies 
H(xi;xj) 
edges 
are 
ordered, 
high 
entropy 
edges 
rst. 
Shown 
is 
the 
adjacency 
matrix 
of 
the 
resulting 
Markov 
network 
whose 
junction 
tree 
has 
cliques 
= 
15 
in 
size 
(white 
represents 
an 
edge). 
(b): 
Indicated 
are 
the 
number 
of 
cliques 
that 
each 
pixel 
is 
a 
member 
of, 
indicating 
a 
degree 
of 
importance. 
Note 
that 
the 
lowest 
clique 
membership 
value 
is 
1, 
so 


(a) 
(b) 
that 
each 
pixel 
is 
a 
member 
of 
at 
least 
one 
clique. 


DRAFT 
March 
9, 
2010 



Maximum 
Likelihood 
for 
Undirected 
models 



Figure 
9.17: 
Learning 
digits 
(from 
Simon 
Lucas’ 
algoval 
system) 
using 
a 
Markov 
network. 
Top 
row: 
the 
36 
training 
examples. 
Each 
example 
is 
a 
binary 
image 
on 
18 
× 
14 
pixels. 
Second 
row: 
the 
training 
data 
with 
50% 
missing 
pixels 
(grey 
represents 
a 
missing 
pixel). 
Third 
row: 
Reconstructions 
from 
the 
missing 
data 
using 
a 
thin-junction-tree 
MN 
with 
maximum 
clique 
size 
15. 
Bottom 
row: 
Reconstructions 
using 
a 
thin-junction-tree 
Boltzmann 
machine 
with 
maximum 
clique 
size 
15, 
trained 
using 
ecient 
IPF. 


9.4.5 
Iterative 
scaling 
We 
consider 
Markov 
networks 
of 
the 
exponential 
form 


Y

1 


cfc(Vc)

p(Vj)= 
e 
(9.4.34)

Z()

c 


where 
fc(Vc) 
= 
0 
and 
c 
ranges 
of 
the 
non-maximal 
cliques 
Vc 
V. 
The 
normalisation 
requirement 
is 


XY

cfc(Vc)

Z()=e 
(9.4.35) 


Vc 


A 
Maximum 
Likelihood 
training 
algorithm 
for 
a 
Markov 
network, 
somewhat 
analogous 
to 
the 
EM 
approach 
of 
section(11.2) 
can 
be 
derived 
as 
follows[32]: 


Consider 
the 
bound, 
for 
positive 
x: 


log 
x 
= 
x 
- 
1 
)- 
log 
x 
= 
1 
- 
x 
(9.4.36) 


Hence 


Z() 
Z() 
Z()

- 
log 
= 
1 
- 
)- 
log 
Z() 
- 
log 
Z(old)+1 
- 
(9.4.37)

Z(old) 
Z(old) 
Z(old) 


Then 
we 
can 
write 
a 
bound 
on 
the 
log 
likelihood 


X

11 
Z()

L() 
= 
cfc(Vc
n) 
- 
log 
Z(old)+1 
- 
(9.4.38)

Z(old)

NN

c;n 


As 
it 
stands, 
the 
bound 
(9.4.38) 
is 
in 
general 
not 
straightforward 
to 
optimise 
since 
the 
parameters 
of 
each 
potential 
are 
coupled 
through 
the 
Z() 
term. 
For 
convenience 
it 
is 
useful 
to 
rst 
reparmameterise 
and 
write 


- 
old 
+old 


(9.4.39)
c 
= 
cc 
c

|{z}X

c 


Then 


XPXPP

fc(Vc)old

fc(Vc)c 
fc(Vc)c

c 
ccc

Z()=e=ee(9.4.40) 


VV 


One 
can 
decouple 
this 
using 
an 
additional 
bound 
derived 
by 
rst 
considering: 


PPP

cfc(Vc) 
pc[cfd(Vd)]

c 
cd

e= 
e(9.4.41) 


DRAFT 
March 
9, 
2010 



Maximum 
Likelihood 
for 
Undirected 
models 


where 


fc(Vc)

pc 
iP(9.4.42)

fd(Vd)

d 


P

Since 
pc 
i0 
andpc 
= 
1 
we 
may 
apply 
Jensen's 
inequality 
to 
give 


c 


PXP

cfc(Vc) 
fd(Vd)c

ec 
pced 
(9.4.43) 


c 


Hence 


XPXP

fc(Vc)old 


cfd(Vc)

Z() 
eccpce 
f 
(9.4.44) 


V 
c 


Plugging 
this 
bound 
into 
(9.4.38) 
we 
have 


()

XXDPE

11 


L() 
fc(Vn)c 
..pce 
cd 
fd(Vc)+1 
..ilog 
Z(old) 
(9.4.45)

c 


p(Vjold)
cn 


NN

|{z}X

LB(c) 


The 
term 
in 
curly 
brackets 
contains 
the 
potential 
parameters 
c 
in 
an 
uncoupled 
fashion. 
Dierentiating 
with 
respect 
to 
c 
the 
gradient 
of 
each 
lower 
bound 
is 
given 
by 


DE

X

@LB(c)1 
(c..old)P

= 
fc(Vn) 
..fc(Vc)ecd 
fd(Vd)(9.4.46)

c

@c 
Np(Vjold)

n 


This 
can 
be 
used 
as 
part 
of 
a 
gradient 
based 
optimisation 
procedure 
to 
learn 
the 
parameters 
c. 
A 
potential 
advantage 
over 
IPF 
is 
that 
all 
the 
parameters 
may 
be 
updated 
simultaneously, 
whereas 
in 
IPF 
they 
must 
be 
updated 
sequentially. 
Intuitively, 
the 
parameters 
converge 
when 
the 
empirical 
average 
of 
the 
functions 
f 
match 
the 
average 
of 
the 
functions 
with 
respect 
to 
samples 
drawn 
from 
the 
distribution, 
in 
line 
with 
our 
general 
condition 
for 
Maximum 
Likelihood 
optimal 
solution. 


P

In 
the 
special 
case 
that 
the 
functions 
sum 
to 
1,fc(Vc) 
= 
1, 
the 
zero 
of 
the 
gradient 
can 
be 
found 


c 


analytically, 
giving 
the 
update 


X

1 


= 
old 


c 
+ 
log 
fc(Vn) 
..iloghfc(Vc)ip(Vcjold) 
(9.4.47)

cc

N

n 


The 
constraint 
that 
the 
features 
fc 
need 
to 
be 
non-negative 
can 
be 
relaxed 
at 
the 
expense 
of 
additional 
variational 
parameters, 
see 
exercise(129). 
In 
cases 
where 
the 
zero 
of 
the 
gradient 
cannot 
be 
computed 
analytically, 
there 
may 
be 
little 
advantage 
in 
general 
in 
using 
IS 
over 
standard 
gradient 
based 
procedures 
on 
the 
log 
likelihood 
directly 
[196]. 


If 
the 
junction 
tree 
formed 
from 
this 
exponential 
form 
Markov 
network 
has 
limited 
tree 
width, 
computational 
savings 
can 
be 
made 
by 
performing 
IPF 
over 
the 
cliques 
of 
the 
junction 
tree 
and 
updating 
the 
parameters 
. 
within 
each 
clique 
using 
IS[11]. 
This 
is 
a 
modied 
version 
of 
the 
constrained 
decomposable 
case. 
See 
also 
[274] 
for 
a 
unied 
treatment 
of 
propagation 
and 
scaling 
on 
junction 
trees. 


9.4.6 
Conditional 
random 
elds 
For 
an 
input 
x 
and 
output 
y, 
a 
CRF 
is 
dened 
by 
a 
conditional 
distribution 
[266, 
166] 


Y

1 


p(yjx)= 
k(y, 
x) 
(9.4.48)

Z(x)

k 


for 
(positive) 
potentials 
k(y, 
x).To 
make 
learning 
more 
straightforward, 
the 
potentials 
are 
usually 
dened 
as 
ekfk(y;x) 
for 
xed 
functions 
f(y, 
x) 
and 
parameters 
k. 
In 
this 
case 
the 
distribution 
of 
the 
output 
conditioned 
on 
the 
input 
is 


Y

1 


kfk(y;x)

p(yjx, 
)= 
e 
(9.4.49)

Z(x, 
)

k 


DRAFT 
March 
9, 
2010 
193 



Maximum 
Likelihood 
for 
Undirected 
models 


n

For 
an 
i.i.d. 
dataset 
of 
input-outputs, 
Di= 
f(x;yn);n 
=1;:::;Ng, 
training 
based 
on 
conditional 
Maximum 
Likelihood 
requires 
the 
maximisation 
of 


NN

XXX

n

L() 
ilog 
p(y 
njx 
n;)= 
kfk(y 
;x 
n) 
..ilog 
Z(x 
n;) 
(9.4.50) 
n=1 
n=1k 


In 
general 
no 
closed 
form 
solution 
for 
the 
optimal 
. 
exists 
and 
this 
needs 
to 
be 
determined 
numerically. 
First 
we 
note 
that 
equation 
(9.4.49) 
is 
equivalent 
to 
equation 
(9.4.34) 
where 
the 
parameters 
. 
are 
here 
denoted 
by 
. 
and 
the 
variables 
v 
are 
here 
denoted 
by 
y. 
In 
the 
CRF 
case 
the 
inputs 
simply 
have 
the 
eect 
of 
determining 
the 
feature 
fk(y, 
x). 
In 
this 
sense 
iterative 
scaling, 
or 
any 
related 
method 
for 
Maximum 
Likelihood 
training 
of 
constrained 
Markov 
networks, 
may 
be 
readily 
adapted, 
taking 
advantage 
also 
of 
any 
computational 
savings 
from 
limited 
width 
junction 
trees. 


As 
an 
alternative 
here 
we 
briey 
describe 
gradient 
based 
training. 
The 
gradient 
has 
components 


X

n

@
L 
=fi(y 
;x 
n) 
..hfi(y, 
x 
n)i(9.4.51)

p(yjxn;)

@i 


n 


The 
terms 
hfi(y, 
xn)ican 
be 
problematic 
and 
their 
tractability 
depends 
on 
the 
structure 
of 
the 


p(yjxn;) 


potentials. 
For 
a 
multivariate 
y, 
provided 
the 
structure 
of 
the 
cliques 
dened 
on 
subsets 
of 
y 
is 
singly-
connected, 
then 
computing 
the 
average 
is 
generally 
tractable. 
More 
generally, 
provided 
the 
cliques 
of 
the 
resulting 
junction 
tree 
have 
limited 
width, 
then 
exact 
marginals 
are 
avaiable. 
An 
example 
of 
this 
is 
given 
for 
a 
linear-chain 
CRF 
in 
section(23.4.3) 
– 
see 
also 
example(46) 
below. 


Another 
quantity 
often 
useful 
for 
numerical 
optimisation 
is 
the 
Hessian 
which 
has 
components 


@2 
XL 
=(hfi(y, 
x 
n)ihfj(y, 
x 
n)i..hfi(y, 
x 
n)fj(y, 
x 
n)i) 
(9.4.52)

@i@j 


n 


where 
the 
averages 
above 
are 
with 
respect 
to 
p(yjxn;). 
This 
expression 
is 
a 
(negated) 
sum 
of 
covariance 
elements, 
and 
is 
therefore 
negative 
(semi) 
denite. 
Hence 
the 
function 
L() 
is 
concave 
and 
has 
only 
a 
single 
global 
optimum. 
Whilst 
no 
closed 
form 
solution 
for 
the 
optimal 
. 
exists, 
the 
optimal 
solutions 
can 
be 
found 
easily 
using 
a 
numerical 
technique 
such 
as 
conjugate 
gradients. 


In 
practice 
regularisation 
terms 
are 
often 
added 
to 
prevent 
overtting 
(see 
section(13.2.3) 
for 
a 
discussion 
of 
regularisation) 
. 
Using 
a 
term 


X

2

..ck2 
(9.4.53)

k 
k 


2

for 
positive 
regularisation 
constants 
cdiscourages 
the 
weights 
. 
from 
being 
too 
large. 
This 
term 
is 
also 


k 


negative 
denite 
and 
hence 
the 
overall 
objective 
function 
remains 
concave. 
Iterative 
Scaling 
may 
also 
be 
used 
to 
train 
a 
CRF 
though 
in 
practice 
gradient 
based 
techniques 
are 
to 
be 
preferred[196]. 




Once 
trained 
a 
CRF 
can 
be 
used 
for 
predicting 
the 
output 
distribution 
for 
a 
novel 
input 
x 
. 
The 
most 




likely 
output 
y 
is 
equivalently 
given 
by 


X

* 


y 
= 
argmax 
log 
p(yjx 
) 
= 
argmax 
kfk(y, 
x 
) 
..ilog 
Z(x 
* 
;) 
(9.4.54) 


yy

k 


Since 
the 
normalisation 
term 
is 
independent 
of 
y, 
nding 
the 
most 
likely 
output 
is 
equivalent 
to 


X

* 


y 
= 
argmax 
kfk(y, 
x 
) 
(9.4.55) 
y

k 


DRAFT 
March 
9, 
2010 



Maximum 
Likelihood 
for 
Undirected 
models 


24681012141618201232468101214161820123246810121416182012324681012141618201232468101214161820123
24681012141618201232468101214161820123246810121416182012324681012141618201232468101214161820123
(a) 
(b) 
Figure 
9.18: 
(a): 
Training 
results 
for 
a 
linear 
chain 
CRF. 
There 
are 
5 
training 
sequences, 
one 
per 
subpanel. 
In 
each 
the 
top 
row 
corresponds 
to 
the 
input 
sequence 
x1:20, 
xt 
2f1;:::, 
5} 
(each 
state 
represented 
by 
a 
dierent 
colour) 
the 
middle 
row, 
the 
correct 
output 
sequence 
y1:20, 
yt 
2f1, 
2, 
3} 
(each 
state 
represented 
by 
a 
dierent 
colour). 
Together 
the 
input 
and 
output 
sequences 
make 
the 
training 
data 
D. 
The 
bottom 
row 
contains 
the 
most 
likely 
output 
sequence 
given 
the 
trained 
CRF, 
arg 
maxy1:20 
p(y1:20jx1:20, 
D). 
(b): 
Five 
additional 
test 
sequences 
along 
with 
the 
correct 
output 
and 
predicted 
output 
sequence. 


Natural 
language 
processing 


In 
a 
natural 
language 
processing 
application, 
xt 
might 
represent 
a 
word 
and 
yt 
a 
corresponding 
linguistic 
tag 
(`noun',`verb', 
etc. 
). 
A 
more 
suitable 
form 
in 
this 
case 
is 
to 
constrain 
the 
CRF 
to 
be 
of 
the 
form 


XX

exp 
kgk(yt;yt..1)+lhl(yt;xt) 
(9.4.56) 
kl 


for 
binary 
functions 
gk 
and 
hl 
and 
parameters 
k 
and 
l. 
The 
grammatical 
structure 
of 
tag-tag 
transitions 
is 
encoded 
in 
gk(yt;yt..1) 
and 
linguistic 
tag 
information 
in 
hk(yt;xt), 
with 
the 
importance 
of 
these 
being 
determined 
by 
the 
corresponding 
parameters[166]. 
In 
this 
case 
inference 
of 
the 
marginals 
hytyt..1jx1:T 
) 
is 
straightforward 
since 
the 
factor 
graph 
corresponding 
to 
the 
inference 
problem 
is 
a 
linear 
chain. 


Variants 
of 
the 
linear 
chain 
CRF 
are 
used 
heavily 
in 
natural 
language 
processing, 
including 
part-of-speech 
tagging 
and 
machine 
translation 
(in 
which 
the 
input 
sequence 
x 
represents 
a 
sentence 
say 
in 
English 
and 
the 
output 
sequence 
y 
the 
corresponding 
translation 
into 
French). 
See, 
for 
example, 
[213]. 


Example 
46 
(Linear 
chain 
CRF). 
We 
consider 
a 
CRF 
with 
X 
= 
5 
input 
states 
and 
Y 
= 
3 
output 
states 
of 
the 
form 


T

YXPP

k 
kgk(yt;yt..1)+lhl(yt;xt)

l

p(y1:T 
jx1:T 
)= 
e(9.4.57) 


t=2 


Here 
the 
binary 
functions 
gk(yt;yt..1)= 
I[yt 
= 
ak] 
I[yt..1 
= 
bk], 
k 
=1;:::, 
9 
model 
the 
transitions 
between 
two 
consecutive 
outputs. 
The 
binary 
functions 
hl(yt;xt)= 
I[yt 
= 
al] 
I[xt 
= 
cl], 
l 
=1;:::, 
15 
model 
the 
translation 
of 
the 
input 
to 
the 
output. 
There 
are 
therefore 
9 
+ 
15 
= 
24 
parameters 
in 
total. 
In 
g(9.18) 
we 
plot 
the 
training 
and 
test 
results 
based 
on 
a 
small 
set 
of 
data. 
The 
training 
of 
the 
CRF 
is 
obtained 
using 
50 
iterations 
of 
gradient 
ascent 
with 
a 
learning 
rate 
of 
0.1. 
See 
demoLinearCRF.m. 


DRAFT 
March 
9, 
2010 



Properties 
of 
Maximum 
Likelihood 


9.4.7 
Pseudo 
likelihood 
Consider 
a 
MN 
on 
variables 
x 
with 
dim 
x 
= 
D 
of 
the 
form 


Y

1 


p(xj)= 
c(Xcjc) 
(9.4.58)

Z

c 


For 
all 
but 
specially 
constrained 
c, 
the 
partition 
function 
Z 
will 
be 
intractable 
and 
the 
likelihood 
of 
a 
set 
of 
i.i.d. 
data 
intractable 
as 
well. 
A 
surrogate 
is 
to 
use 
the 
pseudo 
likelihood 
of 
the 
probability 
of 
each 
variable 
conditioned 
on 
all 
other 
variables 
(which 
is 
equivalent 
to 
conditioning 
on 
only 
the 
variable's 
neighbours 
for 
a 
MN) 


ND

XXY

n

L0() 
= 
log 
p(xi 
jxn 
j) 
(9.4.59)

ni
n=1 
i=1 


n

The 
terms 
p(xjxn 
j) 
are 
usually 
straightforward 
to 
work 
out 
since 
they 
require 
nding 
the 
normalisation

i 
ni

of 
a 
univariate 
distribution 
only. 
In 
this 
case 
the 
gradient 
can 
be 
computed 
exactly, 
and 
learning 
of 
the 
parameters 
. 
carried 
out. 
At 
least 
for 
the 
case 
of 
the 
Boltzmann 
machine, 
this 
forms 
a 
consistent 
estimator[139]. 


9.4.8 
Learning 
the 
structure 
Learning 
the 
structure 
of 
a 
Markov 
network 
can 
also 
be 
based 
on 
independence 
tests, 
as 
for 
Belief 
networks. 
A 
criterion 
for 
nding 
a 
MN 
on 
a 
set 
of 
nodes 
V 
is 
to 
use 
the 
fact 
that 
no 
edge 
exits 
between 
x 
and 
y 
if, 
conditioned 
on 
all 
other 
nodes, 
x 
and 
y 
are 
deemed 
independent. 
This 
is 
the 
pairwise 
Markov 
property 
described 
in 
section(4.2.1). 
By 
checking 
x 
.
??yjVnfx, 
y} 
for 
every 
pair 
of 
variables 
x 
and 
y, 
this 
edge 
deletion 
approach 
in 
principle 
reveals 
the 
structure 
of 
the 
network[219]. 
For 
learning 
the 
structure 
from 
an 
oracle, 
this 
method 
is 
sound. 
However, 
a 
practical 
diculty 
in 
the 
case 
where 
the 
independencies 
are 
determined 
from 
data 
is 
that 
checking 
if 
x 
. 
??yjVnfx, 
y} 
requires 
in 
principle 
enormous 
amounts 
of 
data. 
The 
reason 
for 
this 
is 
that 
the 
conditioning 
selects 
only 
those 
parts 
of 
the 
dataset 
consistent 
with 
the 
conditioning. 
In 
practice 
this 
will 
result 
in 
very 
small 
numbers 
of 
remaining 
datapoints, 
and 
estimating 
independencies 
on 
this 
basis 
is 
unreliable. 


The 
Markov 
boundary 
criterion[219] 
uses 
the 
local 
Markov 
property, 
section(4.2.1), 
namely 
that 
conditioned 
on 
its 
neighbours, 
a 
variable 
is 
independent 
of 
all 
other 
variables 
in 
the 
graph. 
By 
starting 
with 
a 
variable 
x 
and 
an 
empty 
neighbourhood 
set, 
one 
can 
progressively 
include 
neighbours, 
testing 
if 
their 
inclusion 
renders 
the 
remaining 
non-neighbours 
independent 
of 
x. 
A 
dicultly 
with 
this 
is 
that, 
if 
one 
doesn't 
have 
the 
correct 
Markov 
boundary, 
then 
including 
a 
variable 
in 
the 
neighbourhood 
set 
may 
be 
deemed 
necessary. 
To 
see 
this, 
consider 
a 
network 
which 
corresponds 
to 
a 
linear 
chain 
and 
that 
x 
is 
at 
the 
edge 
of 
the 
chain. 
In 
this 
case, 
only 
the 
nearest 
neighbour 
of 
x 
is 
in 
the 
Markov 
boundary 
of 
x. 
However, 
if 
this 
nearest 
neighbour 
were 
not 
currently 
in 
the 
set, 
then 
any 
other 
non-nearest 
neighbour 
would 
be 
included, 
even 
though 
this 
is 
not 
strictly 
required. 
To 
counter 
this, 
the 
neighbourhood 
variables 
included 
in 
the 
neighbourhood 
of 
x 
may 
be 
later 
removed 
if 
they 
are 
deemed 
superuous 
to 
the 
boundary[102]. 


In 
cases 
where 
specic 
constraints 
are 
imposed, 
such 
as 
learning 
structures 
whose 
resulting 
triangulation 
has 
a 
bounded 
tree-width, 
whilst 
still 
formally 
dicult, 
approximate 
procedures 
are 
available[260]. 


In 
terms 
of 
network 
scoring 
methods 
for 
undirected 
networks, 
computing 
a 
score 
is 
hampered 
by 
the 
fact 
that 
the 
parameters 
of 
each 
clique 
become 
coupled 
in 
the 
normalisation 
constant 
of 
the 
distribution. 
This 
issue 
can 
be 
addressed 
using 
hyper 
Markov 
priors[75]. 


9.5 
Properties 
of 
Maximum 
Likelihood 
9.5.1 
Training 
assuming 
the 
correct 
model 
class 
n

Consider 
a 
dataset 
X 
= 
fx;n 
=1;:::;N} 
generated 
from 
an 
underlying 
parametric 
model 
p(xj0). 
Our 
interest 
is 
to 
t 
a 
model 
p(xj) 
of 
the 
same 
form 
as 
the 
correct 
underlying 
model 
p(xj0) 
and 
examine 


196 
DRAFT 
March 
9, 
2010 



Code 


whether 
if, 
in 
the 
limit 
of 
a 
large 
amount 
of 
data, 
the 
parameter 
. 
learned 
by 
Maximum 
Likelihood 
matches 
the 
correct 
parameter 
0. 
Our 
derivation 
below 
is 
non-rigorous, 
but 
highlights 
the 
essence 
of 
the 
argument. 


Assuming 
the 
data 
is 
i.i.d., 
the 
log 
likelihood 
L() 
= 
log 
p(Xj) 
is 


X

1 
N

L() 
= 
log 
p(x 
nj) 
(9.5.1)

N 


n=1 


In 
the 
limit 
N 
!1, 
the 
sample 
average 
can 
be 
replaced 
by 
an 
average 
with 
respect 
to 
the 
distribution 
generating 
the 
data 


..


L() 
N=!8 
hlog 
p(xj)) 
p(xj0) 
= 
..KLp(xj0)jp(xj)+log 
p(xj0)p(xj0) 
(9.5.2) 


Up 
to 
a 
negligible 
constant, 
this 
is 
the 
Kullback-Leibler 
divergence 
between 
two 
distributions 
in 
x, 
just 
with 
dierent 
parameter 
settings. 
The 
. 
that 
maximises 
L() 
is 
that 
which 
minimises 
the 
Kullback-
Leibler 
divergence, 
namely 
. 
= 
0 
. 
In 
the 
limit 
of 
a 
large 
amount 
of 
data 
we 
can, 
in 
principle, 
learn 
the 
correct 
parameters 
(assuming 
we 
know 
the 
correct 
model 
class). 
The 
property 
of 
an 
estimator 
such 
that 
the 
parameter 
. 
converges 
to 
the 
true 
model 
parameter 
0 
as 
the 
sequence 
of 
data 
increase 
is 
termed 
a 
consistency. 


9.5.2 
Training 
when 
the 
assumed 
model 
is 
incorrect 
We 
write 
q(xj) 
for 
the 
assumed 
model, 
and 
p(xj) 
for 
the 
correct 
generating 
model. 
Repeating 
the 
above 
calculations 
in 
the 
case 
of 
the 
assumed 
model 
being 
correct, 
we 
have 
that, 
in 
the 
limit 
of 
a 
large 
amount 
of 
data, 
the 
likelihood 
is 


L()= 
hlog 
q(xj)ip(xj) 
= 
..KL(p(xj)jq(xj)) 
+ 
hlog 
p(xj)ip(xj) 
(9.5.3) 


Since 
q 
and 
p 
are 
not 
of 
the 
same 
form, 
setting 
. 
to 
f 
does 
not 
necessarily 
minimise 
KL(p(xj)jq(xj)), 
and 
therefore 
does 
not 
necessarily 
optimize 
L(). 


9.6 
Code 
condindepEmp.m: 
Bayes 
test 
and 
Mutual 
Information 
for 
empirical 
conditional 
independence 
condMI.m: 
Conditional 
Mutual 
Information 
condMIemp.m: 
Conditional 
Mutual 
Information 
of 
Empirical 
distribution 
MIemp.m: 
Mutual 
Information 
of 
Empirical 
distribution 


9.6.1 
PC 
algorithm 
using 
an 
oracle 
This 
demo 
uses 
an 
oracle 
to 
determine 
x 
. 
??y 
| 
z, 
rather 
than 
using 
data 
to 
determine 
the 
empirical 
dependence. 
The 
oracle 
is 
itself 
a 
Belief 
Network. 
For 
the 
partial 
orientation 
only 
the 
rst 
`unmarried 
collider’ 
rule 
is 
implemented. 
demoPCoracle.m: 
Demo 
of 
PC 
algorithm 
with 
an 
oracle 
PCskeletonOracle.m: 
PC 
algorithm 
using 
an 
oracle 
PCorient.m: 
Orient 
a 
skeleton 


9.6.2 
Demo 
of 
empirical 
conditional 
independence 
For 
half 
of 
the 
experiments, 
the 
data 
is 
drawn 
from 
a 
distribution 
for 
which 
x.
??yjz 
is 
true. 
For 
the 
other 
half 
of 
the 
experiments, 
the 
data 
is 
drawn 
from 
a 
random 
distribution 
for 
which 
x.
??yjz 
is 
false. 
We 
then 
measure 
the 
fraction 
of 
experiments 
for 
which 
the 
Bayes 
test 
correctly 
decides 
x 
.
??y| 
z. 
We 
also 
measure 
the 
fraction 
of 
experiments 
for 
which 
the 
Mutual 
Information 
test 
correctly 
decides 
x 
. 
??y| 
z, 
based 
on 


DRAFT 
March 
9, 
2010 
197 



FuseDrumTonerPaperRollerBurningQualityWrinkledMult.PagesPaperJamFuseDrumTonerPaperRollerBurningQualityWrinkledMult.PagesPaperJam
Exercises 


Figure 
9.19: 
Printer 
Nightmare 
Belief 
Network. 
All 
variables 
are 
binary. 
The 
upper 
variables 
without 
parents 
are 
possible 
problems 
(diagnoses), 
and 
the 
lower 
variables 
consequences 
of 
problems 
(faults). 


setting 
the 
threshold 
equal 
to 
the 
median 
of 
all 
the 
empirical 
conditional 
mutual 
information 
values. 
A 
similar 
empirical 
threshold 
can 
also 
be 
obtained 
for 
the 
Bayes’ 
factor 
(although 
this 
is 
not 
strictly 
kosher 
in 
the 
pure 
Bayesian 
spirit 
since 
one 
should 
in 
principle 
set 
the 
threshold 
to 
zero). 
The 
test 
based 
on 
the 
assumed 
chi-squared 
distributed 
MI 
is 
included 
for 
comparison, 
although 
it 
seems 
to 
be 
impractical 
in 
these 
small 
data 
cases. 
demoCondIndepEmp.m: 
Demo 
of 
empirical 
conditional 
independence 
based 
on 
data 


9.6.3 
Bayes 
Dirichlet 
structure 
learning 
It 
is 
interesting 
to 
compare 
the 
result 
of 
demoPCdata.m 
with 
demoBDscore.m. 
PCskeletonData.m: 
PC 
algorithm 
using 
empirical 
conditional 
independence 
demoPCdata.m: 
Demo 
of 
PC 
algorithm 
with 
data 
BDscore.m: 
Bayes 
Dirichlet 
(BD) 
score 
for 
a 
node 
given 
parents 
learnBayesNet.m: 
Given 
an 
ancestral 
order 
and 
maximal 
parents, 
learn 
the 
network 
demoBDscore.m: 
Demo 
of 
structure 
learning 


9.7 
Exercises 
Exercise 
116 
(Printer 
Nightmare). 
Cheapco 
is, 
quite 
honestly, 
a 
pain 
in 
the 
neck. 
Not 
only 
did 
they 
buy 
a 
dodgy 
old 
laser 
printer 
from 
StopPress 
and 
use 
it 
mercilessly, 
but 
try 
to 
get 
away 
with 
using 
substandard 
components 
and 
materials. 
Unfortunately 
for 
StopPress, 
they 
have 
a 
contract 
to 
maintain 
Cheapco's 
old 
warhorse, 
and 
end 
up 
frequently 
sending 
the 
mechanic 
out 
to 
repair 
the 
printer. 
After 
the 
10th 
visit, 
they 
decide 
to 
make 
a 
statistical 
model 
of 
Cheapco's 
printer, 
so 
that 
they 
will 
have 
a 
reasonable 
idea 
of 
the 
fault 
based 
only 
on 
the 
information 
that 
Cheapco's 
secretary 
tells 
them 
on 
the 
phone. 
In 
that 
way, 
StopPress 
hopes 
to 
be 
able 
to 
send 
out 
to 
Cheapco 
only 
a 
junior 
repair 
mechanic, 
having 
most 
likely 
diagnosed 
the 
fault 
over 
the 
phone. 


Based 
on 
the 
manufacturer's 
information, 
StopPress 
has 
a 
good 
idea 
of 
the 
dependencies 
in 
the 
printer, 
and 
what 
is 
likely 
to 
directly 
aect 
other 
printer 
components. 
The 
Belief 
Network 
in 
g(9.19) 
represents 
these 
assumptions. 
However, 
the 
specic 
way 
that 
Cheapco 
abuse 
their 
printer 
is 
a 
mystery, 
so 
that 
the 
exact 
probabilistic 
relationships 
between 
the 
faults 
and 
problems 
is 
idiosyncratic 
to 
Cheapco. 
StopPress 
has 
the 
following 
table 
of 
faults 
for 
each 
of 
the 
10 
visits. 
Each 
column 
represents 
a 
visit. 


fuse 
assembly 
malfunction 
0 
0 
0 
1 
0 
0 
0 
0 
0 
0 
0 
0 
1 
0 
1 
drum 
unit 
0 
0 
0 
0 
1 
0 
0 
1 
0 
0 
1 
1 
0 
0 
0 
toner 
out 
1 
1 
0 
0 
0 
1 
0 
1 
0 
0 
0 
1 
0 
0 
0 
poor 
paper 
quality 
1 
0 
1 
0 
1 
0 
1 
0 
1 
1 
0 
1 
1 
0 
0 
worn 
roller 
0 
0 
0 
0 
0 
0 
1 
0 
0 
0 
0 
0 
0 
1 
1 
burning 
smell 
0 
0 
0 
1 
0 
0 
0 
0 
0 
0 
0 
0 
1 
0 
0 
poor 
print 
quality 
1 
1 
1 
0 
1 
1 
0 
1 
0 
0 
1 
1 
0 
0 
0 
wrinkled 
pages 
0 
0 
1 
0 
0 
0 
0 
0 
1 
0 
0 
0 
1 
1 
1 
multiple 
pages 
fed 
0 
0 
1 
0 
0 
0 
1 
0 
1 
0 
0 
0 
0 
0 
1 
paper 
jam 
0 
0 
1 
1 
0 
0 
1 
1 
1 
1 
0 
0 
0 
1 
0 


DRAFT 
March 
9, 
2010 



Exercises 


1. 
The 
above 
table 
is 
contained 
in 
printer.mat. 
Learn 
all 
table 
entries 
on 
the 
basis 
of 
Maximum 
Likelihood. 
2. 
Program 
the 
Belief 
Network 
using 
the 
tables 
Maximum 
Likelihood 
tables 
and 
BRMLtoolbox. 
Comp
ute 
the 
probability 
that 
there 
is 
a 
fuse 
assembly 
malfunction 
given 
that 
the 
secretary 
complains 
there 
is 
a 
burning 
smell 
and 
that 
the 
paper 
is 
jammed, 
and 
that 
there 
are 
no 
other 
problems. 
3. 
Repeat 
the 
above 
calculation 
using 
a 
Bayesian 
method 
in 
which 
a 
at 
Beta 
prior 
is 
used 
on 
all 
tables. 
4. 
Given 
the 
above 
information 
from 
the 
secretary, 
what 
is 
the 
most 
likely 
joint 
diagnosis 
over 
the 
diagnostic 
variables 
– 
that 
is 
the 
joint 
most 
likely 
p(F 
use, 
Drum, 
T 
oner, 
P 
aper, 
Rollerjevidence)? 
Use 
the 
max-absorption 
method 
on 
the 
associated 
junction 
tree. 
5. 
Compute 
the 
joint 
most 
likely 
state 
of 
the 
distribution 
p(F 
use, 
Drum, 
T 
oner, 
P 
aper, 
Rollerjburning 
smell, 
paper 
jammed) 


Explain 
how 
to 
compute 
this 
eciently 
using 
the 
max-absorption 
method. 


Exercise 
117. 
Explain 
how 
to 
use 
a 
factorised 
Beta 
prior 
in 
the 
case 
of 
learning 
table 
entries 
in 
Belief 
Networks 
in 
which 
each 
variable 
has 
maximally 
a 
single 
parent. 
Consider 
the 
issues 
around 
Bayesian 
Learning 
of 
binary 
table 
entries 
when 
the 
number 
of 
parental 
variables 
is 
not 
restricted. 


n

Exercise 
118. 
Consider 
data 
x;n 
=1;:::;N. 
Show 
that 
for 
a 
Gaussian 
distribution, 
the 
Maximum 


PN 
PN

1 
n 
1

Likelihood 
estimator 
of 
the 
mean 
is 
m^= 
and 
variance 
is 
^2 
=(xn 
- 
m^)2 
.

Nn=1 
x 
Nn=1

Exercise 
119. 
A 
training 
set 
consists 
of 
one 
dimensional 
examples 
from 
two 
classes. 
The 
training 
examp
les 
from 
class 
1 
are 


0:5, 
0:1, 
0:2, 
0:4, 
0:3, 
0:2, 
0:2, 
0:1, 
0:35, 
0:25 
(9.7.1) 


and 
from 
class 
2 
are 


0:9, 
0:8, 
0:75, 
1:0 
(9.7.2) 


Fit 
a 
(one 
dimensional) 
Gaussian 
using 
Maximum 
Likelihood 
to 
each 
of 
these 
two 
classes. 
Also 
estimate 
the 
class 
probabilities 
p1 
and 
p2 
using 
Maximum 
Likelihood. 
What 
is 
the 
probability 
that 
the 
test 
point 
x 
=0:6 
belongs 
to 
class 
1? 


Exercise 
120. 
For 
a 
set 
of 
N 
observations 
(training 
data), 
X 
= 
x1 
;:::, 
xN 
, 
and 
independently 
gathered 
observations, 
the 
log 
likelihood 
for 
a 
Belief 
network 
to 
generate 
X 
is 


N 
K

XX

nn

log 
p(X 
) 
= 
log 
p 
(xi 
jpa 
(x 
)) 
(9.7.3)

i 
n=1 
i=1 


We 
dene 
the 
notation 


i

(t)= 
p(xi 
= 
sjpa 
(xi)= 
t) 
(9.7.4)
s

meaning 
variable 
xi 
is 
in 
state 
s, 
and 
the 
parents 
of 
variable 
xi 
are 
in 
the 
vector 
of 
states 
t. 
Using 
a 
Lagrangian 


N 
KK

X 
XXX..

nn 
ii

L 
= 
log 
p 
(xi 
jpa 
(x 
)) 
+ 
i 
1 
..t 
(9.7.5)

i 
ti 
s
n=1 
i=1 
i=1ti 
s 


Show 
that 
the 
Maximum 
Likelihood 
setting 
of 
i(t) 
is

s

hihi

PN 


nn

I 
x= 
sIpax= 
tj

n=1 
jj
s 
j(tj)= 
PN 
Phihi(9.7.6) 


nn

I 
x= 
sIpax= 
tj

n=1 
s 
jj

DRAFT 
March 
9, 
2010 



Exercises 


Exercise 
121 
(Conditional 
Likelihood 
training). 
Consider 
a 
situation 
in 
which 
we 
partition 
observable 
variables 
into 
disjoint 
sets 
x 
and 
y 
and 
that 
we 
want 
to 
nd 
the 
parameters 
that 
maximize 
the 
conditional 
likelihood, 


X

1 
N

CL()= 
p(y 
njx 
n;), 
(9.7.7)

N 


n=1 


n

for 
a 
set 
of 
training 
data 
f(x;yn) 
;n 
=1;:::;Ng. 
All 
data 
is 
assumed 
generated 
from 
the 
same 
distribut
ion 
p(x, 
yj0)= 
p(yjx, 
0)p(xj0) 
for 
some 
unknown 
parameter 
0 
. 
In 
the 
limit 
of 
a 
large 
amount 
of 
i.i.d. 
training 
data, 
does 
CL() 
have 
an 
optimum 
at 
0? 


Exercise 
122 
(Moment 
Matching). 
One 
way 
to 
set 
parameters 
of 
a 
distribution 
is 
to 
match 
the 
moments 
of 
the 
distribution 
to 
the 
empirical 
moments. 
This 
sometimes 
corresponds 
to 
Maximum 
Likelihood 
(for 
the 
Gaussian 
distribution 
for 
example), 
though 
generally 
this 
is 
not 
consistent 
with 
Maximum 
Likelihood. 
For 
data 
with 
mean 
m 
and 
variance 
s, 
show 
that 
to 
t 
a 
Beta 
distribution 
by 
moment 
matching, 
we 
use 


m2(1 
..6m)1 
..6m 


a 
= 
..6m, 
ß 
= 
a 
(9.7.8) 


sm 


n

Exercise 
123. 
For 
data 
0 
6x61, 
n 
=1;:::;N, 
generated 
from 
a 
Beta 
distribution 
B 
(xja, 
b), 
show 
that 
the 
log 
likelihood 
is 
given 
by 


NN

XXY

L(a, 
b) 
6(a 
..61) 
log 
x 
n 
+(b 
..61) 
log(1 
..6x 
n) 
..6N 
log 
B(a, 
b) 
(9.7.9) 


n=1 
n=1 


where 
B(a, 
b) 
is 
the 
Beta 
function. 
Show 
that 
the 
derivatives 
are 


NN

XX

@
L 
= 
log 
x 
n 
..6 (a) 
..6 (a 
+ 
b), 
@
L 
= 
log(1 
..6x 
n) 
..6 (b) 
..6 (a 
+ 
b) 
(9.7.10)

@a 
@b 


n=1 
n=1 


where 
 (x) 
6d 
log 
..(x)=dx 
is 
the 
digamma 
function, 
and 
suggest 
a 
method 
to 
learn 
the 
parameters 
a,b. 


Exercise 
124. 
Consider 
the 
Boltzmann 
machine 
as 
dened 
in 
example(44). 


1. 
Derive 
the 
gradient 
with 
respect 
to 
the 
`biases’ 
wii. 
2. 
Write 
down 
the 
pseudo 
likelihood 
for 
a 
set 
of 
i.i.d. 
data 
v1 
;:::, 
vN 
and 
derive 
the 
gradient 
of 
this 
with 
respect 
to 
wij, 
i6
= 
j. 


Exercise 
125. 
Show 
that 
the 
model 
likelihood 
equation 
(9.3.54) 
can 
be 
written 
explicitly 
as 


YYY0

..(i 
ui(v; 
j)) 
..(u(v; 
j))

i

p(DjM)=P(9.7.11)

0

vj 
..((v; 
j))
i 
..(ui(v; 
j))

i 
ui

Exercise 
126. 
Dene 
the 
set 
N6as 
consisting 
of 
8 
node 
Belief 
Networks 
in 
which 
each 
node 
has 
at 
most 
2 
parents. 
For 
a 
given 
ancestral 
order 
a, 
the 
restricted 
set 
is 
written 
Na 


1. 
How 
many 
Belief 
Networks 
are 
in 
Na? 
2. 
What 
is 
the 
computational 
time 
to 
nd 
the 
optimal 
member 
of 
Na 
using 
the 
Bayesian 
Dirichlet 
score, 
assuming 
that 
computing 
the 
BD 
score 
of 
any 
member 
of 
Na 
takes 
1 
second 
and 
bearing 
in 
mind 
the 
decomposability 
of 
the 
BD 
score. 
3. 
What 
is 
the 
time 
to 
nd 
the 
optimal 
member 
of 
N6? 
Exercise 
127. 
For 
the 
Markov 
network 


1 


p(x, 
y, 
z)= 
1(x, 
y)2(y, 
z) 
(9.7.12)

Z 


derive 
an 
iterative 
scaling 
algorithm 
to 
learn 
the 
unconstrained 
tables 
1(x, 
y) 
and 
2(x, 
y) 
based 
on 
a 
set 
of 
i.i.d. 
data 
X6, 
Y, 
Z. 


DRAFT 
March 
9, 
2010 



Exercises 


1

Exercise 
128. 
Given 
training 
data 
x;:::;xn, 
derive 
an 
iterative 
scaling 
algorithm 
for 
Maximum 
Likelih
ood 
training 
of 
CRFs 
of 
the 
form 


Y

1 


cfc(x)

p(xj)= 
e 


Z()

c 


PQ

where 
Z()=ecfc(x) 
and 
non-negative 
features, 
fc(x) 
= 
0 
(you 
may 
assume 
that 
the 
features 


xc 


cannot 
all 
be 
zero 
for 
any 
given 
x). 


Exercise 
129. 
For 
data 
X 
1 
;:::, 
X 
N 
, 
consider 
Maximum 
Likelihood 
learning 
of 
a 
Markov 
network 
p(X 
)=

Q

c(Xc) 
with 
potentials 
of 
the 
form 


cfc(Xc)

c(Xc)= 
e 
(9.7.13) 


with 
fc(Xc) 
being 
general 
real 
valued 
functions 
and 
c 
real 
valued 
parameters. 
By 
considering

XX

cfc(Xc)

cfc(Xc)=pc 
(9.7.14) 
pc 


cc 


P

for 
auxiliary 
variables 
pc 
> 
0 
such 
thatpc 
=1, 
explain 
how 
to 
derive 
a 
form 
of 
iterative 
scaling 
training 


c 
algorithm 
in 
which 
each 
parameter 
c 
can 
be 
learned 
separately. 


DRAFT 
March 
9, 
2010 



Exercises 


DRAFT 
March 
9, 
2010 



CHAPTER 
10 


Naive 
Bayes 


10.1 
Naive 
Bayes 
and 
Conditional 
Independence 
Naive 
Bayes 
(NB) 
is 
a 
popular 
classication 
method 
and 
aids 
our 
discussion 
of 
conditional 
independence, 
overtting 
and 
Bayesian 
methods. 
In 
NB, 
we 
form 
a 
joint 
model 
of 
observations 
x 
and 
the 
corresponding 
class 
label 
c 
using 
a 
Belief 
network 
of 
the 
form 


D

YP

p(x;c)= 
p(c) 
p(xijc) 
(10.1.1) 


i=1 


whose 
Belief 
Network 
is 
depicted 
in 
g(10.1a). 
Coupled 
with 
a 
suitable 
choice 
for 
each 
conditional 
distribution 
p(xijc), 
we 
can 
then 
use 
Bayes’ 
rule 
to 
form 
a 
classier 
for 
a 
novel 
attribute 
vector 
x 
: 


p(x 
jc)p(c) 
p(x 
jc)p(c)

p(cjx 
)= 
= 
P(10.1.2) 


p(x 
) 
p(x 
jc)p(c)

c 


In 
practice 
it 
is 
common 
to 
consider 
only 
two 
classes 
dom(c)= 
f0, 
1g. 
The 
theory 
we 
describe 
below 
is 
valid 
for 
any 
number 
of 
classes 
c, 
though 
our 
examples 
are 
restricted 
to 
the 
binary 
class 
case. 
Also, 
the 
attributes 
xi 
are 
often 
taken 
to 
be 
binary, 
as 
we 
shall 
do 
initially 
below 
as 
well. 
The 
extension 
to 
more 
than 
two 
attribute 
states, 
or 
continuous 
attributes 
is 
straightforward. 


Example 
47. 
EZsurvey.org 
considers 
Radio 
station 
listeners 
conveniently 
fall 
into 
two 
groups 
– 
the 
`young’ 
and 
`old'. 
They 
assume 
that, 
given 
the 
knowledge 
that 
a 
customer 
is 
either 
`young’ 
or 
`old', 
this 
is 
sucient 
to 
determine 
whether 
or 
not 
a 
customer 
will 
like 
a 
particular 
Radio 
station, 
independent 
of 
their 
likes 
or 
dislikes 
for 
any 
other 
stations: 


p(R1;R2;R3;R4jage)= 
p(R1jage)p(R2jage)p(R3jage)p(R4jage) 
(10.1.3) 


where 
each 
of 
the 
variables 
R1;R2;R3;R4 
can 
take 
the 
states 
either 
like 
or 
dislike, 
and 
the 
`age’ 
variable 
can 
take 
the 
value 
either 
young 
or 
old. 
Thus 
the 
information 
about 
the 
age 
of 
the 
customer 
determines 
the 
individual 
product 
preferences 
without 
needing 
to 
know 
anything 
else. 
To 
complete 
the 
specication, 
given 
that 
a 
customer 
is 
young, 
she 
has 
a 
95% 
chance 
to 
like 
Radio1, 
a 
5% 
chance 
to 
like 
Radio2, 
a 
2% 
chance 
to 
like 
Radio3 
and 
a 
20% 
chance 
to 
like 
Radio4. 
Similarly, 
an 
old 
listener 
has 
a 
3% 
chance 
to 
like 
Radio1, 
an 
82% 
chance 
to 
like 
Radio2, 
a 
34% 
chance 
to 
like 
Radio3 
and 
a 
92% 
chance 
to 
like 
Radio4. 
They 
know 
that 
90% 
of 
the 
listeners 
are 
old. 


203 



Estimation 
using 
Maximum 
Likelihood 


c
x1x2x3
cn
x
c
ni
n=1:N
i;c
i=1:D
Figure 
10.1: 
Naive 
Bayes 
classifer. 
(a): 
The 
central 
assumption 
is 
that 
given 
the 
class 
c, 
the 
attributes 
xi 
are 
independent. 
(b): 
Assuming 
the 
data 
is 
i.i.d., 
Maximum 
Likelihood 
learns 
the 
optimal 
parameters 
of 
the 
distribution 
p(c) 
and 
the 
class-dependent 
attribute 
distributions 
p(xijc). 


(a) 
(b) 
Given 
this 
model, 
and 
a 
new 
customer 
that 
likes 
Radio1, 
and 
Radio3, 
but 
dislikes 
Radio2 
and 
Radio4, 
what 
is 
the 
probability 
that 
they 
are 
young? 
This 
is 
given 
by 


p(age 
= 
youngjR1= 
like;R2= 
dislike;R3= 
like;R4= 
dislike) 
p(R1= 
like;R2= 
dislike;R3= 
like;R4= 
dislikejage 
= 
young)p(age 
= 
young)


= 
P(10.1.4) 


p(R1= 
like;R2= 
dislike;R3= 
like;R4= 
dislikejage)p(age)

age 


Using 
the 
Naive 
Bayes 
structure, 
the 
numerator 
above 
is 
given 
by 


p(R1= 
likejage 
= 
young)p(R2= 
dislikejage 
= 
young) 
× 
p(R3= 
likejage 
= 
young)p(R4= 
dislikejage 
= 
young)p(age 
= 
young) 
(10.1.5) 


Plugging 
in 
the 
values 
we 
obtain 


0:95 
× 
0:95 
× 
0:02 
× 
0:8 
× 
0:1=0:0014 
The 
denominator 
is 
given 
by 
this 
value 
plus 
the 
corresponding 
term 
evaluated 
under 
assuming 
the 
customer 
is 
old, 


0:03 
× 
0:18 
× 
0:34 
× 
0:08 
× 
0:9=1:3219 
× 
10..4 
Which 
gives 


0:0014 


p(age 
= 
youngjR1= 
like;R2= 
dislike;R3= 
like;R4= 
dislike)= 
=0:9161 
(10.1.6) 


0:0014 
+ 
1:3219 
× 
10..4 


10.2 
Estimation 
using 
Maximum 
Likelihood 
Learning 
the 
table 
entries 
for 
NB 
is 
a 
straightforward 
application 
of 
the 
more 
general 
BN 
learning 
discussed 
in 
section(9.2.3). 
For 
a 
fully 
observed 
dataset, 
Maximum 
Likelihood 
learning 
of 
the 
table 
entries 
corresponds 
to 
counting 
the 
number 
of 
occurrences 
in 
the 
training 
data, 
as 
we 
show 
below. 


10.2.1 
Binary 
attributes 
n

Consider 
a 
dataset 
fxn;n 
=1;:::;N} 
of 
binary 
attributes, 
x2f0, 
1g, 
i 
=1;:::;D. 
Each 
datapoint 


i 


n

xn 
has 
an 
associated 
class 
label 
c. 
The 
number 
of 
datapoints 
from 
class 
c 
= 
0 
is 
n0 
and 
the 
number 
from 
class 
c 
= 
1 
denoted 
is 
n1. 
For 
each 
attribute 
of 
the 
two 
classes, 
we 
need 
to 
estimate 
the 
values 
p(xi 
=1jc) 
= 
i
c 
. 
The 
other 
probability, 
p(xi 
=0jc) 
is 
given 
by 
the 
normalisation 
requirement, 
p(xi 
=0jc)=1 
- 
p(xi 
=1jc)=1 
- 
i
c 
. 


DRAFT 
March 
9, 
2010 



Estimation 
using 
Maximum 
Likelihood 


Based 
on 
the 
NB 
conditional 
independence 
assumption 
the 
probability 
of 
observing 
a 
vector 
x 
can 
be 
compactly 
written 


DD

YYX

p(xjc)= 
p(xijc)= 
(c)xi 
(1 
- 
c)1..xi 
(10.2.1)

ii 
i=1 
i=1 


In 
the 
above 
expression, 
xi 
is 
either 
0 
or 
1 
and 
hence 
each 
i 
term 
contributes 
a 
factor 
c 
if 
xi 
= 
1 
or 
1 
- 
c 


ii 
if 
xi 
= 
0. 
Together 
with 
the 
assumption 
that 
the 
training 
data 
is 
i.i.d. 
generated, 
the 
log 
likelihood 
of 
the 
attributes 
and 
class 
labels 
is 


XXYX

n

L 
=log 
p(xn 
;c 
n) 
=log 
p(c 
n) 
p(xi 
jc 
n) 
(10.2.2) 
n 
ni

X

nn

=x 
log 
cn 
+ 
(1 
- 
x 
) 
log(1 
- 
cn 
)+ 
n0 
log 
p(c 
= 
0)+ 
n1 
log 
p(c 
= 
1) 
(10.2.3)

iii 
i 
i;n 


This 
can 
be 
written 
more 
explicitly 
in 
terms 
of 
the 
parameters 
as 


X

nn 
n

L 
=I[x 
=1;c 
n 
= 
0] 
log 
0 
+ 
I[x 
=0;c 
n 
= 
0] 
log(1 
- 
0)+ 
I[x 
=1;c 
n 
= 
1] 
log 
1 


iii 
iii 
i;n

	

n

+ 
I[x 
=0;c 
n 
= 
1] 
log(1 
- 
1)+ 
n0 
log 
p(c 
= 
0)+ 
n1 
log 
p(c 
= 
1) 
(10.2.4)
ii 


We 
can 
nd 
the 
Maximum 
Likelihood 
optimal 
c 
by 
dierentiating 
w.r.t. 
c 
and 
equating 
to 
zero, 
giving 


ii 


P

nn

I[x=1;c= 
c]

ni

c 
= 
p(xi 
=1jc)=P(10.2.5)

i 
nn

I[x=0;cn 
= 
c]+ 
I[x=1;cn 
= 
c]

ni 
i 
number 
of 
times 
xi 
= 
1 
for 
class 
c 


= 
(10.2.6)

number 
of 
datapoints 
in 
class 
c 


Similarly, 
optimising 
equation 
(10.2.3) 
with 
respect 
to 
p(c) 
gives 


number 
of 
times 
class 
c 
occurs 


p(c) 
= 
(10.2.7)

total 
number 
of 
data 
points 


Classication 
boundary 


We 
classify 
a 
novel 
input 
x 
* 
as 
class 
1 
if 


p(c 
=1jx 
) 
>p(c 
=0jx 
) 
(10.2.8) 


Using 
Bayes’ 
rule 
and 
writing 
the 
log 
of 
the 
above 
expression, 
this 
is 
equivalent 
to 


log 
p(x 
* 
jc 
= 
1) 
+ 
log 
p(c 
= 
1) 
- 
log 
p(x 
) 
> 
log 
p(x 
* 
jc 
= 
0) 
+ 
log 
p(c 
= 
0) 
- 
log 
p(x 
) 
(10.2.9) 


From 
the 
denition 
of 
the 
classier, 
this 
is 
equivalent 
to 
(the 
normalisation 
constant 
- 
log 
p(x 
) 
can 
be 
dropped 
from 
both 
sides)

XX



log 
p(xi 
jc 
= 
1) 
+ 
log 
p(c 
= 
1) 
>log 
p(xi 
jc 
= 
0) 
+ 
log 
p(c 
= 
0) 
(10.2.10) 


ii 


Using 
the 
binary 
encoding 
xi 
2f0, 
1g, 
we 
classify 
x 
* 
as 
class 
1 
if

X	X	

* 
* 


x 
log 
1 
+ 
(1 
- 
x 
) 
log(1 
- 
1)+log 
p(c 
= 
1) 
>x 
log 
0 
+ 
(1 
- 
x 
) 
log(1 
- 
0)+log 
p(c 
= 
0) 


iiii 
iiii 
ii


(10.2.11) 
P



This 
decision 
rule 
can 
be 
expressed 
in 
the 
form: 
classify 
x 
* 
as 
class 
1 
ifi 
wix 
+ 
a> 
0 
for 
some 
suitable 


i 
choice 
of 
weights 
wi 
and 
constant 
a, 
see 
exercise(133). 
The 
interpretation 
is 
that 
w 
species 
a 
hyperplane 
in 
the 
attribute 
space 
and 
x 
* 
is 
classied 
as 
1 
if 
it 
lies 
on 
the 
positive 
side 
of 
the 
hyperplane. 


DRAFT 
March 
9, 
2010 
205 



Estimation 
using 
Maximum 
Likelihood 


0 
0 
1 
1 
1 
1 
0 
1 
1 
0 
1 
1 
0 
0 
1 
1 
1 
0 
0 
0 
0 
1 
0 
0 
1 
0 
0 
0 
1 
0 


1 
0 
0 
1 
1 
1 
1 
0 
0 
1 
1 
1 
1 
1 
0 
1 
1 
0 
1 
0 
1 
1 
0 
1 
1 
1 
0 
1 
1 
0 
1 
0 
1 
0 
0 


Figure 
10.2: 
(a): 
English 
tastes 
over 
attributes 
(shortbread, 
lager, 
whiskey, 
porridge, 
football). 
Each 
column 
represents 
the 
tastes 
of 
an 
individual. 
(b): 
Scottish 
tastes. 


(a) 
(b) 
Example 
48 
(Are 
they 
Scottish?). 
Consider 
the 
following 
vector 
of 
attributes: 


(likes 
shortbread, 
likes 
lager, 
drinks 
whiskey, 
eats 
porridge, 
watched 
England 
play 
football) 
(10.2.12) 


A 
vector 
x 
= 
(1, 
0, 
1, 
1, 
0)T 
would 
describe 
that 
a 
person 
likes 
shortbread, 
does 
not 
like 
lager, 
drinks 
whiskey, 
eats 
porridge, 
and 
has 
not 
watched 
England 
play 
football. 
Together 
with 
each 
vector 
x, 
there 
is 
a 
label 
nat 
describing 
the 
nationality 
of 
the 
person, 
dom(nat)= 
fscottish, 
englishg, 
see 
g(10.2). 


We 
wish 
to 
classify 
the 
vector 
x 
= 
(1, 
0, 
1, 
1, 
0)T 
as 
either 
scottish 
or 
english. 
We 
can 
use 
Bayes’ 
rule 
to 
calculate 
the 
probability 
that 
x 
is 
Scottish 
or 
English: 


p(xjscottish)p(scottish) 
p(xjscottish)p(scottish) 


p(scottishjx) 
= 
= 
(10.2.13) 


p(x) 
p(xjscottish)p(scottish)+ 
p(xjenglish)p(english) 


By 
Maximum 
Likelihood 
the 
`prior’ 
class 
probability 
p(scottish) 
is 
given 
by 
the 
fraction 
of 
people 
in 
the 
database 
that 
are 
Scottish, 
and 
similarly 
p(english) 
is 
given 
as 
the 
fraction 
of 
people 
in 
the 
database 
that 
are 
English. 
This 
gives 
p(scottish)=7=13 
and 
p(english)=6=13. 


For 
p(xjnat) 
under 
the 
Naive 
Bayes 
assumption: 


p(xjnat)= 
p(x1jnat)p(x2jnat)p(x3jnat)p(x4jnat)p(x5jnat) 
(10.2.14) 


so 
that 
knowing 
whether 
not 
someone 
is 
Scottish, 
we 
don't 
need 
to 
know 
anything 
else 
to 
calculate 
the 
probability 
of 
their 
likes 
and 
dislikes. 
Based 
on 
the 
table 
in 
g(10.2) 
and 
using 
Maximum 
Likelihood 
we 
have: 


p(x1 
=1jenglish) 
=1=2 
p(x1 
=1jscottish) 
=1 
p(x2 
=1jenglish) 
=1=2 
p(x2 
=1jscottish) 
=4=7 
p(x3 
=1jenglish) 
=1=3 
p(x3 
=1jscottish) 
=3=7 
(10.2.15) 
p(x4 
=1jenglish) 
=1=2 
p(x4 
=1jscottish) 
=5=7 
p(x5 
=1jenglish) 
=1=2 
p(x5 
=1jscottish) 
=3=7 


For 
x 
= 
(1, 
0, 
1, 
1, 
0)T, 
we 
get 


3354 
7

1 


7 
7 
7 
7 
13 


p(scottishjx)= 
=0:8076 
(10.2.16)

3354 
7 
1111 
6

+ 
1
1 
× 


7 
7 
7 
7 
13 
2 
2 
3 
2 
2 
13 


Since 
this 
is 
greater 
than 
0.5, 
we 
would 
classify 
this 
person 
as 
being 
Scottish. 


Small 
data 
counts 


In 
example(48), 
consider 
trying 
to 
classify 
the 
vector 
x 
= 
(0, 
1, 
1, 
1, 
1)T 
. 
In 
the 
training 
data, 
all 
Scottish 
people 
say 
they 
like 
shortbread. 
This 
means 
that 
for 
this 
particular 
x, 
p(x, 
scottish) 
= 
0, 
and 
therefore 
that 
we 
make 
the 
extremely 
condent 
classication 
p(scottishjx) 
= 
0. 
This 
demonstrates 
a 
diculty 
using 
Maximum 
Likelihood 
with 
sparse 
data. 
One 
way 
to 
ameliorate 
this 
is 
to 
smooth 
the 
probabilities, 
for 
example 
by 
adding 
a 
certain 
small 
number 
to 
the 
frequency 
counts 
of 
each 
attribute. 
This 
ensures 
that 


DRAFT 
March 
9, 
2010 



Estimation 
using 
Maximum 
Likelihood 


there 
are 
no 
zero 
probabilities 
in 
the 
model. 
An 
alternative 
is 
to 
use 
a 
Bayesian 
approach 
that 
discourages 
extreme 
probabilities, 
as 
discussed 
in 
section(10.3). 


Potential 
pitfalls 
with 
encoding 


In 
many 
o-the-shelf 
packages 
implementing 
Naive 
Bayes, 
binary 
attributes 
are 
assumed. 
In 
practice, 
however, 
the 
case 
of 
non-binary 
attributes 
often 
occurs. 
Consider 
the 
following 
attribute 
: 
age. 
In 
a 
survey, 
a 
person's 
age 
is 
marked 
down 
using 
the 
variable 
a 
. 
1, 
2, 
3. 
a 
= 
1 
means 
the 
person 
is 
between 
0 
and 
10 
years 
old, 
a 
= 
2 
means 
the 
person 
is 
between 
10 
and 
20 
years 
old, 
a 
= 
3 
means 
the 
person 
is 
older 
than 
20. 
One 
way 
to 
transform 
the 
variable 
a 
into 
a 
binary 
representation 
would 
be 
to 
use 
three 
binary 
variables 
(a1;a2;a3) 
with 
(1, 
0, 
0), 
(0, 
1, 
0), 
(0, 
0, 
1) 
representing 
a 
=1;a 
=2;a 
= 
3 
respectively. 
This 
is 
called 
1 
- 
of 
- 
M 
coding 
since 
only 
1 
of 
the 
binary 
variables 
is 
active 
in 
encoding 
the 
M 
states. 
By 
construction, 
means 
that 
the 
variables 
a1;a2;a3 
are 
dependent 
– 
for 
example, 
if 
we 
know 
that 
a1 
= 
1, 
we 
know 
that 
a2 
= 
0 
and 
a3 
= 
0. 
Regardless 
of 
any 
class 
conditioning, 
these 
variables 
will 
always 
be 
dependent, 
contrary 
to 
the 
assumption 
of 
Naive 
Bayes. 
A 
correct 
approach 
is 
to 
use 
variables 
with 
more 
than 
two 
states, 
as 
explained 
in 
section(10.2.2). 


10.2.2 
Multi-state 
variables 
For 
a 
variable 
xi 
with 
more 
than 
two 
states, 
dom(xi)= 
f1;:::;Sg, 
the 
likelihood 
of 
observing 
a 
state 
xi 
= 
s 
is 
denoted 


p(xi 
= 
sjc)= 
i 
(c) 
(10.2.17)

s

P

withp(xi 
= 
sjc) 
= 
1. 
For 
a 
set 
of 
data 
vectors 
xn 
,n 
=1;:::N, 
belonging 
to 
class 
c, 
under 
the 
i.i.d. 


s 


assumption, 
the 
likelihood 
of 
the 
NB 
model 
generating 
data 
from 
class 
c 
is 


NNDSC

YXYYYYX

nn

p(xnjc 
n)= 
i 
(c)I[xi 
=s]I[c=c] 
(10.2.18)

s
n=1 
n=1 
i=1 
s=1 
c=1 


which 
gives 
the 
class 
conditional 
log-likelihood 


NDSC

XXXX

nn

L 
= 
I[x 
= 
s] 
I[c 
= 
c] 
log 
i 
(c) 
(10.2.19)

is
n=1 
i=1 
s=1 
c=1 


We 
can 
optimize 
with 
respect 
to 
the 
parameters 
. 
using 
a 
Lagrange 
multiplier 
(one 
for 
each 
of 
the 
attributes 
i 
and 
classes 
c) 
to 
ensure 
normalisation: 


NDSCCDS

XXXXXXX

nn

L()= 
I[x 
= 
s] 
I[c 
= 
c] 
log 
i 
(c)+ 
c 
1 
- 
i 
(c) 
(10.2.20)

i 
sis
n=1 
i=1 
s=1 
c=1 
c=1 
i=1 
s=1 


To 
nd 
the 
optimum 
of 
this 
function 
we 
may 
dierentiate 
with 
respect 
to 
i 
(c) 
and 
equate 
to 
zero. 
Solving 


s

the 
resulting 
equation 
we 
obtain 


N

XI[xn 
= 
s] 
I[cn 
= 
c]

i 


= 
c 
(10.2.21)

ii

(c)
n=1 
s

Hence, 
by 
normalisation, 


P

nn

I[x= 
s] 
I[c= 
c]

i 
(c)= 
p(xi 
= 
sjc)=Pn 
i 
(10.2.22)

sn' 
0' 


s0;n' 
Ixi 
= 
sI[cn= 
c] 


The 
Maximum 
Likelihood 
setting 
for 
the 
parameter 
p(xi 
= 
sjc) 
equals 
the 
relative 
number 
of 
times 
that 
attribute 
i 
is 
in 
state 
s 
for 
class 
c. 


DRAFT 
March 
9, 
2010 
207 



Bayesian 
Naive 
Bayes 


cn
xni
i;c
n=1:N
c=1:C
i=1:D
c 


Figure 
10.3: 
Bayesian 
Naive 
Bayes 
with 
a 
factorised 
prior 
on 
the 
class 
conditional 
attribute 
probabilities 
p(xi 
= 
sjc). 
For 
simplicity 
we 
assume 
that 
the 
class 
probability 
c 
= 
p(c) 
is 
learned 
with 
Maximum 
Likelihood, 
so 
that 
no 
distribution 
is 
placed 
over 
this 
parameter. 


10.2.3 
Text 
classication 
Consider 
a 
set 
of 
documents 
about 
politics, 
and 
another 
set 
about 
sport. 
Our 
interest 
is 
to 
make 
a 
method 
that 
can 
automatically 
classify 
a 
new 
document 
as 
pertaining 
to 
either 
sport 
or 
politics. 
We 
search 
through 
both 
sets 
of 
documents 
to 
nd 
the 
100 
most 
commonly 
occurring 
words. 
Each 
document 
is 
then 
represented 
by 
a 
100 
dimensional 
vector 
representing 
the 
number 
of 
times 
that 
each 
of 
the 
words 
occurs 
in 
that 
document 
– 
the 
so 
called 
bag 
of 
words 
representation 
(this 
is 
a 
crude 
representation 
of 
the 
document 
since 
it 
discards 
word 
order). 
A 
Naive 
Bayes 
model 
species 
a 
distribution 
of 
these 
number 
of 
occurrences 
p(xijc), 
where 
xi 
is 
the 
count 
of 
the 
number 
of 
times 
word 
i 
appears 
in 
documents 
of 
type 
c. 
One 
can 
achieve 
this 
using 
either 
a 
multistate 
representation 
(as 
discussed 
in 
section(10.2.2)) 
or 
using 
a 
continuous 
xi 
to 
represent 
the 
frequency 
of 
word 
i 
in 
the 
document. 
In 
this 
case 
p(xijc) 
could 
be 
conveniently 
modelled 
using 
for 
example 
a 
Beta 
distribution. 


Despite 
the 
simplicity 
of 
Naive 
Bayes, 
it 
can 
classify 
documents 
surprisingly 
well[125]. 
Intuitively 
a 
potential 
justication 
for 
the 
conditional 
independence 
assumption 
is 
that 
if 
we 
know 
a 
document 
is 
about 
politics, 
this 
is 
a 
good 
indication 
of 
the 
kinds 
of 
other 
words 
we 
will 
nd 
in 
the 
document. 
Because 
Naive 
Bayes 
is 
a 
reasonable 
classier 
in 
this 
sense, 
and 
has 
minimal 
storage 
and 
fast 
training, 
it 
has 
been 
applied 
to 
time-storage 
critical 
applications, 
such 
as 
automatically 
classifying 
webpages 
into 
types[289], 
and 
spam 
ltering[9]. 


10.3 
Bayesian 
Naive 
Bayes 
To 
predict 
the 
class 
c 
of 
an 
input 
x 
we 
use 


p(cjx, 
D) 
. 
p(x, 
D;c)p(cjD) 
. 
p(xjD;c)p(cjD) 
(10.3.1) 


For 
convenience 
we 
will 
simply 
set 
p(cjD) 
using 
Maximum 
Likelihood 


X

1 


n 


p(cjD)= 
I[c 
= 
c] 
(10.3.2)

N

n 


However, 
as 
we've 
seen, 
setting 
the 
parameters 
of 
p(xjD;c) 
using 
Maximum 
Likelihood 
training 
can 
yield 
over-condent 
predictions 
in 
the 
case 
of 
sparse 
data. 
A 
Bayesian 
approach 
that 
addresses 
this 
diculty 
is 
to 
use 
priors 
on 
the 
probabilities 
p(xi 
= 
sjc) 
= 
i 
(c) 
that 
discourage 
extreme 
values. 
The 
model 
is 


s

depicted 
in 
g(10.3). 


The 
prior 


We 
will 
use 
a 
prior 
on 
the 
table 
entries 
and 
make 
the 
global 
factorisation 
assumption 
(see 
section(9.3)) 


Y

p()=p(i(c)) 
(10.3.3) 
i;c 


DRAFT 
March 
9, 
2010 



Bayesian 
Naive 
Bayes 


We 
consider 
discrete 
xi 
each 
of 
which 
take 
states 
from 
1;:::;S. 
In 
this 
case 
p(xi 
= 
sjc) 
corresponds 
to 
a 
multinomial 
distribution, 
for 
which 
the 
conjugate 
prior 
is 
a 
Dirichlet 
distribution. 
Under 
the 
factorised 
prior 
assumption 
(10.3.3) 
we 
dene 
a 
prior 
for 
each 
attribute 
i 
and 
class 
c, 


..

p(i(c)) 
= 
Dirichleti(c)jui(c)(10.3.4) 


where 
ui(c) 
is 
the 
hyperparameter 
vector 
of 
the 
Dirichlet 
distribution 
for 
table 
p(xijc). 


The 
posterior 


First 
let's 
see 
how 
the 
Bayesian 
approach 
is 
used 
to 
classify 
a 
novel 
point 
x 
* 
. 
Let 
D6denote 
the 
training 
data 
(xn;cn);n 
=1;:::;N. 
From 
equation 
(10.3.14), 
the 
term 
p(x 
* 
, 
Djc 
) 
is 
computed 
using 
the 
following 
decomposition: 


ZZZ

* 


p(x 
, 
Djc 
)=p(x 
, 
D;jc 
)=p(x 
* 
j;/
....)p(, 
Dj/
....) 
/p(x 
* 
j(c 
))p((c 
)jD) 
(10.3.5)

D;c 
c 


. 
(c 
) 


Hence 
in 
order 
to 
make 
a 
prediction, 
we 
require 
the 
parameter 
posterior. 
Consistent 
with 
our 
general 
Bayesian 
BN 
training 
result 
in 
section(9.3), 
the 
parameter 
posterior 
factorises 


Y

p((c 
)jD)=p(i(c 
)jD) 
(10.3.6) 


i 


where 


Y

n 


p(i(c 
)jD) 
/6p(i(c 
))p(xi 
ji(c 
)) 
(10.3.7) 


* 


n:cn=c 




By 
conjugacy, 
the 
posterior 
for 
class 
c 
is 
a 
Dirichlet 
distribution, 


..

p(i(c 
)jD) 
= 
Dirichleti(c 
)ju^i(c 
)(10.3.8) 


where 
the 
vector 
u^i(c 
) 
has 
components

X

in

u^i(c 
)s 
= 
u 
(c 
)+I[xi 
= 
s] 
(10.3.9)

s

* 


n:cn=c 


For 
Dirichlet 
hyperparameters 
ui(c 
) 
the 
above 
equation 
updates 
the 
hyperparameter 
by 
the 
number 
of 


times 
variable 
i 
is 
in 
state 
s 
for 
class 
c 
data. 
A 
common 
default 
setting 
is 
to 
take 
all 
components 
of 
u 
to 
be 
1. 


Classication 


The 
class 
distribution 
is 
given 
by 


Y

* 


p(c 
* 
jx 
, 
D) 
/6p(c 
* 
jD)p(x 
* 
jD;c 
)= 
p(c 
* 
jD)p(xi 
jD;c 
) 
(10.3.10) 


i 


To 
compute 
p(x 
jD;c 
) 
we 
use 


ZZZ

* 
* 


i 


p(x 
= 
sjD;c 
)=p(x 
= 
s, 
(c 
)jD;c 
)=p(x 
= 
sj(c 
))p((c 
)jD)=(c 
)p((c 
)jD)

iii 
s
(c 
) 
(c 
) 
(c 
) 


(10.3.11) 
Using 
the 
general 
identity

ZZY

1 
us' 
..1+I[s0Z(u0)

sDirichlet 
(ju) 
d. 
= 
=s]d. 
= 
(10.3.12)

Z(u)
' 
Z(u) 


s

where 
Z(u) 
is 
the 
normalisation 
constant 
of 
the 
Dirichlet 
distribution 
Dirichlet 
(ju) 
and 




0

us 
s=6s

/ 


us 
=
/ 
(10.3.13)

us 
+1 
s 
= 
s

DRAFT 
March 
9, 
2010 
209 



Tree 
Augmented 
Naive 
Bayes 


x1x2x3x4
Figure 
10.4: 
A 
Chow-Liu 
Tree 
in 
which 
each 
variable 
xi 
has 
at 
most 
one 
parent. 
The 
variables 
may 
be 
indexed 
such 
that 
1 
ii 
iD. 


we 
obtain 


YXZ(u 
i(c 
))

* 


p(c 
* 
jx 
, 
D) 
/ip(c 
* 
jD) 
(10.3.14)

Z(u^i(c 
))

i 


where 


ii 
* 


u 
(c 
)= 
u^(c 
)+ 
I[x 
= 
s] 
(10.3.15)

ssi 


Example 
49 
(Bayesian 
Naive 
Bayes). 
Repeating 
the 
previous 
analysis 
for 
the 
`Are 
they 
Scottish?’ 
data 
from 
example(48), 
the 
probability 
under 
a 
uniform 
Dirichlet 
prior 
for 
all 
the 
tables, 
gives 
a 
value 
of 
0:236 
for 
the 
probability 
that 
(1, 
0, 
1, 
1, 
0) 
is 
Scottish, 
compared 
with 
a 
value 
of 
0:192 
under 
the 
standard 
Naive 
Bayes 
assumption. 


10.4 
Tree 
Augmented 
Naive 
Bayes 
A 
natural 
extension 
of 
Naive 
Bayes 
is 
to 
relax 
the 
assumption 
that 
the 
attributes 
are 
independent 
given 
the 
class: 


D

YX

p(xjc)6p(xijc)

= 
(10.4.1) 


i=1 


The 
question 
then 
arises 
– 
which 
structure 
should 
we 
choose 
for 
p(xjc)? 
As 
we 
saw 
in 
section(9.3.5), 
learning 
a 
structure 
is 
computationally 
infeasible 
for 
all 
but 
very 
small 
numbers 
of 
attributes. 
A 
practical 
algorithm 
requires 
a 
specic 
form 
of 
constraint 
on 
the 
structure. 
To 
do 
this 
we 
rst 
make 
a 
digression 
into 
the 
Maximum 
Likelihood 
learning 
of 
trees 
constrained 
to 
have 
at 
most 
a 
single 
parent. 


10.4.1 
Chow-Liu 
Trees 
Consider 
a 
multivariate 
distribution 
p(x) 
that 
we 
wish 
to 
approximate 
with 
a 
distribution 
q(x). 
Furthermore, 
we 
constrain 
the 
approximation 
q(x) 
to 
be 
a 
Belief 
Network 
in 
which 
each 
node 
has 
at 
most 
one 
parent. 
First 
we 
assume 
that 
we 
have 
chosen 
a 
particular 
labelling 
of 
the 
variables 
1 
ii 
iD, 
for 
which 
the 
DAG 
single 
parent 
constraint 
means 


D

YX

q(x)= 
q(xijxpa(i)), 
pa(i) 
< 
i, 
or 
pa(i)= 
;i(10.4.2) 
i=1 


where 
pa(i) 
is 
the 
single 
parent 
index 
of 
node 
i. 
To 
nd 
the 
best 
approximating 
distribution 
q 
in 
this 
constrained 
class, 
we 
may 
minimise 
the 
Kullback-Leibler 
divergence 


D

X


KL(pjq)=hlog 
p(x)ip(x) 
..ilog 
q(xijxpa(i))p(xi;xpa(i)) 
(10.4.3) 
i=1




Since 
p(x) 
is 
xed, 
the 
rst 
term 
is 
constant. 
By 
adding 
a 
termlog 
p(xijxpa(i))that 
depends 


p(xi;xpa(i)) 


on 
p(x) 
alone, 
we 
can 
write 


DE

XD



KL(pjq)= 
const. 
..ilog 
q(xijxpa(i))..log 
p(xijxpa(i))(10.4.4)

p(xijxpa(i)) 
p(xijxpa(i))p(x

i=1pa(i)) 


DRAFT 
March 
9, 
2010 



Tree 
Augmented 
Naive 
Bayes 


Algorithm 
7 
Chow-Liu 
Trees 


1: 
for 
i 
=1 
to 
D 
do 
2: 
for 
j 
=1 
to 
D 
do 
3: 
Compute 
the 
Mutual 
Information 
for 
the 
pair 
of 
variables 
xi;xj: 
wij 
= 
MI(xi; 
xj) 
4: 
end 
for 
5: 
end 
for 
6: 
For 
the 
undirected 
graph 
G 
with 
edge 
weights 
w, 
nd 
a 
maximum 
weight 
undirected 
spanning 
tree 
T 
7: 
Choose 
an 
arbitrary 
variable 
as 
the 
root 
node 
of 
the 
tree 
T 
. 
8: 
Form 
a 
directed 
tree 
by 
orienting 
all 
edges 
away 
from 
the 
root 
node. 
This 
enables 
us 
to 
recognise 
that, 
up 
to 
a 
negligible 
constant, 
the 
overall 
Kullback-Leibler 
divergence 
is 
a 
positive 
sum 
of 
individual 
Kullback-Leibler 
divergences 
so 
that 
the 
optimal 
setting 
is 
therefore 


q(xijxpa(i))= 
p(xijxpa(i)) 
(10.4.5) 


Plugging 
this 
solution 
into 
equation 
(10.4.3) 
and 
using 
log 
p(xijxpa(i)) 
= 
log 
p(xi;xpa(i)) 
- 
log 
p(xpa(i)) 
we 
obtain 


DD

X
X


KL(pjq)= 
const. 
- 
log 
p(xi;xpa(i))+ 
log 
p(xpa(i))(10.4.6)

p(xi;xpa(i)) 
p(xpa(i)) 
i=1i=1

We 
still 
need 
to 
nd 
the 
optimal 
parental 
structure 
pa(i) 
that 
minimises 
the 
above 
expression. 
If 
we 
add 
and 
subtract 
an 
entropy 
term 
we 
can 
write 


DDD

X

X
X


KL(pjq)= 
- 
log 
p(xi;xpa(i))+ 
log 
p(xpa(i))+ 
hlog 
p(xi)ip(xi)

p(xi;xpa(i)) 
p(xpa(i)) 
i=1i=1i=1 


D

X


..hlog 
p(xi)i+ 
const. 
(10.4.7)

p(xi) 
i=1 


For 
two 
variables 
xi 
and 
xj 
and 
distribution 
p(xi;xj), 
the 
mutual 
information 
denition(87) 
can 
be 
written 
as 




p(xi;xj)

MI(xi; 
xj) 
=log 
(10.4.8) 


p(xi)p(xj)

p(xi;xj 
) 


which 
can 
be 
seen 
as 
the 
Kullback-Leibler 
divergence 
KL(p(xi;xj)jp(xi)p(xj)) 
and 
is 
therefore 
nonnegative. 
Using 
this, 
equation 
(10.4.7) 
is 


DD

X..X


KL(pjq)= 
- 
MIxi; 
x..hlog 
p(xi)i+ 
const. 
(10.4.9)

pa(i)p(xi) 
i=1 
i=1 


P

Since 
our 
task 
is 
to 
nd 
the 
parental 
indices 
pa(i), 
and 
the 
entropic 
termhlog 
p(xi)iis 
independent 


ip(xi) 


of 
this 
mapping, 
nding 
the 
optimal 
mapping 
is 
equivalent 
to 
maximising 
the 
summed 
mutual 
informations 


D

X
..

MIxi; 
xpa(i)(10.4.10) 
i=1 


under 
the 
constraint 
that 
pa(i) 
= 
i. 
Since 
we 
also 
need 
to 
choose 
the 
optimal 
initial 
labelling 
of 
the 
variables 
as 
well, 
the 
problem 
is 
equivalent 
to 
computing 
all 
the 
pairwise 
mutual 
informations 


wij 
= 
MI(xi; 
xj) 
(10.4.11) 


and 
then 
nding 
a 
maximal 
spanning 
tree 
for 
the 
graph 
with 
edge 
weights 
w 
(see 
spantree.m). 
This 
can 
be 
thought 
of 
as 
a 
form 
of 
breadth-rst-search[61]. 
Once 
found, 
we 
need 
to 
identify 
a 
directed 
tree 
with 
at 
most 
one 
parent. 
This 
is 
achieved 
by 
choosing 
an 
arbitrary 
node 
and 
then 
orienting 
edges 
consistently 
away 
from 
this 
node. 


DRAFT 
March 
9, 
2010 



Tree 
Augmented 
Naive 
Bayes 


c]
x1x2x3x4
Figure 
10.5: 
Tree 
Augmented 
Naive 
(TAN) 
Bayes. 
Each 
variable 
xi 
has 
at 
most 
one 
parent. 
The 
Maximum 
Likelihood 
optimal 
TAN 
structure 
is 
computed 
using 
a 
modied 
Chow-Liu 
algorithm 
in 
which 
the 
conditional 
mutual 
information 
MI(xi; 
xjjc) 
is 
computed 
for 
all 
i, 
j. 
A 
maximum 
weight 
spanning 
tree 
is 
then 
found 
and 
turned 
into 
a 
directed 
graph 
by 
orienting 
the 
edges 
outwards 
from 
a 
chosen 
root 
node. 
The 
table 
entries 
can 
then 
be 
read 
off 
using 
the 
usual 
Maximum 
Likelihood 
counting 
argument. 


Maximum 
likelihood 
Chow-Liu 
trees 


If 
p(x) 
is 
the 
empirical 
distribution 


X

1 
N

p(x)= 
](x;]x]n) 
(10.4.12)

N]

n=1 


then 


X

1

KL(pjq)= 
const. 
..hlog 
q(x]n) 
(10.4.13)

N]

n 


Hence 
the 
approximation 
q]that 
minimises 
the 
Kullback-Leibler 
divergence 
between 
the 
empirical 
distribution 
and 
p]is 
equivalent 
to 
that 
which 
maximises 
the 
likelihood 
of 
the 
data. 
This 
means 
that 
if 
we 
use 
the 
mutual 
information 
found 
from 
the 
empirical 
distribution, 
with 


p(xi 
= 
a;xj 
= 
b) 
/](xi 
= 
a;xj 
= 
b) 
(10.4.14) 


then 
the 
Chow-Liu 
tree 
produced 
corresponds 
to 
the 
Maximum 
Likelihood 
solution 
amongst 
all 
single-
parent 
trees. 
An 
outline 
of 
the 
procedure 
is 
given 
in 
algorithm(7). 
An 
ecient 
algorithm 
for 
sparse 
data 
is 
also 
available[190]. 


Remark 
10 
(Learning 
Tree 
structured 
Belief 
Networks). 
The 
Chow-Liu 
algorithm 
pertains 
to 
the 
discussion 
in 
section(9.3.5) 
on 
learning 
the 
structure 
of 
Belief 
networks 
from 
data. 
Under 
the 
special 
constraint 
that 
each 
variable 
has 
at 
most 
one 
parent, 
the 
Chow-Liu 
algorithm 
returns 
the 
Maximum 
Likelihood 
structure 
to 
t 
the 
data. 


10.4.2 
Learning 
tree 
augmented 
Naive 
Bayes 
networks 
For 
a 
distribution 
p(xjc) 
of 
the 
form 
of 
a 
tree 
structure 
with 
a 
single-parent 
constraint 
we 
can 
readily 
nd 
the 
class 
conditional 
Maximum 
Likelihood 
solution 
by 
computing 
the 
Chow-Liu 
tree 
for 
each 
class. 
One 
then 
adds 
links 
from 
the 
class 
node 
c]to 
each 
variable 
and 
learns 
the 
class 
conditional 
probabilities 
from 
c]to 
x, 
which 
can 
be 
read 
off 
for 
Maximum 
Likelihood 
using 
the 
usual 
counting 
argument. 
Note 
that 
this 
would 
generally 
result 
in 
a 
dierent 
Chow-Liu 
tree 
for 
each 
class. 


Practitioners 
typically 
constrain 
the 
network 
to 
have 
the 
same 
structure 
for 
all 
classes. 
The 
Maximum 
Likelihood 
objective 
under 
the 
TAN 
constraint 
then 
corresponds 
to 
maximising 
the 
conditional 
mutual 
information[97] 


MI(xi; 
xjjc)=hKL(p(xi;xjjc)jp(xijc)p(xjjc))i(10.4.15)

p(c) 


see 
exercise(136). 
Once 
the 
structure 
is 
learned 
one 
subsequently 
sets 
parameters 
by 
Maximum 
Likelihood 
counting. 
Techniques 
to 
prevent 
overtting 
are 
discussed 
in 
[97] 
and 
can 
be 
addressed 
using 
Dirichlet 
priors, 
as 
for 
the 
simpler 
Naive 
Bayes 
structure. 


One 
can 
readily 
consider 
less 
restrictive 
structures 
than 
single-parent 
Belief 
Networks. 
However, 
the 
complexity 
of 
nding 
optimal 
BN 
structures 
is 
generally 
computationally 
infeasible 
and 
heuristics 
are 
required 
to 
limit 
the 
search 
space. 


DRAFT 
March 
9, 
2010 



Exercises 


10.5 
Code 
NaiveBayesTrain.m: 
Naive 
Bayes 
trained 
with 
Maximum 
Likelihood 
NaiveBayesTest.m: 
Naive 
Bayes 
test 
NaiveBayesDirichletTrain.m: 
Naive 
Bayes 
trained 
with 
Bayesian 
Dirichlet 
NaiveBayesDirichletTest.m: 
Naive 
Bayes 
testing 
with 
Bayesian 
Dirichlet 


demoNaiveBayes.m: 
Demo 
of 
Naive 
Bayes 


10.6 
Exercises 
Exercise 
130. 
A 
local 
supermarket 
specializing 
in 
breakfast 
cereals 
decides 
to 
analyze 
the 
buying 
patterns 
of 
its 
customers. 
They 
make 
a 
small 
survey 
asking 
6 
randomly 
chosen 
people 
their 
age 
(older 
or 
younger 
than 
60 
years) 
and 
which 
of 
the 
breakfast 
cereals 
(Cornakes, 
Frosties, 
Sugar 
Pus, 
Branakes) 
they 
like. 
Each 
respondent 
provides 
a 
vector 
with 
entries 
1 
or 
0 
corresponding 
to 
whether 
they 
like 
or 
dislike 
the 
cereal. 
Thus 
a 
respondent 
with 
(1101) 
would 
like 
Cornakes, 
Frosties 
and 
Branakes, 
but 
not 
Sugar 
Pus. 
The 
older 
than 
60 
years 
respondents 
provide 
the 
following 
data 
(1000), 
(1001), 
(1111), 
(0001). 
The 
younger 
than 
60 
years 
old 
respondents 
responded 
(0110), 
(1110). 
A 
novel 
customer 
comes 
into 
the 
supermarket 
and 
says 
she 
only 
likes 
Frosties 
and 
Sugar 
Pus. 
Using 
Naive 
Bayes 
trained 
with 
maximum 
likelihood, 
what 
is 
the 
probability 
that 
she 
is 
younger 
than 
60? 


Exercise 
131. 
A 
psychologist 
does 
a 
small 
survey 
on 
`happiness'. 
Each 
respondent 
provides 
a 
vector 
with 
entries 
1 
or 
0 
corresponding 
to 
whether 
they 
answer 
`yes’ 
to 
a 
question 
or 
`no', 
respectively. 
The 
question 
vector 
has 
attributes 


x 
=(rich, 
married, 
healthy) 
(10.6.1) 


Thus, 
a 
response 
(1, 
0, 
1) 
would 
indicate 
that 
the 
respondent 
was 
`rich', 
`unmarried', 
`healthy'. 
In 
addition, 
each 
respondent 
gives 
a 
value 
c 
=1 
if 
they 
are 
content 
with 
their 
lifestyle, 
and 
c 
=0 
if 
they 
are 
not. 
The 
foll
owing 
responses 
were 
obtained 
from 
people 
who 
claimed 
also 
to 
be 
`content’ 
: 
(1, 
1, 
1), 
(0, 
0, 
1), 
(1, 
1, 
0), 
(1, 
0, 
1) 
and 
for 
`not 
content': 
(0, 
0, 
0), 
(1, 
0, 
0), 
(0, 
0, 
1), 
(0, 
1, 
0), 
(0, 
0, 
0). 


1. 
Using 
Naive 
Bayes, 
what 
is 
the 
probability 
that 
a 
person 
who 
is 
`not 
rich', 
`married’ 
and 
`healthy’ 
is 
`content'? 
2. 
What 
is 
the 
probability 
that 
a 
person 
who 
is 
`not 
rich’ 
and 
`married’ 
is 
`content'? 
(That 
is, 
we 
do 
not 
know 
whether 
or 
not 
they 
are 
`healthy'). 
3. 
Consider 
the 
following 
vector 
of 
attributes 
: 
x1 
= 
1 
if 
customer 
is 
younger 
than 
20 
; 
x1 
= 
0 
otherwise 
(10.6.2) 
x2 
= 
1 
if 
customer 
is 
between 
20 
and 
30 
years 
old 
; 
x2 
= 
0 
otherwise 
x3 
= 
1 
if 
customer 
is 
older 
than 
30 
; 
x3 
= 
0 
otherwise 
x4 
= 
1 
if 
customer 
walks 
to 
work 
; 
x4 
= 
0 
otherwise 
(10.6.3) 
(10.6.4) 
(10.6.5) 


Each 
vector 
of 
attributes 
has 
an 
associated 
class 
label 
`rich’ 
or 
`poor'. 
Point 
out 
any 
potential 
diculties 
with 
using 
your 
previously 
described 
approach 
to 
training 
using 
Naive 
Bayes. 
Hence 
describe 
how 
to 
extend 
your 
previous 
Naive 
Bayes 
method 
to 
deal 
with 
this 
dataset. 


Exercise 
132. 
Whizzco 
decide 
to 
make 
a 
text 
classier. 
To 
begin 
with 
they 
attempt 
to 
classify 
documents 
as 
either 
sport 
or 
politics. 
They 
decide 
to 
represent 
each 
document 
as 
a 
(row) 
vector 
of 
attributes 
describing 
the 
presence 
or 
absence 
of 
words. 


x 
= 
(goal, 
football, 
golf, 
defence, 
oence, 
wicket, 
oce, 
strategy) 
(10.6.6) 


DRAFT 
March 
9, 
2010 



Exercises 


Training 
data 
from 
sport 
documents 
and 
from 
politics 
documents 
is 
represented 
below 
in 
MATLAB 
using 
a 
matrix 
in 
which 
each 
row 
represents 
the 
8 
attributes. 


xS=[1 
1 
0 
0 
0 
0 
0 
0; 
% 
Sport

xP=[10111011; 
%Politics 


0 
0 
1 
0 
0 
0 
0 
0;

0 
0 
0 
1 
0 
0 
1 
1; 


1 
1 
0 
1 
0 
0 
0 
0;

1 
0 
0 
1 
1 
0 
1 
0; 


1 
1 
0 
1 
0 
0 
0 
1;

0 
1 
0 
0 
1 
1 
0 
1; 


1 
1 
0 
1 
1 
0 
0 
0;

0 
0 
0 
1 
1 
0 
1 
1; 


0 
0 
0 
1 
0 
1 
0 
0;

0 
0 
0 
1 
1 
0 
0 
1] 


1 
1 
1 
1 
1 
0 
1 
0] 


Using 
a 
Naive 
Bayes 
classier, 
what 
is 
the 
probability 
that 
the 
document 
x 
= 
(1, 
0, 
0, 
1, 
1, 
1, 
1, 
0) 
is 
about 
politics? 


Exercise 
133. 
A 
Naive 
Bayes 
Classier 
for 
binary 
attributes 
xi 
2f0, 
1} 
is 
parameterised 
by 
1 
= 
p(xi 
= 


i 


1jclass 
= 
1), 
0 
= 
p(xi 
=1jclass 
= 
0), 
and 
p1 
= 
p(class 
= 
1) 
and 
p0 
= 
p(class 
= 
0). 
Show 
that 
the 


i 


decision 
boundary 
to 
classify 
a 
datapoint 
x 
can 
be 
written 
as 
wTx 
+ 
b> 
0, 
and 
state 
explicitly 
w 
and 
b 
as 
a 
function 
of 
1 
, 
0 
;p1;p0. 


Exercise 
134. 
This 
question 
concerns 
spam 
ltering. 
Each 
email 
is 
represented 
by 
a 
vector 


x 
=(x1;:::;xD) 
(10.6.7) 


where 
xi 
2f0, 
1g. 
Each 
entry 
of 
the 
vector 
indicates 
if 
a 
particular 
symbol 
or 
word 
appears 
in 
the 
email. 
The 
symbols/words 
are 


money, 
cash, 
!!!, 
viagra, 
. 
. 
. 
, 
etc. 
(10.6.8) 


So 
that 
x2 
=1 
if 
the 
word 
`cash’ 
appears 
in 
the 
email. 
The 
training 
dataset 
consists 
of 
a 
set 
of 
vectors 
along 
with 
the 
class 
label 
c, 
where 
c 
=1 
indicates 
the 
email 
is 
spam, 
and 
c 
=0 
not 
spam. 
Hence, 
the 


n

training 
set 
consists 
of 
a 
set 
of 
pairs 
(x;cn);n 
=1;:::;N. 
The 
Naive 
Bayes 
model 
is 
given 
by 


D

YX

p(c, 
x)= 
p(c) 
p(xijc) 
(10.6.9) 


i=1 


1. 
Draw 
a 
Belief 
Network 
for 
this 
distribution. 
2. 
Derive 
expressions 
for 
the 
parameters 
of 
this 
model 
in 
terms 
of 
the 
training 
data 
using 
Maximum 
Likelihood. 
Assume 
that 
the 
data 
is 
independent 
and 
identically 
distributed 
N

YX

1 
N 
1 
n 


p(c 
;:::;c 
;x 
;:::;x 
N 
)= 
p(c 
;x 
n) 
(10.6.10) 
n=1 


Explicitly, 
the 
parameters 
are 


p(c 
= 
1);p(xi 
=1jc 
= 
1);p(xi 
=1jc 
= 
0);i 
=1;:::;D 
(10.6.11) 


3. 
Given 
a 
trained 
model 
p(x, 
c), 
explain 
how 
to 
form 
a 
classier 
p(cjx). 
4. 
If 
`viagra’ 
never 
appears 
in 
the 
spam 
training 
data, 
discuss 
what 
eect 
this 
will 
have 
on 
the 
classic
ation 
for 
a 
new 
email 
that 
contains 
the 
word 
`viagra'. 
5. 
Write 
down 
an 
expression 
for 
the 
decision 
boundary 
p(c 
=1jx)= 
p(c 
=0jx) 
(10.6.12) 


and 
show 
that 
it 
can 
be 
written 
in 
the 
form 


D

X

udxd 
- 
b 
= 
0 
(10.6.13) 
d=1 


for 
suitably 
dened 
u 
and 
b. 


214 
DRAFT 
March 
9, 
2010 



Exercises 


Exercise 
135. 
For 
a 
distribution 
p(x, 
c) 
and 
an 
approximation 
q(x, 
c), 
show 
that 
when 
p(x, 
c) 
corresponds 
to 
the 
empirical 
distribution, 
nding 
q(x, 
c) 
that 
minimises 
the 
Kullback-Leibler 
divergence 


KL(p(x, 
c)jq(x, 
c)) 
(10.6.14) 


corresponds 
to 
Maximum 
Likelihood 
training 
of 
q(x, 
c). 


Exercise 
136. 
Consider 
a 
distribution 
p(x, 
c) 
and 
a 
Tree 
Augmented 
approximation 


Y

q(x, 
c)= 
q(c)q(xijxpa(i);c), 
pa(i) 
<i 
or 
pa(i)= 
Ø 
(10.6.15) 
i 


Show 
that 
for 
the 
optimal 
q(x, 
c) 
constrained 
as 
above, 
the 
solution 
q(x, 
c) 
that 
minimises 
KL(p(x, 
c)jq(x, 
c)) 
when 
plugged 
back 
into 
the 
Kullback-Leibler 
expression 
gives, 
as 
a 
function 
of 
the 
parental 
structure, 




Xp(xi;xpa(i)jc)

KL(p(x, 
c)jq(x, 
c)) 
= 
..log 
+ 
const. 
(10.6.16) 
p(xpa(i)jc)p(xijc)

ip(xi;xpa(i);c) 


This 
shows 
that 
under 
the 
single-parent 
constraint 
and 
that 
each 
tree 
q(xjc) 
has 
the 
same 
structure, 
mini
mising 
the 
Kullback-Leibler 
divergence 
is 
equivalent 
to 
maximising 
the 
sum 
of 
conditional 
mutual 
inform
ation 
terms. 


Exercise 
137. 
Write 
a 
MATLAB 
routine 
A 
= 
ChowLiu(X) 
where 
X 
is 
a 
D 
× 
N 
data 
matrix 
containing 
a 
multivariate 
datapoint 
on 
each 
column 
that 
returns 
a 
Chow-Liu 
Maximum 
Likelihood 
tree 
for 
X. 
The 
tree 
structure 
is 
to 
be 
returned 
in 
the 
sparse 
matrix 
A. 
You 
may 
nd 
the 
routine 
spantree.m 
useful. 
The 
le 
ChowLiuData.mat 
contains 
a 
data 
matrix 
for 
10 
variables. 
Use 
your 
routine 
to 
nd 
the 
Maximum 
Likelihood 
Chow 
Liu 
tree, 
and 
draw 
a 
picture 
of 
the 
resulting 
DAG 
with 
edges 
oriented 
away 
from 
variable 


1. 
DRAFT 
March 
9, 
2010 



Exercises 


DRAFT 
March 
9, 
2010 



CHAPTER 
11 


Learning 
with 
Hidden 
Variables 


11.1 
Hidden 
Variables 
and 
Missing 
Data 
In 
practice 
data 
entries 
are 
often 
missing 
resulting 
in 
incomplete 
information 
to 
specify 
a 
likelihood. 
Observational 
variables 
may 
be 
split 
into 
visible 
(those 
for 
which 
we 
actually 
know 
the 
state) 
and 
missing 
(those 
whose 
states 
would 
nominally 
be 
known 
but 
are 
missing 
for 
a 
particular 
datapoint). 


Another 
scenario 
in 
which 
not 
all 
variables 
in 
the 
model 
are 
observed 
are 
the 
so-called 
hidden 
or 
latent 
variable 
models. 
In 
this 
case 
there 
are 
variables 
which 
are 
essential 
for 
the 
model 
description 
but 
never 
observed. 
For 
example, 
the 
underlying 
physics 
of 
a 
model 
may 
contain 
latent 
processes 
which 
are 
essential 
to 
describe 
the 
model, 
but 
cannot 
be 
directly 
measured. 


11.1.1 
Why 
hidden/missing 
variables 
can 
complicate 
proceedings 
In 
learning 
the 
parameters 
of 
models 
as 
previously 
described 
in 
chapter(9), 
we 
assumed 
we 
have 
complete 
information 
to 
dene 
all 
variables 
of 
the 
joint 
model 
of 
the 
data 
p(vj). 
Consider 
the 
Asbestos-Smoking-
Cancer 
network 
of 
section(9.2.3). 
In 
the 
case 
of 
complete 
data, 
the 
likelihood 
is 


nn 
n 


p(v 
nj)= 
p(a 
;s 
;c 
nj)= 
p(c 
nja 
;s 
n;c)p(a 
nja)p(s 
njs) 
(11.1.1) 


which 
is 
factorised 
in 
terms 
of 
the 
table 
entry 
parameters. 
We 
exploited 
this 
property 
to 
show 
that 
table 
entries 
. 
can 
be 
learned 
by 
considering 
only 
local 
information, 
both 
in 
the 
Maximum 
Likelihood 
and 
Bayesian 
frameworks. 


Now 
consider 
the 
case 
that 
for 
some 
of 
the 
patients, 
only 
partial 
information 
is 
available. 
For 
example, 


n

for 
patient 
n 
with 
record 
v= 
fc 
= 
1;s 
= 
1} 
it 
is 
known 
that 
the 
patient 
has 
cancer 
and 
is 
a 
smoker, 
but 
whether 
or 
not 
they 
had 
exposure 
to 
asbestos 
is 
unknown. 
Since 
we 
can 
only 
use 
the 
`visible’ 
available 
information 
is 
it 
would 
seem 
reasonable 
to 
assess 
parameters 
using 
the 
marginal 
likelihood 


XX

n 


p(v 
nj)=p(a;s 
;c 
nj)=p(c 
nja, 
s 
n;c)p(aja)p(s 
njs) 
(11.1.2) 
aa 


We 
will 
discuss 
when 
this 
approach 
is 
valid 
in 
section(11.1.2). 
Using 
the 
marginal 
likelihood 
may 
result 
in 
computational 
diculties 
since 
equation 
(11.1.2) 
is 
not 
factorised 
over 
the 
tables. 
This 
means 
that 
the 
likelihood 
function 
cannot 
be 
written 
as 
a 
product 
of 
functions, 
one 
for 
each 
separate 
parameter. 
In 
this 
case 
the 
maximisation 
of 
the 
likelihood 
is 
more 
complex 
since 
the 
parameters 
of 
dierent 
tables 
are 
coupled. 


A 
similar 
complication 
holds 
for 
Bayesian 
learning. 
As 
we 
saw 
in 
section(13), 
under 
a 
prior 
factorised 
over 
each 
CPT 
, 
the 
posterior 
is 
also 
factorised. 
However, 
in 
the 
case 
of 
unknown 
asbestos 
exposure, 
a 
term 


217 



xvisxinv

minv
xvisxinv

minv
xvisxinv

minv
xvisxinv

minv
Hidden 
Variables 
and 
Missing 
Data 


Figure 
11.1: 
(a): 
Missing 
at 
random 
assumption. 
(b): 
Missing 
completely 
at 
random 
assumption. 


(a) 
(b) 
is 
introduced 
of 
the 
form 


XX

p(v 
nj)=p(c 
nja, 
s 
n;c)p(aja)p(s 
njs)= 
p(s 
njs)p(c 
nja, 
s 
n;c)p(aja) 
(11.1.3) 
aa 


which 
cannot 
be 
written 
as 
a 
product 
of 
a 
functions 
of 
fs(s)fa(a)fc(c). 
The 
missing 
variable 
therefore 
introduces 
dependencies 
in 
the 
posterior 
parameter 
distribution, 
making 
the 
posterior 
more 
complex. 


In 
both 
the 
Maximum 
Likelihood 
and 
Bayesian 
cases, 
one 
has 
a 
well 
dened 
likelihood 
function 
of 
the 
table 
parameters/posterior. 
The 
diculty 
is 
therefore 
not 
conceptual, 
but 
rather 
computational 
– 
how 
are 
we 
to 
nd 
the 
optimum 
of 
the 
likelihood/summarise 
the 
posterior? 


Note 
that 
missing 
data 
does 
not 
always 
make 
the 
parameter 
posterior 
non-factorised. 
For 
example, 
if 
the 
cancer 
state 
is 
unobserved 
above, 
because 
cancer 
is 
a 
collider 
with 
no 
descendants, 
the 
conditional 
distribution 
simply 
sums 
to 
1, 
and 
one 
is 
left 
with 
a 
factor 
dependent 
on 
a 
and 
another 
on 
s. 


11.1.2 
The 
missing 
at 
random 
assumption 
Under 
what 
circumstances 
is 
it 
valid 
to 
use 
the 
marginal 
likelihood 
to 
assess 
parameters? 
We 
partition 
the 
variables 
x 
into 
those 
that 
are 
`visible', 
xvis 
and 
`invisible', 
xinv, 
so 
that 
the 
set 
of 
all 
variables 
can 
be 
written 
x 
=[xvis;xinv]. 
For 
the 
visible 
variables 
we 
have 
an 
observed 
state 
xvis 
= 
v, 
whereas 
the 
state 
of 
the 
invisible 
variables 
is 
unknown. 
We 
use 
an 
indicator 
minv 
= 
1 
to 
denote 
that 
the 
state 
of 
the 
invisible 
variables 
is 
unknown. 
Then 
for 
a 
datapoint 
which 
contains 
both 
visible 
and 
invisible 
information, 


X

p(xvis 
= 
v;minv 
= 
1j)=p(xvis 
= 
v;xinv;minv 
= 
1j) 
(11.1.4) 


xinv

X

=p(minv 
= 
1jxvis 
= 
v;xinv;)p(xvis 
= 
v;xinvj) 
(11.1.5) 


xinv 


If 
we 
assume 
that 
the 
mechanism 
which 
generates 
invisible 
data 
has 
the 
form 


p(minv 
= 
1jxvis 
= 
v;xinv;)= 
p(minv 
= 
1jxvis 
= 
v) 
(11.1.6) 


then 


X

p(xvis 
= 
v;minv 
= 
1j)= 
p(minv 
= 
1jxvis 
= 
v)p(xvis 
= 
v;xinvj) 
(11.1.7) 


xinv 


= 
p(minv 
= 
1jxvis 
= 
v)p(xvis 
= 
vj) 
(11.1.8) 


Only 
the 
term 
p(xvis 
= 
vj) 
conveys 
information 
about 
the 
model. 
Therefore, 
provided 
the 
mechanism 
by 
which 
the 
data 
is 
missing 
depends 
only 
on 
the 
visible 
states, 
we 
may 
simply 
use 
the 
marginal 
likelihood 
to 
assess 
parameters. 
This 
is 
called 
the 
missing 
at 
random 
assumption. 


Example 
50 
(Not 
missing 
at 
random). 
EZsurvey.org 
stop 
men 
on 
the 
street 
and 
ask 
them 
their 
favourite 
colour. 
All 
men 
whose 
favourite 
colour 
is 
pink 
decline 
to 
respond 
to 
the 
question 
– 
for 
any 
other 
colour, 
all 
men 
respond 
to 
the 
question. 
Based 
on 
the 
data, 
EZsurvey.org 
produce 
a 
histogram 
of 
men's 
favourite 
colour, 
based 
on 
the 
likelihood 
of 
the 
visible 
data 
alone, 
condently 
stating 
that 
none 
of 
them 
likes 
pink. 


DRAFT 
March 
9, 
2010 



Hidden 
Variables 
and 
Missing 
Data 


For 
simplicity, 
assume 
there 
are 
only 
three 
colours, 
blue, 
green 
and 
pink. 
EZsurvey.org 
attempts 
to 
nd 
the 
histogram 
with 
probabilities 
b;g;p 
with 
b 
+ 
g 
+ 
p 
= 
1. 
Each 
respondent 
produces 
a 
visible 
response 
xc 
with 
dom(xc)= 
fblue, 
green, 
pinkg, 
otherwise 
mc 
= 
1 
if 
there 
is 
no 
response. 
Three 
men 
are 
asked 
their 
favourite 
colour, 
giving 
data

	

123 


x 
;x 
;x 
= 
fblue, 
missing, 
green} 
(11.1.9)

ccc

Based 
on 
the 
likelihood 
of 
the 
visible 
data 
alone 
we 
have 
the 
log 
likelihood 
for 
i.i.d. 
data 


L(b;g;p) 
= 
log 
b 
+ 
log 
g 
+ 
. 
(1 
- 
b 
- 
g 
- 
p) 
(11.1.10) 


where 
the 
last 
Lagrange 
term 
ensures 
normalisation. 
Maximising 
the 
expression 
we 
arrive 
at 
(see 
exercise(145)) 


11 


b 
= 
;g 
= 
;p 
= 
0 
(11.1.11)

22

The 
unreasonable 
result 
that 
EZsurvey.org 
produce 
is 
due 
to 
not 
accounting 
correctly 
for 
the 
mechanism 
which 
produces 
the 
data. 


The 
correct 
mechanism 
that 
generates 
the 
data 
(including 
the 
missing 
data 
is) 


1 
23 


p(c 
= 
bluej)p(mc 
=1j)p(c 
= 
greenj)= 
bpg 
= 
b 
(1 
- 
b 
- 
g) 
g 
(11.1.12) 


2

where 
we 
used 
p(m=1j)= 
p 
since 
the 
probability 
that 
a 
datapoint 
is 
missing 
is 
the 
same 
as 
the 


c 


probability 
that 
the 
favourite 
colour 
is 
pink. 
Maximising 
the 
likelihood, 
we 
arrive 
at 


111 


b 
= 
;g 
= 
;p 
= 
(11.1.13)

333 


as 
we 
would 
expect. 
On 
the 
other 
hand 
if 
there 
is 
another 
visible 
variable, 
t, 
denoting 
the 
time 
of 
day, 
and 
the 
probability 
that 
men 
respond 
to 
the 
question 
depends 
only 
on 
the 
time 
t 
alone 
(for 
example 
the 
missing 
probability 
is 
high 
during 
rush 
hour), 
then 
we 
may 
indeed 
treat 
the 
missing 
data 
as 
missing 
at 
random. 


A 
stronger 
assumption 
than 
MAR 
is 


p(minv 
= 
1jxvis 
= 
v;xinv;)= 
p(minv 
= 
1) 
(11.1.14) 


which 
is 
called 
missing 
completely 
at 
random. 
This 
applies 
for 
example 
to 
latent 
variable 
models 
in 
which 
the 
variable 
state 
is 
always 
missing, 
independent 
of 
anything 
else. 


11.1.3 
Maximum 
likelihood 
Throughout 
the 
remaining 
discussion 
we 
will 
assume 
any 
missing 
data 
is 
MAR 
or 
missing 
completely 
at 
random. 
This 
means 
that 
we 
can 
treat 
any 
unobserved 
variables 
by 
summing 
(or 
integrating) 
over 
their 
states. 
For 
Maximum 
Likelihood 
we 
learn 
model 
parameters 
. 
by 
optimising 
the 
marginal 
likelihood 


X

p(vj)=p(v, 
hj) 
(11.1.15) 


h 


with 
respect 
to 
. 


11.1.4 
Identiability 
issues 
The 
marginal 
likelihood 
objective 
function 
depends 
on 
the 
parameters 
only 
through 
p(vj), 
so 
that 
equivalent 
parameter 
solutions 
may 
exist. 
For 
example, 
consider 
a 
latent 
variable 
problem 
with 
distribution 


p(x1;x2j)= 
x1;x2 
(11.1.16) 


DRAFT 
March 
9, 
2010 
219 



Expectation 
Maximisation 


in 
which 
variable 
x2 
is 
never 
observed. 
This 
means 
that 
the 
marginal 
likelihood 
only 
depends 
on 
the 
entry 


P

p(x1j)=x1;x2 
. 
Given 
a 
Maximum 
Likelihood 
solution 
, 
we 
can 
then 
always 
nd 
an 
equivalent 


x2 


Maximum 
Likelihood 
solution 
/ 
provided 
(see 
exercise(146))

XX

/ 
=. 
* 
(11.1.17)

x1;x2 
x1;x2 


x2 
x2 


In 
other 
cases 
there 
is 
an 
inherent 
symmetry 
in 
the 
parameter 
space 
of 
the 
marginal 
likelihood. 
For 
example, 
consider 
the 
network 
over 
binary 
variables 


p(c, 
a, 
s)= 
p(cja, 
s)p(a)p(s) 
(11.1.18) 


Our 
aim 
is 
to 
learn 
the 
table 


p^(a 
= 
1) 
(11.1.19) 


and 
the 
four 
tables 


p^(c 
= 
1ja 
= 
1;s 
= 
1);p^(c 
= 
1ja 
= 
1;s 
= 
0);p^(c 
= 
1ja 
= 
0;s 
= 
1);p^(c 
= 
1ja 
= 
0;s 
= 
0) 
(11.1.20) 


where 
we 
used 
a 
`^’ 
to 
denote 
that 
these 
are 
parameter 
estimates. 


We 
assume 
that 
we 
have 
missing 
data 
such 
that 
the 
states 
of 
variable 
a 
are 
never 
observed. 
In 
this 
case 
an 
equivalent 
solution 
(in 
the 
sense 
that 
it 
has 
the 
same 
marginal 
likelihood) 
is 
given 
by 
interchanging 
the 
states 
of 
a: 


p^0(a 
= 
0)= 
p^(a 
= 
1) 
(11.1.21) 


and 
the 
four 
tables 


p^0(c 
= 
1ja 
= 
0;s 
= 
1)= 
p^(c 
= 
1ja 
= 
1;s 
= 
1);p^0(c 
= 
1ja 
= 
0;s 
= 
0)= 
p^(c 
= 
1ja 
= 
1;s 
= 
0) 


p^0(c 
= 
1ja 
= 
1;s 
= 
1)= 
p^(c 
= 
1ja 
= 
0;s 
= 
1);p^0(c 
= 
1ja 
= 
1;s 
= 
0)= 
p^(c 
= 
1ja 
= 
0;s 
= 
0) 


A 
similar 
situation 
occurs 
in 
a 
more 
general 
setting 
in 
which 
the 
state 
of 
a 
variable 
is 
consistently 
unobserved 
(mixture 
models 
are 
a 
case 
in 
point) 
yielding 
an 
inherent 
symmetry 
in 
the 
solution 
space. 
A 
well 
known 
characteristic 
of 
Maximum 
Likelihood 
algorithms 
is 
that 
`jostling’ 
occurs 
in 
the 
initial 
stages 
of 
training 
in 
which 
these 
symmetric 
solutions 
compete. 


11.2 
Expectation 
Maximisation 
The 
EM 
algorithm 
is 
a 
convenient 
and 
general 
purpose 
iterative 
approach 
to 
maximising 
the 
likelihood 
under 
missing 
data/hidden 
variables[187]. 
It 
is 
generally 
straightforward 
to 
implement 
and 
can 
achieve 
large 
jumps 
in 
parameter 
space, 
particularly 
in 
the 
initial 
iterations. 


11.2.1 
Variational 
EM 
The 
key 
feature 
of 
the 
EM 
algorithm 
is 
to 
form 
an 
alternative 
objective 
function 
for 
which 
the 
parameter 
coupling 
eect 
discussed 
in 
section(11.1.1) 
is 
removed, 
meaning 
that 
individual 
parameter 
updates 
can 
be 
achieved, 
akin 
to 
the 
case 
of 
fully 
observed 
data. 
The 
way 
this 
works 
is 
to 
replace 
the 
marginal 
likelihood 
with 
a 
lower 
bound 
– 
it 
is 
this 
lower 
bound 
that 
has 
the 
decoupled 
form. 


We 
rst 
consider 
a 
single 
variable 
pair 
(v, 
h), 
where 
v 
stands 
for 
`visible’ 
and 
h 
for 
`hidden'. 
To 
derive 
the 
bound 
on 
the 
marginal 
likelihood, 
consider 
the 
Kullback-Leibler 
divergence 
between 
a 
`variational’ 
distribution 
q(hjv) 
and 
the 
parametric 
model 
p(hjv, 
): 


KL(q(hjv)jp(hjv, 
)) 
hlog 
q(hjv) 
- 
log 
p(hjv, 
)iq(hjv) 
= 
0 
(11.2.1) 


220 
DRAFT 
March 
9, 
2010 



Expectation 
Maximisation 


The 
term 
`variational’ 
refers 
to 
the 
fact 
that 
this 
distribution 
will 
be 
a 
parameter 
of 
an 
optimisation 
problem. 
Using 
Bayes’ 
rule, 
p(hjv, 
)= 
p(h, 
vj)=p(vj) 
and 
the 
fact 
that 
p(vj) 
does 
not 
depend 
on 
h, 


hlog 
q(hjv)iq(hjv) 
..hlog 
p(h, 
vj)iq(hjv) 
+ 
log 
p(vj) 
= 
KL(q(hjv)jp(hjv, 
)) 
= 
0 
(11.2.2) 
Rearranging, 
we 
obtain 
a 
bound 
on 
the 
marginal 
likelihood1 


log 
p(vj) 
..hlog 
q(hjv)iq(hjv) 
+ 
hlog 
p(h, 
vj)iq(hjv) 
(11.2.3) 


|{z}|{z}z

Entropy 
Energy 


The 
bound 
is 
potentially 
useful 
since 
it 
is 
similar 
in 
form 
to 
the 
fully 
observed 
case, 
except 
that 
terms 
with 
missing 
data 
have 
their 
log 
likelihood 
weighted 
by 
a 
prefactor. 
Equation(11.2.3) 
is 
a 
marginal 
likelihood 
bound 
for 
a 
single 
training 
example. 
Under 
the 
i.i.d. 
assumption, 
the 
log 
likelihood 
of 
all 
training 
data 


	z

1N

V 
=v;:::;vis 
the 
sum 
of 
the 
individual 
log 
likelihoods: 


N

Xz

log 
p(Vj) 
= 
log 
p(v 
nj) 
(11.2.4) 


n=1 


Summing 
over 
the 
training 
data, 
we 
obtain 
a 
bound 
on 
the 
log 
(marginal) 
likelihood 


NN

XXz

log 
p(Vj) 
- 
hlog 
q(hnjv 
n)iq(hnjvn) 
+ 
hlog 
p(hn 
;v 
nj)iq(hnjvn) 
(11.2.5) 


n=1 
n=1 


Note 
that 
the 
bound 
is 
exact 
(that 
is, 
the 
right 
hand 
side 
is 
equal 
to 
the 
log 
likelihood) 
when 
we 
set 
q(hnjvn)= 
p(hnjvn;);n 
=1;:::;N. 


The 
bound 
suggests 
an 
iterative 
procedure 
to 
optimise 
: 


E-step 
For 
xed 
, 
nd 
the 
distributions 
q(hnjvn) 
that 
maximise 
equation 
(11.2.5). 


M-step 
For 
xed 
fq(hnjvn);n 
=1;:::;Ng, 
nd 
the 
parameters 
. 
that 
maximise 
equation 
(11.2.5). 


11.2.2 
Classical 
EM 
In 
the 
variational 
E-step 
above, 
the 
fully 
optimal 
setting 
is 


q(hnjv 
n)= 
p(hnjv 
n;) 
(11.2.6) 


Since 
q 
does 
not 
depend 
on 
new 
the 
M-step 
is 
equivalent 
to 
maximising 
the 
energy 
term 
alone, 
see 
algorithm(8). 


Example 
51 
(A 
one-parameter 
one-state 
example). 
We 
consider 
a 
model 
small 
enough 
that 
we 
can 
plot 
fully 
the 
evolution 
of 
the 
EM 
algorithm. 
The 
model 
is 
on 
a 
single 
visible 
variable 
v 
and 
single 
two-state 
hidden 
variable 
h 
2f1, 
2g. 
We 
dene 
a 
model 
p(v, 
h)= 
p(vjh)p(h) 
with 


1 
- 
1 
(v..h)2 


p(vjh, 
)= 
v 
e 
22 
(11.2.7)
22 


and 
p(h 
= 
1) 
= 
p(h 
= 
2) 
= 
0:5. 
For 
an 
observation 
v 
=2:75 
and 
2 
=0:5 
our 
interest 
is 
to 
nd 
the 
parameter 
. 
that 
optimises 
the 
likelihood 


X

1 


..(2:75..h)2 


p(v 
=2:75j)= 
v 
e 
(11.2.8)

2p 


h=1;2 


1This 
is 
analogous 
to 
a 
standard 
partition 
function 
bound 
in 
statistical 
physics, 
from 
where 
the 
terminology 
`energy’ 
and 
`entropy’ 
hails. 


DRAFT 
March 
9, 
2010 
221 



Expectation 
Maximisation 


Algorithm 
8 
Expectation 
Maximisation. 
Compute 
Maximum 
Likelihood 
value 
for 
data 
with 
hidden 
variables. 
Input: 
a 
distribution 
p(xj) 
and 
dataset 
V. 
Returns 
ML 
setting 
of 
. 


1: 
t 
=0 
I 
Iteration 
counter 
2: 
Choose 
an 
initial 
setting 
for 
the 
parameters 
0 
. 
I 
Initialisation 
3: 
while 
. 
not 
converged 
(or 
likelihood 
not 
converged) 
do 
4: 
t 
. 
t 
+ 
1 
5: 
for 
n 
= 
1 
to 
N 
do 
I 
Run 
over 
all 
datapoints 
6: 
7: 
8: 
9: 
qn 
t 
(hnjvn) 
= 
p(hnjvn, 
t..1) 
end 
for 
t 
= 
arg 
maxPN 
n=1 
hlog 
p(hn, 
vnj)iqn 
t 
(hnjvn) 
end 
while 
I 
E 
step 
I 
M 
step 


10: 
return 
t 
I 
The 
max 
likelihood 
parameter 
estimate. 
11.522.53-1.55-1.5-1.45-1.4-1.35-1.3-1.25-1.2-1.15-1.1-1.05
µlogp(v|µ)µq(h=2)
11.522.530.10.20.30.40.50.60.70.80.9
µq(h=2)
11.522.530.10.20.30.40.50.60.70.80.9
(a) 
(b) 
(c) 
Figure 
11.2: 
(a): 
The 
log 
likelihood 
for 
the 
model 
described 
in 
example(51). 
(b): 
Contours 
of 
the 
lower 
bound 
LB(q(h 
= 
2);). 
For 
an 
initial 
choice 
q(h 
= 
2) 
= 
0:5 
and 
. 
=1:9, 
successive 
updates 
of 
the 
E 
(vertical) 
and 
M 
(horizontal) 
steps 
are 
plotted. 
(c): 
Starting 
at 
. 
=1:95, 
the 
EM 
algorithm 
converges 
to 
a 
local 
optimum. 


The 
log 
likelihood 
is 
plotted 
in 
g(11.2a) 
with 
optimum 
at 
. 
=1:325. 
The 
EM 
procedure 
iteratively 
optimises 
the 
lower 
bound 


log 
p(v 
=2:75j) 
= 
LB(q(h 
= 
2);) 


X

..q(h 
= 
1) 
log 
q(h 
= 
1) 
- 
q(h 
= 
2) 
log 
q(h 
= 
2) 
..q(h) 
(2:75 
- 
h)2 
+ 
log 
2 
(11.2.9) 


h=1;2 


where 
q(h 
= 
1) 
= 
1 
- 
q(h 
= 
2). 
From 
an 
initial 
starting 
, 
the 
EM 
algorithm 
nds 
the 
q 
distribution 
that 
optimises 
L(q, 
) 
(E-step) 
and 
then 
updates 
. 
(M-step). 
Depending 
on 
the 
initial 
, 
the 
solution 
found 
is 
either 
a 
global 
or 
local 
optimum 
of 
the 
likelihood, 
see 
g(11.2). 





The 
M-step 
is 
easy 
to 
work 
out 
analytically 
in 
this 
case 
with 
new 
= 
v 
hhi=h2. 
Similarly, 
the 


q(h) 
q(h)
E-step 
sets 
qnew(h)= 
p(hjv, 
) 
so 
that 


..(2:75..2)2 


p(v 
=2:75jh 
=2;)p(h 
= 
2) 
e

q 
new(h 
= 
2) 
= 
= 
(11.2.10) 


p(v 
=2:75) 
e..(2:75..2)2 
+ 
e..(2:75..)2 


where 
we 
used 
p(v 
= 
2:75) 
= 
p(v 
= 
2:75jh 
= 
1, 
)p(h 
= 
1) 
+ 
p(v 
= 
2:75jh 
= 
2, 
)p(h 
= 
2) 
(11.2.11) 
222 
DRAFT 
March 
9, 
2010 



Expectation 
Maximisation 


Example 
52. 
Consider 
a 
simple 
model 


p(x1;x2j) 
(11.2.12) 
where 
dom(x1) 
= 
dom(x2)= 
f1, 
2g. 
Assuming 
an 
unconstrained 
distribution 


p(x1;x2j)= 
x1;x2 
;1;1 
+ 
1;2 
+ 
2;1 
+ 
2;2 
= 
1 
(11.2.13) 
our 
aim 
is 
to 
learn 
. 
from 
the 
data 
x1 
=(1, 
1) 
, 
x2 
=(1, 
?) 
, 
x3 
= 
(?, 
2). 
The 
energy 
term 
for 
the 
classical 
EM 
is 


log 
p(x1 
= 
1;x2 
= 
1j)+ 
hlog 
p(x1 
= 
1;x2j)) 
p(x2jx1=1;old) 
+ 
hlog 
p(x1;x2 
= 
2j)) 
p(x1jx2=2;old) 


(11.2.14) 
Writing 
out 
fully 
each 
of 
the 
above 
terms 
on 
a 
separate 
line 
gives 
the 
energy 
log 
1;1 
(11.2.15) 
+p(x2 
= 
1jx1 
= 
1;old) 
log 
1;1 
+ 
p(x2 
= 
2jx1 
= 
1;old) 
log 
1;2 
(11.2.16) 
+p(x1 
= 
1jx2 
= 
2;old) 
log 
1;2 
+ 
p(x1 
= 
2jx2 
= 
2;old) 
log 
2;2 
(11.2.17) 


This 
expression 
resembles 
the 
standard 
log 
likelihood 
of 
fully 
observed 
data 
except 
that 
terms 
with 
missing 
data 
have 
their 
weighted 
log 
parameters. 
The 
parameters 
are 
conveniently 
decoupled 
in 
this 
bound 
(apart 
from 
the 
trivial 
normalisation 
constraint) 
so 
that 
nding 
the 
optimal 
parameters 
is 
straightforward. 
This 
is 
achieved 
by 
the 
M-step 
update 
which 
gives 


1;1 
. 
1+ 
p(x2 
= 
1jx1 
= 
1;old) 
1;2 
. 
p(x2 
= 
2jx1 
= 
1;old)+ 
p(x1 
= 
1jx2 
= 
2;old) 


(11.2.18)
2;1 
=0 
2;2 
. 
p(x1 
= 
2jx2 
= 
2;old) 


where 
p(x2jx1;old) 
. 
old 
(E-step) 
etc. 
The 
E 
and 
M-steps 
are 
iterated 
till 
convergence. 


x1;x2 


The 
EM 
algorithm 
increases 
the 
likelihood 


Whilst, 
by 
construction, 
the 
EM 
algorithm 
cannot 
decrease 
the 
bound 
on 
the 
likelihood, 
an 
important 
question 
is 
whether 
or 
not 
the 
log 
likelihood 
itself 
is 
necessarily 
increased 
by 
this 
procedure. 


We 
use 
' 
for 
the 
new 
parameters, 
and 
. 
for 
the 
previous 
parameters 
in 
two 
consecutive 
iterations. 
Using 
q(hnjvn)= 
p(hnjvn;) 
we 
see 
that 
as 
a 
function 
of 
the 
parameters, 
the 
lower 
bound 
for 
a 
single 
variable 
pair 
(v, 
h) 
depends 
on 
. 
and 
0: 





LB(0j) 
..hlog 
p(hjv, 
)ip(hjv;) 
+log 
p(h, 
vj0)p(hjv;) 
(11.2.19) 
and 


log 
p(vj0)= 
LB(0j)+ 
KL(p(hjv, 
)jp(hjv, 
0)) 
(11.2.20) 
That 
is, 
the 
Kullback-Leibler 
divergence 
is 
the 
dierence 
between 
the 
lower 
bound 
and 
the 
true 
likelihood. 
We 
may 
write 


log 
p(vj)= 
LB(j)+ 
KL(p(hjv, 
)jp(hjv, 
)) 
(11.2.21)

|{z}z

0 


Hence 
log 
p(vj0) 
- 
log 
p(vj)= 
LB(0j) 
- 
LB(j)+ 
KL(p(hjv, 
)jp(hjv, 
0)) 
(11.2.22)

|{z}|{z}z

0 
0 


DRAFT 
March 
9, 
2010 



Expectation 
Maximisation 


s 
c 
1 
1 
0 
0 
1 
1 
1 
0 
1 
1 
0 
0 
0 
1 


Figure 
11.3: 
A 
database 
containing 
information 
about 
being 
a 
Smoker 
(1 
signies 
the 
individual 
is 
a 
smoker), 
and 
lung 
Cancer 
(1 
signies 
the 
individual 
has 
lung 
Cancer). 
Each 
row 
contains 
the 
information 
for 
an 
individual, 
so 
that 
there 
are 
7 
individuals 
in 
the 
database. 


The 
rst 
assertion 
is 
true 
since, 
by 
denition 
of 
the 
M-step, 
we 
search 
for 
a 
' 
which 
has 
a 
higher 
value 
for 
the 
bound 
than 
our 
starting 
value 
. 
The 
second 
assertion 
is 
true 
by 
the 
property 
of 
the 
Kullback-Leibler 
divergence. 


For 
more 
than 
a 
single 
datapoint, 
we 
simply 
sum 
each 
individual 
bound 
for 
log 
p(vnj). 
Hence 
we 
reach 
the 
important 
conclusion 
that 
the 
EM 
algorithm 
increases, 
not 
only 
the 
lower 
bound 
on 
the 
marginal 
likelihood, 
but 
the 
marginal 
likelihood 
itself 
(more 
correctly, 
the 
EM 
cannot 
decrease 
these 
quantities). 


Shared 
parameters 
and 
tables 


The 
case 
of 
tables 
sharing 
parameters 
is 
essentially 
straightforward. 
According 
to 
the 
energy 
term, 
we 
need 
to 
identify 
all 
those 
terms 
in 
which 
the 
shared 
parameter 
occurs. 
The 
objective 
for 
the 
shared 
parameter 
is 
then 
the 
sum 
over 
all 
energy 
terms 
containing 
the 
shared 
parameter. 


11.2.3 
Application 
to 
Belief 
networks 
Conceptually, 
the 
application 
of 
EM 
to 
training 
Belief 
Networks 
with 
missing 
data 
is 
straightforward. 
The 
battle 
is 
more 
notational 
than 
conceptual. 
We 
begin 
the 
development 
with 
an 
example, 
from 
which 
intuition 
about 
the 
general 
case 
can 
be 
gleaned. 


Example 
53. 
Consider 
the 
network. 


p(a, 
c, 
s)= 
p(cja, 
s)p(a)p(s) 
(11.2.23) 


for 
which 
we 
have 
a 
set 
of 
data, 
but 
that 
the 
states 
of 
variable 
a 
are 
never 
observed, 
see 
g(11.3). 
Our 
goal 
is 
to 
learn 
the 
CPTs 
p(cja, 
s) 
and 
p(a) 
and 
p(s). 
To 
apply 
EM, 
algorithm(8) 
to 
this 
case, 
we 
rst 
assume 
initial 
parameters 
0 
, 
0 
, 
0 
.

as 
c 


The 
rst 
E-step, 
for 
iteration 
t 
= 
1 
then 
denes 
a 
set 
of 
distributions 
on 
the 
hidden 
variables 
(here 
the 
hidden 
variable 
is 
a) 


n=1 
n=2 


q 
(a)= 
p(ajc 
=1;s 
=1;0);q 
(a)= 
p(ajc 
=0;s 
=0;0) 
(11.2.24)

t=1 
t=1 


n

and 
so 
on 
for 
the 
7 
training 
examples, 
n 
=2;:::, 
7. 
For 
notational 
convenience, 
we 
write 
q(a) 
in 
place 


t 


n

of 
q(ajvn).

t 


We 
now 
move 
to 
the 
rst 
M-step. 
The 
energy 
term 
for 
any 
iteration 
t 
is: 


7

Xn

n

E()= 
hlog 
p(c 
nja 
;s 
n) 
+ 
log 
p(a 
n) 
+ 
log 
p(s 
n)i

(11.2.25)
q

n
t

(a) 
n=1 


7

Xnno

= 


n

hlog 
p(c 
nja 
;s 
n)i

q

n
t

(a) 
+ 
hlog 
p(a 
n)i
q

n
t

(a) 
+ 
log 
p(s 
n)(11.2.26) 
n=1

The 
nal 
term 
is 
the 
log 
likelihood 
of 
the 
variable 
s, 
and 
p(s) 
appears 
explicitly 
only 
in 
this 
term. 
Hence, 
the 
usual 
maximum 
likelihood 
rule 
applies, 
and 
p(s 
= 
1) 
is 
simply 
given 
by 
the 
relative 
number 
of 
times 


DRAFT 
March 
9, 
2010 



Expectation 
Maximisation 


that 
s 
= 
1 
occurs 
in 
the 
database, 
giving 
p(s 
= 
1) 
= 
4=7, 
p(s 
= 
0) 
= 
3=7. 


The 
parameter 
p(a 
= 
1) 
occurs 
in 
the 
terms

X

nn

fq 
(a 
= 
0) 
log 
p(a 
= 
0)+ 
q 
(a 
= 
1) 
log 
p(a 
= 
1)gh(11.2.27)

tt 
n 


which, 
using 
the 
normalisation 
constraint 
is 


XX

nn

log 
p(a 
= 
0)q 
(a 
= 
0) 
+ 
log(1 
..hp(a 
= 
0))q 
(a 
= 
1) 
(11.2.28)

tt 
nn 


Dierentiating 
with 
respect 
to 
p(a 
= 
0) 
and 
solving 
for 
the 
zero 
derivative 
we 
get 


P

qn(a 
=0)1 
X

nt 
n 


p(a 
= 
0) 
=PP= 
q 
(a 
= 
0) 
(11.2.29)

nn 
t 


q(a 
= 
0)+q(a 
= 
1) 
N

nt 
nt 


n 


That 
is, 
whereas 
in 
the 
standard 
Maximum 
Likelihood 
estimate, 
we 
would 
have 
the 
real 
counts 
of 
the 


nn

data 
in 
the 
above 
formula, 
here 
they 
have 
been 
replaced 
with 
our 
guessed 
values 
q(a 
= 
0) 
and 
q(a 
= 
1). 


tt 


A 
similar 
story 
holds 
for 
p(c 
=1ja 
=0;s 
= 
1). 
The 
contribution 
of 
this 
term 
to 
the 
energy 
is

XX

nn 


q 
(a 
= 
0) 
log 
p(c 
=1ja 
=0;s 
= 
1)+q 
(a 
= 
0) 
log 
(1 
..hp(c 
=1ja 
=0;s 
= 
1)) 


tt 
n:cn=1;sn=1 
n:cn=0;sn=1 


which 
is 


XX

nn

log 
p(c 
=1ja 
=0;s 
= 
1)q 
(a 
= 
0)+log(1..p(c 
=1ja 
=0;s 
= 
1))q 
(a 
= 
0) 
(11.2.30) 


tt 
n:cn=1;sn=1 
n:cn=0;sn=1 


Optimising 
with 
respect 
to 
p(c 
=1ja 
=0;s 
= 
1) 
gives 


P

n

I[cn 
= 
1] 
I[sn 
= 
1] 
q(a 
= 
0)

Pn 
Pt 


p(c 
=1ja 
=0;s 
= 
1) 
=(11.2.31)

nn

I[cn 
= 
1] 
I[sn 
= 
1] 
q(a 
= 
0)+I[cn 
= 
0] 
I[sn 
= 
1] 
q(a 
= 
0) 


n 
tn 
t 


For 
comparison, 
the 
setting 
in 
the 
complete 
data 
case 
is 


P

I[cn 
= 
1] 
I[sn 
= 
1] 
I[an 
= 
0]

n

p(c 
=1ja 
=0;s 
= 
1) 
=PP(11.2.32)

I[cn 
= 
1] 
I[sn 
= 
1] 
I[an 
= 
0]+I[cn 
= 
0] 
I[sn 
= 
1] 
I[an 
= 
0] 


nn 


There 
is 
an 
intuitive 
relationship 
between 
these 
updates: 
in 
the 
missing 
data 
case 
we 
replace 
the 
indicators 
by 
the 
assumed 
distributions 
q. 


Iterating 
the 
E 
and 
M 
steps, 
these 
equations 
will 
converge 
to 
a 
local 
likelihood 
optimum. 


To 
minimise 
the 
notational 
burden, 
we 
assume 
that 
the 
structure 
of 
the 
missing 
variables 
is 
xed 
throughout, 
this 
being 
equivalent 
therefore 
to 
a 
latent 
variable 
model. 
The 
form 
of 
the 
energy 
term 
for 
Belief 
Networks 
is

XXX

nn

hlog 
p(x 
n)iqt(hnjvn) 
=hlog 
p(xi 
jpa 
(xi 
))iqt(hnjvn) 
(11.2.33) 
nni

It 
is 
useful 
to 
dene 
the 
following 
notation: 


n 


q 
(x)= 
qt(hjv 
n)(v, 
v 
n) 
(11.2.34)

t 


n

where 
x 
=(v, 
h) 
represents 
all 
the 
variables 
in 
the 
distribution. 
This 
means 
that 
q(x) 
sets 
the 
visible 


t 


variables 
in 
the 
observed 
state, 
and 
denes 
a 
conditional 
distribution 
on 
the 
unobserved 
variables. 
We 


DRAFT 
March 
9, 
2010 



Expectation 
Maximisation 


Algorithm 
9 
EM 
for 
Belief 
Networks. 
Input: 
a 
BN 
structure 
p(xijpa 
(xi)), 
i 
=1;:::;K 
with 
empty 
tables, 
and 
dataset 
on 
the 
visible 
variables 
V. 
Returns 
the 
Maximum 
Likelihood 
setting 
of 
tables. 


1: 
t 
=1 
I 
Iteration 
counter 
2: 
Set 
pt 
(xijpa 
(xi)) 
to 
initial 
values. 
I 
Initialisation 
3: 
while 
p 
(xijpa 
(xi)) 
not 
converged 
(or 
likelihood 
not 
converged) 
do 
4: 
t 
. 
t 
+1 
5: 
for 
n 
=1 
to 
N 
do 
I 
Run 
over 
all 
datapoints 
n

6: 
q(x)= 
pt 
(hnjvn) 
(v, 
vn) 
I 
E 
step 
t 


7: 
end 
for 
8: 
for 
i 
=1 
to 
K 
do 
I 
Run 
over 
all 
variables 
1 
n

9: 
pt+1(xijpa 
(xi)) 
= 
PN 
(xijpa 
(xi)) 
I 
M 
step 
Nn=1 
qt 


10: 
end 
for 
11: 
end 
while 
12: 
return 
pt(xijpa 
(xi)) 
I 
The 
max 
likelihood 
parameter 
estimate. 
then 
dene 
the 
mixture 
distribution 


X

1 
N

n 


qt(x)= 
q 
(x) 
(11.2.35)

N 
t 


n=1 


The 
energy 
term 
in 
equation 
(11.2.5) 
can 
be 
written 
more 
compactly 
as

X

hlog 
p(x 
n)) 
= 
N 
hlog 
p(x)) 
(11.2.36)

qt(hjvn) 
qt(x) 
n 


To 
see 
this 
consider 
the 
right 
hand 
side 
of 
the 
above 


XXX

1 


N 
hlog 
p(x)) 
qt(x) 
= 
N[log 
p(x)] 
qt(hjv 
n)(v, 
v 
n)=hlog 
p(x 
n)) 
qt(hjvn) 
(11.2.37)

N

xn 
n 


Using 
the 
structure 
of 
the 
Belief 
Network, 
we 
have 


DE

XX

hlog 
p(x)iqt(x) 
=hlog 
p(xijpa 
(xi))iqt(x) 
=hlog 
p(xijpa 
(xi))iqt(xijpa(xi))(11.2.38) 
qt(pa(xi))

ii

This 
means 
that 
maximising 
the 
energy 
is 
equivalent 
to 
minimising

DE

X

hlog 
qt(xijpa 
(xi))) 
qt(xijpa(xi)) 
..hlog 
p(xijpa 
(xi))) 
qt(xijpa(xi))(11.2.39) 
qt(pa(xi))

i

where 
we 
added 
the 
constant 
rst 
term 
to 
make 
this 
into 
the 
form 
of 
a 
Kullback-Leibler 
divergence. 
Since 
this 
is 
a 
sum 
of 
independent 
Kullback-Leibler 
divergences, 
optimally 
the 
M-step 
is 
given 
by 
setting 


p(xijpa 
(xi)) 
= 
qt(xijpa 
(xi)) 
(11.2.40) 


In 
practice, 
storing 
the 
qt(x) 
over 
the 
states 
of 
all 
variables 
x 
is 
prohibitively 
expense. 
Fortunately, 
since 


the 
M-step 
only 
requires 
the 
distribution 
on 
the 
family 
of 
each 
variable 
xi, 
one 
only 
requires 
the 
local 
n

distributions 
q(xijpa 
(xi)). 
We 
may 
therefore 
dispense 
with 
the 
global 
qold(x) 
and 
equivalently 
use 


old

P

n

q(xi, 
pa 
(xi))

n 
old

p 
new(xijpa 
(xi)) 
=P(11.2.41)

0

n

l 
q(pa 
(xi))

nold

Using 
the 
EM 
algorithm, 
the 
optimal 
setting 
for 
the 
E-step 
is 
to 
use 
qt(hnjvn)= 
pold(hnjvn). 
With 
this 
notation, 
the 
EM 
algorithm 
can 
be 
compactly 
stated 
as 
in 
algorithm(9). 
See 
also 
EMbeliefnet.m. 


DRAFT 
March 
9, 
2010 



Expectation 
Maximisation 


Example 
54 
(More 
General 
Belief 
Networks). 
Consider 
a 
ve 
variable 
distribution 
with 
discrete 
variables, 
p(x1;x2;x3;x4;x5)= 
p(x1jx2)p(x2jx3)p(x3jx4)p(x4jx5)p(x5) 
(11.2.42) 
in 
which 
the 
variables 
x2 
and 
x4 
are 
consistently 
hidden 
in 
the 
training 
data, 
and 
training 
data 
for 
x1;x3;x5 
are 
always 
present. 
The 
distribution 
can 
be 
represented 
as 
a 
Belief 
network 


x1x2x3x4x5
In 
this 
case, 
the 
contributions 
to 
the 
energy 
have 
the 
form

X

n 
nn 
nn

hlog 
p(x1 
jx2)p(x2jx3 
)p(x3 
jx4)p(x4jx5 
)p(x5 
iqn(x2;x4jx1;x3;x5) 
(11.2.43) 


n 


which 
may 
be 
written 
as

XX

nn

hlog 
p(x1 
jx2)) 
qn(x2;x4jx1;x3;x5) 
+hlog 
p(x2jx3 
)) 
qn(x2;x4jx1;x3;x5) 
nn


XXX

n 
nn

+hlog 
p(x3 
jx4)) 
qn(x2;x4jx1;x3;x5) 
+hlog 
p(x4jx5 
)) 
qn(x2;x4jx1;x3;x5) 
+log 
p(x5 
) 
(11.2.44) 
nnn 


A 
useful 
property 
can 
now 
be 
exploited, 
namely 
that 
each 
term 
depends 
on 
only 
those 
hidden 
variables 
in 
the 
family 
that 
that 
term 
represents. 
Thus 
we 
may 
write

XX

nn

hlog 
p(x1 
jx2)) 
qn(x2jx1;x3;x5) 
+hlog 
p(x2jx3 
)) 
qn(x2jx1;x3;x5) 
nn


XXX

n 
nn

+hlog 
p(x3 
jx4)) 
qn(x4jx1;x3;x5) 
+hlog 
p(x4jx5 
)) 
qn(x4jx1;x3;x5) 
+log 
p(x5 
) 
nnn 


The 
nal 
term 
can 
be 
set 
using 
Maximum 
Likelihood. 
Let 
us 
consider 
therefore 
a 
more 
dicult 
table, 


n

p(x1jx2). 
When 
will 
the 
table 
entry 
p(x1 
= 
ijx2 
= 
j) 
occur 
in 
the 
energy? 
This 
happens 
whenever 
x

1 


is 
in 
state 
i. 
Since 
there 
is 
a 
summation 
over 
all 
the 
states 
of 
variables 
x2 
(due 
to 
the 
average), 
there 
is 
also 
a 
term 
with 
variable 
x2 
in 
state 
j. 
Hence 
the 
contribution 
to 
the 
energy 
from 
terms 
of 
the 
form 
p(x1 
= 
ijx2 
= 
j) 
is

X

n

I[x 
= 
i] 
q 
n(x2 
= 
jjx1;x3;x5) 
log 
p(x1 
= 
ijx2 
= 
j) 
(11.2.45)

1 


n 


nn

where 
the 
indicator 
function 
I[x= 
i] 
equals 
1 
if 
xis 
in 
state 
i 
and 
is 
zero 
otherwise. 
To 
ensure 
normal


11 


isation 
of 
the 
table, 
we 
add 
a 
Lagrange 
term:

()

XX

n

I[x 
= 
i] 
q 
n(x2 
= 
jjx1;x3;x5) 
log 
p(x1 
= 
ijx2 
= 
j)+ 
1 
..p(x1 
= 
kjx2 
= 
j)(11.2.46)

1 


nk 


Dierentiating 
with 
respect 
to 
p(x1 
= 
ijx2 
= 
j) 
we 
get

Xqn(x2 
= 
jjx1;x3;x5)

n

I[x 
= 
i]= 
. 
(11.2.47)

1 


p(x1 
= 
ijx2 
= 
j)

n 


or 


X

n 


p(x1 
= 
ijx2 
= 
j) 
/I[x 
= 
i] 
q 
n(x2 
= 
jjx1;x3;x5). 
(11.2.48)

1 


n 


Hence 


P

n

I[x= 
i] 
qn(x2 
= 
jjx1;x3;x5)

n 
1 


p(x1 
= 
ijx2 
= 
j)=P(11.2.49)

n

I[x= 
k] 
qn(x2 
= 
jjx1;x3;x5)

n;k 
1 


DRAFT 
March 
9, 
2010 
227 



Expectation 
Maximisation 


024681012-120-110-100-90-80-70-60-50log likelihood
Figure 
11.4: 
Evolution 
of 
the 
log-likelihood 
versus 
iterations 
under 
the 
EM 
training 
procedure 
(from 
solving 
the 
Printer 
Nightmare 
with 
missing 
data, 
exercise(138). 
Note 
how 
rapid 
progress 
is 
made 
at 
the 
beginning, 
but 
convergence 
can 
be 
slow. 


Using 
the 
EM 
algorithm, 
we 
have 


nnn 


q 
n(x2 
= 
jjx1;x3;x5)= 
p(x2 
= 
jjx1 
;x 
3 
;x 
) 
(11.2.50)

5 


This 
optimal 
distribution 
is 
easy 
to 
compute 
since 
this 
is 
the 
marginal 
on 
the 
family, 
given 
some 
evidential 
variables. 
Hence, 
the 
M-step 
update 
for 
the 
table 
is 


P

new(x1 
n 
I[x1 
n 
= 
i] 
pold(x2 
= 
jjx1 
n;x3 
n;x5 
n)

p 
= 
ijx2 
= 
j)=P(11.2.51)

I[xn 
= 
k] 
pold(x2 
= 
jjx1 
n;x3 
n;xn)

n;k 
15 


What 
about 
the 
table 
p(x2 
= 
ijx3 
= 
j)? 
To 
ensure 
normalisation 
of 
the 
table, 
we 
add 
a 
Lagrange 
term:

()

XX

n

I[x 
= 
j] 
q 
n(x2 
= 
ijx1;x3;x5) 
log 
p(x2 
= 
ijx3 
= 
j)+ 
1 
..p(x2 
= 
kjx3 
= 
j)(11.2.52)

3 


nk 


As 
before, 
dierentiating, 
and 
using 
the 
EM 
settings, 
we 
have 


P

new(x2 
n 
I[x3 
n 
= 
j] 
pold(x2 
= 
ijx1 
n;x3 
n;x5 
n)

p 
= 
ijx3 
= 
j)=Pn 
old(x2 
nnn 
(11.2.53)

I[x= 
j] 
p= 
kjx1 
;x3 
;x)

n;k 
35 


There 
is 
a 
simple 
intuitive 
pattern 
to 
equation 
(11.9.2) 
and 
equation 
(11.2.53) 
: 
If 
there 
were 
no 
hidden 
data, 
equation 
(11.9.2) 
would 
read 


X

p 
new(x1 
= 
ijx2 
= 
j) 
/I[x 
n 
= 
i] 
I[x 
n 
= 
j] 
(11.2.54)

12 


n 


and 
equation 
(11.2.53) 
would 
be 


X

new(x2 
nn 


p 
= 
ijx3 
= 
j) 
/I[x 
= 
j] 
I[x 
= 
i] 
(11.2.55)

32 


n 


All 
that 
we 
do, 
therefore, 
in 
the 
general 
EM 
case, 
is 
to 
replace 
those 
deterministic 
functions 
such 
as 
I[xn 
= 
i] 
by 
their 
missing 
variable 
equivalents 
pold(x2 
= 
ijx1 
n;x3 
n;xn). 
This 
is 
merely 
a 
restatement 
of 
the 


25 


general 
update 
given 
in 
equation 
(11.2.41) 
under 
the 
denition 
(11.2.34). 


11.2.4 
Application 
to 
Markov 
networks 
Q

1

For 
a 
MN 
dened 
over 
visible 
and 
hidden 
variables 
p(v, 
hj)= 
c(h, 
v) 
the 
EM 
variational 
bound 


Z()c 


is 


X

log 
p(vj) 
..H(q(h)) 
+hlog 
c(h, 
vj)i- 
log 
Z() 
(11.2.56)

q(h) 
c 


Whilst 
the 
bound 
decouples 
the 
parameters 
in 
the 
second 
term, 
the 
parameters 
are 
nevertheless 
coupled 
in 
the 
normalisation 
Z(). 
Because 
of 
this 
we 
cannot 
optimise 
the 
above 
bound 
on 
a 
parameter 
by 
parameter 
basis. 
One 
approach 
is 
to 
use 
an 
additional 
bound 
log 
Z() 
from 
above, 
as 
for 
iterative 
scaling. 


228 
DRAFT 
March 
9, 
2010 



Extensions 
of 
EM 


11.2.5 
Convergence 
Convergence 
of 
EM 
can 
be 
slow, 
particularly 
when 
the 
number 
of 
missing 
observations 
is 
greater 
than 
the 
number 
of 
visible 
observations. 
In 
practice, 
one 
often 
combines 
the 
EM 
with 
gradient 
based 
procedures 
to 
improve 
convergence, 
see 
section(11.7). 
Note 
also 
that 
the 
log 
likelihood 
is 
typically 
a 
non-convex 
function 
of 
the 
parameters. 
This 
means 
that 
there 
may 
be 
multiple 
local 
optima 
and 
the 
solution 
found 
often 
depends 
on 
the 
initialisation. 


11.3 
Extensions 
of 
EM 
11.3.1 
Partial 
M 
step 
It 
is 
not 
necessary 
to 
nd 
the 
full 
optimum 
of 
the 
energy 
term 
at 
each 
iteration. 
As 
long 
as 
one 
nds 
a 
parameter 
' 
which 
has 
a 
higher 
energy 
than 
that 
of 
the 
current 
parameter 
, 
then 
the 
conditions 
required 
in 
section(11.2.2) 
still 
hold, 
and 
the 
likelihood 
cannot 
decrease 
at 
each 
iteration. 


11.3.2 
Partial 
E 
step 
The 
E-step 
requires 
us 
to 
nd 
the 
optimum 
of 
NN 
NN 
log 
p(Vj) 
= 
- 
n=1 
hlog 
q(hnjv 
n)iq(hnjvn) 
+ 
n=1 
hlog 
p(hn 
, 
v 
nj)iq(hnjvn) 
(11.3.1) 
with 
respect 
to 
q(hnjvn). 
The 
fully 
optimal 
setting 
is 
q(hnjv 
n) 
= 
p(hnjv 
n) 
(11.3.2) 


For 
a 
guaranteed 
increase 
in 
likelihood 
at 
each 
iteration, 
from 
section(11.2.2) 
we 
required 
that 
the 
fully 
optimal 
setting 
of 
q 
is 
used. 
Unfortunately, 
therefore, 
one 
cannot 
in 
general 
guarantee 
that 
such 
a 
partial 
E 
step 
would 
always 
increase 
the 
likelihood. 
Of 
course, 
it 
is 
guaranteed 
to 
increase 
the 
lower 
bound 
on 
the 
likelihood, 
though 
not 
the 
likelihood 
itself. 


Intractable 
energy 


The 
EM 
algorithm 
assumes 
that 
we 
can 
calculate 


hlog 
p(h, 
vj)iq(hjv) 
(11.3.3) 


However, 
in 
general, 
it 
may 
be 
that 
we 
can 
only 
carry 
out 
the 
average 
over 
q 
for 
a 
very 
restricted 
class 


= 


of 
distributions 
-for 
example, 
factorised 
distributions 
q(hjv)= 
j 
q(hjjv). 
In 
such 
cases 
one 
may 
use 
a 


= 


simpler 
class 
of 
distributions, 
Q, 
e.g. 
Q 
= 
factorised 
q(hjv)= 
i 
q(hijv), 
for 
which 
the 
averaging 
required 
for 
the 
energy 
may 
be 
simpler. 


We 
can 
nd 
the 
best 
distribution 
in 
class 
Q 
by 
minimising 
the 
KL 
divergence 
between 
q(hjv, 
Q) 
and 
p(hjv, 
) 
numerically 
using 
a 
non-linear 
optimisation 
routine: 


qopt 
= 
argmin 
KL(q(h)jp(hjv, 
)) 
(11.3.4) 


q2Q 


Alternatively, 
one 
can 
assume 
a 
certain 
structured 
form 
for 
the 
q 
distribution, 
and 
learn 
the 
optimal 
factors 
of 
the 
distribution 
by 
free 
form 
functional 
calculus. 


Viterbi 
training 


An 
extreme 
case 
is 
to 
restrict 
q(hnjvn) 
to 
a 
delta-function. 
In 
this 
case, 
the 
entropic 
term 
hlog 
q(hnjvn)iq(hnjvn) 
is 
constant, 
so 
that 
the 
optimal 
delta 
function 
q 
is 
to 
set 


q(hnjv 
n)= 
d 
(hn;hn) 
(11.3.5)

* 


DRAFT 
March 
9, 
2010 



A 
Failure 
Case 
for 
EM 


where 


hn 


= 
argmax 
p(h, 
vnj) 
(11.3.6)

* 


h 


This 
is 
called 
Viterbi 
training 
and 
is 
common 
in 
training 
HMMs, 
see 
section(23.2). 
EM 
training 
with 
this 
restricted 
class 
of 
q 
distribution 
is 
therefore 
guaranteed 
to 
increase 
the 
lower 
bound 
on 
the 
log 
likelihood, 
though 
not 
the 
likelihood 
itself. 
A 
practical 
advantage 
of 
Viterbi 
training 
is 
that 
the 
energy 
is 
always 
tractable 
to 
compute, 
becoming 
simply 


N

XZ

log 
p(hn 
* 
;v 
nj) 
(11.3.7) 
n=1 


which 
is 
amenable 
to 
optimisation. 


Provided 
there 
is 
sucient 
data, 
one 
might 
hope 
that 
the 
likelihood 
as 
a 
function 
of 
the 
parameter 
. 
will 
be 
sharply 
peaked 
around 
the 
optimum 
value. 
This 
means 
that 
at 
convergence 
the 
approximation 
of 
the 
posterior 
p(hjv, 
opt) 
by 
a 
delta 
function 
will 
be 
reasonable, 
and 
an 
update 
of 
EM 
using 
Viterbi 
training 
will 
produce 
a 
new 
. 
approximately 
the 
same 
as 
opt. 
For 
any 
highly 
suboptimal 
, 
however, 
p(hjv, 
) 
will 
be 
far 
from 
a 
delta 
function, 
and 
therefore 
a 
Viterbi 
update 
is 
less 
reliable 
in 
terms 
of 
leading 
to 
an 
increase 
in 
the 
likelihood 
itself. 
This 
suggests 
that 
the 
initialisation 
of 
. 
for 
Viterbi 
training 
is 
more 
critical 
than 
for 
the 
standard 
EM. 


Stochastic 
training 


Another 
approximate 
q(hnjvn) 
distribution 
would 
be 
to 
use 
an 
empirical 
distribution 
formed 
by 
samples 
from 
the 
fully 
optimal 
distribution 
p(hnjvn;). 
That 
is 
one 
draws 
samples 
(see 
chapter(27) 
for 
a 
discussion 
on 
sampling) 
hn 
1 
;:::;hn 
from 
p(hnjvn;) 
and 
forms 
a 
q 
distribution

L 


X

1 
L

q(hnjv 
n)= 
d 
(hn;hn) 
(11.3.8)

l

L 


l=1 


The 
energy 
becomes 
then 
proportional 
to 


NL

XXZ

log 
p(hn
l 
;v 
nj) 
(11.3.9) 
n=1 
l=1 


so 
that, 
as 
in 
Viterbi 
training, 
the 
energy 
is 
always 
computationally 
tractable 
for 
this 
restricted 
q 
class. 
Provided 
that 
the 
samples 
from 
p(hnjvn) 
are 
reliable, 
stochastic 
training 
will 
produce 
an 
energy 
function 
with 
(on 
average) 
the 
same 
characteristics 
as 
the 
true 
energy 
under 
the 
classical 
EM 
algorithm. 
This 
means 
that 
the 
solution 
obtained 
from 
stochastic 
training 
should 
tend 
to 
that 
from 
the 
classical 
EM 
as 
the 
number 
of 
samples 
increases. 


11.4 
A 
Failure 
Case 
for 
EM 
Consider 
a 
likelihood 
of 
the 
form 


Z

p(vj)=d 
(v, 
f(hj)) 
p(h) 
(11.4.1) 


h 


If 
we 
attempt 
an 
EM 
approach 
for 
this, 
this 
will 
fail 
(see 
also 
exercise(75)). 
For 
a 
more 
general 
model 
of 
the 
form 


Z

p(vj)=p 
(vjh, 
) 
p(h) 
(11.4.2) 


h 


The 
E-step 
is 


q(hjold) 
. 
p 
(vjh, 
old) 
p(h) 
(11.4.3) 


230 
DRAFT 
March 
9, 
2010 



Variational 
Bayes 


and 
the 
M-step 
sets 


new 
= 
argmax 
hlog 
p(v, 
hj)ip(hjold) 
= 
argmax 
hlog 
p(vjh, 
)ip(hjold) 
(11.4.4) 
. 


where 
we 
used 
the 
fact 
that 
for 
this 
model 
p(h) 
is 
independent 
of 
. 
In 
the 
case 
that 
p 
(vjh, 
)= 
d 
(v, 
f(hj)) 
then 


p(hjold) 
/6d 
(v, 
f(hj)) 
p(h) 
(11.4.5) 


so 
that 
optimising 
the 
energy 
requires 


new 
= 
argmax 
hlog 
d 
(v, 
f(hj))i(11.4.6)

p(hjold)

. 


Since 
p(hjold) 
is 
zero 
everywhere 
expect 
that 
h 
for 
which 
v 
= 
f(hj), 
then 
the 
energy 
is 
eectively 
negative 
innity 
if 
6However, 
when 
. 
= 
old 
the 
energy 
is 
maximal2 
. 
This 
is 
therefore 
the 
optimum 
of 
the 


= 
old. 
energy, 
and 
represents 
therefore 
a 
failure 
in 
updating 
for 
EM. 
This 
situation 
occurs 
in 
practice, 
and 
has 
been 
noted 
in 
particular 
in 
the 
context 
of 
Independent 
Component 
Analysis[222]. 


One 
can 
attempt 
to 
heal 
this 
behaviour 
by 
deriving 
an 
EM 
algorithm 
based 
on 
the 
distribution 


p(vjh, 
) 
= 
(1 
..6)d 
(v, 
f(hj)) 
+ 
n(h), 
0 
6E 
61 
(11.4.7) 


where 
n(h) 
is 
an 
arbitrary 
distribution 
on 
the 
hidden 
variable 
h. 
The 
original 
deterministic 
model 
corresponds 
to 
p0(vjh, 
). 
Dening 


Z

p(vj)=p(vjh, 
)p(h) 
(11.4.8) 


h 


we 
have 


p(vj) 
= 
(1 
..6)p0(vj)+ 
E 
hn(h)i(11.4.9)

p(h) 


An 
EM 
algorithm 
for 
p(vj), 
0 
<< 
1 
satises 


p(vjnew) 
..6p(vjold) 
= 
(1 
..6)(p0(vjnew) 
..6p0(vjold)) 
> 
0 
(11.4.10) 


which 
implies 


p0(vjnew) 
..6p0(vjold) 
> 
0 
(11.4.11) 


This 
means 
that 
the 
EM 
algorithm 
for 
the 
non-deterministic 
case 
0 
<< 
1 
is 
guaranteed 
to 
increase 
the 
likelihood 
under 
the 
deterministic 
model 
p0(vj) 
at 
each 
iteration 
(unless 
we 
are 
at 
convergence). 
See 


[100] 
for 
an 
application 
of 
this 
`antifreeze’ 
technique 
to 
learning 
Markov 
Decision 
Processes 
with 
EM. 
11.5 
Variational 
Bayes 
As 
discussed 
in 
section(9.2) 
Maximum 
Likelihood 
corresponds 
to 
a 
form 
of 
Bayesian 
approach 
in 
which 
the 
parameter 
posterior 
distribution 
(under 
a 
at 
prior) 
is 
approximated 
with 
a 
delta 
function 
p(jV) 
6
(, 
opt). 
Variational 
Bayes 
is 
analogous 
to 
EM 
in 
that 
it 
attempts 
to 
deal 
with 
hidden 
variables 
but 
using 
a 
distribution 
that 
better 
represents 
the 
posterior 
distribution 
than 
given 
by 
Maximum 
Likelihood. 


To 
keep 
the 
notation 
simple, 
we'll 
initially 
assume 
only 
a 
single 
datapoint 
with 
observation 
v. 
Our 
interest 
is 
then 
the 
parameter 
posterior 


X

p(jv) 
/6p(vj)p() 
/p(v, 
hj)p() 
(11.5.1) 


h 


The 
VB 
approach 
assumes 
a 
factorised 
approximation 
of 
the 
joint 
hidden 
and 
parameter 
posterior: 


p(h, 
jv) 
6q(h)q() 
(11.5.2) 


2For 
discrete 
variables 
and 
the 
Kronecker 
delta, 
the 
energy 
attains 
the 
maximal 
value 
of 
zero 
when 
. 
= 
old. 
In 
the 
case 
of 
continuous 
variables, 
however, 
the 
log 
of 
the 
Dirac 
delta 
function 
is 
not 
well 
dened. 
Considering 
the 
delta 
function 
as 
the 
limit 
of 
a 
narrow 
width 
Gaussian, 
for 
any 
small 
but 
nite 
width, 
the 
energy 
is 
largest 
when 
. 
= 
old. 


DRAFT 
March 
9, 
2010 
231 



Variational 
Bayes 


A 
bound 
on 
the 
marginal 
likelihood 


By 
minimising 
the 
KL 
divergence, 
KL(q(h)q()jp(h, 
jv)) 
= 
hlog 
q(h)i+ 
hlog 
q()i..hlog 
p(h, 
jv)i= 
0 
(11.5.3)

q(h) 
q() 
q(h)q() 


we 
arrive 
at 
the 
bound 
log 
p(v) 
..hlog 
q(h)i..hlog 
q()i+ 
hlog 
p(v, 
h, 
)i(11.5.4)

q(h) 
q() 
q(h)q() 


For 
xed 
q() 
if 
we 
minimize 
the 
Kullback-Leibler 
divergence, 
we 
get 
the 
tightest 
lower 
bound 
on 
log 
p(v). 
If 
then 
for 
xed 
q(h) 
we 
minimise 
the 
Kullback-Leibler 
divergence 
w.r.t. 
q() 
we 
are 
maximising 
the 
term 
..hlog 
q()i+ 
hlog 
p(v, 
h, 
)iand 
hence 
pushing 
up 
the 
bound 
on 
the 
marginal 
likelihood. 
This 


q() 
q(h)q() 


simple 
co-ordinate 
wise 
procedure 
in 
which 
we 
rst 
x 
the 
q() 
and 
solve 
for 
q(h) 
and 
then 
vice 
versa 
is 
analogous 
to 
the 
E 
and 
M 
step 
of 
the 
EM 
algorithm: 


E-step 




q 
new(h) 
= 
argmin 
KLq(h)q 
old()jp(h, 
jv)(11.5.5) 
q(h) 


M-step 
q 
new() 
= 
argmin 
q() 
KL(q 
new(h)q()jp(h, 
jv)) 
(11.5.6) 
In 
full 
generality 
for 
a 
set 
of 
observations 
V 
and 
hidden 
variables 
H, 
algorithm(7). 
For 
distributions 


q(H) 
and 
q() 
which 
are 
parametersised/constrained, 
the 
best 
distributions 
in 
the 
minimal 
KL 
sense 
are 
returned. 
In 
general, 
each 
iteration 
of 
VB 
is 
guaranteed 
to 
increase 
the 
bound 
on 
the 
marginal 
likelihood, 
but 
not 
the 
marginal 
likelihood 
itself. 
Like 
the 
EM 
algorithm, 
VB 
can 
(and 
often 
does) 
suer 
from 
local 
maxima 
issues. 
This 
means 
that 
the 
converged 
solution 
can 
be 
dependent 
on 
the 
initialisation. 


Unconstrained 
approximations 


For 
xed 
q() 
the 
contribution 
to 
the 
KL 
divergence 
is 
hlog 
q(h)i..hlog 
p(v, 
h, 
)i= 
KL(q(h)jp~(h)) 
+ 
const. 
(11.5.7)

q(h) 
q(h)q() 


where 


1 
hlog 
p(v;h;)iq()

p~(h) 
= 
e(11.5.8)

~

Z 
where 
Z~is 
a 
normalising 
constant. 
Hence, 
for 
xed 
q(), 
the 
optimal 
q(h) 
is 
given 
by 
~p, 


hlog 
p(v;h;)iq()

q(h) 
. 
e(11.5.9) 
Similarly, 
for 
xed 
q(h), 
optimally 


hlog 
p(v;h;)iq(h)

q() 
. 
e(11.5.10) 


i.i.d. 
Data 
Under 
the 
i.i.d. 
assumption, 
we 
obtain 
a 
bound 
on 
the 
marginal 
likelihood 
for 
the 
whole 
dataset: 


no

X

log 
p(Vj) 
..hlog 
q(hn)) 
..hlog 
q()) 
+ 
hlog 
p(v 
n;hn;)) 
(11.5.11)

q(hn) 
q() 
q(hn)q()
n

The 
bound 
holds 
for 
any 
q(hn) 
and 
q() 
but 
is 
tightest 
for 
the 
converged 
estimates 
from 
the 
VB 
procedure. 


For 
an 
i.i.d. 
dataset, 
it 
is 
straightforward 
to 
show 
that 
without 
loss 
of 
generality 
we 
may 
assume 


Y

q(h1;:::;hN 
)=q(hn) 
(11.5.12) 
n 


Under 
this 
we 
arrive 
at 
algorithm(11). 


DRAFT 
March 
9, 
2010 



Variational 
Bayes 


Algorithm 
10 
Variational 
Bayes. 


1: 
t 
=0 
I 
Iteration 
counter 
2: 
Choose 
an 
initial 
distribution 
q0(). 
I 
Initialisation 
3: 
while 
. 
not 
converged 
(or 
likelihood 
bound 
not 
converged) 
do 
4: 
t 
. 
t 
+1 
n

5: 
q(H) 
= 
arg 
minq(H) 
KL(q(H)qt..1()jp(H;jV)) 
I 
E 
step 
t 


nn

6: 
q() 
= 
arg 
minq() 
KL(q(H)q()jp(H;jV)) 
I 
M 
step 
tt 


7: 
end 
while 
n

8: 
return 
q() 
I 
The 
posterior 
parameter 
approximation. 
t 


vn
hn

N
hn

N
Figure 
11.5: 
(a): 
Generic 
form 
of 
a 
model 


with 
hidden 
variables. 
(b): 
A 
factorised 
posterior 
approximation 
uses 
in 
Variational 
Bayes. 


(a) 
(b) 
11.5.1 
EM 
is 
a 
special 
case 
of 
variational 
Bayes 
If 
we 
wish 
to 
nd 
a 
summary 
of 
the 
parameter 
distribution 
corresponding 
to 
only 
the 
most 
likely 
point 
, 
then 


q()= 
d 
(, 
) 
(11.5.13) 
where 
* 
is 
the 
single 
optimal 
value 
of 
the 
parameter. 
If 
we 
plug 
this 
assumption 
into 
equation 
(11.5.4) 
we 
obtain 
the 
bound 


log 
p(vj) 
..hlog 
q(h)i+ 
hlog 
p(v, 
h, 
)i+ 
const. 
(11.5.14)

q(h) 
q(h) 


The 
M-step 
is 
then 
given 
by 




* 
= 
argmax 
hlog 
p(vjh, 
)p(hj)i+ 
log 
p()(11.5.15)

q(h)



For 
a 
at 
prior 
p()= 
const., 
this 
is 
therefore 
equivalent 
to 
energy 
maximisation 
in 
the 
EM 
algorithm. 
Using 
this 
single 
optimal 
value 
in 
the 
VB 
update 
for 
q(hn) 
we 
have 


n 


q 
(h) 
. 
p(v, 
hj) 
. 
p(hjv, 
) 
(11.5.16)

t 


which 
is 
the 
standard 
E-step 
of 
EM. 
Hence 
EM 
is 
a 
special 
case 
of 
VB, 
under 
a 
at 
prior 
p()= 
const. 
and 
a 
delta 
function 
approximation 
of 
the 
parameter 
posterior. 


11.5.2 
Factorising 
the 
parameter 
posterior 
Let's 
reconsider 
Bayesian 
learning 
in 
the 
binary 
variable 
network 


p(a, 
c, 
s)= 
p(cja, 
s)p(a)p(s) 
(11.5.17) 
in 
which 
we 
use 
a 
factorised 
parameter 
prior 


p(c)(a)p(s) 
(11.5.18) 
When 
all 
the 
data 
is 
observed, 
the 
parameter 
posterior 
factorises. 
As 
we 
discussed 
in 
section(11.1.1) 
if 
the 


state 
of 
a 
is 
not 
observed, 
the 
parameter 
posterior 
no 
longer 
factorises: 
p(a, 
s, 
cjV) 
. 
p(a)p(s)p(c)p(Vja, 
s, 
c) 
Y
(11.5.19) 
. 
p(a)p(s)p(c)
n 
p(v 
nja, 
s, 
c) 
YX
(11.5.20) 
. 
p(a)p(s)p(c)
n 
p(s 
njs)
an 
p(c 
njs 
n 
, 
a 
n, 
c)p(a 
nja) 
(11.5.21) 
DRAFT 
March 
9, 
2010 
233 



Variational 
Bayes 


Algorithm 
11 
Variational 
Bayes 
(i.i.d. 
data). 


1: 
t 
=0 
I 
Iteration 
counter 
2: 
Choose 
an 
initial 
distribution 
q0(). 
I 
Initialisation 
3: 
while 
. 
not 
converged 
(or 
likelihood 
bound 
not 
converged) 
do 
4: 
t 
. 
t 
+1 
5: 
for 
n 
=1 
to 
N 
do 
I 
Run 
over 
all 
datapoints 
hlog 
p(vn;hn;)i

n 
qt..1()

6: 
q(hn) 
. 
eI 
E 
step 
t 


7: 
end 
for 
P
hlog 
p(vn;hnj)in

nq(hn)

8: 
qt() 
. 
p()et 
I 
M 
step 
9: 
end 
while 
n

10: 
return 
q() 
I 
The 
posterior 
parameter 
approximation. 
t 


c
as
c
N
Figure 
11.6: 
(a): 
A 
model 
for 
the 
relationship 
between 
lung 
Cancer, 
Asbestos 
exposure 
and 
Smoking 
with 
factorised 
parameter 
priors. 
Variables 
c 
and 
s 
are 
observed, 
but 
variable 
a 
is 
consistently 
missing. 
(b): 
A 
factorised 
parameter 
posterior 
approximation. 


a
N
c
(a) 
(b) 
where 
the 
summation 
over 
a 
prevents 
the 
factorisation 
into 
a 
product 
of 
the 
individual 
table 
parameters. 


Since 
it 
is 
convenient 
in 
terms 
of 
representations 
to 
work 
with 
factorised 
posteriors, 
we 
can 
apply 
VB 
but 
with 
a 
factorised 
constraint 
on 
the 
form 
of 
the 
q. 
In 
VB 
we 
dene 
a 
distribution 
over 
the 
visible 
and 


nn

hidden 
variables. 
In 
this 
case 
the 
hidden 
variables 
are 
the 
an 
and 
the 
visible 
are 
s;c. 
The 
joint 
posterior 
over 
all 
unobserved 
variables 
(parameters 
and 
missing 
observations) 
is 


Y

1 
n 


p(a;s;c;a 
;:::;a 
N 
jV) 
. 
p(a)p(s)p(c)p(c 
njs 
;a 
n;c)p(s 
njs)p(a 
nja) 
(11.5.22) 
n 


To 
make 
a 
factorised 
posterior 
approximation 
we 
use 


Y

1 


p(a;s;c;a 
;:::;a 
N 
jV) 
˜ 
q(a)q(c)q(s)q(a 
n) 
(11.5.23) 
n 


and 
minimise 
the 
Kullback-Leibler 
divergence 
between 
the 
left 
and 
right 
of 
the 
above. 


M-step 


Hence 


Y

hlog 
p(anja)) 


q(an)

q(a) 
. 
p(a)e 
(11.5.24) 
n 


nn

hlog 
p(a 
nja)i= 
q(a 
= 
1) 
log 
a 
+ 
q(a 
= 
0) 
log 
(1 
- 
a) 
(11.5.25)

q(an) 


Hence 


hlog 
p(anja)) 
=1)=0)

e 
q(an) 
= 
q(an(1 
- 
a)q(an(11.5.26)

a 


It 
is 
convenient 
to 
use 
a 
Beta 
distribution 
prior, 


)..1 


p(a) 
. 
..1 
(1 
- 
a(11.5.27)

a 


DRAFT 
March 
9, 
2010 


a
s
as

Variational 
Bayes 


since 
the 
posterior 
approximation 
is 
then 
also 
a 
Beta 
distribution: 


 !

XX

nn 


q(a)= 
Baja 
+q(a 
= 
1);ß 
+q(a 
= 
0(11.5.28) 
nn 


A 
similar 
calculation 
gives 


 !

XX

nn 


q(s)= 
Bsja 
+I[s 
= 
1] 
;ß 
+I[s 
= 
0](11.5.29) 
nn 


and 
four 
tables, 
one 
for 
each 
of 
the 
parental 
states 
of 
c. 
For 
example 


 !

XX

nn 
nn 


q(c(a 
= 
0;s 
= 
1)) 
= 
Bcja 
+I[s 
= 
1] 
q(a 
= 
0);ß 
+I[s 
= 
0] 
q(a 
= 
1)(11.5.30) 


nn 


These 
are 
reminiscent 
of 
the 
standard 
Bayesian 
equations, 
equation 
(9.3.17) 
except 
that 
the 
counts 
have 
been 
replaced 
by 
q's. 


E-step 


We 
still 
need 
to 
determine 
q(an). 
The 
optimal 
value 
is 
given 
by 
minimising 
the 
Kullback-Leibler 
divergence 
with 
respect 
to 
q(an). 
This 
gives 
the 
solution 
that 
optimally, 


hlog 
p(cnjs;an;c)) 
+hlog 
p(anja)) 


q(a)

q(a 
n) 
. 
e 
n
q(c)(11.5.31) 


For 
example, 
if 
assume 
that 
for 
datapoint 
n, 
s 
is 
in 
state 
1 
and 
c 
in 
state 
0, 
then 


n 
hlog(1..c(s=1;a=1))i+hlog 
ai

q(c(s=1;a=1))q(a)

q(a 
= 
1) 
. 
e(11.5.32) 


and 


n 
hlog(1..c(s=1;a=0))) 
q(c(s=1;a=1))+hlog(1..a)) 


q(a)

q(a 
= 
0) 
. 
e 
(11.5.33) 


To 
compute 
such 
quantities 
explicitly, 
we 
need 
the 
values 
hlog 
iB(j;) 
and 
hlog 
(1 
- 
)iB(j;). 
For 
a 
Beta 
distribution, 
these 
are 
straightforward 
to 
compute, 
see 
exercise(95). 


The 
complete 
VB 
procedure 
is 
then 
given 
by 
iterating 
equations 
(11.5.28,11.5.29,11.5.30) 
and 
(11.5.32,11.5.33) 
until 
convergence. 


Given 
a 
converged 
factorised 
approximation, 
computing 
a 
marginal 
table 
p(a 
= 
1jV) 
is 
then 
straightforward 
under 
the 
approximation 


p(a 
= 
1jV) 
˜ 
q(a 
= 
1ja)q(ajV)= 
aq(ajV) 
(11.5.34) 


a 
a 


Since 
q(ajV) 
is 
a 
Beta 
distribution 
B 
(aj, 
), 
the 
mean 
is 
straightforward. 
Using 
this 
for 
both 
states 
of 
a 
leads 
to 


P

n

a 
+q(a= 
1)

n

p(a 
= 
1jV)= 
PP(11.5.35)

a 
+q(an 
= 
0)+ 
ß 
+q(an 
= 
1)

nn 


The 
application 
of 
VB 
to 
learning 
the 
tables 
in 
arbitrarily 
structured 
BNs 
is 
a 
straightforward 
extension 
of 
the 
technique 
outlined 
here. 
Under 
the 
factorised 
approximation, 
q(h, 
)= 
q(h)q(), 
one 
will 
always 
obtain 
a 
simple 
updating 
equation 
analogous 
to 
the 
full 
data 
case, 
but 
with 
the 
missing 
data 
replaced 
by 
variational 
approximations. 
Nevertheless, 
if 
a 
variable 
has 
many 
missing 
parents, 
the 
number 
of 
states 
in 
the 
average 
with 
respect 
to 
the 
q 
distribution 
can 
become 
intractable, 
and 
further 
constraints 
on 
the 
form 
of 
the 
approximation, 
or 
additional 
bounds 
are 
required. 


One 
may 
readily 
extend 
the 
above 
to 
the 
case 
of 
Dirichlet 
distributions 
on 
multinomial 
variables, 
see 
exercise(142). 
Indeed, 
the 
extension 
to 
the 
exponential 
family 
is 
straightforward. 


DRAFT 
March 
9, 
2010 



00
Optimising 
the 
Likelihood 
by 
Gradient 
Methods 


Figure 
11.7: 
(a): 
Standard 
ML 
learning. 
The 
best 
parameter 
. 
is 
found 


. 


v
by 
maximising 
the 
probability 
that 
the 
model 
generates 
the 
observed 
data 
opt 
= 
arg 
max. 
p(vj). 
(b): 
ML-II 
learning. 
In 
cases 
where 
we 
have 
a 
prior 
preference 
for 
the 
parameters 
, 
but 
with 
unspecied 
hyperparameter 
0, 
we 
can 
nd 
' 
by 
0= 
arg 
max' 
p(vj0) 
= 
arg 
max' 
hp(vj)i


v
opt 
p(j0). 


(a) 
(b) 
11.6 
Bayesian 
Methods 
and 
ML-II 
Consider 
a 
parameterised 
distribution 
p(vj), 
for 
which 
we 
wish 
to 
the 
learn 
the 
optimal 
parameters 
. 
given 
some 
data. 
The 
model 
p(vj) 
is 
depicted 
in 
g(11.7a), 
where 
a 
dot 
indicates 
that 
no 
distribution 
is 
present 
on 
that 
variable. 
For 
a 
single 
observed 
datapoint 
v, 
setting 
. 
by 
Maximum 
Likelihood 
corresponds 
to 
nding 
the 
parameter 
. 
that 
maximises 
p(vj). 


In 
some 
cases 
we 
may 
have 
an 
idea 
about 
which 
parameters 
. 
are 
more 
appropriate 
and 
can 
express 
this 
prior 
preference 
using 
a 
distribution 
p(). 
If 
the 
prior 
were 
fully 
specied, 
then 
there 
is 
nothing 
to 
`learn’ 
since 
p(jv) 
is 
now 
fully 
known. 
However, 
in 
many 
cases 
in 
practice, 
we 
are 
unsure 
of 
the 
exact 
parameter 
settings 
of 
the 
prior, 
and 
hence 
specify 
a 
parametersised 
prior 
using 
a 
distribution 
p(j0) 
with 
hyperparameter 
0. 
This 
is 
depicted 
in 
g(11.7b). 
The 
learning 
corresponds 
to 
nding 
the 
optimal 
0

R

that 
maximises 
the 
likelihood 
p(vj0)=p(vj)p(j0). 
This 
is 
known 
as 
an 
ML-II 
procedure 
since 
it 


. 


corresponds 
to 
maximum 
likelihood, 
but 
at 
the 
higher, 
hyperparameter 
level[33, 
183]. 
This 
is 
a 
form 
of 
approximate 
Bayesian 
analysis 
since, 
although 
' 
is 
set 
using 
maximum 
likelihood, 
after 
training, 
we 
have 
a 
distribution 
over 
parameters, 
p(jv, 
0). 


11.7 
Optimising 
the 
Likelihood 
by 
Gradient 
Methods 
11.7.1 
Directed 
models 
The 
EM 
algorithm 
typically 
works 
well 
when 
the 
amount 
of 
missing 
information 
is 
small 
compared 
to 
the 
complete 
information. 
In 
this 
case 
EM 
exhibits 
approximately 
the 
same 
convergence 
as 
Newton 
based 
gradient 
method[237]. 
However, 
if 
the 
fraction 
of 
missing 
information 
approaches 
unity, 
EM 
can 
converge 
very 
slowly. 
In 
the 
case 
of 
continuous 
parameters 
, 
an 
alternative 
is 
to 
compute 
the 
gradient 
of 
the 
likelihood 
directly 
and 
use 
this 
as 
part 
of 
a 
standard 
continuous 
variable 
optimisation 
routine. 
The 
gradient 
is 
straightforward 
to 
compute 
using 
the 
following 
identity. 
Consider 
the 
log 
likelihood 


L() 
= 
log 
p(vj) 
(11.7.1) 


The 
derivative 
can 
be 
written 


Z

11 


@L()= 
@p(vj)= 
@p(v, 
hj) 
(11.7.2) 


p(vj)p(vj)

h 


At 
this 
point, 
we 
take 
the 
derivative 
inside 
the 
integral 


ZZ

1 


@L()= 
p(vj)@p(v, 
hj)=p(hjv, 
)@. 
log 
p(v, 
hj)= 
h@. 
log 
p(v, 
hj)ip(hjv;) 
(11.7.3) 


hh 


where 
we 
used 
. 
log 
f(x) 
= 
(1=f(x))@f(x). 
The 
right 
hand 
side 
is 
the 
average 
of 
the 
derivative 
of 
the 
log 
complete 
likelihood. 
This 
is 
closely 
related 
to 
the 
derivative 
of 
the 
energy 
term 
in 
the 
EM 
algorithm, 
though 
note 
that 
the 
average 
here 
is 
performed 
with 
respect 
the 
current 
distribution 
parameters 
. 
and 
not 
old 
as 
in 
the 
EM 
case. 
Used 
in 
this 
way, 
computing 
the 
derivatives 
of 
latent 
variable 
models 
is 
relatively 
straightforward. 
These 
derivatives 
may 
then 
be 
used 
as 
part 
of 
a 
standard 
optimisation 
routine 
such 
as 
conjugate 
gradients[237]. 


236 
DRAFT 
March 
9, 
2010 



Exercises 


11.7.2 
Undirected 
models 
Consider 
an 
undirected 
model 
which 
contains 
both 
hidden 
and 
visible 
variables 
1 


(v;h)

p(v, 
hj)= 
e 
(11.7.4)

Z()
For 
i.i.d. 
data, 
the 
log 
likelihood 
on 
the 
visible 
variables 
is 
(assuming 
discrete 
v 
and 
h) 


.


. 


X

X

X

(v(v;hj)

L()=@loge 
n;hj) 
- 
loge

.


(11.7.5) 
n 
h 
h;v 


which 
has 
gradient 


1

0X

BBBB
.


CCCC
. 










 


. 


@. 


(v 
n;hj)..(v, 
hj)

@. 
@. 


p(hjvn)

}

(11.7.6)
L 
=


@. 


}

p(h;v)

clamped 
average 
free 
average 


For 
a 
Markov 
Network 
that 
is 
intractable 
(the 
partition 
function 
Z 
cannot 
be 
computed 
eciently), 
the 
gradient 
is 
particularly 
dicult 
to 
estimate 
since 
it 
is 
the 
dierence 
of 
two 
quantities, 
each 
of 
which 
needs 
to 
be 
estimated. 
Even 
getting 
the 
sign 
of 
the 
gradient 
correct 
can 
therefore 
be 
computationally 
dicult. 
For 
this 
reason 
learning 
in 
models, 
such 
as 
the 
Boltzmann 
machine 
with 
hidden 
units, 
is 
particularly 
dicult. 


11.8 
Code 
In 
the 
demo 
code 
we 
take 
the 
original 
Chest 
Clinic 
network 
[170] 
and 
draw 
data 
samples 
from 
this 
network. 
Our 
interest 
is 
then 
to 
see 
if 
we 
can 
use 
the 
EM 
algorithm 
to 
estimate 
the 
tables 
based 
on 
the 
data 
(with 
some 
parts 
of 
the 
data 
missing 
at 
random). 
We 
assume 
that 
we 
know 
the 
correct 
BN 
structure, 
only 
that 
the 
CPTs 
are 
unknown. 
We 
assume 
the 
logic 
gate 
table 
is 
known, 
so 
we 
do 
not 
need 
to 
learn 
this. 
demoEMchestclinic.m: 
Demo 
of 
EM 
in 
learning 
the 
Chest 
Clinic 
Tables 


The 
following 
code 
implements 
Maximum 
Likelihood 
learning 
of 
BN 
tables 
based 
on 
data 
with 
possibly 
missing 
values. 
EMbeliefnet.m: 
EM 
training 
of 
a 
Belief 
Network 


11.9 
Exercises 
Exercise 
138 
(Printer 
Nightmare 
continued). 
Continuing 
with 
the 
BN 
given 
in 
g(9.19), 
the 
following 
table 
represents 
data 
gathered 
on 
the 
printer, 
where 
? 
indicates 
that 
the 
entry 
is 
missing. 
Each 
column 
represents 
a 
datapoint. 
Use 
the 
EM 
algorithm 
to 
learn 
all 
CPTs 
of 
the 
network. 


n

 


{ 


 


{ 


fuse 
assembly 
malfunction 
? 
? 
? 
1 
0 
0 
? 
0 
? 
0 
0 
? 
1 
? 
1 
drum 
unit 
? 
0 
? 
0 
1 
0 
0 
1 
? 
? 
1 
1 
? 
0 
0 
toner 
out 
1 
1 
0 
? 
? 
1 
0 
1 
0 
? 
0 
1 
? 
0 
? 
poor 
paper 
quality 
1 
0 
1 
0 
1 
? 
1 
0 
1 
1 
? 
1 
1 
? 
0 
worn 
roller 
0 
0 
? 
? 
? 
0 
1 
? 
0 
0 
? 
0 
? 
1 
1 
burning 
smell 
0 
? 
? 
1 
0 
0 
0 
0 
0 
? 
0 
? 
1 
0 
? 
poor 
print 
quality 
1 
1 
1 
0 
1 
1 
0 
1 
0 
0 
1 
1 
? 
? 
0 
wrinkled 
pages 
0 
0 
1 
0 
0 
0 
? 
0 
1 
? 
0 
0 
1 
1 
1 
multiple 
pages 
fed 
0 
? 
1 
0 
? 
0 
1 
0 
1 
? 
0 
0 
? 
0 
1 
paper 
jam 
? 
0 
1 
1 
? 
0 
1 
1 
1 
1 
0 
? 
0 
1 
? 


DRAFT 
March 
9, 
2010 



Exercises 


The 
table 
is 
contained 
in 
EMprinter.mat, 
using 
states 
1, 
2, 
nan 
in 
place 
of 
0, 
1, 
? 
(since 
BRMLtoolbox 
requires 
states 
to 
be 
numbered 
1,2,....). 
Given 
no 
wrinkled 
pages, 
no 
burning 
smell 
and 
poor 
print 
quality, 
what 
is 
the 
probability 
there 
is 
a 
drum 
unit 
problem? 


Exercise 
139. 
Consider 
the 
following 
distribution 
over 
discrete 
variables, 


p(x1;x2;x3;x4;x5)= 
p(x1jx2)p(x2jx3)p(x3jx4)p(x4jx5)p(x5), 
(11.9.1) 


in 
which 
the 
variables 
x2 
and 
x4 
are 
consistently 
hidden 
in 
the 
training 
data, 
and 
training 
data 
for 
x1;x3;x5 
are 
always 
present. 
Show 
that 
the 
EM 
update 
for 
the 
table 
p(x1jx2) 
is 
given 
by 


P

I[xn 
= 
i] 
pold(x2 
= 
jjx1 
n;x3 
n;xn)

p 
new(x1 
= 
ijx2 
= 
j)=Pn 
1 
n 
old(x2 
nn 
5 
n 
(11.9.2)

I[x= 
k] 
p= 
jjx1 
;x3 
;x)

n;k 
15 


Exercise 
140. 
Consider 
a 
simple 
two 
variable 
BN 


p(y, 
x)= 
p(yjx)p(x) 
(11.9.3) 


where 
both 
y 
and 
x 
are 
binary 
variables, 
dom(x)= 
f1, 
2g, 
dom(y)= 
f1, 
2g. 
You 
have 
a 
set 
of 
training 


nn

data 
f(y;xn) 
;n 
=1;:::;Ng, 
in 
which 
for 
some 
cases 
xmay 
be 
missing. 
We 
are 
specically 
interested 
in 
learning 
the 
table 
p(x) 
from 
this 
data. 
A 
colleague 
suggests 
that 
one 
can 
set 
p(x) 
by 
simply 
looking 
at 
datapoints 
where 
x 
is 
observed, 
and 
then 
setting 
p(x 
= 
1) 
to 
be 
the 
fraction 
of 
observed 
x 
that 
is 
in 
state 


1. 
Explain 
how 
this 
suggested 
procedure 
relates 
to 
Maximum 
Likelihood 
and 
EM. 
Exercise 
141. 
Assume 
that 
a 
sequence 
is 
generated 
by 
a 
Markov 
chain. 
For 
a 
single 
chain 
of 
length 
T 
, 
we 
have 


T 
..1

YP

p(v1;:::;vT 
)= 
p(v1) 
p(vt+1jvt) 
(11.9.4) 
t=1 


For 
simplicity, 
we 
denote 
the 
sequence 
of 
visible 
variables 
as 


v 
=(v1;:::;vT 
) 
(11.9.5) 


For 
a 
single 
Markov 
chain 
labelled 
by 
h, 


T 
..1

YP

p(vjh)= 
p(v1jh) 
p(vt+1jvt;h) 
(11.9.6) 


t=1 


In 
total 
there 
are 
a 
set 
of 
H 
such 
Markov 
chains 
(h 
=1;:::;H). 
The 
distribution 
on 
the 
visible 
variables 
is 
therefore 


H

XP

p(v)= 
p(vjh)p(h) 
(11.9.7) 


h=1 


1. 
There 
are 
a 
set 
of 
training 
sequences, 
vn;n 
=1;:::;N. 
Assuming 
that 
each 
sequence 
vn 
is 
indep
endently 
and 
identically 
drawn 
from 
a 
Markov 
chain 
mixture 
model 
with 
H 
components, 
derive 
the 
Expectation 
Maximisation 
algorithm 
for 
training 
this 
model. 
2. 
Write 
a 
general 
MATLAB 
function 
in 
the 
form 
function 
[q,ph,pv,A]=mchain_mix(v,V,H,num_em_loops) 


n

to 
perform 
EM 
learning 
for 
any 
set 
of 
(the 
same 
length) 
sequences 
of 
integers 
v. 
[1 
: 
V 
],

t 


t 
=1;:::;T 
. 
v 
is 
a 
cell 
array 
of 
the 
training 
data: 
v{2}(4) 
is 
the 
4th 
time 
element 
of 
the 
seco
nd 
training 
sequence. 
Each 
element, 
say 
v{2}(4) 
must 
be 
an 
integer 
from 
1 
to 
V 
. 
V 
is 
the 
number 
of 
states 
of 
the 
visible 
variables 
(in 
the 
bio-sequence 
case 
below, 
this 
will 
be 
4). 
H 
is 
the 


DRAFT 
March 
9, 
2010 



Exercises 


number 
of 
mixture 
components. 
num_em_loops 
is 
the 
number 
of 
EM 
iterations. 
A 
is 
the 
transit
ion 
matrix 
A{h}(i,j)=p(v(t+1)=i|v(t)=j,h). 
pv 
is 
the 
prior 
state 
of 
the 
rst 
visible 
variable, 
pv{h}(i)=p(v(t=1)=i|h). 
ph 
is 
a 
vector 
of 
prior 
probabilities 
for 
the 
mixture 
state 
ph(h)=p(h). 
q 
is 
the 
cell 
array 
of 
posterior 
probabilities 
q{mu}(h)=p(h|v{mu}). 
Your 
routine 
must 
also 
display, 
for 
each 
EM 
iteration, 
the 
value 
of 
the 
log 
likelihood. 
As 
a 
check 
on 
your 
routine, 
the 
log 
likelihood 
must 
increase 
at 
each 
iteration. 


3. 
The 
le 
sequences.mat 
contains 
a 
set 
of 
ctitious 
bio-sequence 
in 
a 
cell 
array 
sequences{mu}(t). 
Thus 
sequences{3}(:) 
is 
the 
third 
sequence, 
GTCTCCTGCCCTCTCTGAAC 
which 
consists 
of 
20 
timesteps. 
There 
are 
20 
such 
sequences 
in 
total. 
Your 
task 
is 
to 
cluster 
these 
sequences 
into 
two 
clusters, 
assuming 
that 
each 
cluster 
is 
modelled 
by 
a 
Markov 
chain. 
State 
which 
of 
the 
sequences 
belong 
together 
by 
assigning 
a 
sequence 
vn 
to 
that 
state 
for 
which 
p(hjvn) 
is 
highest. 
Exercise 
142. 
Write 
a 
general 
purpose 
routine 
VBbeliefnet(pot,x,pars) 
along 
the 
lines 
of 
EMbeliefnet.m 
that 
performs 
Variational 
Bayes 
under 
a 
Dirichlet 
prior, 
using 
a 
factorised 
parameter 
approximation. 
Ass
ume 
both 
global 
and 
local 
parameter 
independence 
for 
the 
prior 
and 
the 
approximation 
q, 
section(9.3.1). 


Exercise 
143. 
Consider 
a 
3 
`layered’ 
Boltzmann 
Machine 
which 
has 
the 
form 


1 


p(v, 
h1, 
h2, 
h3j)= 
(v, 
h1j1)(h1, 
h2j2)(h2, 
h3j3) 
(11.9.8)

Z 


where 
dim 
v 
= 
dim 
h1 
= 
dim 
h2 
= 
dim 
h3 
= 
V 


PV 


i;j=1 
Wij 
xiyj 
+Aij 
xixj 
+Bij 
yiyj

(x, 
yj)= 
e(11.9.9) 


	

All 
variables 
are 
binary 
with 
states 
0, 
1 
and 
the 
parameters 
for 
each 
layer 
l 
are 
l 
=Wl 
, 
Al 
, 
Bl. 


1. 
In 
terms 
of 
tting 
the 
model 
to 
visible 
data 
v1 
;:::, 
vN 
, 
is 
the 
3 
layered 
model 
above 
any 
more 
powerful 
than 
tting 
a 
two-layered 
model 
(the 
factor 
(h2, 
h3j3) 
is 
not 
present 
in 
the 
two-layer 
case)? 
2. 
If 
we 
use 
a 
restricted 
potential 
P

i;j 
Wij 
xiyj

(x, 
yj)= 
e(11.9.10) 


is 
the 
three 
layered 
model 
more 
powerful 
in 
being 
able 
to 
t 
the 
visible 
data 
than 
the 
two-layered 
model? 


Exercise 
144. 
The 
sigmoid 
Belief 
Network 
is 
dened 
by 
the 
layered 
network 


L

Y

p(xL) 
p(xl..1jxl) 
(11.9.11) 
l=1 


where 
vector 
variables 
have 
binary 
components 
xl 
2f0, 
1gwl 
and 
the 
width 
of 
layer 
l 
is 
given 
by 
wl. 
In 
addition 


wl

Y

l..1 


p(xl..1jxl)= 
p(x 
jxl) 
(11.9.12)

i 
i=1 


and 




l..1 
T 
l

p(x 
=1jxl)= 
w;(x)=1=(1 
+ 
e 
..x) 
(11.9.13)

i 
i;lx

for 
a 
weight 
vector 
wi;l 
describing 
the 
interaction 
from 
the 
parental 
layer. 
The 
top 
layer, 
p(xL) 
describes 
LL

a 
factorised 
distribution 
p(x1 
);:::;p(xwL 
). 


1. 
Draw 
the 
Belief 
Network 
structure 
of 
this 
distribution. 
2. 
For 
the 
layer 
x0, 
what 
is 
the 
computational 
complexity 
of 
computing 
the 
likelihood 
p(x0), 
assuming 
that 
all 
layers 
have 
equal 
width 
w? 
DRAFT 
March 
9, 
2010 



Exercises 


3. 
Assuming 
a 
fully 
factorised 
approximation 
for 
an 
equal 
width 
network, 
Lw

YYP

1 
l 


p(x;:::, 
xLjx0) 
˜ 
q(x 
) 
(11.9.14)

i
l=1 
i=1 


write 
down 
the 
energy 
term 
of 
the 
Variational 
EM 
procedure 
for 
a 
single 
data 
observation 
x0, 
and 
discuss 
the 
tractability 
of 
computing 
the 
energy. 


Exercise 
145. 
Show 
how 
to 
nd 
the 
components 
0 
= 
(b;g;p) 
= 
1 
that 
maximise 
equation 
(11.1.10). 


P2 
P2

Exercise 
146. 
A 
2 
× 
2 
probability 
table, 
p(x1 
= 
i, 
x2 
= 
j)= 
i;j, 
with 
0 
= 
i;j 
= 
1,i=1j=1 
i;j 
=1 
is 
learned 
using 
maximal 
marginal 
likelihood 
in 
which 
x2 
is 
never 
observed. 
Show 
that 
if 




0:30:3 
. 
=(11.9.15)

0:20:2
is 
given 
as 
a 
maximal 
marginal 
likelihood 
solution, 
then 




0:20:4 
. 
=(11.9.16)

0:40
has 
the 
same 
marginal 
likelihood 
score. 


DRAFT 
March 
9, 
2010 



CHAPTER 
12 


Bayesian 
Model 
Selection 


12.1 
Comparing 
Models 
the 
Bayesian 
Way 
Given 
two 
models 
M1 
and 
M2 
with 
parameters 
1, 
2 
and 
associated 
parameter 
priors, 


p(x, 
1jM1)= 
p(xj1;M1)p(1jM1);p(x, 
2jM2)= 
p(xj2;M2)p(2jM2) 
(12.1.1) 


how 
can 
we 
compare 
the 
performance 
of 
the 
models 
in 
tting 
a 
set 
of 
data 
D 
= 
fx1;:::;xN 
g? 
The 
application 
of 
Bayes’ 
rule 
to 
models 
gives 
a 
framework 
for 
answering 
questions 
like 
this 
– 
a 
form 
of 
Bayesian 
Hypothesis 
testing, 
applied 
at 
the 
model 
level. 
More 
generally, 
given 
an 
indexed 
set 
of 
models 
M1;:::;Mm, 
and 
associated 
prior 
beliefs 
in 
the 
appropriateness 
of 
each 
model 
p(Mi), 
our 
interest 
is 
the 
model 
posterior 
probability 


p(DjMi)p(Mi) 


p(MijD) 
= 
(12.1.2) 


p(D) 


where 


m

XZ

p(D)= 
p(DjMi)p(Mi) 
(12.1.3) 


i=1 


Model 
Mi 
is 
parameterised 
by 
i, 
and 
the 
model 
likelihood 
is 
given 
by 


Z

p(DjMi)=p(Dji;Mi)p(ijMi)di 
(12.1.4) 


In 
discrete 
parameter 
spaces, 
the 
integral 
is 
replaced 
with 
summation. 
Note 
that 
the 
number 
of 
parameters 
dim 
(i) 
need 
not 
be 
the 
same 
for 
each 
model. 


A 
point 
of 
caution 
here 
is 
that 
p(MijD) 
only 
refers 
to 
the 
probability 
relative 
to 
the 
set 
of 
models 
specied 
M1;:::;Mm. 
This 
is 
not 
the 
absolute 
probability 
that 
model 
M 
ts 
`well'. 
To 
compute 
such 
a 
quantity 
would 
require 
one 
to 
specify 
all 
possible 
models. 
Whilst 
interpreting 
the 
posterior 
p(MijD) 
requires 
some 
care, 
comparing 
two 
competing 
model 
hypotheses 
Mi 
and 
Mj 
is 
straightforward 
and 
only 
requires 
the 


Bayes’ 
factor 


p(MijD) 
p(DjMi) 
p(Mi)

= 
(12.1.5) 


p(MjjD) 
p(DjMj) 
p(Mj)

|{z}Z

Bayes’ 
Factor 


which 
does 
not 
require 
integration/summation 
over 
all 
possible 
models. 
241 



Illustrations 
: 
coin 
tossing 


00.10.20.30.40.50.60.70.80.9100.20.40.60.8
00.10.20.30.40.50.60.70.80.9100.050.10.15
(a) 
(b) 
Figure 
12.1: 
(a): 
Discrete 
prior 
model 
of 
a 
`fair’ 
coin. 
(b): 
Prior 
for 
a 
biased 
`unfair’ 
coin. 
In 
both 
cases 
we 
are 
making 
explicit 
choices 
here 
about 
what 
we 
consider 
to 
be 
a 
`fair’ 
and 
and 
`unfair'. 


12.2 
Illustrations 
: 
coin 
tossing 
We'll 
consider 
two 
illustrations. 
The 
rst 
uses 
a 
discrete 
parameter 
space 
to 
keep 
the 
mathematics 
simple. 
In 
the 
second 
we 
use 
a 
continuous 
parameter 
space. 


12.2.1 
A 
discrete 
parameter 
space 
A 
simple 
choice 
would 
be 
to 
consider 
two 
competing 
models, 
one 
corresponding 
to 
a 
fair 
coin, 
and 
the 
other 
a 
biased 
coin. 
The 
bias 
of 
the 
coin, 
namely 
the 
probability 
that 
the 
coin 
will 
land 
heads, 
is 
specied 
by 
, 
so 
that 
a 
truly 
fair 
coin 
has 
. 
=0:5. 
For 
simplicity 
we 
assume 
dom()= 
f0:1, 
0:2;:::, 
0:9g. 
For 
the 
fair 
coin 
we 
use 
the 
distribution 
p(jMfair) 
in 
g(12.1a) 
and 
for 
the 
biased 
coin 
the 
distribution 
p(jMbiased) 
in 
g(12.1b). 


For 
each 
model 
M, 
the 
likelihood 
is 
given 
by 


XX

p(DjM)=p(Dj, 
M)p(jM)=NH 
(1 
- 
)NT 
p(jM) 
(12.2.1) 


. 


=0:1NH 
(1 
- 
0:1)NT 
p(. 
=0:1jM)+ 
::. 
+0:9NH 
(1 
- 
0:9)NT 
p(. 
=0:9jM) 
(12.2.2) 


Assuming 
that 
p(Mfair)= 
p(Mbiased) 
the 
Bayes’ 
factor 
is 
given 
by 
the 
ratio 
of 
the 
two 
model 
likelihoods. 


Example 
55 
(Discrete 
parameter 
space). 


5 
Heads 
and 
2 
Tails 
Here 
p(DjMfair)=0:00786 
and 
p(DjMbiased)=0:0072. 
The 
Bayes’ 
factor 
is 


p(MfairjD) 


=1:09 
(12.2.3) 


p(MbiasedjD) 
indicating 
that 
there 
is 
little 
to 
choose 
between 
the 
two 
models. 
50 
Heads 
and 
20 
Tails 
Here 
p(DjMfair) 
= 
and 
p(DjMbiased) 
= 
. 
The 
Bayes’ 


1:5 
× 
10..20 
1:4 
× 
10..19 
factor 
is 
p(MfairjD) 


=0:109 
(12.2.4) 


p(MbiasedjD) 
indicating 
that 
have 
around 
10 
times 
the 
belief 
in 
the 
biased 
model 
as 
opposed 
to 
the 
fair 
model. 


12.2.2 
A 
continuous 
parameter 
space 
Here 
we 
repeat 
the 
above 
calculation 
but 
for 
continuous 
parameter 
spaces. 


DRAFT 
March 
9, 
2010 



Illustrations 
: 
coin 
tossing 


00.20.40.60.8102468
00.20.40.60.8100.511.5
(a) 
(b) 
Figure 
12.2: 
Probability 
density 
priors 
on 
the 
probability 
of 
a 
Head 
p(). 
(a): 
For 
a 
fair 
coin, 
p(jMfair)= 
B 
(j50, 
50). 
(b): 
For 
an 
biased 
coin, 
p(jMbiased)=0:5(B 
(j3, 
10) 
+ 
B 
(j10, 
3)). 
Note 
the 
dierent 
vertical 
scales 
in 
the 
two 
cases. 


Fair 
coin 


For 
the 
fair 
coin, 
a 
uni-modal 
prior 
is 
appropriate. 
We 
use 
Beta 
distribution 
1 


p()= 
B 
(ja, 
b) 
;B 
(ja, 
b) 
= 
a..1 
(1 
- 
)b..1 
(12.2.5)

B(a, 
b)

for 
convenience 
since 
as 
this 
is 
conjugate 
to 
the 
binomial 
distribution 
the 
required 
integrations 
are 
trivial. 
A 
reasonable 
choice 
for 
a 
fair 
coin 
is 
a 
= 
50, 
b 
= 
50, 
as 
shown 
in 
g(12.2a). 


In 
general, 


ZZ

1 


p(DjMfair)=p()NH 
(1 
- 
)NT 
= 
a..1 
(1 
- 
)b..1 
NH 
(1 
- 
)NT 
(12.2.6)

B(a, 
b)



Z

1 
B(NH 
+ 
a, 
NT 
+ 
b)

= 
NH 
+a..1 
(1 
- 
)NT 
+b..1 
= 
(12.2.7)

B(a, 
b)B(a, 
b)

. 


Biased 
coin 


For 
the 
biased 
coin, 
we 
use 
a 
bimodal 
distribution 
formed, 
for 
convenience, 
as 
a 
mixture 
of 
two 
Beta 
distributions: 


1 


p(jMbiased)= 
[B 
(ja1;b1)+ 
B 
(ja2;b2)] 
(12.2.8)

2 
as 
shown 
in 
g(12.2b). 
The 
model 
likelihood 
p(DjMbiased) 
is 
given 
by

Z

p(jMbiased)NH 
(1 
- 
)NT 
(12.2.9) 


. 


ZZ

11 
1 


= 
a1..1 
(1)b1..1 
NH 
(1 
- 
)NT 
+ 
a2..1 
(1 
- 
)b2..1 
NH 
(1 
- 
)NT

2B(a1;b1)B(a2;b2)

. 


(12.2.10)


1 
B(NH 
+ 
a1;NT 
+ 
b1) 
B(NH 
+ 
a2;NT 
+ 
b2)

= 
+ 
(12.2.11)

2B(a1;b1) 
B(a2;b2)

Assuming 
no 
prior 
preference 
for 
either 
a 
fair 
or 
biased 
coin 
p(M)= 
const., 
and 
repeating 
the 
above 
scenario 
in 
the 
discrete 
parameter 
case: 


Example 
56 
(Continuous 
parameter 
space). 


DRAFT 
March 
9, 
2010 



Occam's 
Razor 
and 
Bayesian 
Complexity 
Penalisation 


000.10.2000.10.2000.10.2000.10.212345678910111213141516171819202122232425262728293000.10.2
Figure 
12.3: 
The 
likelihood 
of 
the 
total 
dice 
score, 
p(tjn) 
for 
n 
= 
1 
(top) 
to 
n 
= 
5 
(bottom) 
die. 
Plotted 
along 
the 
horizontal 
axis 
is 
the 
total 
score 
t. 
The 
vertical 
line 
marks 
the 
comparison 
for 
p(t 
=9jn) 
for 
the 
dierent 
number 
of 
die. 
The 
more 
complex 
models, 
which 
can 
reach 
more 
states, 
have 
lower 
likelihood, 
due 
to 
normalisation 
over 
t. 


5 
Heads 
and 
2 
Tails 
Here 
p(DjMfair)=0:0079 
and 
p(DjMbiased)=0:00622. 
The 
Bayes’ 
factor 
is 


p(MfairjD) 


=1:27 
(12.2.12) 
p(MbiasedjD) 


indicating 
that 
there 
is 
little 
to 
choose 
between 
the 
two 
models. 


9:4 
× 
10..21 
1:09 
× 
10..19 
factor 
is 
50 
Heads 
and 
20 
Tails 
Here 
p(DjMfair) 
= 
and 
p(DjMbiased) 
= 
. 
The 
Bayes’ 


p(MfairjD) 


=0:087 
(12.2.13) 
p(MbiasedjD) 


indicating 
that 
have 
around 
11 
times 
the 
belief 
in 
the 
biased 
model 
as 
opposed 
to 
the 
fair 
model. 


12.3 
Occam's 
Razor 
and 
Bayesian 
Complexity 
Penalisation 
We 
return 
to 
the 
dice 
scenario 
of 
section(1.3.1). 
There 
we 
assumed 
there 
are 
two 
die 
whose 
scores 
s1 
and 
s2 
are 
not 
known. 
Only 
the 
sum 
of 
the 
two 
scores 
t 
= 
s1 
+ 
s2 
is 
known. 
We 
then 
computed 
the 
posterior 
joint 
score 
distribution 
p(s1;s2jt 
= 
9) 
for 
the 
two 
die. 
We 
repeat 
the 
calculation 
but 
now 
for 
multiple 
dice 
and 
with 
the 
twist 
that 
we 
don't 
know 
how 
many 
dice 
there 
are1, 
only 
that 
the 
sum 
of 
the 
scores 
is 
9. 


PX

n

That 
is, 
we 
know 
t 
= 
i=1 
si 
and 
are 
given 
the 
value 
t 
= 
9. 
However, 
we 
are 
not 
told 
the 
number 
of 
die 
involved 
n. 
Assuming 
that 
any 
number 
n 
is 
equally 
likely, 
what 
is 
the 
posterior 
distribution 
over 
n? 


From 
Bayes’ 
rule, 
we 
need 
to 
compute 
the 
posterior 
distribution 
over 
models 


p(tjn)p(n) 


p(njt) 
= 
(12.3.1) 


p(t) 


In 
the 
above 


"#

n

XXYXXY

p(tjn)=p(t, 
s1;:::;snjn)=p(tjs1;:::;sn)p(si)=It 
= 
sip(si) 
s1;:::;sn 
s1;:::;sn 
is1;:::;sn 
i=1 
i 


(12.3.2) 
1This 
description 
of 
Occam's 
razor 
is 
due 
to 
Taylan 
Cemgil. 


244 
DRAFT 
March 
9, 
2010 



A 
continuous 
example 
: 
curve 
tting 


1 
2 
3 
n
4 
5 
6 
Figure 
12.4: 
The 
posterior 
distribution 
p(njt= 
9) 
of 
the 
number 
of 
die 
given 
the 
observed 
summed 
score 
of 
9. 
where 
p(si) 
= 
1=6 
for 
all 
scores 
si. 
By 
enumerating 
all 
6n 
states, 
we 
can 
explicitly 
compute 
p(tjn), 
as 


00:5
displayed 
in 
g(12.3). 
The 
important 
observation 
is 
that 
as 
the 
models 
explaining 
the 
data 
become 
more 
`complex’ 
(nincreases), 
more 
states 
become 
accessible 
and 
the 
probability 
mass 
typically 
reduces. 
We 
see 
this 
eect 
at 
p(t=9jn) 
where, 
apart 
from 
n= 
1, 
the 
value 
of 
p(t=9jn) 
decreases 
with 
increasing 
nsince 
the 
higher 
nhave 
mass 
in 
more 
states, 
becoming 
more 
spread 
out. 
Assuming 
p(n)= 
const., 
the 
posterior 
p(njt= 
9) 
is 
plotted 
in 
g(12.4). 
A 
posteriori, 
there 
are 
only 
3 
plausible 
models, 
namely 
n=2;3;4 
since 
the 
rest 
are 
either 
too 
complex, 
or 
impossible. 
This 
demonstrates 
the 
Occam's 
razor 
eect 
which 
penalises 
models 
which 
are 
over 
complex. 


12.4 
A 
continuous 
example 
: 
curve 
tting 
Consider 
an 
additive 
set 
of 
periodic 
functions 


0 


y= 
w0 
+ 
w1 
cos(x)+ 
w2 
cos(2x)+ 
:::+ 
wK 
cos(Kx) 
(12.4.1) 


This 
can 
be 
conveniently 
written 
in 
vector 
form 


0 


y= 
wT(x) 
(12.4.2) 


where 
(x) 
is 
a 
K+ 
1 
dimensional 
vector 
with 
elements 
(1;cos(x);cos(2x);:::;cos(Kx))T 
and 
the 
vector 


n

w 
contains 
the 
weights 
of 
the 
additive 
function. 
We 
are 
given 
a 
set 
of 
data 
D 
= 
f(x;yn);n=1;:::;Ngdrawn 
from 
this 
distribution, 
where 
yis 
the 
clean 
y0(x) 
corrupted 
with 
additive 
zero 
mean 
Gaussian 
noise 
with 
variance 
2 
, 


..



n 


y= 
y0(xn)+n 
;n 
Nn 


0;2(12.4.3) 


see 
g(12.5). 
Assuming 
i.i.d. 
data, 
we 
are 
interested 
in 
the 
posterior 
probability 
of 
the 
number 
of 
coecients, 
given 
the 
observed 
data: 


Q

p(DjK)p(K) 
p(K)p(x

n 
11 


p(KjD)= 
= 
n) 
p(y;:::;yN 
jx;:::;xN 
;K) 
(12.4.4) 


p(D) 
p(D) 
We 
will 
assume 
p(K)= 
const. 
The 
likelihood 
term 
above 
is 
given 
by 
the 
integral 


ZN

N 


11 
n 


p(y;:::;yN 
jx;:::;xN 
;K)=p(wjK) 
p(ynjx;w;K) 
(12.4.5) 


w 
n=1 


For 
p(wjK)= 
N 
(w 


0;IK=), 
the 
integrand 
is 
a 
Gaussian 
in 
w 
for 
which 
it 
is 
straightforward 
to 
evaluate 
the 
integral, 
(see 
section(8.6) 
and 
exercise(149)) 


N

... 
(yn)2 
11

2 
log 
p(y;:::;yN 
jx;:::;yN 
;K)= 
Nlog22- 
+ 
bTA..1b 
- 
log 
det 
(2A)+ 
Klog 
(2)

2 


n=1 


w
Kxn
yn
0
yn
N
Figure 
12.5: 
Belief 
Network 
representation 
of 
a 
Hierarchical 
Bayesian 
Model 
for 
regression 
under 
the 
i.i.d. 
data 
assumption. 
Note 
that 


n

the 
intermediate 
nodes 
on 
yare 
included 
to 
highlight 
the 
role 
of 
the 


0 
g 


`clean’ 
underlying 
model. 
Since 
p(yjw;x)= 
p(yjy0)p(y0jw;x)= 


y0

....

g 
..

y0 
Ny

y0;2y0 
- 
wTx= 
Ny

wTx;2, 
we 
can 
if 
desired 
do 
away 


with 
the 
intermediate 
node 
y0 
and 
place 
directly 
arrows 
from 
w 
and 
xn 
to 
n

y. 
DRAFT 
March 
9, 
2010 



Approximating 
the 
Model 
Likelihood 


-10-50510-3-2-10123
1234567891000.20.40.60.81
-10-50510-3-2-10123
(a) 
(b) 
(c) 
Figure 
12.6: 
(a) 
The 
data 
generated 
with 
additive 
Gaussian 
noise 
s 
=0:5 
from 
a 
K 
= 
5 
component 
model. 


(b) 
The 
posterior 
p(KjD). 
(c) 
The 
reconstruction 
of 
the 
data 
using 
hwiT 
(x) 
where 
hw) 
is 
the 
mean 
posterior 
vector 
of 
the 
optimal 
dimensional 
model 
p(wjD;K 
= 
5). 
Plotted 
in 
the 
continuous 
line 
is 
the 
reconstruction. 
Plotted 
in 
dots 
is 
the 
true 
underlying 
clean 
data. 
(12.4.6) 
where 


XX

1 
N1 
N

A 
= 
I 
+ 
(x 
n)T(x 
n), 
b 
= 
y 
n(x 
n) 
(12.4.7)

2 
2 


n=1 
n=1 


Assuming 
a 
= 
1 
and 
s 
=0:5, 
we 
sampled 
some 
data 
from 
a 
model 
with 
K 
= 
5 
components, 
g(12.6a). 
We 
assume 
that 
we 
know 
the 
correct 
noise 
level 
. 
The 
posterior 
p(KjD) 
plotted 
in 
g(12.6b) 
is 
sharply 
peaked 
at 
K 
= 
5, 
which 
is 
the 
`correct’ 
value 
used 
to 
generate 
the 
data. 
The 
clean 
reconstructions 
for 
K 
= 
5 
are 
plotted 
in 
g(12.6c). 


12.5 
Approximating 
the 
Model 
Likelihood 
For 
a 
model 
with 
continuous 
parameter 
vector 
, 
dim 
()= 
K 
and 
data 
D, 
the 
model 
likelihood 
is 


p(DjM)= 
p(Dj;M)p(jM)d. 
(12.5.1) 


. 


For 
a 
generic 
expression 


..f()

p(Dj;M)p(jM)= 
e 
(12.5.2) 


unless 
f 
is 
of 
a 
particularly 
simple 
form 
(quadratic 
in 
. 
for 
example), 
one 
cannot 
compute 
the 
integral 
in 


(12.5.1) 
and 
approximations 
are 
required. 
12.5.1 
Laplace's 
method 
A 
simple 
approximation 
of 
(12.5.1) 
is 
given 
by 
Laplace's 
method, 
section(28.2), 


1 
..

log 
p(DjM) 
..f(. 
)+ 
logdet2H..1(12.5.3)

2 


where 
. 
* 
is 
the 
MAP 
solution 


. 
* 
= 
argmax 
p(Dj;M)p(jM) 
(12.5.4) 


. 


and 
H 
is 
the 
Hessian 
of 
f() 
at 
. 
* 
. 


246 
DRAFT 
March 
9, 
2010 



Exercises 


	

1N

For 
data 
D 
=x;:::;xthat 
is 
i.i.d. 
generated 
the 
above 
specialises 
to 


ZN

Y

p(DjM)=p(jM) 
p(x 
nj;M)d. 
(12.5.5) 


. 


n=1 


In 
this 
case 
Laplace's 
method 
computes 
the 
optimum 
of 
the 
function 


N

X

..f() 
= 
log 
p(jM) 
+ 
log 
p(x 
nj;M) 
(12.5.6) 


n=1 


12.5.2 
Bayes 
information 
criterion 
(BIC) 
For 
i.i.d. 
data 
the 
Hessian 
scales 
with 
the 
number 
of 
training 
examples, 
N, 
and 
a 
crude 
approximation 


log 
p(DjM) 
˜ 
log 
p(Dj. 
* 
;M) 
+ 
log 
p(. 
* 
jM)+ 
log2p 
- 
log 
N 
(12.5.7) 


is 
to 
set 
H 
˜ 
NIK 
where 
K 
= 
dim 
. 
In 
this 
case 
one 
may 
take 
as 
a 
model 
comparison 
procedure 
the 
function 
K 
K 


22 


For 
a 
simple 
prior 
that 
penalises 
the 
length 
of 
the 
parameter 
vector, 
p(jM)= 
N 
(. 


0, 
I), 
the 
above 
reduces 
to 


1 
K

log 
p(DjM) 
˜ 
log 
p(Dj. 
* 
;M) 
- 
(. 
)T 
. 
* 
- 
log 
N 
(12.5.8)

22 


The 
Bayes 
Information 
Criterion[244] 
approximates 
(12.5.7) 
by 
ignoring 
the 
penalty 
term, 
giving 


K 


BIC 
= 
log 
p(Dj. 
* 
;M) 
- 
log 
N 
(12.5.9)

2 


The 
BIC 
criterion 
may 
be 
used 
as 
an 
approximate 
way 
to 
compare 
models, 
where 
the 
term 
..K 
log 
N

2 


penalises 
model 
complexity. 
In 
general, 
the 
Laplace 
approximation, 
equation 
(12.5.3), 
is 
to 
be 
preferred 
to 
the 
BIC 
criterion 
since 
it 
more 
correctly 
accounts 
for 
the 
uncertainty 
in 
the 
posterior 
parameter 
estimate. 
Other 
techniques 
that 
aim 
to 
improve 
on 
the 
Laplace 
method 
are 
discussed 
in 
section(28.3) 
and 
section(28.7). 


12.6 
Exercises 
Exercise 
147. 
Write 
a 
program 
to 
implement 
the 
fair/biased 
coin 
tossing 
model 
selection 
example 
of 
section(12.2.1) 
using 
a 
discrete 
domain 
for 
. 
Explain 
how 
to 
overcome 
potential 
numerical 
issues 
in 
dealing 
with 
large 
NH 
and 
NT 
(of 
the 
order 
of 
1000). 


Exercise 
148. 
You 
work 
at 
Dodder's 
hedge 
fund 
and 
the 
manager 
wants 
to 
model 
next 
day 
`returns’ 
yt+1 
based 
on 
current 
day 
information 
xt. 
The 
vector 
of 
`factors’ 
each 
day, 
xt 
captures 
essential 
aspects 
of 
the 
market. 
He 
argues 
that 
a 
simple 
linear 
model 


K

X

yt+1 
= 
wkxkt 
(12.6.1) 


k=1 


should 
be 
reasonable 
and 
asks 
you 
to 
nd 
the 
weight 
vector 
w, 
based 
on 
historical 
information 
D 
= 
f(xt;yt+1);t 
=1;:::;T 
- 
1g. 
In 
addition 
he 
also 
gives 
you 
a 
measure 
of 
the 
`volatility’ 
2 
for 
each 
day. 


t 


1. 
Under 
the 
assumption 
that 
the 
returns 
are 
i.i.d. 
Gaussian 
distributed 
TT

YY

T

p(y1:T 
jx1:T 
, 
w)= 
p(ytjxt..1, 
w)= 
Nyt 


wxt..1;2 
(12.6.2)

t

t=2 
t=2 


explain 
how 
to 
set 
the 
weight 
vector 
w 
by 
Maximum 
Likelihood. 


DRAFT 
March 
9, 
2010 



Exercises 


2. 
Your 
hedge 
fund 
manager 
is 
however 
convinced 
that 
some 
of 
the 
factors 
are 
useless 
for 
prediction 
and 
wishes 
to 
remove 
as 
many 
as 
possible. 
To 
do 
this 
you 
decide 
to 
use 
a 
Bayesian 
model 
selection 
method 
in 
which 
you 
use 
a 
prior 
p(wjM)= 
N 
(w 


0, 
I) 
(12.6.3) 


where 
M 
=1;:::, 
2K 
- 
1 
indexes 
the 
model. 
Each 
model 
uses 
only 
a 
subset 
of 
the 
factors. 
By 
translating 
the 
integer 
M 
into 
a 
binary 
vector 
representation, 
the 
model 
describes 
which 
factors 
are 
to 
be 
used. 
For 
example 
if 
K 
= 
3, 
there 
would 
be 
7 
models 


f0, 
0, 
1} 
, 
f0, 
1, 
0} 
, 
f1, 
0, 
0} 
, 
f0, 
1, 
1} 
, 
f1, 
0, 
1} 
, 
f1, 
1, 
0} 
, 
f1, 
1, 
1} 
(12.6.4) 


where 
the 
rst 
model 
is 
yt 
= 
w3x3 
with 
weight 
prior 
p(w3)= 
N 
(w3 


0, 
1). 
Similarly 
model 
7 
would 
be 
yt 
= 
w1x1 
+ 
w2x2 
+ 
w3x3 
with 
p(w1;w2;w3)= 
N 
((w1;w2;w3) 


(0, 
0, 
0), 
I3). 
You 
decide 
to 
use 
a 
at 
prior 
p(M)= 
const. 
Draw 
the 
hierarchical 
Bayesian 
network 
for 
this 
model 
and 
explain 
how 
to 
nd 
the 
best 
model 
for 
the 
data 
using 
Bayesian 
model 
selection 
by 
suitably 
adapting 
equation 
(12.4.6). 


3. 
Using 
the 
data 
dodder.mat, 
perform 
Bayesian 
model 
selection 
as 
above 
for 
K 
= 
6 
and 
nd 
which 
of 
the 
factors 
x1;:::;x6 
are 
most 
likely 
to 
explain 
the 
data. 
Exercise 
149. 
Here 
we 
will 
derive 
the 
expression 
(12.4.6) 
and 
also 
an 
alternative 
form. 


1. 
Starting 
from 
N

YY

n 


p(w) 
p(y 
njw;x 
n;K)= 
N 
(w 


0, 
I=) 
Ny 


wT(x 
n);2(12.6.5) 
n=1 
n 


P

1 
- 
a 
wTw 
1 
- 
2
1
2(yn..wT(xn))2 


n

= 
v 
e 
2 
e 
(12.6.6)
2..1 
(22)N=2 


Show 
that 
this 
can 
be 
expressed 
as 


P

11 
- 
1 
(yn)2 
- 
1 
wTAw+bTw

n

v 
e 
22e 
2 
(12.6.7)
2..1 
(22)N=2 


where 


XX

11

A 
= 
I 
+ 
2(x 
n)T(x 
n) 
b 
= 
2y 
n(x 
n) 
(12.6.8) 
nn 


2. 
By 
completing 
the 
square 
(see 
section(8.6.2)), 
derive 
(12.4.6). 
n

3. 
Since 
each 
y, 
n 
=1;:::;N 
is 
linearly 
related 
through 
w 
and 
w 
is 
Gaussian 
distributed, 
the 
joint 
1N

vector 
y;:::;yis 
Gaussian 
distributed. 
Using 
the 
Gaussian 
propagation 
results, 
section(8.6.3), 


11

derive 
an 
alternative 
expression 
for 
log 
p(y;:::;yN 
jx;:::;xN 
). 


DRAFT 
March 
9, 
2010 



Part 
III 
Machine 
Learning 


249 



CHAPTER 
13 


Machine 
Learning 
Concepts 


13.1 
Styles 
of 
Learning 
Broadly 
speaking 
the 
main 
two 
subelds 
of 
machine 
learning 
are 
supervised 
learning 
and 
unsupervised 
learning. 
In 
supervised 
learning 
the 
focus 
is 
on 
accurate 
prediction, 
whereas 
in 
unsupervised 
learning 
the 
aim 
is 
to 
nd 
accurate 
compact 
descriptions 
of 
the 
data. 


Particularly 
in 
supervised 
learning, 
one 
is 
interested 
in 
methods 
that 
perform 
well 
on 
previously 
unseen 
data. 
That 
is, 
the 
method 
`generalises’ 
to 
unseen 
data. 
In 
this 
sense, 
one 
distinguishes 
between 
data 
that 
is 
used 
to 
train 
a 
model, 
and 
data 
that 
is 
used 
to 
test 
the 
performance 
of 
the 
trained 
model, 
see 
g(13.1). 


13.1.1 
Supervised 
learning 
Consider 
a 
database 
of 
face 
images, 
each 
represented 
by 
a 
vector1 
x. 
Along 
with 
each 
image 
x 
is 
an 
output 
class 
y 
2fmale, 
female} 
that 
states 
if 
the 
image 
is 
of 
a 
male 
or 
female. 
A 
database 
of 
10000 
such 
image-class 
pairs 
is 
available, 
D 
= 
f(xn;yn) 
;n 
=1;:::, 
10000g. 
The 
task 
is 
to 
make 
an 
accurate 
predictor 
y(x 
) 
of 
the 
sex 
of 
a 
novel 
image 
x 
* 
. 
This 
is 
an 
example 
application 
that 
would 
be 
hard 
to 
program 
in 
a 
traditional 
`programming’ 
manner 
since 
formally 
specifying 
how 
male 
faces 
dier 
from 
female 
faces 
is 
dicult. 
An 
alternative 
is 
to 
give 
examples 
faces 
and 
their 
gender 
labels 
and 
let 
a 
machine 
automatically 
`learn’ 
a 
rule 
to 
dierentiate 
male 
from 
female 
faces. 


n

Denition 
88 
(Supervised 
Learning). 
Given 
a 
set 
of 
data 
D 
= 
f(x;yn) 
;n 
=1;:::;N} 
the 
task 
is 
to 




`learn’ 
the 
relationship 
between 
the 
input 
x 
and 
output 
y 
such 
that, 
when 
given 
a 
new 
input 
x 
the 




predicted 
output 
y 
is 
accurate. 
To 
specify 
explicitly 
what 
accuracy 
means 
one 
denes 
a 
loss 
function 
L(ypred;ytrue) 
or, 
conversely, 
a 
utility 
function 
U 
= 
..L. 


In 
supervised 
learning 
our 
interest 
is 
describing 
y 
conditioned 
on 
knowing 
x. 
From 
a 
probabilistic 
modelling 
perspective, 
we 
are 
therefore 
concerned 
primarily 
with 
the 
conditional 
distribution 
p(yjx, 
D). 
The 
term 
`supervised’ 
indicates 
that 
there 
is 
a 
`supervisor’ 
specifying 
the 
output 
y 
for 
each 
input 
x 
in 
the 
available 
data 
D. 
The 
output 
is 
also 
called 
a 
`label', 
particularly 
when 
discussing 
classication. 


Predicting 
tomorrow's 
stock 
price 
y(T 
+1) 
based 
on 
past 
observations 
y(1);:::;y(T 
) 
is 
a 
form 
of 
supervised 
learning. 
We 
have 
a 
collection 
of 
times 
and 
prices 
D 
= 
f(t, 
y(t)) 
;t 
=1;:::;T 
} 
where 
time 
t 
is 
the 
`input’ 
and 
the 
price 
y(t) 
is 
the 
output. 


1For 
an 
m 
× 
n 
face 
image 
with 
elements 
Fmn 
we 
can 
form 
a 
vector 
by 
stacking 
the 
entries 
of 
the 
matrix. 
In 
MATLAB 
one 
may 
achieve 
this 
using 
x=F(:). 


251 



Styles 
of 
Learning 


Train 


Test 


Figure 
13.1: 
In 
training 
and 
evaluating 
a 
model, 
conceptually 
there 
are 
two 
sources 
of 
data. 
The 
parameters 
of 
the 
model 
are 
set 
on 
the 
basis 
of 
the 
train 
data 
only. 
If 
the 
test 
data 
is 
generated 
from 
the 
same 
underlying 
process 
that 
generated 
the 
train 
data, 
an 
unbiased 
estimate 
of 
the 
generalisation 
performance 
can 
be 
obtained 
by 
measuring 
the 
test 
data 
performance 
of 
the 
trained 
model. 
Importantly, 
the 
test 
performance 
should 
not 


be 
used 
to 
adjust 
the 
model 
parameters 
since 
we 
would 
then 
no 
longer 
have 
an 
independent 
measure 
of 
the 
performance 
of 
the 
model. 


Example 
57. 
A 
father 
decides 
to 
teach 
his 
young 
son 
what 
a 
sports 
car 
is. 
Finding 
it 
dicult 
to 
explain 
in 
words, 
he 
decides 
to 
give 
some 
examples. 
They 
stand 
on 
a 
motorway 
bridge 
and, 
as 
each 
car 
passes 
underneath, 
the 
father 
cries 
out 
`that's 
a 
sports 
car!’ 
when 
a 
sports 
car 
passes 
by. 
After 
ten 
minutes, 
the 
father 
asks 
his 
son 
if 
he's 
understood 
what 
a 
sports 
car 
is. 
The 
son 
says, 
`sure, 
it's 
easy'. 
An 
old 
red 
VW 
Beetle 
passes 
by, 
and 
the 
son 
shouts 
– 
`that's 
a 
sports 
car!'. 
Dejected, 
the 
father 
asks 
– 
`why 
do 
you 
say 
that?'. 
`Because 
all 
sports 
cars 
are 
red!', 
replies 
the 
son. 


This 
is 
an 
example 
scenario 
for 
supervised 
learning. 
Here 
the 
father 
plays 
the 
role 
of 
the 
supervisor, 
and 
his 
son 
is 
the 
`student’ 
(or 
`learner'). 
It's 
indicative 
of 
the 
kinds 
of 
problems 
encountered 
in 
machine 
learning 
in 
that 
it 
is 
not 
really 
clear 
anyway 
what 
a 
sports 
car 
is 
– 
if 
we 
knew 
that, 
then 
we 
wouldn't 
need 
to 
go 
through 
the 
process 
of 
learning. 
This 
example 
also 
highlights 
the 
issue 
that 
there 
is 
a 
dierence 
between 
performing 
well 
on 
training 
data 
and 
performing 
well 
on 
novel 
test 
data. 
The 
main 
interest 
in 
supervised 
learning 
is 
to 
discover 
an 
underlying 
rule 
that 
will 
generalise 
well, 
leading 
to 
accurate 
prediction 
on 
new 
inputs. 


For 
an 
input 
x, 
if 
the 
output 
is 
one 
of 
a 
discrete 
number 
of 
possible 
`classes', 
this 
is 
called 
a 
classication 
problem. 
In 
classication 
problems 
we 
will 
generally 
use 
c 
for 
the 
output. 


For 
an 
input 
x, 
if 
the 
output 
is 
continuous, 
this 
is 
called 
a 
regression 
problem. 
For 
example, 
based 
on 
historical 
information 
of 
demand 
for 
sun-cream 
in 
your 
supermarket, 
you 
are 
asked 
to 
predict 
the 
demand 
for 
the 
next 
month. 
In 
some 
cases 
it 
is 
possible 
to 
discretise 
a 
continuous 
output 
and 
then 
consider 
a 
corresponding 
classication 
problem. 
However, 
in 
other 
cases 
it 
is 
impractical 
or 
unnatural 
to 
do 
this; 
for 
example 
if 
the 
output 
y 
is 
a 
high 
dimensional 
continuous 
valued 
vector, 
or 
if 
the 
ordering 
of 
states 
of 
the 
variable 
is 
meaningful. 


13.1.2 
Unsupervised 
learning 
n

Denition 
89 
(Unsupervised 
learning). 
Given 
a 
set 
of 
data 
D 
= 
fx;n 
=1;:::;N} 
in 
unsupervised 
learning 
we 
aim 
to 
to 
`learn’ 
a 
plausible 
compact 
description 
of 
the 
data. 
An 
objective 
is 
used 
to 
quantify 
the 
accuracy 
of 
the 
description. 


In 
unsupervised 
learning 
there 
is 
no 
special 
`prediction’ 
variable. 
From 
a 
probabilistic 
perspective 
we 
are 
interested 
in 
modelling 
the 
distribution 
p(x). 
The 
likelihood 
of 
the 
data 
under 
the 
i.i.d. 
assumption, 
for 
example, 
would 
be 
one 
objective 
measure 
of 
the 
accuracy 
of 
the 
description. 


DRAFT 
March 
9, 
2010 



Styles 
of 
Learning 


Example 
58. 
A 
supermarket 
chain 
wishes 
to 
discover 
how 
many 
dierent 
basic 
consumer 
buying 
behaviours 
there 
are 
based 
on 
a 
large 
database 
of 
supermarket 
checkout 
data. 
Items 
brought 
by 
a 
customer 
on 
a 
visit 
to 
a 
checkout 
are 
represented 
by 
a 
(very 
sparse) 
10,000 
dimensional 
vector 
x 
which 
contains 
a 
1 
in 
the 
ith 
element 
if 
the 
customer 
bought 
product 
i 
and 
0 
otherwise. 
Based 
on 
10 
million 


i 


xn

such 
checkout 
vectors 
from 
stores 
across 
the 
country, 
D 
= 
;n 
=1;:::, 
107 


the 
supermarket 
chain 


wishes 
to 
discover 
patterns 
of 
buying 
behaviour. 


In 
the 
table 
each 
column 
represents 
the 
buying 
patterns 
of 
a 
customer 
(7 
customer 
records 
and 
just 
the 
rst 
6 
of 
the 
10,000 
products 
are 
shown). 
A 
1 
indicates 
that 
the 
customer 
bought 
that 
item. 
We 
wish 
to 
nd 
common 
patterns 
in 
the 
data, 
such 
as 
if 
someone 
buys 
coee 
they 
are 
also 
likely 
to 
buy 
milk. 


Example 
59 
(Clustering). 


The 
table 
on 
the 
right 
represents 
a 
collection 
of 
unlabelled 
two-dimensional 
points. 
We 
can 
visualise 
this 
data 
by 
plotting 
it 
in 
2 
dimensions. 


x1-2-6-111-14633423245
x272211-85240335439
By 
simply 
eye-balling 
the 
data, 
we 
can 
see 
that 
there 
are 
two 
apparent 
clusters 
here, 
one 
centred 
around 
(0,5) 
and 
the 
other 
around 
(35,45). 
A 
reasonable 
model 
to 
describe 
this 
data 
might 
therefore 
be 
to 
describe 
it 
as 
two 
clusters, 
centred 
at 
(0,0) 
and 
(35,35), 
each 
with 
a 
standard 
deviation 
of 
around 
10. 


coee 
tea 
milk 
beer 
diapers 
aspirin 


1 
0 
1 
0 
0 
0 


0 
0 
0 
0 
0 
1 


-100102030405001020304050
0 
1 
1 
0 
1 
0 


1 
0 
1 
1 
0 
0 


0 
0 
0 
1 
1 
1 


0 
0 
1 
0 
0 
0 


0 
0 
1 
1 
1 
1 


· 
· 
· 
· 
· 
· 


13.1.3 
Anomaly 
detection 
A 
baby 
processes 
a 
mass 
of 
initially 
confusing 
sensory 
data. 
After 
a 
while 
the 
baby 
begins 
to 
understand 
her 
environment 
in 
the 
sense 
that 
novel 
sensory 
data 
from 
the 
same 
environment 
is 
familiar 
or 
expected. 
When 
a 
strange 
face 
presents 
itself, 
the 
baby 
recognises 
that 
this 
is 
not 
familiar 
and 
may 
be 
upset. 
The 
baby 
has 
learned 
a 
representation 
of 
the 
familiar 
and 
can 
distinguish 
the 
expected 
from 
the 
unexpected; 
this 
is 
an 
example 
of 
unsupervised 
learning. 
Models 
that 
can 
detect 
irregular 
events 
are 
used 
in 
plant 
monitoring 
and 
require 
a 
model 
of 
normality 
which 
will 
in 
most 
cases 
be 
based 
on 
unlabelled 
data. 


13.1.4 
Online 
(sequential) 
learning 
In 
the 
above 
situations, 
we 
assumed 
that 
the 
data 
D 
was 
given 
beforehand. 
In 
online 
learning 
data 
arrives 
sequentially 
and 
we 
want 
to 
continually 
update 
our 
model 
as 
new 
data 
becomes 
available. 
Online 
learning 
may 
occur 
in 
either 
a 
supervised 
or 
unsupervised 
context. 


13.1.5 
Interacting 
with 
the 
environment 
In 
many 
real-world 
situations, 
an 
agent 
is 
able 
to 
interact 
in 
some 
manner 
with 
its 
environment. 


Query 
(Active) 
Learning 
Here 
the 
agent 
has 
the 
ability 
to 
request 
data 
from 
the 
environment. 
For 
example, 
a 
predictor 
might 
recognise 
that 
it 
is 
less 
condently 
able 
to 
predict 
in 
certain 
regions 
of 
the 
space 
x 
and 
therefore 
requests 
more 
training 
data 
in 
this 
region. 
Active 
Learning 
can 
also 
be 
considered 
in 
an 
unsupervised 
context 
in 
which 
the 
agent 
might 
request 
information 
in 
regions 
where 
p(x) 
looks 
uninformative 
or 
`at'. 


DRAFT 
March 
9, 
2010 



Supervised 
Learning 


Reinforcement 
Learning 
One 
might 
term 
this 
also 
`survival 
learning'. 
One 
has 
in 
mind 
scenarios 
such 
as 
encountered 
in 
real-life 
where 
an 
organism 
needs 
to 
learn 
the 
best 
actions 
to 
take 
in 
its 
environment 
in 
order 
to 
survive 
as 
long 
as 
possible. 
In 
each 
situation 
in 
which 
the 
agent 
nds 
itself 
it 
needs 
to 
take 
an 
action. 
Some 
actions 
may 
eventually 
be 
benecial 
(lead 
to 
food 
for 
example), 
whilst 
others 
may 
be 
disastrous 
(lead 
to 
being 
eaten 
for 
example). 
Based 
on 
accumulated 
experience, 
the 
agent 
needs 
to 
learn 
which 
action 
to 
take 
in 
a 
given 
situation 
in 
order 
to 
obtain 
a 
desired 
long 
term 
goal. 
Essentially 
actions 
that 
lead 
to 
long 
term 
rewards 
need 
to 
reinforced. 
Reinforcement 
learning 
has 
connections 
with 
control 
theory, 
Markov 
decision 
processes 
and 
game 
theory. 
Whilst 
we 
discussed 
MDPs 
and 
briey 
mentioned 
how 
an 
environment 
can 
be 
learned 
based 
on 
delayed 
rewards 
in 
section(7.8.3), 
we 
will 
not 
discuss 
this 
topic 
further 
in 
this 
book. 


13.1.6 
Semi-supervised 
learning 
In 
machine 
learning, 
a 
common 
scenario 
is 
to 
have 
a 
small 
amount 
of 
labelled 
and 
a 
large 
amount 
of 
unlabelled 
data. 
For 
example, 
it 
may 
be 
that 
we 
have 
access 
to 
many 
images 
of 
faces; 
however, 
only 
a 
small 
number 
of 
them 
may 
have 
been 
labelled 
as 
instances 
of 
known 
faces. 
In 
semi-supervised 
learning, 
one 
tries 
to 
use 
the 
unlabelled 
data 
to 
make 
a 
better 
classier 
than 
that 
based 
on 
the 
labelled 
data 
alone. 


13.2 
Supervised 
Learning 
Supervised 
and 
unsupervised 
learning 
are 
mature 
elds 
with 
a 
wide 
range 
of 
practical 
tools 
and 
associated 
theoretical 
analyses. 
Our 
aim 
here 
is 
to 
give 
a 
brief 
introduction 
to 
the 
issues 
and 
`philosophies’ 
behind 
the 
approaches. 
We 
focus 
here 
mainly 
on 
supervised 
learning 
and 
classication 
in 
particular. 


13.2.1 
Utility 
and 
Loss 
To 
more 
fully 
specify 
a 
supervised 
problem 
we 
need 
to 
be 
clear 
what 
`cost’ 
is 
involved 
in 
making 
a 
correct 
or 
incorrect 
prediction. 
In 
a 
two 
class 
problem 
dom(c)= 
f1, 
2g, 
we 
assume 
here 
that 
everything 
we 
know 
about 
the 
environment 
is 
contained 
in 
a 
model 
p(x, 
c). 
Given 
a 
new 
input 
x 
, 
the 
optimal 
prediction 
also 
depends 
on 
how 
costly 
making 
an 
error 
is. 
This 
can 
be 
quantied 
using 
a 
loss 
function 
(or 
conversely 
a 
utility). 
In 
forming 
a 
decision 
function 
c(x 
) 
that 
will 
produce 
a 
class 
label 
for 
the 
new 
input 
x 
, 
we 
don't 
know 
the 
true 
class, 
only 
our 
presumed 
distribution 
p(cjx 
). 
The 
expected 
utility 
for 
the 
decision 
function 
is 


X

U(c(x 
)) 
=U(c 
true 
;c(x 
))p(c 
truejx 
) 
(13.2.1) 
ctrue 


and 
the 
optimal 
decision 
is 
that 
which 
maximises 
the 
expected 
utility. 


Zero-one 
loss 


A 
`count 
the 
correct 
predictions’ 
measure 
of 
prediction 
performance 
is 
based 
on 
the 
`zero-one’ 
utility 
(or 
conversely 
the 
zero-one 
loss): 




* 
true 
1 
if 
c 
= 
c

true 


U(c 
;c 
) 
=(13.2.2)

true 


0 
if 
c 
6

= 
c

For 
the 
two 
class 
case, 
we 
then 
have 




true 


p(c= 
1jx 
) 
for 
c(x 
)= 
1 


U(c(x 
)) 
=(13.2.3)

true 


p(c= 
2jx 
) 
for 
c(x 
)= 
2 


Hence, 
in 
order 
to 
have 
the 
highest 
expected 
utility, 
the 
decision 
function 
c(x 
) 
should 
correspond 
to 
selecting 
the 
highest 
class 
probability 
p(cjx 
): 




1 
if 
p(c 
= 
1jx 
) 
60:5 


c(x 
) 
=(13.2.4)

2 
if 
p(c 
= 
2jx 
) 
60:5 


In 
the 
case 
of 
a 
tie, 
either 
class 
is 
selected 
at 
random 
with 
equal 
probability. 


254 
DRAFT 
March 
9, 
2010 



Supervised 
Learning 


General 
loss 
functions 


In 
general, 
for 
a 
two-class 
problem, 
we 
have 




true 
* 
true 
true 
* 
true 


U(c= 
1;c 
= 
1)p(c= 
1jx 
)+ 
U(c= 
2;c 
= 
1)p(c= 
2jx 
) 
for 
c(x 
)= 
1 


U(c(x 
)) 
=

true 
* 
true 
true 
* 
true 


U(c= 
1;c 
= 
2)p(c= 
1jx 
)+ 
U(c= 
2;c 
= 
2)p(c= 
2jx 
) 
for 
c(x 
)= 
2 


(13.2.5) 
and 
the 
optimal 
decision 
function 
c(x 
) 
chooses 
that 
class 
with 
highest 
expected 
utility. 


One 
can 
readily 
generalise 
this 
to 
multiple-class 
situations 
using 
a 
utility 
matrix 
with 
elements 


true 


Ui;j 
= 
U(c 
= 
i, 
cpred 
= 
j) 
(13.2.6) 


where 
the 
i, 
j 
element 
of 
the 
matrix 
contains 
the 
utility 
of 
predicting 
class 
j 
when 
the 
true 
class 
is 
i. 
Conversely 
one 
could 
think 
of 
a 
loss-matrix 
with 
entries 
Lij 
= 
..Uij. 
The 
expected 
loss 
with 
respect 
to 
p(cjx) 
is 
then 
termed 
the 
risk. 


In 
some 
applications 
the 
utility 
matrix 
is 
highly 
non-symmetric. 
Consider 
a 
medical 
scenario 
in 
which 
we 
are 
asked 
to 
predict 
whether 
or 
not 
the 
patient 
has 
cancer 
dom(c)= 
fcancer, 
benigng. 
If 
the 
true 
class 
is 
cancer 
yet 
we 
predict 
benign, 
this 
could 
have 
terrible 
consequences 
for 
the 
patient. 
On 
the 
other 
hand, 
if 
the 
class 
is 
benign 
yet 
we 
predict 
cancer, 
this 
may 
be 
less 
disastrous 
for 
the 
patient. 
Such 
asymmetric 
utilities 
can 
bias 
the 
predictions 
in 
favour 
of 
conservative 
decisions 
– 
in 
the 
cancer 
case, 
we 
would 
be 
more 
inclined 
to 
decide 
the 
sample 
is 
cancerous 
than 
benign, 
even 
if 
the 
predictive 
probability 
of 
the 
two 
classes 
is 
equal. 


13.2.2 
What's 
the 
catch? 
In 
solving 
for 
the 
optimal 
decision 
function 
c(x 
) 
above 
we 
are 
assuming 
that 
the 
model 
p(cjx) 
is 
`correct'. 
The 
catch 
is 
therefore 
that 
in 
practice 
: 


• 
We 
typically 
don't 
know 
the 
correct 
model 
underlying 
the 
data 
– 
all 
we 
have 
is 
a 
dataset 
of 
examples 
n

D 
= 
f(x;yn) 
;n 
=1;:::;N} 
and 
our 
domain 
knowledge. 


• 
We 
want 
our 
method 
to 
perform 
well 
not 
just 
on 
a 
specically 
chosen 
x 
, 
but 
any 
new 
input 
that 
could 
come 
along 
– 
that 
is 
we 
want 
it 
to 
generalise 
to 
novel 
inputs. 
This 
means 
we 
also 
need 
a 
model 
for 
p(x) 
in 
order 
to 
measure 
what 
the 
expected 
performance 
of 
our 
decision 
function 
would 
be. 
Hence 
we 
require 
knowledge 
of 
the 
joint 
distribution 
p(c, 
x)= 
p(cjx)p(x). 
We 
therefore 
need 
to 
form 
a 
distribution 
p(x, 
cjD) 
which 
should 
ideally 
be 
close 
to 
the 
true 
but 
unknown 
joint 
data 
distribution. 
Communities 
of 
researchers 
in 
machine 
learning 
form 
around 
dierent 
strategies 
to 
address 
the 
lack 
of 
knowledge 
about 
the 
true 
p(c, 
x). 


13.2.3 
Using 
the 
empirical 
distribution 
A 
direct 
approach 
to 
not 
knowing 
the 
correct 
model 
ptrue(c, 
x) 
is 
to 
replace 
it 
with 
the 
empirical 
distribution 


X

1 
N

p(x, 
cjD)= 
d 
(x, 
x 
n) 
d 
(c, 
c 
n) 
(13.2.7)

N 


n=1 


That 
is, 
we 
assume 
that 
the 
underlying 
distribution 
is 
approximated 
by 
placing 
equal 
mass 
on 
each 
of 
the 


n

points 
(x;cn) 
in 
the 
dataset. 
Using 
this 
gives 
the 
empirical 
utility 


X

1 


n

hU(c, 
c(x))i= 
U(c 
;c(x 
n)) 
(13.2.8)

p(c;xjD) 


N 


n 


or 
conversely 
the 
empirical 
risk 


R 
= 
1 
N 
XL(c 
n 
, 
c(x 
n)) 
(13.2.9) 
n 
DRAFT 
March 
9, 
2010 
255 



Supervised 
Learning 


Train 
Validate 


Figure 
13.2: 
Models 
can 
be 
trained 
using 
the 
train 
data 
based 
on 


dierent 
regularisation 
parameters. 
The 
optimal 
regularisation 


parameter 
is 
determined 
by 
the 
empirical 
performance 
on 
the 


Test 
validation 
data. 
An 
independent 
measure 
of 
the 
generalisation 


performance 
is 
obtained 
by 
using 
a 
separate 
test 
set. 


Assuming 
the 
loss 
is 
minimal 
when 
the 
correct 
class 
is 
predicted, 
the 
optimal 
decision 
c(x) 
for 
any 
input 


n 


in 
the 
train 
set 
is 
trivially 
given 
by 
c(xn)= 
c. 
However, 
for 
any 
new 
x 
not 
contained 
in 
D 
then 
c(x 
) 
is 
undened. 
In 
order 
to 
dene 
the 
class 
of 
a 
novel 
input, 
one 
may 
use 
a 
parametric 
function 


c(x)= 
f(xj) 
(13.2.10) 


For 
example 
for 
a 
two 
class 
problem 
dom(c)= 
f1, 
2g, 
a 
linear 
decision 
function 
is 
given 
by 




1 
if 
Tx 
+ 
0 
= 
0 


f(xj) 
=(13.2.11)

2 
if 
Tx 
+ 
0 
< 
0 


If 
the 
vector 
input 
x 
is 
on 
the 
positive 
side 
of 
a 
hyperplane 
dened 
by 
the 
vector 
. 
and 
bias 
0, 
we 
assign 
it 
to 
class 
1, 
otherwise 
to 
class 
2. 
(We 
return 
to 
the 
geometric 
interpretation 
of 
this 
in 
chapter(17)). 
The 
empirical 
risk 
then 
becomes 
a 
function 
of 
the 
parameters 
. 
= 
f;0g, 


X

1 


R(jD)= 
L(c 
n;f(x 
nj)) 
(13.2.12)

N

n 


The 
optimal 
parameters 
. 
are 
given 
by 
minimising 
the 
empirical 
risk 
with 
respect 
to 
, 


opt 
= 
argmin 
R(jD) 
(13.2.13) 


. 




The 
decision 
for 
a 
new 
datapoint 
x 
is 
then 
given 
by 
f(x 
jopt). 


In 
this 
empirical 
risk 
minimisation 
approach, 
as 
we 
make 
the 
decision 
function 
f(xj) 
more 
complex, 
the 
empirical 
risk 
goes 
down. 
If 
we 
make 
f(xj) 
too 
complex 
we 
will 
have 
no 
condence 
f(xj) 
will 
perform 




well 
on 
a 
novel 
input 
x 
. 
To 
constrain 
the 
complexity 
of 
f(xj) 
we 
may 
minimise 
the 
penalised 
empirical 
risk 


R0(jD)= 
R(jD)+ 
P 
() 
(13.2.14) 


For 
the 
linear 
decision 
function 
above, 
it 
is 
reasonable 
to 
penalise 
wildly 
changing 
classications 
in 
the 
sense 
that 
if 
we 
change 
the 
input 
x 
by 
only 
a 
small 
amount 
we 
expect 
(on 
average) 
minimal 
change 
in 
the 


..2

class 
label. 
The 
squared 
dierence 
in 
Tx 
+ 
0 
for 
two 
inputs 
x1 
and 
x2 
isTx 
where 
x 
= 
x2 
- 
x1. 
By 
constraining 
the 
length 
of 
. 
to 
be 
small 
we 
would 
then 
limit 
the 
ability 
of 
the 
classier 
to 
change 
class 
for 
only 
a 
small 
change 
in 
input 
space2 
. 
This 
motivates 
a 
penalised 
risk 
of 
the 
form 


R0(, 
0jD)= 
R(, 
0jD)+ 
T. 
(13.2.15) 


where 
. 
is 
a 
regularising 
constant,. 
We 
subsequently 
minimise 
this 
penalised 
empirical 
risk 
with 
respect 
to 
, 
0. 
We 
discuss 
how 
to 
nd 
an 
appropriate 
setting 
for 
the 
regularisation 
constant 
. 
below. 


Validation 


In 
penalised 
empirical 
risk 
minimisation 
we 
need 
to 
set 
the 
regularisation 
parameter 
. 
This 
can 
be 
achieved 
by 
evaluating 
the 
performance 
of 
the 
learned 
classier 
f(xj) 
on 
validation 
data 
Dvalidate 
for 
several 
dierent 
. 
values, 
and 
choosing 
the 
one 
with 
the 
best 
performance. 
It's 
important 
that 
the 
validation 
data 
is 
not 
the 
data 
on 
which 
the 
model 
was 
trained 
since 
we 
know 
that 
the 
optimal 
setting 
for 
. 
in 
that 
case 
is 
zero, 
and 
again 
we 
will 
have 
no 
condence 
in 
the 
generalisation 
ability. 


2Assuming 
the 
distance 
between 
two 
datapoints 
is 
distributed 
according 
to 
an 
isotropic 
multivariate 
Gaussian 
with 
zero 


D..2 
E

mean 
and 
covariance 
2I, 
the 
average 
squared 
change 
is 
Tx= 
2T, 
motivating 
the 
choice 
of 
the 
Euclidean 
squared 
length 
of 
the 
parameter 
. 
as 
the 
penalty 
term. 


256 
DRAFT 
March 
9, 
2010 



Supervised 
Learning 


Algorithm 
12 
Setting 
regularisation 
parameters 
using 
cross-validation. 


1: 
Choose 
a 
set 
of 
regularisation 
parameters 
1;:::;A
, 
	

2: 
Choose 
a 
set 
of 
training 
and 
validation 
set 
splits 
Di 
;i 
=1;:::;I 
train, 
Di 


validate

3: 
for 
a 
=1 
to 
A 
do 
4: 
for 
i 
=1 
to 
I 
do 


i

5: 
= 
argmin 
R(jDi 
)+ 
aP 
()
a 
train



6: 
end 
for 
7: 
L(a)= 
1 
PI 
R(i 
jDi 
)
Ii=1 
avalidate

8: 
end 
for 
9: 
opt 
= 
argmin 
L(a) 
a 
Given 
an 
original 
dataset 
D 
we 
split 
this 
into 
disjoint 
parts, 
Dtrain, 
Dvalidate, 
where 
the 
size 
of 
the 
validation 
set 
is 
usually 
chosen 
to 
be 
smaller 
than 
the 
train 
set. 
For 
each 
parameter 
a 
one 
then 
nds 
the 
minimal 
empirical 
risk 
parameter 
a. 
This 
splitting 
procedure 
is 
repeated, 
each 
time 
producing 
a 
separate 
training 
Di 
and 
validation 
Di 
set, 
along 
with 
an 
optimal 
penalised 
empirical 
risk 
parameter 
i 


train 
validation 
a 


and 
associated 
(unregularised) 
validation 
performance 
R(i 
jDi 
). 
The 
performance 
of 
regularisation 


avalidate

parameter 
a 
is 
taken 
as 
the 
average 
of 
the 
validation 
performances 
over 
i. 
The 
best 
regularisation 
parameter 
is 
then 
given 
as 
that 
with 
the 
minimal 
average 
validation 
error, 
see 
algorithm(12) 
and 
g(13.2). 
Using 
the 
optimal 
regularisation 
parameter 
, 
many 
practitioners 
retrain 
. 
on 
the 
basis 
of 
the 
whole 
dataset 
D. 


In 
cross-validation 
a 
dataset 
is 
partitioned 
into 
training 
and 
validation 
sets 
multiple 
times 
with 
validation 
results 
obtained 
for 
each 
partition. 
More 
specically, 
in 
K-fold 
cross 
validation 
the 
data 
D 
is 
split 
into 
K 
equal 
sized 
disjoint 
parts 
D1;:::, 
DK 
. 
Then 
Di 
= 
Di 
and 
Di 
= 
DnDi 
This 
gives 
a 
total 
of 


validate 
train 
validate. 


K 
dierent 
training-validation 
sets 
over 
which 
performance 
is 
averaged, 
see 
g(13.3). 
In 
practice 
10-fold 
cross 
validation 
is 
popular, 
as 
is 
leave-one-out 
cross 
validation 
in 
which 
the 
validation 
sets 
consist 
of 
only 
a 
single 
example. 


Benets 
of 
the 
empirical 
risk 
approach 


true

For 
a 
utility 
U(c;cpred) 
and 
penalty 
P 
(), 
the 
empirical 
risk 
approach 
is 
summarised 
in 
g(13.4). 


• 
In 
the 
limit 
of 
a 
large 
amount 
of 
training 
data 
the 
empirical 
distribution 
will 
tend 
towards 
the 
correct 
distribution. 
• 
The 
discriminant 
function 
is 
chosen 
on 
the 
basis 
of 
minimal 
risk, 
which 
is 
the 
quantity 
we 
are 
ultimately 
interested 
in. 
• 
The 
procedure 
is 
conceptually 
straightforward. 
Train 
Validate 


Train 
Validate 
Train 


Figure 
13.3: 
In 
cross-validation 
the 
original 
dataset 
is 
split 
into 
several 
train-validation 
sets. 
Depicted 
is 
3-fold 
cross-validation. 
For 
a 
range 
of 
regularisation 
parameters, 
the 
optimal 
regularisation 
parameter 
is 
found 
based 
on 
the 
empirical 
validation 
performance 
averaged 
across 
the 
dierent 
splits. 


Validate 
Train 


Test 


DRAFT 
March 
9, 
2010 



Supervised 
Learning 


X 
, 
C 
x 
* 
Figure 
13.4: 
Empirical 
risk 
approach. 
Given 
the 
dataset 
X 
, 
C, 
a 
model 
of 
the 
data 
p(x, 
c) 
is 
made, 
usually 
using 
the 
empirical 
distribution. 
For 
a 
classier 
f(xj), 
the 
parameter 
. 
is 
learned 
by 
maximising 
the 
penalised 
empirical 
utility 
(or 
minimising 
empirical 
risk) 
with 
respect 
to 
. 
The 
penalty 
parameter 
. 
is 
set 
by 




validation. 
A 
novel 
input 
x 
is 
then 
assigned 
to 
class 
f(x 
j), 
given 
this 
optimal 
. 


p(x;c)
U(c;f(xj))..P()
c=f(xj)
-3-2-10123-1.5-1-0.500.511.5
-3-2-10123-1.5-1-0.500.511.5
(a) 
(b) 
Figure 
13.5: 
(a): 
The 
unregularised 
t 
(. 
= 
0) 
to 
training 
given 
by 
. 
Whilst 
the 
training 
data 
is 
well 
tted, 
the 
error 
on 
the 
validation 
examples, 
+ 
is 
high. 
(b): 
The 
regularised 
t 
(. 
=0:5). 
Whilst 
the 
train 
error 
is 
high, 
the 
validation 
error 
(which 
is 
all 
important) 
is 
low. 
The 
true 
function 
which 
generated 
this 
noisy 
data 
is 
the 
dashed 
line; 
the 
function 
learned 
from 
the 
data 
is 
given 
by 
the 
solid 
line. 


Drawbacks 
of 
the 
empirical 
risk 
approach 


• 
It 
seems 
extreme 
to 
assume 
that 
the 
data 
follows 
the 
empirical 
distribution, 
particularly 
for 
small 
amounts 
of 
training 
data. 
To 
generalise 
well, 
we 
need 
to 
make 
sensible 
assumptions 
as 
to 
p(x) 
– 
that 
is 
the 
distribution 
for 
all 
x 
that 
could 
arise. 
• 
If 
the 
utility 
(or 
loss) 
function 
changes, 
the 
discriminant 
function 
needs 
to 
be 
retrained. 
• 
Some 
problems 
require 
an 
estimate 
of 
the 
condence 
of 
the 
prediction. 
Whilst 
there 
may 
be 
heuristic 
ways 
to 
evaluating 
condence 
in 
the 
prediction, 
this 
is 
not 
inherent 
in 
the 
framework. 
• 
When 
there 
are 
multiple 
penalty 
parameters, 
performing 
cross 
validation 
in 
a 
discretised 
grid 
of 
the 
parameters 
becomes 
infeasible. 
• 
It 
seems 
a 
shame 
to 
discard 
all 
those 
trained 
models 
in 
cross-validation 
– 
can't 
they 
be 
combined 
in 
some 
manner 
and 
used 
to 
make 
a 
better 
predictor? 
Example 
60 
(Finding 
a 
good 
regularisation 
parameter). 
In 
g(13.5), 
we 
t 
the 
function 
a 
sin(wx) 
to 
data, 
learning 
the 
parameters 
a 
and 
w. 
The 
unregularised 
solution 
g(13.5a) 
badly 
overts 
the 
data, 
and 


2

has 
a 
high 
validation 
error. 
To 
encourage 
a 
smoother 
solution, 
a 
regularisation 
term 
Ereg 
= 
wis 
used. 
The 
validation 
error 
based 
on 
several 
dierent 
values 
of 
the 
regularisation 
parameter 
. 
was 
computed, 
nding 
that 
. 
=0:5 
gave 
a 
low 
validation 
error. 
The 
resulting 
t 
to 
novel 
data, 
g(13.5b) 
is 
reasonable. 


13.2.4 
Bayesian 
decision 
approach 
An 
alternative 
to 
using 
the 
empirical 
distribution 
is 
to 
t 
a 
model 
p(c, 
xj) 
to 
the 
train 
data 
D. 
Given 
this 
model, 
the 
decision 
function 
c(x) 
is 
automatically 
determined 
from 
the 
maximal 
expected 
utility 
(or 


DRAFT 
March 
9, 
2010 



Supervised 
Learning 


X;C
p(x;cj)
U(c;c)

c
p(cjx;)
x
Figure 
13.6: 
Bayesian 
decision 
approach. 
A 
model 
p(x, 
cj) 
is 
tted 
to 
the 
data. 
After 
leaning, 
this 
model 
is 
used 
to 
compute 
p(cjx, 
). 
For 
a 
novel 
x 
, 
we 
then 
nd 
the 
distribution 
of 
the 
assumed 
`truth', 
p(cjx 
;). 
The 
prediction 
(decision) 
is 
then 
given 
by 
that 
c 
* 
which 
maximises 
the 
expected 
utility 
hU(c, 
c 
)ip(cjx 
;). 


minimal 
risk), 
with 
respect 
to 
this 
model, 
as 
in 
equation 
(13.2.5), 
in 
which 
the 
unknown 
p(ctruejx) 
is 
replaced 
with 
p(cjx, 
). 
This 
approach 
therefore 
divorces 
learning 
the 
parameters 
. 
of 
p(c, 
xj) 
from 
the 
utility 
(or 
loss). 


Benets 
of 
the 
Bayesian 
decision 
approach 


• 
This 
is 
a 
conceptually 
`clean’ 
approach, 
in 
which 
one 
tries 
ones 
best 
to 
model 
the 
environment, 
independent 
of 
the 
subsequent 
decision 
process. 
In 
this 
case 
learning 
the 
environment 
is 
separated 
from 
the 
ultimate 
eect 
this 
will 
have 
on 
the 
expected 
utility. 
* 


• 
The 
ultimate 
decision 
c 
for 
a 
novel 
input 
x 
can 
be 
a 
highly 
complex 
function 
of 
x 
due 
to 
the 
maximisation 
operation. 
Drawbacks 
of 
the 
Bayesian 
decision 
approach 




• 
If 
the 
environment 
model 
p(c, 
xj) 
is 
poor, 
the 
prediction 
c 
could 
be 
highly 
inaccurate 
since 
modelling 
the 
environment 
is 
divorced 
from 
prediction. 
• 
To 
avoid 
fully 
divorcing 
the 
learning 
of 
the 
model 
p(c, 
xj) 
from 
its 
eect 
on 
decisions, 
in 
practice 
one 
often 
includes 
regularisation 
terms 
in 
the 
environment 
model 
p(c, 
xj) 
which 
are 
set 
by 
validation 
based 
on 
an 
empirical 
utility. 
There 
are 
two 
main 
approaches 
to 
tting 
p(c, 
xj) 
to 
data 
D. 
We 
could 
parameterise 
the 
joint 
distribution 
using 


p(c, 
xj) 
= 
p(cjx, 
cjx)p(xjx) 
discriminative 
approach 
(13.2.16) 
or 
p(c, 
xj) 
= 
p(xjc, 
xjc)p(cjc) 
generative 
approach 
(13.2.17) 


We'll 
consider 
these 
two 
approaches 
below 
in 
the 
context 
of 
trying 
to 
make 
a 
system 
that 
can 
distinguish 
between 
a 
male 
and 
female 
face. 
We 
have 
a 
database 
of 
face 
images 
in 
which 
each 
image 
is 
represented 
as 
a 
real-valued 
vector 
xn;n 
=1;:::;N), 
along 
with 
a 
label 
cn 
2f0, 
1} 
stating 
if 
the 
image 
is 
male 
or 
female. 


Generative 
approach 
p(x;cj)= 
p(xjc, 
xjc)p(cjc) 


For 
simplicity 
we 
use 
Maximum 
Likelihood 
training 
for 
the 
parameters 
. 
Assuming 
the 
data 
D 
is 
i.i.d., 
we 
have 
a 
log 
likelihood 


 
X

log 
p(Dj) 
=log 
p(xnjc 
n;xjc) 
+log 
p(c 
njc) 
(13.2.18) 
nn 


As 
we 
see 
the 
dependence 
on 
xjc 
occurs 
only 
in 
the 
rst 
term, 
and 
c 
only 
occurs 
in 
the 
second. 
This 
means 
that 
learning 
the 
optimal 
parameters 
is 
equivalent 
to 
isolating 
the 
data 
for 
the 
male-class 
and 


DRAFT 
March 
9, 
2010 



Supervised 
Learning 


cnxn
cxjc
N
cnxn
cjxx
N
Figure 
13.7: 
Two 
generic 
strategies 
for 
probabilistic 
classication. 
(a): 
Class 
dependent 
generative 
model 
of 
x. 
After 
learning 
parameters, 
classication 
is 
obtained 
by 
making 
x 
evidential 
and 
inferring 
p(cjx). 
(b): 
A 
discriminative 
classication 
method 
p(cjx).

(a) 
(b) 
tting 
a 
model 
p(xjc 
= 
male;xjmale). 
We 
similarly 
isolate 
the 
female 
data 
and 
t 
a 
separate 
model 
p(xjc 
= 
female;xjfemale). 
The 
class 
distribution 
p(cjc) 
can 
be 
easily 
set 
according 
to 
the 
ratio 
of 
males/females 
in 
the 
set 
of 
training 
data. 


To 
make 
a 
classication 
of 
a 
new 
image 
x 
* 
as 
either 
male 
or 
female, 
we 
may 
use 


p(x 
* 
;c 
= 
malejxjmale) 


p(c 
= 
malejx 
) 
= 
(13.2.19) 


p(x* 
;c 
= 
malejxjmale)+ 
p(x* 
;c 
= 
femalejxjfemale) 


Based 
on 
zero-one 
loss, 
if 
this 
probability 
is 
greater 
than 
0.5 
we 
classify 
x 
* 
as 
male, 
otherwise 
female. 
More 
generally, 
we 
may 
use 
this 
probability 
as 
part 
of 
a 
decision 
process, 
as 
in 
equation 
(13.2.5). 


Advantages 
Prior 
information 
about 
the 
structure 
of 
the 
data 
is 
often 
most 
naturally 
specied 
through 
a 
generative 
model 
p(xjc). 
For 
example, 
for 
male 
faces, 
we 
would 
expect 
to 
see 
heavier 
eyebrows, 
a 
squarer 
jaw, 
etc. 


Disadvantages 
The 
generative 
approach 
does 
not 
directly 
target 
the 
classication 
model 
p(cjx) 
since 
the 
goal 
of 
generative 
training 
is 
rather 
to 
model 
p(xjc). 
If 
the 
data 
x 
is 
complex, 
nding 
a 
suitable 
generative 
data 
model 
p(xjc) 
is 
a 
dicult 
task. 
On 
the 
other 
hand 
it 
might 
be 
that 
making 
a 
model 
of 
p(cjx) 
is 
simpler, 
particularly 
if 
the 
decision 
boundary 
between 
the 
classes 
has 
a 
simple 
form, 
even 
if 
the 
data 
distribution 
of 
each 
class 
is 
complex, 
see 
g(13.8). 
Furthermore, 
since 
each 
generative 
model 
is 
separately 
trained 
for 
each 
class, 
there 
is 
no 
competition 
amongst 
the 
models 
to 
explain 
the 
data. 


Discriminative 
approach 
p(c, 
x)= 
p(cjx;cjx)p(xjx) 


Assuming 
i.i.d. 
data, 
the 
log 
likelihood 
is

XXX

log 
p(Dj) 
=log 
p(c 
njxn;cjx) 
+log 
p(xnjx) 
(13.2.20) 
nn 
n 


The 
parameters 
are 
isolated 
in 
the 
two 
terms 
so 
that 
Maximum 
Likelihood 
training 
is 
equivalent 
to 
nding 
the 
parameters 
of 
cjx 
that 
will 
best 
predict 
the 
class 
c 
for 
a 
given 
training 
input 
x. 
The 
parameters 
x 
for 
modelling 
the 
data 
occur 
separately 
in 
the 
second 
term 
above, 
and 
setting 
them 
can 
therefore 
be 
treated 
as 
a 
separate 
unsupervised 
learning 
problem. 
This 
approach 
therefore 
isolates 
modelling 
the 
decision 
boundary 
from 
modelling 
the 
input 
distribution, 
see 
g(13.8). 


Classication 
of 
a 
new 
point 
x 
* 
is 
based 
on 


p(cjx;opt) 
(13.2.21)

cjx 


As 
for 
the 
generative 
case, 
this 
approach 
still 
learns 
a 
joint 
distribution 
p(c, 
x)= 
p(cjx)p(x) 
which 
can 
be 
used 
as 
part 
of 
a 
decision 
process 
if 
required. 


Advantages 
The 
discriminative 
approach 
directly 
addresses 
making 
an 
accurate 
classier 
based 
on 
p(cjx), 
modelling 
the 
decision 
boundary, 
as 
opposed 
to 
the 
class 
conditional 
data 
distribution 
in 
the 
generative 
approach. 
Whilst 
the 
data 
from 
each 
class 
may 
be 
distributed 
in 
a 
complex 
way, 
it 
could 
be 
that 
the 
decision 
boundary 
between 
them 
is 
relatively 
easy 
to 
model. 


DRAFT 
March 
9, 
2010 



Supervised 
Learning 


*
mmmmmmmffffffffffx
Figure 
13.8: 
Each 
point 
represents 
a 
high 
dimensional 
vector 
with 
an 
associated 
class 
label, 
either 
male 
or 
female. 
The 
point 
x 
6is 
a 
new 
point 
for 
which 
we 
would 
like 
to 
predict 
whether 
this 
should 
be 
male 
or 
female. 
In 
the 
generative 
approach, 
a 
male 
model 
p(xjmale) 
generates 
data 
similar 
to 
the 
`m’ 
points. 
Similarly, 
the 
female 
model 
p(xjfemale) 
generates 
points 
that 
are 
similar 
to 
the 
`f’ 
points 
above. 
We 
then 
use 
Bayes’ 
rule 
to 
calculate 
the 
probability 
p(malejx 
) 
using 
the 
two 
tted 
models, 
as 
given 
in 
the 
text. 
In 
the 
discriminative 
approach, 
we 
directly 
make 
a 
model 
of 
p(malejx 
), 
which 
cares 
less 
about 
how 
the 
points 
`m’ 
or 
`f’ 
are 
distributed, 
but 
more 
about 
describing 
the 
boundary 
which 
can 
separate 
the 
two 
classes, 
as 
given 
by 
the 
line. 


cnxnhn
cjhhxjh
N
n

Figure 
13.9: 
A 
strategy 
for 
semi-supervised 
learning. 
When 
cis 
missing, 
the 
term 
p(cnjhn) 
is 
absent. 
The 
large 
amount 
of 
training 
data 
helps 
the 
model 
learn 
a 
good 
lower 
dimension/compressed 
representation 
h 
of 
the 
data 
x. 
Fitting 
then 
a 
classication 
model 
p(cjh) 
using 
this 
lower 
dimensional 
representation 
may 
be 
much 
easier 
than 
tting 
a 
model 
directly 
from 
the 
complex 
data 
to 
the 
class, 
p(cjx). 


Disadvantages 
Discriminative 
approaches 
are 
usually 
trained 
as 
`black-box’ 
classiers, 
with 
little 
prior 
knowledge 
built 
in 
as 
to 
how 
data 
for 
a 
given 
class 
might 
look. 
Domain 
knowledge 
is 
often 
more 
easily 
expressed 
using 
the 
generative 
framework. 


Hybrid 
generative-discriminative 
approaches 


One 
could 
use 
a 
generative 
description, 
p(xjc), 
building 
in 
prior 
information, 
and 
use 
this 
to 
form 
a 
joint 
distribution 
p(x, 
c), 
from 
which 
a 
discriminative 
model 
p(cjx) 
may 
be 
formed, 
using 
Bayes’ 
rule. 
Specically, 
we 
can 
use 


p(xjc, 
xjc)p(cjc)

p(cjx, 
)= 
P(13.2.22) 


p(xjc, 
xjc)p(cjc)

c 


..

and 
use 
a 
separate 
model 
for 
p(xjx). 
Subsequently 
the 
parameters 
. 
=xjc;c, 
for 
this 
hybrid 
model 
can 
be 
found 
by 
maximising 
the 
probability 
of 
being 
in 
the 
correct 
class. 
This 
approach 
would 
appear 
to 
leverage 
the 
advantages 
of 
both 
the 
discriminative 
and 
generative 
frameworks 
since 
we 
can 
more 
readily 
incorporate 
domain 
knowledge 
in 
the 
generative 
model 
p(xjc, 
xjc) 
yet 
train 
this 
in 
a 
discriminative 
way. 
This 
approach 
is 
rarely 
taken 
in 
practice 
since 
the 
resulting 
functional 
form 
of 
the 
likelihood 
depends 
in 
a 
complex 
manner 
on 
the 
parameters. 
In 
this 
case 
no 
separation 
occurs 
(as 
was 
previously 
the 
case 
for 
the 
generative 
and 
discriminative 
approaches). 


13.2.5 
Learning 
lower-dimensional 
representations 
in 
semi-supervised 
learning 
One 
way 
to 
exploit 
a 
large 
amount 
of 
unlabelled 
training 
data 
to 
improve 
classication 
modelling 
is 
to 
try 
to 
nd 
a 
lower 
dimensional 
representation 
h 
of 
the 
data 
x. 
Based 
on 
this, 
the 
mapping 
from 
h 
to 
c 
may 
be 
rather 
simpler 
to 
learn 
than 
a 
mapping 
from 
x 
to 
c 
directly. 
To 
do 
so 
we 
can 
form 
the 
likelihood 
using, 
see 
g(13.9), 


Y	I[cn=;]

6

p(C, 
X 
, 
Hj)=p(c 
njhn;cjh)p(x 
njhn;xjh)p(hjh) 
(13.2.23) 
n

and 
then 
set 
any 
parameters 
for 
example 
by 
using 
Maximum 
Likelihood 


X

opt 


= 
argmax 
p(C, 
X 
, 
Hj) 
(13.2.24) 




H6

DRAFT 
March 
9, 
2010 



Bayes 
versus 
Empirical 
Decisions 


13.2.6 
Features 
and 
preprocessing 
It 
is 
often 
the 
case 
that 
when 
attempting 
to 
make 
a 
predictive 
model, 
transforming 
the 
raw 
input 
x 
into 
a 
form 
that 
more 
directly 
captures 
the 
relevant 
label 
information 
can 
greatly 
improve 
performance. 
For 
example, 
in 
the 
male-female 
classication 
case, 
it 
might 
be 
that 
building 
a 
classier 
directly 
in 
terms 
of 
the 
elements 
of 
the 
face 
vector 
x 
is 
dicult. 
However, 
using 
`features’ 
which 
contain 
geometric 
information 
such 
as 
the 
distance 
between 
eyes, 
width 
of 
mouth, 
etc. 
may 
make 
nding 
a 
classier 
easier. 
In 
practice 
data 
is 
often 
preprocessed 
to 
remove 
noise, 
centre 
an 
image 
etc. 


13.3 
Bayes 
versus 
Empirical 
Decisions 
The 
empirical 
risk 
and 
Bayesian 
approaches 
are 
at 
the 
extremes 
of 
the 
philosophical 
spectrum. 
In 
the 
empirical 
risk 
approach 
one 
makes 
a 
seemingly 
over-simplistic 
data 
generating 
assumption. 
However 
decision 
function 
parameters 
are 
set 
based 
on 
the 
task 
of 
making 
decisions. 
On 
the 
other 
hand, 
the 
Bayesian 
approach 
attempts 
to 
learn 
p(c, 
x) 
without 
regard 
to 
its 
ultimate 
use 
as 
part 
of 
a 
larger 
decision 
process. 
What 
`objective’ 
criterion 
can 
we 
use 
to 
learn 
p(c, 
x), 
particularly 
if 
we 
are 
only 
interested 
in 
classication 
with 
a 
low 
test-risk? 
The 
following 
example 
is 
intended 
to 
recapitulate 
the 
two 
generic 
Bayes 
and 
empirical 
risk 
approaches 
we've 
been 
considering. 


Example 
61 
(The 
two 
generic 
decision 
strategies). 
Consider 
a 
situation 
in 
which, 
based 
on 
patient 
information 
x, 
we 
need 
to 
take 
a 
decision 
d 
as 
whether 
or 
not 
to 
operate. 
The 
utility 
of 
operating 
u(d, 
c) 
depends 
on 
whether 
or 
not 
the 
patient 
has 
cancer. 
For 
example 


u(operate, 
cancer) 
= 
100 
u(operate, 
benign) 
= 
30 


(13.3.1)
u(don't 
operate, 
cancer)=0 
u(don't 
operate, 
benign) 
= 
70 


We 
have 
independent 
true 
assessments 
of 
whether 
or 
not 
a 
patient 
had 
cancer, 
giving 
rise 
to 
a 
set 
of 


n

historical 
records 
D 
= 
f(x;cn);n 
=1;:::;Ng. 
Faced 
with 
a 
new 
patient 
with 
information 
x, 
we 
need 
to 
make 
a 
decision 
whether 
or 
not 
to 
operate. 


In 
the 
Bayesian 
decision 
approach 
one 
would 
rst 
make 
a 
model 
p(cjx, 
D) 
(for 
example 
Logistic 
regression). 
Using 
this 
model 
the 
decision 
is 
given 
by 
that 
which 
maximises 
the 
expected 
utility 


d 
= 
argmax 
[p(cancerjx, 
D)u(d, 
cancer)+ 
p(benignjx, 
D)u(d, 
benign)] 
(13.3.2) 


d 


In 
this 
approach 
learning 
the 
model 
p(cjx, 
D) 
is 
divorced 
from 
the 
ultimate 
use 
of 
the 
model 
in 
the 
decision 
making 
process. 
An 
advantage 
of 
this 
approach 
is 
that, 
from 
the 
viewpoint 
of 
expected 
utility, 
it 
is 
optimal 
– 
provided 
the 
model 
p(cjx, 
D) 
is 
`correct'. 
Unfortunately, 
this 
is 
rarely 
the 
case. 
Given 
the 
limited 
model 
resources, 
it 
might 
make 
sense 
to 
focus 
on 
ensuring 
the 
prediction 
of 
cancer 
is 
correct 
since 
this 
has 
a 
more 
signicant 
eect 
on 
the 
utility. 
However, 
formally, 
this 
is 
not 
possible 
in 
this 
framework. 


The 
alternative 
empirical 
utility 
approach 
recognises 
that 
the 
task 
can 
be 
stated 
as 
to 
translate 
patient 
information 
x 
into 
an 
operation 
decision 
d. 
To 
do 
so 
one 
could 
parameterise 
this 
as 
d(x)= 
f(xj) 
and 
then 
learn 
. 
under 
maximising 
the 
empirical 
utility 


X

u()=u(f(x 
nj);c 
n) 
(13.3.3) 
n 


For 
example, 
if 
x 
is 
a 
vector 
representing 
the 
patient 
information 
and 
. 
the 
parameter, 
we 
might 
use 
a 
linear 
decision 
function 
such 
as 




Tx 
= 
0 
d 
= 
operate 


f(xj) 
=(13.3.4)

Tx 
< 
0 
d 
= 
don't 
operate 


The 
advantage 
of 
this 
approach 
is 
that 
the 
parameters 
of 
the 
decision 
are 
directly 
related 
to 
the 
utility 
of 
making 
the 
decision. 
A 
disadvantage 
is 
that 
we 
cannot 
easily 
incorporate 
domain 
knowledge 
into 
the 
decision 
function. 
It 
may 
be 
that 
we 
have 
a 
good 
model 
of 
p(cjx) 
and 
would 
wish 
to 
make 
use 
of 
this. 


262 
DRAFT 
March 
9, 
2010 



Bayesian 
Hypothesis 
Testing 
for 
Outcome 
Analysis 


Both 
approaches 
are 
heavily 
used 
in 
practice 
and 
which 
is 
to 
be 
preferred 
depends 
very 
much 
on 
the 
problem. 
Whilst 
the 
Bayesian 
approach 
appears 
formally 
optimal, 
it 
is 
prone 
to 
model 
mis-specication. 
A 
pragmatic 
alternative 
Bayesian 
approach 
is 
to 
t 
a 
parameterised 
distribution 
p(c, 
xj) 
to 
the 
data 
D, 
where 
. 
penalises 
`complexity’ 
of 
the 
tted 
distribution, 
setting 
. 
using 
validation 
on 
the 
risk. 
This 
has 
the 
potential 
advantage 
of 
allowing 
one 
to 
incorporate 
sensible 
prior 
information 
about 
p(c, 
x) 
whilst 
assessing 
competing 
models 
in 
the 
light 
of 
their 
actual 
predictive 
risk. 
Similarly, 
for 
the 
empirical 
risk 
approach, 
one 
can 
modify 
the 
extreme 
empirical 
distribution 
assumption 
by 
using 
a 
more 
plausible 
model 
p(x, 
c) 
of 
the 
data. 


13.4 
Representing 
Data 
The 
numeric 
encoding 
of 
data 
can 
have 
a 
signicant 
eect 
on 
performance 
and 
an 
understanding 
of 
the 
options 
for 
representing 
data 
is 
therefore 
of 
considerable 
importance. 


13.4.1 
Categorical 
For 
categorical 
(or 
nominal) 
data, 
the 
observed 
value 
belongs 
to 
one 
of 
a 
number 
of 
classes, 
with 
no 
intrinsic 
ordering 
of 
the 
classes. 
An 
example 
of 
a 
categorical 
variable 
would 
be 
the 
description 
of 
the 
type 
of 
job 
that 
someone 
does, 
e.g. 
healthcare, 
education, 
nancial 
services, 
transport, 
homeworker, 
unemployed, 
engineering 
etc. 


One 
way 
to 
transform 
this 
data 
into 
numerical 
values 
would 
be 
to 
use 
1-of-m 
encoding. 
Here's 
an 
example: 
There 
are 
4 
kinds 
of 
jobs: 
soldier, 
sailor, 
tinker, 
spy. 
A 
soldier 
is 
represented 
as 
(1,0,0,0), 
a 
sailer 
as 
(0,1,0,0), 
a 
tinker 
as 
(0,0,1,0) 
and 
a 
spy 
as 
(0,0,0,1). 
In 
this 
encoding 
the 
distance 
between 
the 
vectors 
representing 
two 
dierent 
professions 
is 
constant. 
It 
is 
clear 
that 
1-of-m 
encoding 
induces 
dependencies 
in 
the 
profession 
attributes 
since 
if 
one 
of 
the 
profession 
attributes 
is 
1, 
the 
others 
must 
be 
zero. 


13.4.2 
Ordinal 
An 
ordinal 
variable 
consists 
of 
categories 
with 
an 
ordering 
or 
ranking 
of 
the 
categories, 
e.g. 
cold, 
cool, 
warm, 
hot. 
In 
this 
case, 
to 
preserve 
the 
ordering 
we 
could 
perhaps 
use 
-1 
for 
cold, 
0 
for 
cool, 
+1 
for 
warm 
and 
+2 
for 
hot. 
This 
choice 
is 
somewhat 
arbitrary, 
and 
one 
should 
bear 
in 
mind 
that 
results 
will 
generally 
be 
dependent 
on 
the 
numerical 
coding 
used. 


13.4.3 
Numerical 
Numerical 
data 
takes 
on 
values 
that 
are 
real 
numbers, 
e.g. 
a 
temperature 
measured 
by 
a 
thermometer, 
or 
the 
salary 
that 
someone 
earns. 


13.5 
Bayesian 
Hypothesis 
Testing 
for 
Outcome 
Analysis 
How 
can 
we 
assess 
whether 
two 
classiers 
are 
performing 
dierently? 
For 
techniques 
which 
are 
based 
on 
Bayesian 
classiers 
p(c, 
jD;M) 
there 
will 
always 
be, 
in 
principle, 
a 
direct 
way 
to 
estimate 
the 
suitability 
of 
the 
model 
M 
by 
computing 
p(MjD). 
We 
consider 
here 
the 
less 
fortunate 
situation 
where 
the 
only 
information 
presumed 
available 
is 
the 
test 
performance 
of 
the 
two 
classiers. 


To 
outline 
the 
basic 
issue, 
let's 
consider 
two 
classiers 
A 
and 
B 
which 
predict 
the 
class 
of 
55 
test 
examples. 
Classier 
A 
makes 
20 
errors, 
and 
35 
correct 
classications, 
whereas 
classier 
B 
makes 
23 
errors 
and 
32 
correct 
classications. 
Is 
classier 
A 
better 
than 
classier 
B? 
Our 
lack 
of 
condence 
in 
pronouncing 
that 
A 
is 
better 
than 
B 
results 
from 
the 
small 
number 
of 
test 
examples. 
On 
the 
other 
hand 
if 
classier 
A 
makes 
200 
errors 
and 
350 
correct 
classications, 
whilst 
classier 
B 
makes 
230 
errors 
and 
320 
correct 
classications, 
intuitively, 
we 
would 
be 
more 
condent 
that 
classier 
A 
is 
better 
than 
classier 
B. 


Perhaps 
the 
most 
practically 
relevant 
question 
from 
a 
Machine 
Learning 
perspective 
is 
the 
probability 
that 
classier 
A 
outperforms 
classier 
B, 
given 
the 
available 
test 
information. 
Whilst 
this 
question 
can 


DRAFT 
March 
9, 
2010 
263 



Bayesian 
Hypothesis 
Testing 
for 
Outcome 
Analysis 


be 
addressed 
using 
a 
Bayesian 
procedure, 
section(13.5.5), 
we 
rst 
focus 
on 
a 
simpler 
question, 
namely 
whether 
classier 
A 
and 
B 
are 
the 
same[16]. 


13.5.1 
Outcome 
analysis 
The 
treatment 
in 
this 
section 
refers 
to 
outcomes 
and 
quanties 
if 
data 
is 
likely 
to 
come 
from 
the 
same 
multinomial 
distribution. 
In 
the 
main 
we 
will 
apply 
this 
to 
assessing 
if 
two 
classiers 
are 
essentially 
performing 
the 
same, 
although 
one 
should 
bear 
in 
mind 
that 
the 
method 
applies 
more 
generally 
to 
assessing 
if 
outcomes 
are 
likely 
to 
have 
been 
generated 
from 
the 
same 
or 
dierent 
underlying 
processes. 


Consider 
a 
situation 
where 
two 
classiers 
A 
and 
B 
have 
been 
tested 
on 
some 
data, 
so 
that 
we 
have, 
for 
each 
example 
in 
the 
test 
set, 
an 
outcome 
pair 


(oa(n);ob(n)) 
;n 
=1;:::;N 
(13.5.1) 
where 
N 
is 
the 
number 
of 
test 
data 
points, 
and 
oa 
2f1;:::;Q} 
(and 
similarly 
for 
ob). 
That 
is, 
there 
are 
Q 
possible 
types 
of 
outcomes 
that 
can 
occur. 


For 
example, 
for 
binary 
classication 
we 
will 
typically 
have 
the 
four 
cases 
dom(o)= 
fTruePositive, 
FalsePositive, 
TrueNegative, 
FalseNegative} 
(13.5.2) 
If 
the 
classier 
predicts 
class 
c 
2ftrue, 
false} 
and 
the 
truth 
is 
class 
t 
2ftrue, 
false} 
these 
are 
dened 
as 
TruePositive 
c 
= 
true 
t 
= 
true 
FalsePositive 
c 
= 
true 
t 
= 
false 


(13.5.3)
TrueNegative 
c 
= 
false 
t 
= 
false 
FalseNegative 
c 
= 
false 
t 
= 
true 


We 
call 
oa 
= 
foa(n);n 
=1;:::;Ng, 
the 
outcomes 
for 
classier 
A, 
and 
similarly 
for 
ob 
= 
fob(n);n 
=1;:::;Ngfor 
classier 
B. 
To 
be 
specic 
we 
have 
two 
hypotheses 
we 
wish 
to 
test: 


1. 
Hdiff 
: 
oa 
and 
ob 
are 
from 
dierent 
categorical 
distributions. 
2. 
Hsame 
: 
oa 
and 
ob 
are 
from 
the 
same 
categorical 
distribution. 
In 
both 
cases 
we 
will 
use 
categorical 
models 
p(oc 
= 
qj;H)= 
c, 
with 
unknown 
parameters 
a 
(hypothesis 
q
2 
will 
correspond 
to 
using 
the 
same 
parameters 
a 
= 
b 
for 
both 
classiers, 
and 
hypothesis 
1 
to 
using 
dierent 
parameters, 
as 
we 
will 
discuss 
below). 
In 
the 
Bayesian 
framework, 
we 
want 
to 
nd 
how 
likely 
it 
is 
that 
a 
model/hypothesis 
is 
responsible 
for 
generating 
the 
data. 
For 
any 
hypothesis 
H 
calculate 


p(oa, 
objH)p(H) 


p(Hjoa, 
ob) 
= 
(13.5.4) 


p(oa, 
ob) 
where 
p(H) 
is 
our 
prior 
belief 
that 
H 
is 
the 
correct 
hypothesis. 
Note 
that 
the 
normalising 
constant 
p(oa, 
ob) 
does 
not 
depend 
on 
the 
hypothesis. 
Under 
all 
hypotheses 
we 
will 
make 
the 
independence 
of 
trials 
assumption 


N

N 


p(oa, 
objH)= 
p(oa(n);ob(n)jH). 
(13.5.5) 
n=1 


To 
make 
further 
progress 
we 
need 
to 
state 
what 
the 
specic 
hypotheses 
mean. 


DRAFT 
March 
9, 
2010 



Bayesian 
Hypothesis 
Testing 
for 
Outcome 
Analysis 



oa

ob

oaob
P
oa;ob
Figure 
13.10: 
(a): 
Hdiff 
: 
Corresponds 
to 
the 
outcomes 
for 
the 
two 
classiers 
being 
independently 
generated. 
(b): 
Hsame: 
both 
outcomes 
are 
generated 
from 
the 
same 
distribution. 
(c): 
Hdep 
: 
the 
outcomes 
are 
dependent 
(`correlated'). 


(a) 
(b) 
(c) 
13.5.2 
Hdiff 
: 
model 
likelihood 
We 
now 
use 
the 
above 
assumptions 
to 
compute 
the 
Hypothesis 
likelihood: 


p(oa, 
objHdi)p(Hdi) 


p(Hdijoa, 
ob) 
= 
(13.5.6) 


p(oa, 
ob) 


The 
outcome 
model 
for 
classier 
A 
is 
specied 
using 
parameters, 
, 
giving 
p(oaj;Hdi), 
and 
similarly 
we 
use 
ß 
for 
classier 
B. 
The 
nite 
amount 
of 
data 
means 
that 
we 
are 
uncertain 
as 
to 
these 
parameter 
values, 
and 
therefore 
the 
joint 
term 
in 
the 
numerator 
above 
is 


Z

p(oa, 
ob)p(Hdijoa, 
ob)=p(oa, 
obj, 
;Hdi)p(, 
jHdi)p(Hdi)ddß 
(13.5.7) 


ZZ

= 
p(Hdi)p(oaj;Hdi)p(jHdi)dp(obj;Hdi)p(jHdi)dß 
(13.5.8) 


where 
we 
assumed 


p(, 
jHdi)= 
p(jHdi)p(jHdi) 
and 
p(oa, 
obj, 
;Hdi)= 
p(oaj;Hdi)p(obj;Hdi) 
(13.5.9) 


Note 
that 
one 
might 
expect 
there 
to 
be 
a 
specic 
constraint 
that 
the 
two 
models 
A 
and 
B 
are 
dierent. 
However 
since 
the 
models 
are 
assumed 
independent 
and 
each 
has 
parameters 
sampled 
from 
an 
eectively 
innite 
set 
(a 
and 
ß 
are 
continuous), 
the 
probability 
that 
sampled 
parameters 
a 
and 
ß 
of 
the 
two 
models 
are 
the 
same 
is 
zero. 


Since 
we 
are 
dealing 
with 
categorical 
distributions, 
it 
is 
convenient 
to 
use 
the 
Dirichlet 
prior, 
which 
is 
conjugate 
to 
the 
categorical 
distribution: 


QQ

Y..(uq)

1 
uq..1 
q=1 


p(jHdi)= 
q 
;Z(u)=PQ 
(13.5.10)

Z(u)

q 
..q=1 
uq

The 
prior 
hyperparameter 
u 
controls 
how 
strongly 
the 
mass 
of 
the 
distribution 
is 
pushed 
to 
the 
corners 
of 
the 
simplex, 
see 
g(8.5). 
Setting 
uq 
= 
1 
for 
all 
q 
corresponds 
to 
a 
uniform 
prior. 
The 
likelihood 
of 
oa 
is 
given 
by

Z

ZYY

]a
q 
1 
uq..1 
Z(u 
+ 
]a) 


p(oaj;Hdi)p(jHdi)da 
=q 
q 
da 
= 
(13.5.11)

Z(u)Z(u)

qq 


where 
]a 
is 
a 
vector 
with 
components 
]a 
being 
the 
number 
of 
times 
that 
variable 
a 
is 
is 
state 
q 
in 
the 
data. 


q 


Hence 
Z(u 
+ 
]a) 
Z(u 
+ 
]b) 


p(Hdijoa, 
ob)= 
p(Hdi)(13.5.12)

Z(u) 
Z(u) 


where 
Z(u) 
is 
given 
by 
equation 
(13.5.10). 


13.5.3 
Hsame 
: 
model 
likelihood 
In 
Hsame, 
the 
hypothesis 
is 
that 
the 
outcomes 
for 
the 
two 
classiers 
are 
generated 
from 
the 
same 
categorical 
distribution. 
Hence 


Z

p(oa, 
ob)p(Hsamejoa, 
ob)= 
p(Hsame)p(oaj;Hsame)p(obj;Hsame)p(jHsame)da 
(13.5.13) 


Z(u 
+ 
]a 
+ 
]b)

= 
p(Hsame)(13.5.14)

Z(u) 


DRAFT 
March 
9, 
2010 



Bayesian 
Hypothesis 
Testing 
for 
Outcome 
Analysis 


Bayes’ 
factor 


If 
we 
assume 
that 
we 
have 
no 
prior 
preference 
for 
either 
hypothesis 
(p(Hdi)= 
p(Hsame)), 
then 


p(Hdijoa, 
ob) 
Z(u 
+ 
]a)Z(u 
+ 
]b)

= 
(13.5.15) 
p(Hsamejoa, 
ob) 
Z(u)Z(u 
+ 
]a 
+ 
]b) 


This 
is 
the 
evidence 
to 
suggest 
that 
the 
data 
were 
generated 
by 
two 
dierent 
categorical 
distributions. 


Example 
62. 
Two 
people 
classify 
the 
expression 
of 
each 
image 
into 
happy, 
sad 
or 
normal, 
using 
states 
1, 
2, 
3 
respectively. 
Each 
column 
of 
the 
data 
below 
represents 
an 
image 
classed 
by 
the 
two 
people 
(person 
1 
is 
the 
top 
row 
and 
person 
2 
the 
second 
row). 
Are 
the 
two 
people 
essentially 
in 
agreement? 


1 
1 
3 
3 
1 
1 
3 
2 
1 
2 
1 
3 
3 
3 
2 
3 
2 
2 
3 
3 
1 
3 
1 
2 
1 
2 
1 
2 
1 
2 
1 
1 
1 
2 
1 
1 
1 
3 
2 
2 


To 
help 
answer 
this 
question, 
we 
perform 
a 
Hdiff 
versus 
Hsame 
test. 
From 
this 
data, 
the 
count 
vector 
for 
person 
1 
is 
[13, 
3, 
4] 
and 
for 
person 
2, 
[4, 
9, 
7]. 
Based 
on 
a 
at 
prior 
for 
the 
categorical 
distribution 
and 
assuming 
no 
prior 
preference 
for 
either 
hypothesis, 
we 
have 
the 
Bayes’ 
factor 


p(persons 
1 
and 
2 
classify 
dierently) 
Z([14, 
4, 
5])Z([5, 
10, 
8])

= 
=12:87 
(13.5.16) 


p(persons 
1 
and 
2 
classify 
the 
same) 
Z([1, 
1, 
1])Z([18, 
13, 
12]) 


where 
the 
Z 
function 
is 
given 
in 
equation 
(13.5.10). 
This 
is 
strong 
evidence 
the 
two 
people 
are 
classifying 
the 
images 
dierently. 


Below 
we 
discuss 
some 
further 
examples 
for 
the 
Hdiff 
versus 
Hsame 
test. 
As 
above, 
the 
only 
quantities 
we 
need 
for 
this 
test 
are 
the 
vector 
counts 
from 
the 
data. 
Let's 
assume 
that 
there 
are 
three 
kinds 
of 
outcomes, 
Q 
= 
3. 
For 
example 
dom(o)= 
fgood, 
bad, 
ugly} 
are 
our 
set 
of 
outcomes 
and 
we 
want 
to 
test 
if 
two 
classiers 
are 
essentially 
producing 
the 
same 
outcome 
distributions, 
or 
dierent. 


Example 
63 
(Hdiff 
versus 
Hsame). 


• 
We 
have 
the 
two 
outcome 
counts 
]a 
= 
[39, 
26, 
35] 
and 
]b 
= 
[63, 
12, 
25]. 
Then, 
the 
Bayes’ 
factor 
equation 
(13.5.15) 
is 
20:7 
– 
strong 
evidence 
in 
favour 
of 
the 
two 
classiers 
being 
dierent. 
• 
Alternatively, 
consider 
the 
two 
outcome 
counts 
]a 
= 
[52, 
20, 
28] 
and 
]b 
= 
[44, 
14, 
42]. 
Then, 
the 
Bayes’ 
factor 
equation 
(13.5.15) 
is 
0:38 
– 
weak 
evidence 
against 
the 
two 
classiers 
being 
dierent. 
• 
As 
a 
nal 
example, 
consider 
counts 
]a 
= 
[459, 
191, 
350] 
and 
]b 
= 
[465, 
206, 
329]. 
This 
gives 
a 
Bayes’ 
factor 
equation 
(13.5.15) 
of 
0:008 
– 
strong 
evidence 
that 
the 
two 
classiers 
are 
statistically 
the 
same. 
In 
all 
cases 
the 
results 
are 
consistent 
with 
the 
model 
in 
fact 
used 
to 
generate 
the 
count 
data 
– 
the 
two 
outcomes 
for 
A 
and 
B 
were 
indeed 
from 
dierent 
categorical 
distributions. 
The 
more 
test 
data 
we 
have, 
the 
more 
condent 
we 
are 
in 
our 
statements. 


13.5.4 
Dependent 
outcome 
analysis 
Here 
we 
consider 
the 
(perhaps 
more 
common) 
case 
that 
outcomes 
are 
dependent. 
For 
example, 
it 
is 
often 
the 
case 
that 
if 
classier 
A 
works 
well, 
then 
classier 
B 
will 
also 
work 
well. 
Thus 
we 
want 
to 
evaluate 
the 


DRAFT 
March 
9, 
2010 



Bayesian 
Hypothesis 
Testing 
for 
Outcome 
Analysis 


hypothesis: 


Hdep 
: 
the 
outcomes 
that 
the 
two 
classiers 
make 
are 
dependent 
(13.5.17) 
To 
do 
so 
we 
assume 
a 
categorical 
distribution 
over 
the 
joint 
states: 


p(oa(n);ob(n)jP;Hdep) 
(13.5.18) 
Here 
P 
is 
a 
Q 
× 
Q 
matrix 
of 
probabilities: 


[P 
]ij 
= 
p(oa 
= 
i, 
ob 
= 
j) 
(13.5.19) 
so 
[P 
]ij 
is 
the 
probability 
that 
A 
makes 
outcome 
i, 
and 
B 
makes 
outcome 
j. 
Then, 


ZZ

p(ojHdep)=p(o, 
PjHdep)dP 
=p(ojP;Hdep)p(PjHdep)dP 


where, 
for 
convenience, 
we 
write 
o 
=(oa, 
ob). 
Assuming 
a 
Dirichlet 
prior 
on 
P, 
with 
hyperparameters 
U, 
we 
have 


Z(vec 
(U 
+ 
])) 


p(o)p(Hdepjo)= 
p(Hdep)(13.5.20)

Z(vec(U)) 


where 
vec(D) 
is 
a 
vector 
formed 
from 
concatenating 
the 
rows 
of 
the 
matrix 
D. 
Here 
m 
is 
the 
count 
matrix, 
with 
[]]ij 
equal 
to 
the 
number 
of 
times 
that 
joint 
outcome 
(oa 
= 
i, 
ob 
= 
j) 
occurred 
in 
the 
N 
datapoints. 
As 
before, 
we 
can 
then 
use 
this 
in 
a 
Bayes 
factor 
calculation. 
For 
the 
uniform 
prior, 
[U]ij 
=1, 
8i, 
j. 


Testing 
for 
dependencies 
in 
the 
outcomes: 
Hdep 
versus 
Hdiff 


To 
test 
whether 
or 
not 
the 
outcomes 
of 
the 
classiers 
are 
dependent 
Hdep 
against 
the 
hypothesis 
that 
they 
are 
independent 
Hdiff 
we 
may 
use, 
assuming 
p(Hdi)= 
p(Hdep), 


p(Hdijo) 
Z(u 
+ 
]a) 
Z(u 
+ 
]b) 
Z(vec(U))

= 
(13.5.21) 


p(Hdepjo) 
Z(u) 
Z(u) 
Z(vec 
(U 
+ 
])) 


Example 
64 
(Hdep 
versus 
Hdi). 


• 
Consider 
the 
outcome 
count 
matrix 
m 
01Z

98 
7 
93 


@Z168 
13 
163 
AZ(13.5.22) 
245 
12 
201 


so 
that 
]a 
= 
[511, 
32, 
457], 
and 
]b 
= 
[198, 
344, 
458]. 
Then 


p(Hdijo) 


= 
3020 
(13.5.23) 
p(Hdepjo) 


-strong 
evidence 
that 
the 
classiers 
perform 
independently. 


• 
Consider 
the 
outcome 
count 
matrix 
m 
01Z

82 
120 
83 


@Z107 
162 
4 
AZ(13.5.24) 
170 
203 
70 


so 
that 
]a 
= 
[359, 
485, 
156], 
and 
]b 
= 
[284, 
273, 
443]. 
Then 


p(Hdijo) 


=2 
× 
10..18 
(13.5.25) 
p(Hdepjo) 
-strong 
evidence 
that 
the 
classiers 
perform 
dependently. 
These 
results 
are 
in 
fact 
consistent 
with 
the 
way 
the 
data 
was 
generated 
in 
each 
case. 


DRAFT 
March 
9, 
2010 



Bayesian 
Hypothesis 
Testing 
for 
Outcome 
Analysis 


Testing 
for 
dependencies 
in 
the 
outcomes: 
Hdep 
versus 
Hsame 


In 
practice, 
it 
is 
reasonable 
to 
believe 
that 
dependencies 
are 
quite 
likely 
in 
the 
outcomes 
that 
classiers. 
For 
example 
two 
classiers 
will 
often 
do 
well 
on 
`easy’ 
test 
examples, 
and 
badly 
on 
`dicult’ 
examples. 
Are 
these 
dependencies 
strong 
enough 
to 
make 
us 
believe 
that 
the 
outcomes 
are 
coming 
from 
the 
same 
process? 
In 
this 
sense, 
we 
want 
to 
test 


p(Hsamejo) 
Z(u 
+ 
]a]+ 
]b) 
Z(vec(U))

= 
(13.5.26) 


p(Hdepjo) 
Z(u) 
Z(vec 
(U 
+ 
])) 


Example 
65 
(Hdep 
versus 
Hsame). 


• 
Consider 
an 
experiment 
which 
gives 
the 
test 
outcome 
count 
matrix 
m 
0. 


105 
42 
172 
. 
42 
45 
29 
. 
(13.5.27) 
192 
203 
170 


so 
that 
]a]= 
[339, 
290, 
371], 
and 
]b]= 
[319, 
116, 
565]. 
Then 


p(Hsamejo) 


=4:5 
× 
10..38 
(13.5.28) 


p(Hdepjo) 


– 
strong 
evidence 
that 
the 
classiers 
are 
performing 
dierently. 
• 
Consider 
an 
experiment 
which 
gives 
the 
test 
outcome 
count 
matrix 
m 
0. 


15 
8 
10 
. 
548 
. 
(13.5.29) 
13 
12 
25 


so 
that 
]a]= 
[33, 
24, 
43], 
and 
]b]= 
[33, 
17, 
50]. 
Then 


p(Hsamejo) 


= 
42 
(13.5.30) 


p(Hdepjo) 


– 
strong 
evidence 
that 
the 
classiers 
are 
performing 
the 
same. 
These 
results 
are 
in 
fact 
consistent 
with 
the 
way 
the 
data 
was 
generated. 


13.5.5 
Is 
classier 
A 
better 
than 
B? 
We 
return 
to 
the 
question 
with 
which 
we 
began 
this 
outcome 
analysis. 
Given 
the 
common 
scenario 
of 
observing 
a 
number 
of 
errors 
for 
classier 
A 
on 
a 
test 
set 
and 
a 
number 
for 
B, 
can 
we 
say 
which 
classier 
is 
better? 
This 
corresponds 
to 
the 
special 
case 
of 
binary 
classes 
Q 
= 
2 
with 
dom(e)= 
fcorrect, 
incorrectg. 
Under 
the 
Hdiff 
for 
this 
special 
case 
it 
makes 
sense 
to 
use 
a 
Beta 
distribution 
(which 
corresponds 
to 
the 
Dirichlet 
when 
Q 
= 
2). 
Then 
for 
a]being 
the 
probability 
that 
classier 
A 
generates 
a 
correct 
label 
we 
have 


]a 


incorrect 


p(oAja)= 
a]
correct 
(1 
- 
a)]a 
(13.5.31) 


Similarly 


]b 


correct 


incorrect 


p(oBjb)= 
(1 
- 
b)]b 
(13.5.32)

b]

We 
assume 
independent 
identical 
Beta 
distribution 
priors 


p(a)= 
B 
(aju1;u2) 
;p(b)= 
B 
(bju1;u2) 
(13.5.33) 


DRAFT 
March 
9, 
2010 



Code 


00.10.20.30.40.50.60.70.80.9101234567
00.10.20.30.40.50.60.70.80.9105101520
(a) 
(b) 
Figure 
13.11: 
Two 
classiers 
A 
and 
B 
and 
their 
posterior 
distributions 
of 
the 
probability 
that 
they 
classify 
correctly 
(using 
a 
uniform 
Beta 
prior). 
(a): 
For 
A 
with 
35 
correct 
and 
20 
incorrect 
labels, 
B 
(xj1 
+ 
35, 
1 
+ 
20) 
(solid 
curve); 
B 
with 
32 
correct 
23 
incorrect 
B 
(yj1 
+ 
32, 
1 
+ 
23) 
(dashed 
curve). 
p(x> 
y)=0:719 
(b): 
For 
A 
with 
350 
correct 
and 
200 
incorrect 
labels 
(solid 
curve), 
B 
(xj1 
+ 
350, 
1 
+ 
200); 
B 
with 
320 
correct 
230 
incorrect 
B 
(yj1 
+ 
320, 
1 
+ 
230) 
(dashed 
curve), 
p(x>y)=0:968. 
As 
the 
amount 
of 
data 
increases 
the 
overlap 
between 
the 
distributions 
decreases 
and 
the 
certainty 
that 
one 
classier 
is 
better 
than 
the 
other 
correspondingly 
increases. 


where 
a 
at 
prior 
corresponds 
to 
using 
the 
hyperparameter 
setting 
u1 
= 
u2 
= 
1. 
The 
posterior 
distributions 
for 
a 
and 
b 
are 
independent: 


p(ajoA)= 
B 
(aj]a 
+ 
u1;]a 
p(bjoB)= 
Bbj]b 
+ 
u1;]b 
(13.5.34)

correct 
incorrect 
+ 
u2) 
, 
correct 
incorrect 
+ 
u2 


The 
question 
of 
whether 
A 
is 
better 
than 
B 
can 
then 
be 
addressed 
by 
computing 


. 
1 
. 
1

1 


p(a 
>bjoA, 
oB)= 
x 
a..1 
(1 
- 
x)b..1 
y 
c..1 
(1 
- 
y)d..1 
dydx 
(13.5.35)

B(a, 
b)B(c, 
d) 


0 
x 


where 
a 
= 
]a 
+ 
u1;b 
= 
]a 
c 
= 
]b 
+ 
u1;d 
= 
]b 
(13.5.36)

correct 
incorrect 
+ 
u2, 
correct 
incorrect 
+ 
u2 


Example 
66. 
Classier 
A 
makes 
20 
errors, 
and 
35 
correct 
classications, 
whereas 
classier 
B 
makes 
23 
errors 
and 
32 
correct 
classications. 
Using 
a 
at 
prior 
this 
gives 


p(a 
>bjoA, 
oB)= 
betaXbiggerY(1+35,1+20,1+32,1+23) 
=0:719 
(13.5.37) 


On 
the 
other 
hand 
if 
classier 
A 
makes 
200 
errors 
and 
350 
correct 
classications, 
whilst 
classier 
B 
makes 
230 
errors 
and 
320 
correct 
classications, 
we 
have 


p(a 
>bjoA, 
oB)= 
betaXbiggerY(1+350,1+200,1+320,1+230) 
=0:968 
(13.5.38) 


This 
demonstrates 
the 
intuitive 
eect 
that 
even 
though 
the 
proportion 
of 
correct/incorrect 
classications 
doesn't 
change 
for 
the 
two 
scenarios, 
as 
we 
have 
more 
data 
our 
condence 
in 
determining 
the 
better 
classier 
increases. 


13.6 
Code 
demoBayesErrorAnalysis.m: 
Demo 
for 
Bayesian 
Error 
Analysis 
betaXbiggerY.m: 
p(x>y) 
for 
x 
~ 
B 
(xja, 
b), 
y 
~ 
B 
(yjc, 
d) 


DRAFT 
March 
9, 
2010 
269 



Exercises 


13.7 
Notes 
A 
general 
introduction 
to 
machine 
learning 
is 
given 
in 
[200]. 
An 
excellent 
reference 
for 
Bayesian 
decision 
theory 
is 
[33]. 
Approaches 
based 
on 
empirical 
risk 
are 
discussed 
in 
[282]. 


13.8 
Exercises 
..

a 
..

a 


Exercise 
150. 
Given 
the 
distributions 
p(xjclass1) 
= 
Nx 


1;2 
and 
p(xjclass2) 
= 
Nx 


2;2 
, 
with 


1 


2 


corresponding 
prior 
occurrence 
of 
classes 
p1 
and 
p2 
(p1 
+p2 
=1), 
calculate 
the 
decision 
boundary 
explicitly 
as 
a 
function 
of 
1;2;12;22;p1;p2. 
How 
many 
solutions 
are 
there 
to 
the 
decision 
boundary, 
and 
are 
they 
all 
reasonable? 


Exercise 
151. 
Suppose 
that 
instead 
of 
using 
the 
Bayes’ 
decision 
rule 
to 
choose 
class 
k 
if 
p(Ckjx) 
>p(Cjjx) 
for 
all 
j6Calculate 
the 


= 
k, 
we 
use 
a 
randomized 
decision 
rule, 
choosing 
class 
j 
with 
probability 
Q(Cjjx). 
error 
for 
this 
decision 
rule, 
and 
show 
that 
the 
error 
is 
minimized 
by 
using 
Bayes’ 
decision 
rule. 


Exercise 
152. 
Consider 
datapoints 
generated 
from 
two 
dierent 
classes. 
Class 
1 
has 
the 
distribution 


..

a 
..

a 


p(xjc 
= 
1) 
Nx 


m1;2 
and 
class 
2 
has 
the 
distribution 
p(xjc 
= 
2) 
Nx 


m2;2 
. 
The 
prior 
probabilities 
of 
each 
class 
are 
p(c 
= 
1) 
= 
p(c 
= 
2) 
= 
1=2. 
Show 
that 
the 
posterior 
probability 
p(c 
=1jx) 
is 
of 
the 
form 


1 


p(c 
=1jx) 
= 
(13.8.1)

1 
+ 
exp 
..(ax 
+ 
b) 


and 
determine 
a 
and 
b 
in 
terms 
of 
m1;m2 
and 
2 
. 


Exercise 
153. 
WowCo.com 
is 
a 
new 
startup 
prediction 
company. 
After 
years 
of 
failures, 
they 
eventually 
nd 
a 
neural 
network 
with 
a 
trillion 
hidden 
units 
that 
achieves 
zero 
test 
error 
on 
every 
learning 
problem 
posted 
on 
the 
internet 
up 
last 
week. 
Each 
learning 
problem 
included 
a 
training 
and 
test 
set. 
Proud 
of 
their 
achievement, 
they 
market 
their 
product 
aggressively 
with 
the 
claim 
that 
it 
`predicts 
perfectly 
on 
all 
known 
problems'. 
Would 
you 
buy 
this 
product? 
Justify 
your 
answer. 


Exercise 
154. 
Three 
people 
classify 
images 
into 
1 
of 
three 
categories. 
Each 
column 
in 
the 
table 
below 
represents 
the 
classications 
of 
each 
image, 
with 
the 
top 
row 
being 
the 
class 
from 
person 
1, 
the 
middle 
from 
person 
2 
and 
the 
bottom 
from 
person 
3. 


1 
1 
1 
3 
3 
2 
1 
1 
1 
3 
2 
1 
1 
2 
1 
1 
3 
3 
3 
3 
2 
2 
3 
2 
2 
2 
2 
3 
3 
3 
1 
3 
1 
1 
2 
2 
1 
2 
1 
1 
2 
2 
1 
2 
1 
1 
1 
1 
1 
2 
2 
1 
1 
3 
1 
3 
3 
2 
2 
2 


Assuming 
no 
prior 
preference 
amongst 
hypotheses 
and 
a 
uniform 
prior 
on 
counts, 
compute 
p(persons 
1, 
2 
and 
3 
classify 
dierently) 


(13.8.2) 
p(persons 
1, 
2 
and 
3 
classify 
the 
same) 


Exercise 
155 
(Better 
than 
random 
guessing?). 
Consider 
a 
classier 
that 
makes 
R 
correct 
classications 
and 
W 
wrong 
classications. 
Is 
the 
classier 
better 
than 
random 
guessing? 
Let 
D6be 
the 
fact 
that 
there 
are 
R 
right 
and 
W 
wrong 
answers. 
Assume 
also 
that 
the 
classications 
are 
i.i.d. 


1. 
Show 
that 
under 
the 
hypothesis 
the 
data 
is 
generated 
purely 
at 
random, 
p(DjHrandom)=0:5R+W 
(13.8.3) 


2. 
Dene 
. 
to 
be 
the 
probability 
that 
the 
classier 
makes 
an 
error. 
Then 
p(Dj)= 
R 
(1 
..6)W 
(13.8.4) 


DRAFT 
March 
9, 
2010 



Exercises 


Then 
consider 


Z

p(DjHnon 
random)=p(Dj)p() 
(13.8.5) 


. 


Show 
that 
for 
a 
Beta 
prior, 
p()= 
B 
(ja, 
b) 


B(R 
+ 
a, 
W 
+ 
b) 


p(DjHnon 
random) 
= 
(13.8.6)

B(a, 
b) 


where 
B(a, 
b) 
is 
the 
beta-function. 


3. 
Considering 
the 
random 
and 
non-random 
Hypotheses 
as 
a 
priori 
equally 
likely, 
show 
that 
0:5R+W 


(13.8.7)
p(HrandomjD)= 
0:5R+W 
+ 
B(R+a;W 
+b) 
B(a;b) 


4. 
For 
a 
at 
prior 
a 
= 
b 
=1 
compute 
the 
probability 
that 
for 
10 
correct 
and 
12 
incorrect 
classications, 
the 
data 
is 
from 
a 
purely 
random 
distribution 
(according 
to 
equation 
(13.8.7)). 
Repeat 
this 
for 
100 
correct 
and 
120 
incorrect 
classications. 
v 


5. 
Show 
that 
the 
standard 
deviation 
in 
the 
number 
of 
errors 
of 
a 
random 
classier 
is 
0:5 
R 
+ 
W 
and 
relate 
this 
to 
the 
above 
computation. 
Exercise 
156. 
For 
a 
prediction 
model 
p~(yjx) 
and 
true 
data 
generating 
distribution 
p(x, 
y), 
we 
dene 
the 
accuracy 
as 


Z

A 
=p(x, 
y)~p(yjx) 
(13.8.8) 


x;y 


1. 
By 
dening 
^p(x, 
y) 
= 
p(x, 
y)~p(yjx) 
A 
(13.8.9) 
and 
considering 
KL(q(x, 
y)j^p(x, 
y)) 
= 
0 
(13.8.10) 
show 
that 
for 
any 
distribution 
q(x, 
y), 
log 
A 
= 
..KL(q(x, 
y)jp(x, 
y)) 
+ 
hlog 
~p(yjx)iq(x;y) 
(13.8.11) 


nn

2. 
You 
are 
given 
a 
set 
of 
training 
data 
D 
= 
fx;y;n 
=1;:::;Ng. 
By 
taking 
q(x, 
y) 
to 
be 
the 
empirical 
distribution 
n

X

1 


q(x, 
y)= 
d 
(x, 
x 
n) 
d 
(y, 
y 
n) 
(13.8.12)

N 


n=1 


show 
that 


n

X

1

log 
A 
..KL(q(x, 
y)jp(x, 
y))+ 
log~p(y 
njx 
n) 
(13.8.13)

N 


n=1 


This 
shows 
that 
the 
prediction 
accuracy 
is 
lower 
bounded 
by 
the 
training 
accuracy 
and 
the 
`gap’ 
bet
ween 
the 
empirical 
distribution 
and 
the 
unknown 
true 
data 
generating 
mechanism. 
In 
theories 
such 
as 
PAC 
Bayes, 
one 
may 
bound 
this 
gap, 
resulting 
in 
a 
bound 
on 
the 
predictive 
accuracy. 
According 
to 
this 
naive 
bound, 
the 
best 
thing 
to 
do 
to 
increase 
the 
prediction 
accuracy 
is 
to 
increase 
the 
training 
accuracy 
(since 
the 
rst 
Kullback-Leibler 
term 
is 
independent 
of 
the 
predictor). 
As 
N 
increases, 
the 
rst 
term 
Kullback-Leibler 
term 
becomes 
small, 
and 
minimising 
the 
training 
error 
is 
justiable. 


DRAFT 
March 
9, 
2010 



Exercises 


Assuming 
that 
the 
training 
data 
are 
drawn 
from 
a 
distribution 
p(yjx) 
which 
is 
deterministic, 
show 
that 


n

X

1

log 
A 
..KL(q(x)jp(x))+ 
log~p(y 
njx 
n) 
(13.8.14)

N 


n=1 


and 
hence 
that, 
provided 
the 
training 
data 
is 
correctly 
predicted, 
(q(ynjxn)= 
p(ynjxn), 
the 
accuracy 
can 
be 
related 
to 
the 
empirical 
input 
distribution 
and 
true 
input 
distribution 
by 


..KL(q(x)jp(x))

A 
= 
e 
(13.8.15) 


DRAFT 
March 
9, 
2010 



CHAPTER 
14 


Nearest 
Neighbour 
Classication 


14.1 
Do 
As 
Your 
Neighbour 
Does 
Successful 
prediction 
typically 
relies 
on 
smoothness 
in 
the 
data 
– 
if 
the 
class 
label 
can 
change 
arbitrarily 
as 
we 
move 
a 
small 
amount 
in 
the 
input 
space, 
the 
problem 
is 
essentially 
random 
and 
no 
algorithm 
will 
generalise 
well. 
machine 
learning 
researchers 
therefore 
construct 
appropriate 
measures 
of 
smoothness 
for 
the 
problem 
they 
have 
at 
hand. 
Nearest 
neighbour 
methods 
are 
a 
good 
starting 
point 
since 
they 
readily 
encode 
basic 
smoothness 
intuitions 
and 
are 
easy 
to 
program, 
forming 
a 
useful 
baseline 
method. 


n

In 
a 
classication 
problem 
each 
input 
vector 
x 
has 
a 
corresponding 
class 
label, 
c2f1;:::;Cg. 
Given 
a 
dataset 
of 
N 
such 
training 
examples, 
D 
= 
fxn;cn} 
;n 
=1;:::;N, 
and 
a 
novel 
x, 
we 
aim 
to 
return 
the 
correct 
class 
c(x). 
A 
simple, 
but 
often 
eective, 
strategy 
for 
this 
supervised 
learning 
problem 
can 
be 
stated 
as: 
for 
novel 
x, 
nd 
the 
nearest 
input 
in 
the 
training 
set 
and 
use 
the 
class 
of 
this 
nearest 
input, 
algorithm(13). 


For 
vectors 
x 
and 
x' 
representing 
two 
dierent 
datapoints, 
we 
measure 
`nearness’ 
using 
a 
dissimilarity 
function 
d(x, 
x0). 
A 
common 
dissimilarity 
is 
the 
squared 
Euclidean 
distance 


d(x, 
x0)=(x 
- 
x0)T(x 
- 
x0) 
(14.1.1) 


which 
can 
be 
more 
conveniently 
written 
(x 
- 
x0)2 
. 
Based 
on 
the 
squared 
Euclidean 
distance, 
the 
decision 
boundary 
is 
determined 
by 
the 
lines 
which 
are 
the 
perpendicular 
bisectors 
of 
the 
closest 
training 
points 
with 
dierent 
training 
labels, 
see 
g(14.1). 
This 
is 
called 
a 
Voronoi 
tessellation. 


The 
nearest 
neighbour 
algorithm 
is 
simple 
and 
intuitive. 
There 
are, 
however, 
some 
issues: 


• 
How 
should 
we 
measure 
the 
distance 
between 
points? 
Whilst 
the 
Euclidean 
square 
distance 
is 
popular, 
this 
may 
not 
always 
be 
appropriate. 
A 
fundamental 
limitation 
of 
the 
Euclidean 
distance 
is 
that 
it 
does 
not 
take 
into 
account 
how 
the 
data 
is 
distributed. 
For 
example 
if 
the 
length 
scales 
of 
x 
vary 
greatly 
the 
largest 
length 
scale 
will 
dominate 
the 
squared 
distance, 
with 
potentially 
useful 
class-specic 
information 
in 
other 
components 
of 
x 
lost. 
The 
Mahalanobis 
distance 
..T 
..

0..10

d(x, 
x0)=x 
- 
xx 
- 
x 
(14.1.2) 


where 
S 
is 
the 
covariance 
matrix 
of 
the 
inputs 
(from 
all 
classes) 
can 
overcome 
some 
of 
these 
problems 
since 
it 
rescales 
all 
length 
scales 
to 
be 
essentially 
equal. 


• 
The 
whole 
dataset 
needs 
to 
be 
stored 
to 
make 
a 
classication. 
This 
can 
be 
addressed 
by 
a 
method 
called 
data 
editing 
in 
which 
datapoints 
which 
have 
little 
or 
no 
eect 
on 
the 
decision 
boundary 
are 
removed 
from 
the 
training 
dataset. 
273 



K-Nearest 
Neighbours 



Figure 
14.1: 
In 
nearest 
neighbour 
classication 
a 
new 
vector 
is 
assigned 
the 
label 
of 
the 
nearest 
vector 
in 
the 
training 
set. 
Here 
there 
are 
three 
classes, 
with 
training 
points 
given 
by 
the 
circles, 
along 
with 
their 
class. 
The 
dots 
indicate 
the 
class 
of 
the 
nearest 
training 
vector. 
The 
decision 
boundary 
is 
piecewise 
linear 
with 
each 
segment 
corresponding 
to 
the 
perpendicular 
bisector 
between 
two 
datapoints 
belonging 
to 
different 
classes, 
giving 
rise 
to 
a 
Voronoi 
tessellation 
of 
the 
input 
space. 


Algorithm 
13 
Nearest 
neighbour 
algorithm 
to 
classify 
a 
new 
vector 
x, 
given 
a 
set 
of 
training 
data 
D 
= 
f(xn;cn);n 
=1;:::;Ng: 


1: 
Calculate 
the 
dissimilarity 
of 
the 
test 
point 
x 
to 
each 
of 
the 
stored 
points, 
dn 
= 
d 
(x, 
xn), 
n 
=1;:::;N. 
2: 
Find 
the 
training 
point 
xn 
* 
which 
is 
nearest 
to 
x 
: 
n 
* 
= 
argmin 
d 
(x, 
xn) 


n 


n

3: 
Assign 
the 
class 
label 
c(x)= 
c* 
. 
4: 
In 
the 
case 
that 
there 
are 
two 
or 
more 
`equidistant’ 
neighbours 
with 
dierent 
class 
labels, 
the 
most 
numerous 
class 
is 
chosen. 
If 
there 
is 
no 
one 
single 
most 
numerous 
class, 
we 
use 
the 
K-nearest-neighbours 
case 
described 
in 
the 
next 
section. 
• 
Each 
distance 
calculation 
can 
be 
expensive 
if 
the 
datapoints 
are 
high 
dimensional. 
Principal 
Components 
Analysis, 
see 
chapter(15), 
is 
one 
way 
to 
address 
this 
and 
replaces 
xn 
with 
a 
low 
dimensional 
..2

projection 
p. 
The 
Euclidean 
distance 
of 
two 
datapointsxa 
- 
xbis 
then 
approximately 
given 
by

..2

pa 
- 
pb, 
see 
section(15.2.4). 
This 
is 
both 
faster 
to 
compute 
and 
can 
also 
improve 
classication 
accuracy 
since 
only 
the 
large 
scale 
characteristics 
of 
the 
data 
are 
retained 
in 
the 
PCA 
projections. 


• 
It 
is 
not 
clear 
how 
to 
deal 
with 
missing 
data 
or 
incorporate 
prior 
beliefs 
and 
domain 
knowledge. 
• 
For 
large 
databases, 
computing 
the 
nearest 
neighbour 
of 
a 
novel 
point 
x 
* 
can 
be 
very 
time-consuming 
since 
x 
* 
needs 
to 
be 
compared 
to 
each 
of 
the 
training 
points. 
Depending 
on 
the 
geometry 
of 
the 
training 
points, 
nding 
the 
nearest 
neighbour 
can 
accelerated 
by 
examining 
the 
values 
of 
each 
of 
the 
components 
xi 
of 
x 
in 
turn. 
Such 
an 
axis-aligned 
space-split 
is 
called 
a 
KD-tree[202] 
and 
can 
reduce 
the 
possible 
set 
of 
candidate 
nearest 
neighbours 
in 
the 
training 
set 
to 
the 
novel 
x 
, 
particularly 
in 
low 
dimensions. 
14.2 
K-Nearest 
Neighbours 
Basing 
the 
classication 
on 
only 
the 
single 
nearest 
neighbour 
can 
lead 
to 
inaccuracies. 
If 
your 
neighbour 
is 
simply 
mistaken 
(has 
an 
incorrect 
training 
class 
label), 
or 
is 
not 
a 
particularly 
representative 
example 
of 
his 
class, 
then 
these 
situations 
will 
typically 
result 
in 
an 
incorrect 
classication. 
By 
including 
more 
than 
the 
single 
nearest 
neighbour, 
we 
hope 
to 
make 
a 
more 
robust 
classier 
with 
a 
smoother 
decision 
boundary 
(less 
swayed 
by 
single 
neighbour 
opinions). 
For 
datapoints 
which 
are 
somewhat 
anomalous 
compared 
with 


274 
DRAFT 
March 
9, 
2010 



A 
Probabilistic 
Interpretation 
of 
Nearest 
Neighbours 



Figure 
14.2: 
In 
K-nearest 
neighbours, 
we 
centre 
a 
hypersphere 
around 
the 
point 
we 
wish 
to 
classify 
(here 
the 
central 
dot). 
The 
inner 
circle 
corresponds 
to 
the 
nearest 
neighbour. 
However, 
using 
the 
3 
nearest 
neighbours, 
we 
nd 
that 
there 
are 
two 
`blue’ 
classes 
and 
one 
`red’ 
– 
and 
we 
would 
therefore 
class 
the 
point 
as 
`blue’ 
class. 
In 
the 
case 
of 
a 
tie, 
one 
can 
extend 
K 
until 
the 
tie 
is 
broken. 


neighbours 
from 
the 
same 
class, 
their 
inuence 
will 
be 
outvoted. 


If 
we 
assume 
the 
Euclidean 
distance 
as 
the 
dissimilarity 
measure, 
the 
K-Nearest 
Neighbour 
algorithm 
considers 
a 
hypersphere 
centred 
on 
the 
test 
point 
x. 
We 
increase 
the 
radius 
r 
until 
the 
hypersphere 
contains 
exactly 
K 
points 
in 
the 
training 
data. 
The 
class 
label 
c(x) 
is 
then 
given 
by 
the 
most 
numerous 
class 
within 
the 
hypersphere. 


Choosing 
K 


Whilst 
there 
is 
some 
sense 
in 
making 
K> 
1, 
there 
is 
certainly 
little 
sense 
in 
making 
K 
= 
N 
(N 
being 
the 
number 
of 
training 
points). 
For 
K 
very 
large, 
all 
classications 
will 
become 
the 
same 
– 
simply 
assign 
each 
novel 
x 
to 
the 
most 
numerous 
class 
in 
the 
training 
data. 
This 
suggests 
that 
there 
is 
an 
`optimal’ 
intermediate 
setting 
of 
K 
which 
gives 
the 
best 
generalisation 
performance. 
This 
can 
be 
determined 
using 
cross-validation, 
as 
described 
in 
section(13.2.3). 


Example 
67 
(Handwritten 
Digit 
Example). 
Consider 
two 
classes 
of 
handwritten 
digits, 
zeros 
and 
ones. 
Each 
digit 
contains 
28 
× 
28 
= 
784 
pixels. 
The 
training 
data 
consists 
of 
300 
zeros, 
and 
300 
ones, 
a 
subset 
of 
which 
are 
plotted 
in 
g(14.3a,b). 
To 
test 
the 
performance 
of 
the 
nearest 
neighbour 
method 
(based 
on 
Euclidean 
distance) 
we 
use 
an 
independent 
test 
set 
containing 
a 
further 
600 
digits. 
The 
nearest 
neighbour 
method, 
applied 
to 
this 
data, 
correctly 
predicts 
the 
class 
label 
of 
all 
600 
test 
points. 
The 
reason 
for 
the 
high 
success 
rate 
is 
that 
examples 
of 
zeros 
and 
ones 
are 
suciently 
dierent 
that 
they 
can 
be 
easily 
distinguished. 


A 
more 
dicult 
task 
is 
to 
distinguish 
between 
ones 
and 
sevens. 
We 
repeat 
the 
above 
experiment, 
now 
using 
300 
training 
examples 
of 
ones, 
and 
300 
training 
examples 
of 
sevens, 
g(14.3b,c). 
Again, 
600 
new 
test 
examples 
(containing 
300 
ones 
and 
300 
sevens) 
were 
used 
to 
assess 
the 
performance. 
This 
time, 
18 
errors 
are 
found 
using 
nearest 
neighbour 
classication 
– 
a 
3% 
error 
rate 
for 
this 
two 
class 
problem. 
The 
18 
test 
points 
on 
which 
the 
nearest 
neighbour 
method 
makes 
errors 
are 
plotted 
in 
g(14.4). 
If 
we 
use 
K 
= 
3 
nearest 
neighbours, 
the 
classication 
error 
reduces 
to 
14 
– 
a 
slight 
improvement. 
As 
an 
aside, 
the 
best 
machine 
learning 
methods 
classify 
real 
world 
digits 
(over 
all 
10 
classes) 
to 
an 
error 
of 
less 
than 
1% 
(yann.lecun.com/exdb/mnist) 
– 
better 
than 
the 
performance 
of 
an 
`average’ 
human. 


14.3 
A 
Probabilistic 
Interpretation 
of 
Nearest 
Neighbours 
Consider 
the 
situation 
where 
we 
have 
(for 
simplicity) 
data 
from 
two 
classes 
– 
class 
0 
and 
class 
1. 
We 
make 
the 
following 
mixture 
model 
for 
data 
from 
class 
0: 


XX

1 
..

11 


..(x..xn)2=(22)

p(xjc 
= 
0) 
= 
Nx 


xn;2I= 
e 
(14.3.1)

N0

N0 
(22)D=2

n. 
class 
0 
n. 
class 
0 


where 
D 
is 
the 
dimension 
of 
a 
datapoint 
x 
and 
N0 
are 
the 
number 
of 
training 
datapoints 
of 
class 
0, 
and 
2 
is 
the 
variance. 
This 
is 
a 
Parzen 
estimator, 
which 
models 
the 
data 
distribution 
as 
a 
uniform 
weighted 
sum 
of 
distributions 
centred 
on 
the 
training 
points, 
g(14.5). 


DRAFT 
March 
9, 
2010 



A 
Probabilistic 
Interpretation 
of 
Nearest 
Neighbours 



(a) 
(b) 
(c) 
Figure 
14.3: 
Some 
of 
the 
training 
examples 
of 
the 
digit 
zero 
and 
(a), 
one 
(b) 
and 
seven 
(c). 
There 
are 
300 
training 
examples 
of 
each 
of 
these 
three 
digit 
classes. 


Figure 
14.4: 
`1’ 
versus 
`7’ 
classication 
us


ing 
the 
NN 
method. 
(Top) 
The 
18 
out 
of 
600 
test 
examples 
that 
are 
incorrectly 
classi
ed; 
(Bottom) 
the 
nearest 
neighbours 
in 
the 
training 
set 
corresponding 
to 
each 
test-

point 
above. 


Similarly, 
for 
data 
from 
class 
1: 


XX

1 
..

11 


..(x..xn)2=(22)

p(xjc 
= 
1) 
= 
Nx 


xn;2I= 
e 
(14.3.2)

N1

N1 
(22)D=2

n. 
class 
1 
n. 
class 
1 


To 
classify 
a 
new 
datapoint 
x 
, 
we 
use 
Bayes’ 
rule 


p(x 
jc 
= 
0)p(c 
= 
0) 


p(c 
=0jx 
) 
= 
(14.3.3) 


p(xjc 
= 
0)p(c 
= 
0)+ 
p(xjc 
= 
1)p(c 
= 
1) 


The 
Maximum 
Likelihood 
setting 
of 
p(c 
= 
0) 
is 
N0=(N0 
+ 
N1), 
and 
p(c 
= 
1) 
= 
N1=(N0 
+ 
N1). 
An 
analogous 
expression 
to 
equation 
(14.3.3) 
holds 
for 
p(c 
=1jx 
). 
To 
see 
which 
class 
is 
most 
likely 
we 
may 
use 
the 
ratio 


p(c 
=0jx 
) 
p(x 
jc 
= 
0)p(c 
= 
0) 


= 
(14.3.4) 


p(c 
=1jx 
) 
p(x 
jc 
= 
1)p(c 
= 
1) 


If 
this 
ratio 
is 
greater 
than 
one, 
we 
classify 
x 
* 
as 
0, 
otherwise 
1. 


Equation(14.3.4) 
is 
a 
complicated 
function 
of 
x 
* 
. 
However, 
if 
2 
is 
very 
small, 
the 
numerator, 
which 
is 
a 
sum 
of 
exponential 
terms, 
will 
be 
dominated 
by 
that 
term 
for 
which 
datapoint 
xn0 
in 
class 
0 
is 
closest 
to 
the 
point 
x 
* 
. 
Similarly, 
the 
denominator 
will 
be 
dominated 
by 
that 
datapoint 
xn1 
in 
class 
1 
which 
is 
closest 
to 
x 
* 
. 
In 
this 
case, 
therefore, 


..(x 
..xn0 
)2=(22)..(x 
..xn0 
)2=(22)

p(c 
=0jx 
) 
ep(c 
= 
0)=N0 
e

˜ 
= 
(14.3.5) 


p(c 
=1jx) 
e..(x 
..xn1 
)2=(22)p(c 
= 
1)=N1 
e..(x 
..xn1 
)2=(22) 


Taking 
the 
limit 
2 
. 
0, 
with 
certainty 
we 
classify 
x 
* 
as 
class 
0 
if 
x 
* 
has 
a 
point 
in 
the 
class 
0 
data 
which 
is 
closer 
than 
the 
closest 
point 
in 
the 
class 
1 
data. 
The 
nearest 
(single) 
neighbour 
method 
is 
therefore 
recovered 
as 
the 
limiting 
case 
of 
a 
probabilistic 
generative 
model, 
see 
g(14.5). 


The 
motivation 
of 
using 
K 
nearest 
neighbours 
is 
to 
produce 
a 
result 
that 
is 
robust 
against 
unrepresentative 
nearest 
neighbours. 
To 
ensure 
a 
similar 
kind 
of 
robustness 
in 
the 
probabilistic 
interpretation, 
we 
may 
use 
a 
nite 
value 
2 
> 
0. 
This 
smoothes 
the 
extreme 
probabilities 
of 
classication 
and 
means 
that 
more 
points 
(not 
just 
the 
nearest) 
will 
have 
an 
eective 
contribution 
in 
equation 
(14.3.4). 
The 
extension 
to 
more 
than 


DRAFT 
March 
9, 
2010 



Exercises 



Figure 
14.5: 
A 
probabilistic 
interpretation 
of 
nearest 
neighbours. 
For 
each 
class 
we 
use 
a 
mixture 
of 
Gaussians 
to 
model 
the 
data 
from 
that 
class 
p(xjc), 
placing 
at 
each 
training 
point 
an 
isotropic 
Gaussian 
of 
width 
2 
. 
The 
width 
of 
each 
Gaussian 
is 
represented 
by 
the 
circle. 
In 
the 
limit 
2 
. 
0 
a 
novel 
point 
(black) 
is 
assigned 
the 
class 
of 
its 
nearest 
neighbour. 
For 
nite 
2 
> 
0 
the 
inuence 
of 
non-nearest 
neighbours 
has 
an 
eect, 
resulting 
in 
a 
soft 
version 
of 
nearest 
neighbours. 


two 
classes 
is 
straightforward, 
requiring 
a 
class 
conditional 
generative 
model 
for 
each 
class. 


To 
go 
beyond 
nearest 
neighbour 
methods, 
we 
can 
relax 
the 
assumption 
of 
using 
a 
Parzen 
estimator, 
and 
use 
a 
richer 
generative 
model. 
We 
will 
examine 
such 
cases 
in 
some 
detail 
in 
later 
chapters, 
in 
particular 
chapter(20). 


14.3.1 
When 
your 
nearest 
neighbour 
is 
far 
away 
For 
a 
novel 
input 
x 
* 
that 
is 
far 
from 
all 
training 
points, 
Nearest 
Neighbours, 
and 
its 
soft 
probabilistic 
variant 
will 
condently 
classify 
x 
* 
as 
belonging 
to 
the 
class 
of 
the 
nearest 
training 
point. 
This 
is 
arguably 
opposite 
to 
what 
we 
would 
like, 
namely 
that 
the 
classication 
should 
tend 
to 
the 
prior 
probabilities 
of 
the 
class 
based 
on 
the 
number 
of 
training 
data 
per 
class. 
A 
way 
to 
avoid 
this 
problem 
is, 
for 
each 
class, 
to 
include 
a 
ctitious 
mixture 
component 
at 
the 
mean 
of 
all 
the 
data 
with 
large 
variance, 
equal 
for 
each 
class. 
For 
novel 
inputs 
close 
to 
the 
training 
data, 
this 
extra 
ctitious 
datapoint 
will 
have 
no 
appreciable 
eect. 
However, 
as 
we 
move 
away 
from 
the 
high 
density 
regions 
of 
the 
training 
data, 
this 
additional 
ctitious 
component 
will 
dominate. 
Since 
the 
distance 
from 
x 
* 
to 
each 
ctitious 
class 
point 
is 
the 
same, 
in 
the 
limit 
that 
x 
* 
is 
far 
from 
the 
training 
data, 
the 
eect 
is 
that 
no 
class 
information 
from 
the 
position 
of 
x 
* 
occurs. 
See 
section(20.3.3) 
for 
an 
example. 


14.4 
Code 
nearNeigh.m: 
K 
Nearest 
Neighbour 


14.4.1 
Utility 
Routines 
majority.m: 
Find 
the 
majority 
entry 
in 
each 
column 
of 
a 
matrix 


14.4.2 
Demonstration 
demoNearNeigh.m: 
K 
Nearest 
Neighbour 
Demo 


14.5 
Exercises 
Exercise 
157. 
The 
le 
NNdata.mat 
contains 
training 
and 
test 
data 
for 
the 
handwritten 
digits 
5 
and 
9. 
Using 
leave 
one 
out 
cross-validation, 
nd 
the 
optimal 
K 
in 
K-nearest 
neighours, 
and 
use 
this 
to 
compute 
the 
classication 
accuracy 
of 
the 
method 
on 
the 
test 
data. 


Exercise 
158. 
Write 
a 
routine 
SoftNearNeigh(xtrain,xtest,trainlabels,sigma) 
to 
implement 
soft 
nearest 
neighbours, 
analogous 
to 
nearNeigh.m. 
Here 
sigma 
is 
the 
variance 
2 
in 
equation 
(14.3.1). 
As 
above, 
the 
le 
NNdata.mat 
contains 
training 
and 
test 
data 
for 
the 
handwritten 
digits 
5 
and 
9. 
Using 
leave 
one 
out 
cross-validation, 
nd 
the 
optimal 
2 
and 
use 
this 
to 
compute 
the 
classication 
accuracy 
of 
the 
method 
on 
the 
test 
data. 
Hint: 
you 
may 
have 
numerical 
diculty 
with 
this 
method. 
To 
avoid 
this, 


..

b

consider 
using 
the 
logarithm, 
and 
how 
to 
numerically 
compute 
logea 
+ 
efor 
large 
(negative) 
a 
and 
b. 
See 
also 
logsumexp.m. 


DRAFT 
March 
9, 
2010 



Exercises 


Exercise 
159. 
In 
the 
text 
we 
suggested 
the 
use 
of 
the 
Mahalanobis 
distance 


d(x, 
y)=(x 
- 
y)T 
..1 
(x 
- 
y) 


as 
a 
way 
to 
improve 
on 
the 
Euclidean 
distance, 
with 
S 
the 
covariance 
matrix 
of 
the 
combined 
data 
from 
both 
classes. 
Consider 
a 
modication 
based 
on 
using 
a 
mixture 
model 


X

1 


xn 
, 
0)

p(xjc 
= 
0) 
= 
N 
(x 


N0

n2class 
0 


and 


X

1 


xn 
, 
1)

p(xjc 
= 
1) 
= 
N 
(x 


N1

n2class 
1 


1. 
Explain 
how 
the 
soft 
Nearest 
Neighbours 
algorithm 
can 
deal 
with 
the 
issue 
that 
the 
distribution 
of 
data 
from 
the 
dierent 
classes 
can 
be 
very 
dierent. 
2. 
For 
the 
case 
0 
= 
20and 
1 
= 
20and 
2 
small, 
derive 
a 
simple 
expression 
that 
approximates 
01 




p(c 
=0jx 
)
log
p(c 
=1jx 
)


Exercise 
160. 
The 
editor 
at 
YoMan! 
(a 
`mens’ 
magazine) 
has 
just 
had 
a 
great 
idea. 
Based 
on 
the 
success 
of 
a 
recent 
national 
poll 
to 
test 
IQ, 
she 
decides 
to 
make 
a 
`Beauty 
Quotient’ 
(BQ) 
test. 
She 
collects 
as 
many 
images 
of 
male 
faces 
as 
she 
can, 
taking 
care 
to 
make 
sure 
that 
all 
the 
images 
are 
scaled 
to 
roughly 
the 
same 
size 
and 
under 
the 
same 
lighting 
conditions. 
She 
then 
gives 
each 
male 
face 
a 
BQ 
score 
from 
0 
(`Severely 
Aesthetically 
Challenged') 
to 
100 
(`Generously 
Aesthetically 
Gifted'). 
Thus, 
for 
each 
image 
x, 
there 
is 
an 
associated 
value 
b 
in 
the 
range 
0 
to 
100. 
In 
total 
she 
collects 
N 
images 
and 
associated 
scores, 
f(xn;bn) 
;n 
=1;:::;Ng, 
where 
each 
image 
is 
represented 
by 
a 
D-dimensional 
real-valued 
vector 
x. 
One 
morning, 
she 
bounces 
into 
your 
oce 
and 
tells 
you 
the 
good 
news 
: 
it 
is 
your 
task 
to 
make 
a 
test 
for 
the 
male 
nation 
to 
determine 
their 
Beauty 
Quotient. 
The 
idea, 
she 
explains, 
is 
that 
a 
man 
can 
send 
online 
an 
image 
of 
their 
face 
x 
, 
to 
YoMan! 
and 
will 
`instantly’ 
receive 
an 
automatic 
BQ 
response 
b* 
. 


1. 
As 
a 
rst 
step, 
you 
decide 
to 
use 
the 
K 
nearest 
neighbour 
method 
(KNN) 
to 
assign 
a 
BQ 
score 
b* 
to 
a 
novel 
test 
image 
x 
* 
. 
Describe 
how 
to 
determine 
the 
optimal 
number 
of 
neighbours 
K 
to 
use. 


2. 
Your 
line 
manager 
is 
pleased 
with 
your 
algorithm 
but 
is 
disappointed 
that 
it 
does 
not 
provide 
any 
simple 
explanation 
of 
Beauty 
that 
she 
can 
present 
in 
a 
future 
version 
of 
YoMan! 
magazine. 
To 
address 
this, 
you 
decide 
to 
make 
a 
model 
based 
on 
linear 
regression. 
That 
is 


T

b 
= 
wx 
(14.5.1) 


where 
w 
is 
a 
parameter 
(weight) 
vector 
chosen 
to 
minimise 


X2 


T

E(w)=bn 
- 
wxn 


n 


(a) 
After 
training 
(nding 
a 
suitable 
w), 
how 
can 
YoMan! 
explain 
to 
its 
readership 
in 
a 
simple 
way 
what 
facial 
features 
are 
important 
for 
determining 
one's 
BQ? 
(b) 
Describe 
fully 
and 
mathematically 
a 
method 
to 
train 
this 
linear 
regression 
model. 
Your 
explan
ation 
must 
be 
detailed 
enough 
so 
that 
a 
programmer 
can 
directly 
implement 
it. 
(c) 
Discuss 
any 
implications 
of 
the 
situation 
D>N. 
(d) 
Discuss 
any 
advantages/disadvantages 
of 
using 
the 
linear 
regression 
model 
compared 
with 
using 
the 
KNN 
approach. 
DRAFT 
March 
9, 
2010 



CHAPTER 
15 


Unsupervised 
Linear 
Dimension 
Reduction 


15.1 
High-Dimensional 
Spaces 
– 
Low 
Dimensional 
Manifolds 
In 
machine 
learning 
problems 
data 
is 
often 
high 
dimensional 
– 
images, 
bag-of-word 
descriptions, 
geneexpresssions 
etc. 
In 
such 
cases 
we 
cannot 
expect 
the 
training 
data 
to 
densely 
populate 
the 
space, 
meaning 
that 
there 
will 
be 
large 
parts 
in 
which 
little 
is 
known 
about 
the 
data. 
For 
the 
hand-written 
digits 
from 
chapter(14), 
the 
data 
is 
784 
dimensional. 
For 
binary 
valued 
pixels 
the 
possible 
number 
of 
images 
that 
could 
ever 
exist 
is 
2784 
˜ 
10236 
. 
Nevertheless, 
we 
would 
expect 
that 
only 
a 
handful 
of 
examples 
of 
a 
digit 
should 
be 
sucient 
(for 
a 
human) 
to 
understand 
how 
to 
recognise 
a 
7. 
Digit-like 
images 
must 
therefore 
occupy 
a 
highly 
constrained 
subspace 
of 
the 
784 
dimensions 
and 
we 
expect 
only 
a 
small 
number 
of 
directions 
to 
be 
relevant 
for 
describing 
the 
data 
to 
a 
reasonable 
accuracy. 
Whilst 
the 
data 
vectors 
may 
be 
very 
high 
dimensional, 
they 
will 
therefore 
typically 
lie 
close 
to 
a 
much 
lower 
dimensional 
`manifold’ 
(informally, 
a 
two-dimensional 
manifold 
corresponds 
to 
a 
warped 
sheet 
of 
paper 
embedded 
in 
a 
high 
dimensional 
space), 
meaning 
that 
the 
distribution 
of 
the 
data 
is 
heavily 
constrained. 


Here 
we 
concentrate 
on 
linear 
dimension 
reduction 
techniques 
for 
which 
there 
exist 
computationally 
ecient 
approaches. 
In 
this 
approach 
a 
high 
dimensional 
datapoint 
x 
is 
`projected 
down’ 
to 
a 
lower 
dimensional 
vector 
y 
by 


y 
= 
Fx 
+ 
const. 
(15.1.1) 


where 
the 
non-square 
matrix 
F 
has 
dimensions 
dim 
(y) 
× 
dim 
(x), 
with 
dim 
(y) 
< 
dim 
(x). 
The 
methods 
in 
this 
chapter 
are 
largely 
non-probabilistic, 
although 
many 
have 
natural 
probabilistic 
interpretations. 
For 
example, 
PCA 
is 
closely 
related 
to 
Factor 
Analysis, 
described 
in 
chapter(21). 


15.2 
Principal 
Components 
Analysis 
If 
data 
lies 
close 
to 
a 
hyperplane, 
as 
in 
g(15.1) 
we 
can 
accurately 
approximate 
each 
data 
point 
by 
using 
vectors 
that 
span 
the 
hyperplane 
alone. 
Eectively, 
we 
are 
trying 
to 
discover 
a 
low 
dimensional 
co-ordinate 
system 
in 
which 
we 
can 
approximately 
represent 
the 
data. 
We 
express 
the 
approximation 
for 
datapoint 


xn 


as 


M

M 


n

xn 
˜ 
c 
+ 
y 
bi 
= 
x~n 
(15.2.1)

i 
i=1 


Here 
the 
vector 
c 
is 
a 
constant 
and 
denes 
a 
point 
in 
the 
hyperplane 
and 
the 
bi 
dene 
vectors 
in 
the 
hyn


perplane 
(also 
known 
as 
`principal 
component 
coecients’ 
or 
`loadings'). 
The 
yare 
the 
low 
dimensional 


i 
co-ordinates 
of 
the 
data. 
Equation(15.2.1) 
expresses 
how 
to 
nd 
the 
reconstruction 
x~n 
given 
the 
lower 
n

dimensional 
representation 
yn 
(which 
has 
components 
y;i 
=1;:::;M). 
For 
a 
data 
space 
of 
dimension 


i 


279 



Principal 
Components 
Analysis 


-2024-2024-10123
Figure 
15.1: 
In 
linear 
dimension 
reduction 
a 
hyperplane 
is 
tted 
such 
that 
the 
average 
distance 
between 
datapoints 
(red 
rings) 
and 
their 
projections 
onto 
the 
plane 
(black 
dots) 
is 
minimal. 


dim 
(x)= 
D, 
we 
hope 
to 
accurately 
describe 
the 
data 
using 
only 
a 
small 
number 
M 
« 
D 
of 
co-ordinates 
y. 


To 
determine 
the 
best 
lower 
dimensional 
representation 
it 
is 
convenient 
to 
use 
the 
square 
distance 
error 
between 
x 
and 
its 
reconstruction 
x~: 


XX

E(B, 
Y, 
c)= 
[x 


D
N


ni

- 
x˜


ni

]2 


(15.2.2) 
n=1 
i=1 


P

It 
is 
straightforward 
to 
show 
that 
the 
optimal 
bias 
c 
is 
given 
by 
the 
mean 
of 
the 
data 


P

xn=N. 
We 


n 


therefore 
assume 
that 
the 
data 
has 
been 
centred 
(has 
zero 
mean 


xn 
= 
0), 
so 
that 
we 
can 
set 
c 
to 
zero, 


n 


and 
concentrate 
on 
nding 
the 
optimal 
basis 
B 
below. 


15.2.1 
Deriving 
the 
optimal 
linear 
reconstruction 


= 
b


j
i

b1 
;:::, 
bM

(dening 
[B]i;j 


) 
and 
corresponding 
low 
dimensional 


N

To 
nd 
the 
best 
basis 
vectors 
B 
=



coordinates 
Y 
= 
y1 


, 
we 
may 
minimize 
the 
sum 
of 
squared 
dierences 
between 
each 
vector 
x


;:::, 
y


M

D

~anditsreconstruction 
:x
23N

2 


XXX

 


j
i

(X 
- 
BY)T 
(X 
- 
BY)

4x 


.


E(B, 
Y)= 


(15.2.3)
ni

n
j

= 
trace


- 


y 


b


n=1 
i=1 
j=1 




where 
X 
= 
x1 
;:::, 
xN. 


Consider 
a 
transformation 
Q 
of 
the 
basis 
B 
so 
that 
B~
= 
BQ 
is 
an 
orthonormal 
matrix, 
B~
TB~
= 
I. 
Since 
~B~
~

Q 
is 
invertible, 
we 
may 
write 
BY 
= 
BQ..1Y 
= 
Y, 
which 
is 
of 
then 
same 
form 
as 
BY, 
albeit 
with 
an 
orthonormality 
constraint 
on 
B~
. 
Hence, 
without 
loss 
of 
generality, 
we 
may 
consider 
equation 
(15.2.3) 
under 
the 
orthonormality 
constraint 
BTB 
= 
I, 
namely 
that 
the 
basis 
vectors 
are 
mutually 
orthogonal 
and 
of 
unit 
length. 


By 
dierentiating 
equation 
(15.2.3) 
with 
respect 
to 
y


n
k

we 
obtain 
(using 
the 
orthonormality 
constraint) 


23XXXXXX

1 
. 


4x 


ni

- 


y 


n
j

b


j
i 


5b


k
i 


= 


x 


ni

b


ki

- 


y 


n
j

b


j
i

b


k
i 


= 


x 


ni

b


ki

- 
y 


n
k

E(B, 
Y)= 


-


2 
@y


n
k

j

i

j

i

i

i

|{z}

jk 


The 
squared 
error 
E(B, 
Y) 
therefore 
has 
zero 
derivative 
when 


X

y 


n
k 


= 
b


ki

x 


ni

(15.2.4) 
DRAFT 
March 
9, 
2010 



Principal 
Components 
Analysis 


We 
now 
substitute 
this 
solution 
into 
equation 
(15.2.3) 
to 
write 
the 
squared 
error 
only 
as 
a 
function 
of 
B. 
Using

XXX

njjjn 
n 


yj 
bi 
=bi 
bkxk 
=Bi;jBk;jxk 
=[BBTxn]i 
(15.2.5) 


j 
j;k 
j;k 


The 
objective 
E(B) 
becomes 


X

2 


E(B)=I 
- 
BBT 
xn(15.2.6) 


n

..2

SinceI 
- 
BBT 
= 
I 
- 
BBT, 
(using 
BTB 
= 
I) 




XX

E(B)=(xn)TI 
- 
BBT 
xn 
= 
trace 
(xn)(xn)TI 
- 
BBT 
(15.2.7) 


nn 


Hence 
the 
objective 
becomes 


hXiX

E(B)=(N 
- 
1) 
trace 
(S) 
- 
traceSBBT 
(15.2.8) 


where 
S 
is 
the 
sample 
covariance 
matrix 
of 
the 
data1 


XX

1 
N1 
N

m 
= 
xn 
, 
S 
=(xn 
- 
m)(xn 
- 
m)T 
(15.2.9)

NN 
- 
1 


n=1 
n=1 


To 
minimise 
equation 
(15.2.8) 
under 
the 
constraint 
BTB 
= 
I 
we 
use 
a 
set 
of 
Lagrange 
multipliers 
L, 
so 
that 
the 
objective 
is 
to 
minimize 


XX

..traceSBBT 
+ 
traceLBTB 
- 
I 
(15.2.10) 


(neglecting 
the 
constant 
prefactor 
N 
- 
1). 
Since 
the 
constraint 
is 
symmetric, 
we 
can 
assume 
that 
L 
is 
also 
symmetric. 
Dierentiating 
with 
respect 
to 
B 
and 
equating 
to 
zero 
we 
obtain 
that 
at 
the 
optimum 


SB 
= 
BL 
(15.2.11) 


This 
is 
a 
form 
of 
eigen-equation 
so 
that 
a 
solution 
is 
given 
by 
taking 
L 
to 
be 
diagonal 
and 
B 
as 
the 
matrix 


..X

whose 
columns 
are 
the 
corresponding 
eigenvectors 
of 
S. 
In 
this 
case, 
traceSBBT 
= 
trace 
(L), 
which 
is 
the 
sum 
of 
the 
eigenvalues 
corresponding 
to 
the 
eigenvectors 
forming 
B. 
Since 
we 
wish 
to 
minimise 
E(B), 
we 
take 
the 
eigenvectors 
with 
largest 
corresponding 
eigenvalues. 


If 
we 
order 
the 
eigenvalues 
1 
= 
2;:::, 
the 
squared 
error 
is 
given 
by, 
from 
equation 
(15.2.8) 


1 
DMD

XXX

E(B) 
= 
trace 
(S) 
- 
trace 
(L)= 
i 
- 
i 
= 
i 
(15.2.12)

N 
- 
1

i=1 
i=1 
i=M+1 


Whilst 
the 
solution 
to 
this 
eigen-problem 
is 
unique, 
this 
only 
serves 
to 
dene 
the 
solution 
subspace 
since 
one 
may 
rotate 
and 
scale 
B 
and 
Y 
such 
that 
the 
value 
of 
the 
squared 
loss 
is 
exactly 
the 
same. 
The 
justication 
for 
choosing 
the 
non-rotated 
eigen 
solution 
is 
given 
by 
the 
additional 
requirement 
that 
the 
principal 
components 
corresponds 
to 
directions 
of 
maximal 
variance, 
as 
explained 
in 
section(15.2.2). 


1Here 
we 
use 
the 
unbiased 
sample 
covariance, 
simply 
because 
this 
is 
standard 
in 
the 
literature. 
If 
we 
were 
to 
replace 
this 
with 
the 
sample 
covariance 
as 
dened 
in 
chapter(8), 
the 
only 
change 
required 
is 
to 
replace 
N 
- 
1 
by 
N 
throughout, 
which 
has 
no 
eect 
on 
the 
form 
of 
the 
solutions 
found 
by 
PCA. 


DRAFT 
March 
9, 
2010 
281 



Principal 
Components 
Analysis 


-2-10123-2-1.5-1-0.500.511.52
Figure 
15.2: 
Projection 
of 
two 
dimensional 
data 
using 
one 
dimensional 
PCA. 
Plotted 
are 
the 
original 
datapoints 
x 
(larger 
rings) 
and 
their 
reconstructions 
x~
(small 
dots) 
using 
1 
dimensional 
PCA. 
The 
lines 
represent 
the 
orthogonal 
projection 
of 
the 
original 
datapoint 
onto 
the 
rst 
eigenvector. 
The 
arrows 
are 
the 
two 
eigenvectors 
scaled 
by 
the 
square 
root 
of 
their 
corresponding 
eigenvalues. 
The 
data 
has 
been 
centred 
to 
have 
zero 
mean. 
For 
each 
`high 
dimensional’ 
datapoint 
x, 
the 
`low 
dimensional’ 
representation 
y 
is 
given 
in 
this 
case 
by 
the 
distance 
(possibly 
negative) 
from 
the 
origin 
along 
the 
rst 
eigenvector 
direction 
to 
the 
corresponding 
orthogonal 
projection 
point. 


15.2.2 
Maximum 
variance 
criterion 
We 
search 
rst 
for 
the 
single 
direction 
b 
such 
that, 
when 
the 
data 
is 
projected 
onto 
this 
direction, 
the 
variance 
of 
this 
projection 
is 
maximal 
amongst 
all 
possible 
such 
projections. 
Using 
equation 
(15.2.4) 
for 
a 
single 
vector 
b 
we 
have 


X

nn 


y 
=bix 
(15.2.13)

i 
i 


The 
projection 
of 
a 
datapoint 
onto 
a 
direction 
b 
is 
bTxn 
for 
a 
unit 
length 
vector 
b. 
Hence 
the 
sum 
of 
squared 
projections 
is

X2 
X

bTxn 
= 
bT 
xn 
(xn)T 
b 
=(N 
- 
1)bTSb 
= 
(N 
- 
1) 
(15.2.14) 


nn 


which, 
ignoring 
constants, 
is 
simply 
the 
negative 
of 
equation 
(15.2.8) 
for 
a 
single 
retained 
eigenvector 
b 
(with 
Sb 
= 
b). 
Hence 
the 
optimal 
single 
b 
which 
maximises 
the 
projection 
variance 
is 
given 
by 
the 
eigenvector 
corresponding 
to 
the 
largest 
eigenvalue 
of 
S. 
Under 
the 
criterion 
that 
the 
next 
optimal 
direction 
should 
be 
orthonormal 
to 
the 
rst, 
one 
can 
readily 
show 
that 
b(2) 
is 
given 
by 
the 
second 
largest 
eigenvector, 
and 
so 
on. 
This 
explains 
why, 
despite 
the 
squared 
loss 
equation 
(15.2.8) 
being 
invariant 
with 
respect 
to 
arbitrary 
rotation 
(and 
scaling) 
of 
the 
basis 
vectors, 
the 
ones 
given 
by 
the 
eigen-decomposition 
have 
the 
additional 
property 
that 
they 
correspond 
to 
directions 
of 
maximal 
variance. 
These 
maximal 
variance 
directions 
found 
by 
PCA 
are 
called 
the 
principal 
directions. 


15.2.3 
PCA 
algorithm 
The 
routine 
for 
PCA 
is 
presented 
in 
algorithm(14). 
In 
the 
notation 
of 
y 
= 
Fx, 
the 
projection 
matrix 
F 
corresponds 
to 
ET 
. 
Similarly 
for 
the 
reconstruction 
equation 
(15.2.1), 
the 
coordinate 
yn 
corresponds 
to 
ETxn 
and 
bi 
corresponds 
to 
ei 
. 
The 
PCA 
reconstructions 
are 
orthogonal 
projections 
of 
the 
data 
onto 
the 
subspace 
spanned 
by 
the 
eigenvectors 
corresponding 
to 
the 
M 
largest 
eigenvalues 
of 
the 
covariance 
matrix, 
see 
g(15.2). 



Figure 
15.3: 
Top 
row 
: 
a 
selection 
of 
the 
digit 
5 
taken 
from 
the 
database 
of 
892 
examples. 
Plotted 
beneath 
each 
digit 
is 
the 
reconstruction 
using 
100, 
30 
and 
5 
eigenvectors 
(from 
top 
to 
bottom). 
Note 
how 
the 
reconstructions 
for 
fewer 
eigenvectors 
express 
less 
variability 
from 
each 
other, 
and 
resemble 
more 
a 
mean 
5 
digit. 


DRAFT 
March 
9, 
2010 



Principal 
Components 
Analysis 


Algorithm 
14 
Principal 
Components 
Analysis 
to 
form 
an 
M-dimensional 
approximation 
of 
a 
dataset 


n

fxn;n 
=1;:::;Ng, 
with 
dim 
x= 
D. 


1: 
Find 
the 
D 
× 
1 
sample 
mean 
vector 
and 
D 
× 
D 
covariance 
matrix 
N 
X

1 
N1 
N

m 
= 
xn 
, 
S 
=(xn 
- 
m)(xn 
- 
m)T 


NN 
- 
1 


n=1 
n=1 


2: 
Find 
the 
eigenvectors 
e1 
;:::, 
eM 
of 
the 
covariance 
matrix 
S, 
sorted 
so 
that 
the 
eigenvalue 
of 
ei 
is 
larger 
than 
ej 
for 
i<j. 
Form 
the 
matrix 
E 
=[e1 
;:::, 
eM 
]. 
3: 
The 
lower 
dimensional 
representation 
of 
each 
data 
point 
xn 
is 
given 
by 
yn 
= 
ET(xn 
- 
m) 
(15.2.15) 
4: 
The 
approximate 
reconstruction 
of 
the 
original 
datapoint 
xn 
is 
xn 
˜ 
m 
+ 
Eyn 
(15.2.16) 
5: 
The 
total 
squared 
error 
over 
all 
the 
training 
data 
made 
by 
the 
approximation 
is 
N 
D

N 
N 


(xn 
- 
x~n)2 
=(N 
- 
1) 
j 
(15.2.17) 
n=1 
j=M+1 


where 
M+1 
:::N 
are 
the 
eigenvalues 
discarded 
in 
the 
projection. 


Example 
68 
(Reducing 
the 
dimension 
of 
digits). 
We 
have 
892 
examples 
of 
handwritten 
5's, 
where 
each 
image 
consists 
of 
28 
× 
28 
real-values 
pixels, 
see 
g(15.3). 
Each 
image 
matrix 
is 
stacked 
to 
form 
a 
784 
dimensional 
vector, 
giving 
a 
784 
× 
892 
dimensional 
data 
matrix 
X. 
The 
covariance 
matrix 
of 
this 
data 
has 
eigenvalue 
spectrum 
as 
plotted 
in 
g(15.4), 
where 
we 
plot 
only 
the 
100 
largest 
eigenvalues. 
Note 
how 
after 
around 
40 
components, 
the 
mean 
squared 
reconstruction 
error 
is 
small, 
indicating 
that 
the 
data 
indeed 
lie 
close 
to 
a 
40 
dimensional 
hyperplane. 
The 
eigenvalues 
are 
computed 
using 
pca.m. 


The 
reconstructions 
using 
dierent 
numbers 
of 
eigenvectors 
(100, 
30 
and 
5) 
are 
plotted 
in 
g(15.3). 
Note 
how 
using 
only 
a 
small 
number 
of 
eigenvectors, 
the 
reconstruction 
more 
closely 
resembles 
the 
mean 
image. 


Example 
69 
(Eigenfaces). 
In 
g(15.5) 
we 
present 
example 
images 
for 
which 
we 
wish 
to 
nd 
a 
lower 
dimensional 
representation. 
Using 
PCA 
the 
rst 
49 
`eigenfaces’ 
are 
presented 
along 
with 
reconstructions 
of 
the 
original 
data 
using 
these 
eigenfaces, 
see 
g(15.6). 


02040608010000.20.40.60.81eigenvalue numbereigenvalue
Figure 
15.4: 
For 
the 
digits 
data 
consisting 
of 
892 
examples 
of 
the 
digit 
5, 
each 
image 
being 
represented 
by 
a 
784 
dimensional 
vector. 
Plotted 
as 
the 
largest 
100 
eigenvalues 
(scaled 
so 
that 
the 
largest 
eigenvalue 
is 
1) 
of 
the 
sample 
covariance 
matrix. 


DRAFT 
March 
9, 
2010 



Principal 
Components 
Analysis 



Figure 
15.5: 
100 
of 
the 
120 
training 
images 
(40 
people, 
with 
3 
images 
of 
each 
person). 
Each 
image 
consists 
of 
92112 
= 
10304 
greyscale 
pixels. 
The 
training 
data 
is 
scaled 
so 
that, 
represented 
as 
an 
image, 
the 
components 
of 
each 
image 
sum 
to 
1. 
The 
average 
value 
of 
each 
pixel 
across 
all 
images 
is 
9:7010..5 
. 
This 
is 
a 
subset 
of 
the 
400 
images 
in 
the 
full 
Olivetti 
Research 
Face 
Database. 



Figure 
15.6: 
(a): 
SVD 
reconstruction 
of 
the 
images 
in 
g(15.5) 
using 
a 
combination 
of 
the 
49 
eigen-images. 
(b): 
The 
eigen-images 
are 
found 
using 
SVD 
of 
the 
g(15.5) 
and 
taking 
the 
49 
eigenvectors 
with 
largest 
eigenvalue. 
The 
images 
corresponding 
to 
the 
largest 
eigenvalues 
are 
contained 
in 
the 
rst 
row, 
and 
the 
next 
7 
in 
the 
row 
below, 
etc. 
The 
root 
mean 
square 
reconstruction 
error 
is 


1:121 
× 
10..5 
, 
a 
small 
improvement 
over 
PLSA 
(see 
g(15.14)). 
(a) 
(b) 
15.2.4 
PCA 
and 
nearest 
neighbours 
For 
high-dimensional 
data 
computing 
the 
squared 
Euclidean 
distance 
between 
vectors 
can 
be 
expensive, 
and 
also 
sensitive 
to 
noise. 
It 
is 
therefore 
often 
useful 
to 
project 
the 
data 
to 
form 
a 
lower 
dimensional 
representation 
rst. 
For 
example, 
in 
making 
a 
classier 
to 
distinguish 
between 
the 
digit 
1 
and 
the 
digit 
7, 
example(67), 
we 
can 
form 
a 
lower 
dimensional 
representation 
rst 
by 
ignoring 
the 
class 
label 
(to 
make 
a 
dataset 
of 
1200 
training 
points). 
Each 
of 
the 
training 
points 
xn 
is 
then 
projected 
to 
a 
lower 
dimensional 
PCA 
representation 
yn 
. 
Subsequently, 
any 
distance 
calculations 
(xa 
- 
xb)2 
are 
replaced 
by 
(ya 
- 
yb)2 
. 
To 
justify 
this, 
consider 


(xa 
- 
xb)T(xa 
- 
xb) 
˜ 
(Eya 
+ 
m 
- 
Eyb 
- 
m)T(Eya 
+ 
m 
- 
Eyb 
- 
m) 
=(ya 
- 
yb)TETE(ya 
- 
yb) 
=(ya 
- 
yb)T(ya 
- 
yb) 
(15.2.18) 


where 
the 
last 
equality 
is 
due 
to 
the 
orthonormality 
of 
eigenvectors 
: 
ETE 
= 
I. 


Using 
19 
principal 
components 
(see 
example(70) 
why 
this 
number 
was 
chosen) 
and 
the 
nearest 
neighbour 
rule 
to 
classify 
ones 
and 
sevens 
gave 
a 
test-set 
error 
of 
14 
in 
600 
examples, 
compared 
to 
18 
from 
the 
standard 
method 
on 
the 
non-projected 
data. 
How 
can 
it 
be 
that 
the 
classication 
performance 
has 
improved? 
A 
plausible 
explanation 
is 
that 
the 
new 
PCA 
representation 
of 
the 
data 
is 
more 
robust 
since 
only 
the 
large 
scale 
change 
directions 
in 
the 
space 
are 
retained, 
with 
low 
variance 
directions 
discarded. 


DRAFT 
March 
9, 
2010 



High 
Dimensional 
Data 


020406080100024681012number of eigenvaluesnumber of errors
Figure 
15.7: 
Finding 
the 
optimal 
PCA 
dimension 
to 
use 
for 
classifying 
hand-written 
digits 
using 
nearest 
neighbours. 
400 
training 
examples 
are 
used, 
and 
the 
validation 
error 
plotted 
on 
200 
further 
examples. 
Based 
on 
the 
validation 
error, 
we 
see 
that 
a 
dimension 
of 
19 
is 
reasonable. 


Example 
70 
(Finding 
the 
best 
PCA 
dimension). 
There 
are 
600 
examples 
of 
the 
digit 
1 
and 
600 
examples 
of 
the 
digit 
7. 
We 
will 
use 
half 
the 
data 
for 
training 
and 
the 
other 
half 
for 
testing. 
The 
600 
training 
examples 
were 
further 
split 
into 
a 
training 
set 
of 
400 
examples, 
and 
a 
separate 
validation 
set 
of 
200 
examples. 
PCA 
was 
used 
to 
reduce 
the 
dimensionality 
of 
the 
inputs, 
and 
then 
nearest 
neighbours 
used 
to 
classify 
the 
200 
validation 
examples. 
Dierent 
reduced 
dimensions 
were 
in 
vestigated 
and, 
based 
on 
the 
validation 
results, 
19 
was 
selected 
as 
the 
optimal 
number 
of 
PCA 
components 
retained, 
see 
g(15.7). 
The 
independent 
test 
error 
on 
600 
independent 
examples 
using 
19 
dimensions 
is 
14. 


15.2.5 
Comments 
on 
PCA 
The 
`intrinsic’ 
dimension 
of 
data 


How 
many 
dimensions 
should 
the 
linear 
subspace 
have? 
From 
equation 
(15.2.12), 
the 
reconstruction 
error 
is 
proportional 
to 
the 
sum 
of 
the 
discarded 
eigenvalues. 
If 
we 
plot 
the 
eigenvalue 
spectrum 
(the 
set 
of 
eigenvalues 
ordered 
by 
decreasing 
value), 
we 
might 
hope 
to 
see 
a 
few 
large 
values 
and 
many 
small 
values. 
If 
the 
data 
does 
lie 
close 
to 
an 
M 
dimensional 
hyperplane, 
we 
would 
see 
M 
large 
eigenvalues 
with 
the 
rest 
being 
very 
small. 
This 
would 
give 
an 
indication 
of 
the 
number 
of 
degrees 
of 
freedom 
in 
the 
data, 
or 
the 
intrinsic 
dimensionality. 
Directions 
corresponding 
to 
the 
small 
eigenvalues 
are 
then 
interpreted 
as 
`noise'. 


Non-linear 
dimension 
reduction 


In 
PCA 
we 
are 
presupposing 
that 
the 
data 
lies 
close 
to 
a 
hyperplane. 
Is 
this 
really 
a 
good 
description? 
More 
generally, 
we 
would 
expect 
data 
to 
lie 
on 
low 
dimensional 
curved 
manifolds. 
Also, 
data 
is 
often 
clustered 
– 
examples 
of 
handwritten 
`4's 
look 
similar 
to 
each 
other 
and 
form 
a 
cluster, 
separate 
from 
the 
`8's 
cluster. 
Nevertheless, 
since 
linear 
dimension 
reduction 
is 
computationally 
relatively 
straightforward, 
this 
is 
one 
of 
the 
most 
common 
dimensionality 
reduction 
techniques. 


15.3 
High 
Dimensional 
Data 
..z 


The 
computational 
complexity 
of 
computing 
an 
eigen-decomposition 
of 
a 
D 
× 
D 
matrix 
is 
OD3 
. 
You 
might 
be 
wondering 
therefore 
how 
it 
is 
possible 
to 
perform 
PCA 
on 
high 
dimensional 
data. 
For 
example, 
if 
we 
have 
500 
images 
each 
of 
1000 
× 
1000 
= 
106 
pixels, 
the 
covariance 
matrix 
will 
be 
106 
× 
106 
dimensional. 
It 
would 
appear 
a 
signicant 
computational 
challenge 
to 
compute 
the 
eigen-decomposition 
of 
this 
matrix. 
In 
this 
case, 
however, 
since 
there 
are 
only 
500 
such 
vectors, 
the 
number 
of 
non-zero 
eigenvalues 
cannot 


..z 


exceed 
500. 
One 
can 
exploit 
this 
fact 
to 
bound 
the 
complexity 
by 
O 
min(D, 
N)3 
, 
as 
described 
below. 


DRAFT 
March 
9, 
2010 



High 
Dimensional 
Data 


15.3.1 
Eigen-decomposition 
for 
N<D 
First 
note 
that 
for 
zero 
mean 
data, 
the 
sample 
covariance 
matrix 
can 
be 
expressed 
as 


X

1 
N

nn

[S]ij 
= 
xi 
xj 
(15.3.1)

N 
- 
1 


n=1 


In 
matrix 
notation 
this 
can 
be 
written 
1

S 
= 
XXT 
(15.3.2)

N 
- 
1where 
the 
D 
× 
N 
matrix 
X 
contains 
all 
the 
data 
vectors: 




1 
N

X 
=x;:::, 
x(15.3.3) 


Since 
the 
eigenvectors 
of 
a 
matrix 
M 
are 
equal 
to 
those 
of 
M 
for 
scalar 
, 
one 
can 
consider 
more 
simply 
the 
eigenvectors 
of 
XXT 
. 
Writing 
the 
D 
× 
N 
matrix 
of 
eigenvectors 
as 
E 
(this 
is 
a 
non-square 
thin 
matrix 
since 
there 
will 
be 
fewer 
eigenvalues 
than 
data 
dimensions) 
and 
the 
eigenvalues 
as 
an 
N 
× 
N 
diagonal 
matrix 
, 
the 
eigen-decomposition 
of 
the 
covariance 
S 
is 


~

XXTE 
= 
E. 
. 
XTXXTE 
= 
XTE. 
. 
XTXE~
= 
E. 
(15.3.4) 
where 
we 
dened 
E~
= 
XTE. 
The 
nal 
expression 
above 
represents 
the 
eigenvector 
equation 
for 
XTX.

..

This 
is 
a 
matrix 
of 
dimensions 
N 
N 
so 
that 
calculating 
the 
eigen-decomposition 
takes 
ON3 
operations, 


..

compared 
with 
OD3 
operations 
in 
the 
original 
high-dimensional 
space 
. 
We 
then 
can 
therefore 
calculate 
the 
eigenvectors 
E~
and 
eigenvalues 
. 
of 
this 
matrix 
more 
easily. 
Once 
found, 
we 
use 
the 
fact 
that 
the 
eigenvalues 
of 
S 
are 
given 
by 
the 
diagonal 
entries 
of 
. 
and 
the 
eigenvectors 
by 


E 
= 
X 
~(15.3.5)

E..1 


15.3.2 
PCA 
via 
Singular 
value 
decomposition 
An 
alternative 
to 
using 
an 
eigen-decomposition 
routine 
to 
nd 
the 
PCA 
solution 
is 
to 
make 
use 
of 
the 
Singular 
Value 
Decomposition 
(SVD) 
of 
an 
D 
× 
N 
dimensional 
matrix 
X. 
This 
is 
given 
by 


X 
= 
UDVT 
(15.3.6) 
where 
UTU 
= 
ID 
and 
VTV 
= 
IN 
and 
D 
is 
a 
diagonal 
matrix 
of 
the 
(positive) 
singular 
values. 
We 
assume 
that 
the 
decomposition 
has 
ordered 
the 
singular 
values 
so 
that 
the 
upper 
left 
diagonal 
element 
of 
D 
contains 
the 
largest 
singular 
value. 
The 
matrix 
XXT 
can 
then 
be 
written 
as 


XXT 
= 
UDVTVDUT 
= 
UD2UT 
(15.3.7) 
Since 
UD2UT 
is 
in 
the 
form 
of 
an 
eigen-decomposition, 
the 
PCA 
solution 
is 
equivalently 
given 
by 
per


forming 
the 
SVD 
decomposition 
of 
X, 
for 
which 
the 
eigenvectors 
are 
then 
given 
by 
U, 
and 
corresponding 
eigenvalues 
by 
the 
square 
of 
the 
singular 
values. 
Equation(15.3.6) 
shows 
that 
PCA 
is 
a 
form 
of 
matrix 
decomposition 
method: 


X 
= 
UDVT 
˜ 
UM 
DM 
VT 
(15.3.8)

M 
where 
UM 
, 
DM 
, 
VM 
correspond 
to 
taking 
only 
the 
rst 
M 
singular 
values 
of 
the 
full 
matrices. 


DRAFT 
March 
9, 
2010 



Latent 
Semantic 
Analysis 


Figure 
15.8: 
Document 
data 
for 
a 
dictionary 
contain


20040060080010001200246810
ing 
10 
words 
and 
1200 
documents. 
Black 
indicates 
that 
a 
word 
was 
present 
in 
a 
document. 
The 
data 
consists 
of 
two 
`similar’ 
topics 
(diering 
only 
in 
their 
usage 
of 
the 
rst 
two 
words) 
and 
a 
random 
back


ground 
topic. 


15.4 
Latent 
Semantic 
Analysis 
In 
the 
document 
analysis 
literature 
PCA 
is 
also 
called 
Latent 
Semantic 
Analysis 
and 
is 
concerned 
with 
analysing 
a 
set 
of 
N 
documents, 
where 
each 
document 
dn 
, 
n 
=1;:::;N 
is 
represented 
by 
a 
vector 


nn

xn 
=(x1 
;:::;x 
) 
(15.4.1)

D

n

of 
word 
occurrences. 
For 
example 
the 
rst 
element 
xmight 
count 
how 
many 
times 
the 
word 
`cat’ 
appears 


1 


n

in 
document 
n, 
xthe 
number 
of 
occurrences 
of 
`dog', 
etc. 
This 
bag 
of 
words2 
is 
formed 
by 
rst 
choosing 


2 


n

a 
dictionary 
of 
D 
words. 
The 
vector 
element 
xis 
the 
(possibly 
normalised) 
number 
of 
occurrences 
of 
the 


i 


word 
i 
in 
the 
document 
n. 
Typically 
D 
will 
be 
large, 
of 
the 
order 
106, 
and 
x 
will 
be 
very 
sparse 
since 
any 
document 
contains 
only 
a 
small 
fraction 
of 
the 
available 
words 
in 
the 
dictionary. 
Given 
a 
set 
of 
documents 
D, 
the 
aim 
in 
LSA 
is 
to 
form 
a 
lower 
dimensional 
representation 
of 
each 
document. 
The 
whole 
document 
database 
is 
represented 
by 
the 
so-called 
term-document 
matrix 




1 
N

X 
=x;:::, 
x(15.4.2) 


which 
has 
dimension 
D 
× 
N, 
see 
for 
example 
g(15.8). 
In 
this 
small 
example 
the 
term-document 
matrix 
is 
`short 
and 
fat', 
whereas 
in 
practice 
most 
often 
the 
matrix 
will 
be 
`tall 
and 
thin'. 


Example 
71 
(Latent 
Topic). 
We 
have 
a 
small 
dictionary 
containing 
the 
words 
inuenza, 
u, 
headache, 
nose, 
temperature, 
bed, 
cat, 
tree, 
car, 
foot. 
The 
database 
contains 
a 
large 
number 
of 
articles 
that 
discuss 
ailments, 
and 
articles 
which 
seem 
to 
talk 
about 
the 
eects 
of 
inuenza, 
in 
addition 
to 
some 
background 
documents 
that 
are 
not 
specic 
to 
ailments. 
Some 
of 
the 
more 
formal 
documents 
exclusively 
use 
the 
term 
inuenza, 
whereas 
the 
other 
more 
`tabloid’ 
documents 
use 
the 
informal 
term 
u. 
Each 
document 
is 
represented 
by 
a 
simple 
bag-of-words 
style 
description, 
namely 
a 
10-dimensional 
vector 
in 
which 
element 
i 
of 
that 
vector 
is 
set 
to 
1 
if 
word 
i 
occurs 
in 
the 
document, 
and 
0 
otherwise. 
The 
data 
is 
represented 
in 
g(15.8). 
The 
data 
is 
generated 
using 
the 
articial 
mechanism 
described 
in 
demoLSI.m. 


The 
result 
of 
using 
PCA 
on 
this 
data 
is 
represented 
in 
g(15.9) 
where 
we 
plot 
the 
eigenvectors, 
scaled 
by 
their 
eigenvalue. 
The 
rst 
eigenvector 
groups 
all 
the 
`inuenza’ 
words 
together, 
and 
the 
second 
deals 
with 
the 
dierent 
usage 
of 
the 
terms 
inuenza 
and 
u. 


Rescaling 


In 
LSA 
it 
is 
common 
to 
scale 
the 
transformation 
so 
that 
the 
projected 
vectors 
have 
approximately 
unit 
covariance 
(assuming 
centred 
data). 
Using 


v 


N 
- 
1D..1UT

y 
= 
M 
x 
(15.4.3)

M 


the 
covariance 
of 
the 
projections 
is 
obtained 
from 


1 
yn 
(yn)T 
= 
D..1UT 
xn 
(xn)T 
UM 
D..1 
= 
D..1UT 


UD2UTUM 
D..1 
˜ 
I

MM 
MMMM

N 
- 
1 


nn 


XXT 


2More 
generally 
one 
can 
consider 
term-counts, 
in 
which 
terms 
can 
can 
be 
single 
words, 
or 
sets 
of 
words, 
or 
even 
sub-words. 


DRAFT 
March 
9, 
2010 



Latent 
Semantic 
Analysis 


1234567891010987654321
Figure 
15.9: 
Hinton 
Diagram 
of 
the 
eigenvector 
matrix 
E 
where 
each 
eigenvector 
column 
is 
scaled 
by 
the 
corresponding 
eigenvalue. 
Red 
indicates 
positive 
and 
green 
negative 
(the 
area 
of 
each 
square 
corresponds 
to 
the 
magnitude), 
showing 
that 
there 
are 
only 
a 
few 
large 
eigenvalues. 
Note 
that 
the 
overall 
sign 
of 
any 
eigenvector 
is 
irrelevant. 
The 
rst 
eigenvector 
corresponds 
to 
a 
topic 
in 
which 
the 
words 
inuenza, 
u, 
headache, 
nose, 
temperature, 
bed 
are 
prevalent. 
The 
second 
eigenvector 
denotes 
that 
there 
is 
negative 
correlation 
between 
the 
occurrence 
of 
inuenza 
and 
u. 


Given 
y, 
the 
approximate 
reconstruction 
x~is 


1 


x~= 
v 
UM 
DM 
y 
(15.4.4)

N 
- 
1 


The 
Euclidean 
distance 
between 
two 
points 
xa 
and 
xb 
is 
then 
approximately 


. 
T 
. 
T 


11 


xabb 
bb 
D2 
b

d 
~, 
x~= 
ya 
- 
yDM 
UT 
UM 
DM 
ya 
- 
y˜ 
ya 
- 
yM 
ya 
- 
y

N 
- 
1 
M 
N 
- 
1 


It 
is 
common 
to 
ignore 
the 
D2 
term 
(and 
1/ 
(N 
- 
1) 
factor), 
and 
consider 
a 
measure 
of 
dissimilarity 
in 


M 


the 
projected 
space 
just 
to 
be 
the 
Euclidean 
distance 
between 
the 
y 
vectors. 


15.4.1 
LSA 
for 
information 
retrieval 
Consider 
a 
large 
collection 
of 
documents 
from 
the 
web, 
creating 
a 
database 
D. 
Our 
interest 
it 
to 
nd 
the 
most 
similar 
document 
to 
a 
specied 
query 
document. 
Using 
a 
bag-of-words 
style 
representation 
for 
document 
n, 
xn, 
and 
similarly 
for 
the 
query 
document, 
x 
* 
we 
address 
this 
task 
by 
rst 
dening 
a 
measure 
of 
dissimilarity 
between 
documents, 
for 
example 


d(xn 
, 
xm)=(xn 
- 
xm)T 
(xn 
- 
xm) 
(15.4.5) 


One 
then 
searches 
for 
the 
document 
that 
minimises 
this 
dissimilarity: 


nopt 
= 
argmin 
d(xn 
, 
x 
) 
(15.4.6) 


n 


and 
returns 
document 
xnopt 
as 
the 
result 
of 
the 
search 
query. 
A 
diculty 
with 
this 
approach 
is 
that 
the 
bag-of-words 
representation 
will 
have 
mostly 
zeros 
(i.e. 
be 
very 
sparse). 
Hence 
dierences 
may 
be 
due 
to 
`noise’ 
rather 
than 
any 
real 
similarity 
between 
the 
query 
and 
database 
document. 
LSA 
alleviates 
this 
problem 
by 
using 
a 
lower 
dimensional 
representation 
y 
of 
the 
high-dimensional 
x. 
The 
y 
capture 
the 
main 
variations 
in 
the 
data 
and 
are 
less 
sensitive 
to 
random 
uncorrelated 
noise. 
Using 
the 
dissimilarity 
dened 
in 
terms 
of 
the 
lower 
dimensional 
y 
is 
therefore 
more 
robust 
and 
likely 
to 
retrieve 
more 
useful 
documents. 


The 
squared 
dierence 
between 
two 
documents 
can 
also 
be 
written

..T..

00T0TT0

x 
- 
xx 
- 
x= 
xx 
+ 
xx/ 
- 
2xx(15.4.7) 


If, 
as 
is 
commonly 
done, 
the 
bag-of-words 
representations 
are 
scaled 
to 
have 
unit 
length, 


x 


x^= 
v 
(15.4.8) 


xTx 


so 
that 
x^Tx^= 
1, 
the 
distance 
is

..

0T..0. 
/ 
. 


x^- 
x^x^- 
x^=2 
1 
- 
x^Tx^(15.4.9) 


DRAFT 
March 
9, 
2010 



PCA 
With 
Missing 
Data 



Figure 
15.10: 
(a): 
Two 
bag-of-word 
vectors. 
The 
Euclidean 
distance 
between 
the 
two 
is 
large. 
(b): 
Normalised 
vectors. 
The 
Euclidean 
distance 
is 
now 
related 
directly 
to 
the 
angle 
between 
the 
vectors. 
In 
this 
case 
two 
documents 
which 
have 
the 
same 
relative 
frequency 
of 
words 
will 
both 
have 
the 
same 
dissimilarly, 
even 
though 
the 
number 
of 
occurrences 
of 
the 
words 


(a) 
(b) 
is 
dierent. 
5010015020025030020406080100
5010015020025030020406080100
5010015020025030020406080100
Figure 
15.11: 
Top: 
original 
data 
matrix 
X. 
Black 
is 
missing, 
white 
present. 
The 
data 
is 
constructed 
from 
a 
set 
of 
only 
5 
basis 
vectors. 
Middle 
: 
X 
with 
missing 
data 
(80% 
sparsity). 
Bottom 
: 
reconstruction 
found 
using 
svdm.m, 
SVD 
for 
missing 
data. 
This 
problem 
is 
essentially 
easy 
since, 
despite 
there 
being 
many 
missing 
elements, 
the 
data 
is 
indeed 
constructed 
from 
a 
model 
for 
which 
SVD 
is 
appropriate. 
Such 
techniques 
have 
application 
in 
collaborative 
ltering 
and 
recommender 
systems 
where 
one 
wishes 
to 
`ll 
in’ 
missing 
values 
in 
a 
matrix. 


and 
one 
may 
equivalently 
consider 
the 
cosine 
similarity 


s(x^, 
x^0)= 
x^Tx^' 
= 
cos 
() 
(15.4.10) 


where 
. 
is 
the 
angle 
between 
the 
unit 
vectors 
x^and 
x^0, 
g(15.10). 


PCA 
is 
arguably 
suboptimal 
for 
document 
analysis 
since 
we 
would 
expect 
the 
presence 
of 
a 
latent 
topic 
to 
contribute 
only 
positive 
counts 
to 
the 
data. 
A 
related 
version 
of 
PCA 
in 
which 
the 
decomposition 
is 
constrained 
to 
have 
positive 
elements 
is 
called 
PLSA, 
and 
discussed 
in 
section(15.6). 


Example 
72. 
Continuing 
the 
Inuenza 
example, 
someone 
who 
uploads 
a 
query 
document 
which 
uses 
the 
term 
`u’ 
might 
also 
be 
interested 
in 
documents 
about 
`inuenza'. 
However, 
the 
search 
query 
term 
`u’ 
does 
not 
contain 
the 
word 
`inuenza', 
so 
how 
can 
one 
retrieve 
such 
documents? 
Since 
the 
rst 
component 
using 
PCA 
(LSA) 
groups 
all 
`inuenza’ 
terms 
together, 
if 
we 
use 
only 
the 
rst 
component 
of 
the 
representation 
y 
to 
compare 
documents, 
this 
will 
retrieve 
documents 
independent 
of 
whether 
the 
term 
`u’ 
or 
`inuenza’ 
is 
used. 


15.5 
PCA 
With 
Missing 
Data 
When 
values 
of 
the 
data 
matrix 
X 
are 
missing, 
the 
standard 
PCA 
algorithm 
as 
described 
cannot 
be 


n

implemented. 
Unfortunately, 
there 
is 
no 
`quick 
x’ 
PCA 
solution 
when 
some 
of 
the 
xare 
missing 
and 


i 


more 
complex 
numerical 
procedures 
need 
to 
invoked. 
A 
naive 
approach 
in 
this 
case 
is 
to 
require 
the 


DRAFT 
March 
9, 
2010 
289 



PCA 
With 
Missing 
Data 


squared 
reconstruction 
error 
to 
be 
small 
only 
for 
the 
existing 
elements 
of 
X. 
That 
is 


23

2 


ND

XXX

n 
nj

E(B, 
Y)= 
i
n 
4xi 
..yj 
bi 
5X(15.5.1) 
n=1 
i=1 
j 


where 
n 
= 
1 
if 
the 
ith 
entry 
of 
the 
nth 
vector 
is 
available, 
and 
is 
zero 
otherwise. 
Dierentiating, 
as 
before, 


i 


we 
nd 
that 
the 
optimal 
weights 
satisfy 
(assuming 
BTB 
= 
I), 


X

k 
nn 


y 
=i 
xi 
bk 
(15.5.2)

ni 


One 
then 
substitutes 
this 
expression 
into 
the 
squared 
error, 
and 
minimises 
the 
error 
with 
respect 
to 
B 
under 
the 
orthonormality 
constraint. 
An 
alternative 
iterative 
optimisation 
procedure 
is 
as 
follows: 
First 
select 
a 
random 
D 
× 
M 
matrix 
B^
Then 
iterate 
until 
convergence 
the 
following 
two 
steps: 


Optimize 
Y 
for 
xed 
B 


23

2 


ND

XXX

n 
nj

E(B^
, 
Y)= 
n 
4x 
..y 
^b
5X(15.5.3)

ii 
ji 
n=1 
i=1 
j 


For 
xed 
B^
the 
above 
E(B^
, 
Y) 
is 
a 
quadratic 
function 
of 
the 
matrix 
Y, 
which 
can 
be 
optimised 
directly. 
By 
dierentiating 
and 
equating 
to 
zero, 
one 
obtains 
the 
xed 
point 
condition

 !

XX

nn 
n^b
l 
^b
k 


x 
..y 
= 
0 
(15.5.4)

ii 
lii 
il 


Dening

hihi

XX

(n)l 
M(n)^b
l^b
k 
nn^b
k

w= 
yn;=ii 
i
n 
, 
[cn]k 
=i 
xii 
, 
(15.5.5) 
l 
kl 


ii 


in 
matrix 
notation, 
we 
then 
have 
a 
set 
of 
linear 
systems: 


(n) 
= 
M(n)(n)
cy;n 
=1;:::;N 
(15.5.6) 


One 
may 
solve 
each 
linear 
system 
using 
Gaussian 
elimination 
(one 
can 
avoid 
explicit 
matrix 
inversion 
by 
using 
the 
\ 
operator 
in 
MATLAB). 
It 
can 
be 
that 
one 
or 
more 
of 
the 
above 
linear 
systems 
is 
underdetermined– 
this 
can 
occur 
when 
there 
are 
less 
observed 
values 
in 
the 
nth 
data 
column 
of 
X 
than 
there 
are 
components 
M. 
In 
this 
case 
one 
may 
use 
the 
pseudo-inverse 
to 
provide 
a 
minimal 
length 
solution. 


Optimize 
B 
for 
xed 
Y 
One 
now 
freezes 
Y^
and 
considers 
the 
function 


23

2 


ND

XXX

n 
nj

E(B, 
Y^
)= 
n 
4x 
..y^j 
b5X(15.5.7)

ii 
i 
n=1 
i=1 
j 


For 
xed 
Y^
the 
above 
expression 
is 
quadratic 
in 
the 
matrix 
B, 
which 
can 
again 
be 
optimised 
using 
linear 
algebra. 
This 
corresponds 
to 
solving 
a 
set 
of 
linear 
systems 
for 
the 
ith 
row 
of 
B: 


(i) 
= 
F(i)b(i)
m(15.5.8) 


where

hihi

XX

(i)nnn 
F(i)nnn

m=i 
xi 
y^k 
;=i 
y^j 
y^k 
(15.5.9) 
k 
kj 


nn 


= 
F(i)..1 
m(i)

Mathematically, 
this 
is 
b(i) 
. 


In 
this 
manner 
one 
is 
guaranteed 
to 
iteratively 
decrease 
the 
value 
of 
the 
squared 
error 
loss 
until 
a 
minimum 
is 
reached. 
This 
technique 
is 
implemented 
in 
svdm.m. 
Note 
that 
ecient 
techniques 
based 
on 
updating 
the 
solution 
as 
a 
new 
column 
of 
X 
arrives 
one 
at 
a 
time 
(`online’ 
updating) 
are 
available, 
see 
for 
example 
[49]. 


DRAFT 
March 
9, 
2010 



PCA 
With 
Missing 
Data 


15.5.1 
Finding 
the 
principal 
directions 
For 
the 
missing 
data 
case 
the 
basis 
B 
found 
using 
the 
above 
technique 
is 
based 
only 
on 
minimising 
the 
squared 
reconstruction 
error 
and 
therefore 
does 
not 
necessarily 
satisfy 
the 
maximal 
variance 
(or 
principal 
directions) 
criterion, 
namely 
that 
the 
columns 
of 
B 
point 
along 
the 
eigen-directions. 
For 
a 
given 
B, 
Y 
with 
approximate 
decomposition 
X 
˜ 
BY 
we 
can 
return 
a 
new 
orthonormal 
basis 
U 
by 
performing 
SVD 
on 
the 
completed 
data, 
BY 
= 
USVT 
to 
return 
an 
orthonormal 
basis 
B 
. 
U. 
In 
general, 
however, 
this 
is 
potentially 
computationally 
expensive. 
If 
only 
the 
principal 
directions 
are 
required, 
an 
alternative 
is 
to 
explicitly 
transform 
the 
solution 
B 
using 
an 
invertible 
matrix 
Q: 


X 
˜ 
BQQ..1Y 
(15.5.10) 


Calling 
the 
new 
basis 
B~
BQ, 
for 
a 
solution 
to 
be 
aligned 
with 
the 
principal 
directions, 
we 
need 


B~
~

B 
= 
I 
(15.5.11) 
In 
other 
words 
QTBTBQ 
= 
I 
(15.5.12) 
Forming 
the 
SVD 
of 
B, 
B 
= 
UDVT 
(15.5.13) 
and 
substituting 
in 
equation 
(15.5.12), 
we 
have 
the 
requirement 
QTVDUTUDVTQ 
= 
I 
(15.5.14) 
Since 
UTU 
= 
I 
and 
VTV 
= 
VVT 
= 
I, 
we 
can 
use 
Q 
= 
VTD..1 
(15.5.15) 
Hence, 
given 
a 
solution 
B, 
we 
can 
nd 
the 
principal 
directions 
from 
the 
SVD 
of 
B 
using 
B~
= 
UTD..1B 


(15.5.16) 
If 
the 
D 
× 
M 
matrix 
B 
is 
non-square 
M 
<D, 
then 
the 
matrix 
D 
will 
be 
non-square 
and 
non-invertible. 
To 
make 
the 
above 
well 
dened, 
one 
may 
append 
D 
with 
the 
columns 
of 
the 
identity: 
D' 
=[D, 
IM+1;:::, 
ID] 
(15.5.17) 
where 
IK 
is 
the 
Kth 
column 
of 
the 
identity 
matrix, 
and 
use 
D' 
in 
place 
of 
D 
above. 


15.5.2 
Collaborative 
ltering 
using 
PCA 
with 
missing 
data 
A 
database 
contains 
a 
set 
of 
vectors, 
each 
describing 
the 
lm 
ratings 
for 
a 
user 
in 
the 
database. 
The 
ith 




entry 
in 
the 
vector 
x 
species 
the 
rating 
the 
user 
gives 
to 
the 
ith 
lm. 
The 
matrix 
X 
=x1 
;:::, 
xNfor 
all 
the 
N 
users, 
has 
many 
missing 
values 
since 
any 
single 
user 
will 
only 
have 
given 
a 
rating 
for 
a 
small 
selection 
of 
the 
possible 
D 
lms. 
In 
a 
practical 
example 
one 
might 
have 
D 
= 
10, 
000 
lms 
and 
N 
=1, 
000, 
000 
users. 
For 
any 
user 
n 
the 
task 
is 
to 
predict 
reasonable 
values 
for 
the 
missing 
entries 
of 
their 
rating 
vector 
xn, 
thereby 
providing 
a 
suggestion 
as 
to 
which 
lms 
they 
might 
like 
to 
view. 
Viewed 
as 
a 
missing 
data 
problem, 
one 
can 
t 
B 
and 
Y 
using 
svdm.m 
as 
above. 
Given 
B 
and 
Y 
we 
can 
form 
a 
reconstruction 
on 


all 
the 
entries 
of 
X, 
by 
using 
~X 
= 
BY 
(15.5.18) 
giving 
therefore 
a 
prediction 
for 
the 
missing 
values. 
DRAFT 
March 
9, 
2010 
291 



Matrix 
Decomposition 
Methods 



Figure 
15.12: 
(a): 
Under-complete 
representation. 
There 
are 
too 
few 
basis 
vectors 
to 
represent 
the 
datapoints. 
(b): 
Over-complete 
representation. 
There 
are 
too 
many 
basis 
vectors 
to 
form 
a 
unique 
representation 
of 
a 


datapoint 
in 
terms 
of 
a 
linear 
combination 
of 
the 
basis 
vectors.

(a) 
(b) 
Figure 
15.13: 
(a): 
Joint 
PLSA. 
(b): 
Conditional 


z
xyzxy
PLSA. 
Whilst 
written 
as 
a 
graphical 
model, 
some 
care 
is 
required 
in 
the 
interpretation, 
see 
text. 


(a) 
(b) 
15.6 
Matrix 
Decomposition 
Methods 
Given 
a 
data 
matrix 
X 
for 
which 
each 
column 
represents 
a 
datapoint, 
an 
approximate 
matrix 
decomposition 
is 
of 
the 
form 
X 
˜ 
BY 
into 
a 
basis 
matrix 
B 
and 
weight 
(or 
coordinate) 
matrix 
Y. 
Symbolically, 
matrix 
decompositions 
are 
of 
the 
form 


1

0{

1

01

0{

BBBB
@{

CCCC
A{

BBBB
@{

CCCC
A{

@

A

X 
: 
Data 
B 
: 
Basis 
Y 
: 
Weights/Components
˜ 


|

{z

MN 


}{

|

{z

|}

{z

}{

DNDM 


(15.6.1) 
In 
this 
section 
we 
will 
consider 
some 
common 
matrix 
decomposition 
methods. 


Under-complete 
decompositions 


When 
M 
<D, 
there 
are 
fewer 
basis 
vectors 
than 
dimensions, 
g(15.12a). 
The 
matrix 
B 
is 
then 
called 
`tall’ 
or 
`thin'. 
In 
this 
case 
the 
matrix 
Y 
forms 
a 
lower 
dimensional 
approximate 
representation 
of 
the 
data 
X, 
PCA 
being 
a 
classic 
example. 


Over-complete 
decompositions 


For 
M>D 
the 
basis 
is 
over-complete, 
there 
being 
more 
basis 
vectors 
than 
dimensions, 
g(15.12b). 
In 
such 
cases 
additional 
constraints 
are 
placed 
on 
either 
the 
basis 
or 
components. 
For 
example, 
one 
might 
require 
that 
only 
a 
small 
number 
of 
the 
large 
number 
of 
available 
basis 
vectors 
is 
used 
to 
form 
the 
representation 
for 
any 
given 
x. 
Such 
sparse-representations 
are 
common 
in 
theoretical 
neurobiology 
where 
issues 
of 
energy 
eciency, 
rapidity 
of 
processing 
and 
robustness 
are 
of 
interest[214, 
155, 
252]. 


15.6.1 
Probabilistic 
latent 
semantic 
analysis 
Consider 
two 
objects, 
x 
and 
y, 
where 
dom(x)= 
f1;:::;I} 
and 
dom(y)= 
f1;:::;Jg. 
We 
have 
a 
count 
matrix 
with 
elements 
Cij 
which 
describes 
the 
number 
of 
times 
that 
x 
= 
i, 
y 
= 
j 
was 
observed. 
We 
can 
transform 
this 
count 
matrix 
into 
a 
`frequency’ 
matrix 
p 
with 
elements 


p(x 
= 
i, 
y 
= 
j)=


P

Cij

(15.6.2) 
ij 
Cij 


Our 
interest 
is 
to 
nd 
a 
decomposition 
of 
this 
frequency 
matrix 
of 
the 
form 
in 
g(15.13a) 


 


}

|

}

|

p~(x 
= 
ijz 
= 
k) 
p~(y 
= 
jjz 
= 
k)~p(z 
= 
k) 


k

Xij 
Bik 
Ykj 


DRAFT 
March 
9, 
2010 


˜


= 
p~(x 
= 
i, 
y 
= 
j) 
(15.6.3)


p
p|

) 
)
}

(x 
= 
i, 
y 
= 
j

{z

{ 


{ 



Matrix 
Decomposition 
Methods 


which 
is 
a 
form 
of 
matrix 
decomposition 
into 
basis 
B 
and 
coordinates 
Y. 
This 
has 
the 
interpretation 
of 
discovering 
latent 
topics 
z 
that 
describe 
the 
joint 
behaviour 
of 
x 
and 
y. 


An 
EM 
style 
training 
algorithm 


In 
order 
to 
nd 
the 
approximate 
decomposition 
we 
rst 
need 
a 
measure 
of 
dierence 
between 
the 
matrix 
with 
elements 
pij 
and 
the 
approximation 
with 
elements 
~pij. 
Since 
all 
elements 
are 
bounded 
between 
0 
and 
1 
and 
sum 
to 
1, 
we 
may 
interpret 
p 
as 
a 
joint 
probability 
and 
~p 
as 
an 
approximation 
to 
this. 
For 
probabilities, 
a 
useful 
measure 
of 
discrepancy 
is 
the 
Kullback-Leibler 
divergence 


KL(pjp~) 
= 
hlog 
pi..hlog 
~pi(15.6.4)

pp 


Since 
p 
is 
xed, 
minimising 
the 
Kullback-Leibler 
divergence 
with 
respect 
to 
the 
approximation 
~p 
is 
equivalent 
to 
maximising 
the 
`likelihood’ 
term 
hlog 
~pi. 
This 
is

p 


X

p(x, 
y) 
log 
~p(x, 
y) 
(15.6.5) 


x;y 


It's 
convenient 
to 
derive 
an 
EM 
style 
algorithm 
to 
learn 
~p(xjz), 
~p(yjz) 
and 
~p(z). 
To 
do 
this, 
consider 


XX

KL(q(zjx, 
y)jp~(zjx, 
y)) 
=q(zjx, 
y) 
log 
q(zjx, 
y) 
..q(zjx, 
y) 
log 
~p(zjx, 
y) 
= 
0 
(15.6.6) 


zz 


P

whereimplies 
summation 
over 
all 
states 
of 
the 
variable 
z. 
Rearranging, 
this 
gives 
the 
bound, 


z 


XX

log 
~p(x, 
y) 
..q(zjx, 
y) 
log 
q(zjx, 
y)+q(zjx, 
y) 
log 
~p(z, 
x, 
y) 
(15.6.7) 


zz 


Plugging 
this 
into 
the 
`likelihood’ 
term 
above, 
we 
have 
the 
bound

XXX

p(x, 
y) 
log 
~p(x, 
y) 
..p(x, 
y)q(zjx, 
y) 
log 
q(zjx, 
y) 


x;y 
x;y 
z 


XX

+p(x, 
y)q(zjx, 
y) 
[log 
~p(xjz)+log 
~p(yjz)+log 
~p(z)] 
(15.6.8) 


x;y 
z 


M-step 


For 
xed 
~p(xjz);p~(yjz), 
the 
contribution 
to 
the 
bound 
from 
~p(z) 
is

XX

p(x, 
y)q(zjx, 
y) 
log 
~p(z) 
(15.6.9) 


x;y 
z 


It 
is 
straightforward 
to 
see 
that 
the 
optimal 
setting 
of 
~p(z) 
is 


X

p~(z)=q(zjx, 
y)p(x, 
y) 
(15.6.10) 


x;y 


P

since 
equation 
(15.6.9) 
is, 
up 
to 
a 
constant, 
KLq(zjx, 
y)p(x, 
y)jp~(z). 
Similarly, 
for 
xed 
~p(yjz);p~(z),

x;y 


the 
contribution 
to 
the 
bound 
from 
~p(xjz) 
is

XX

p(x, 
y)q(zjx, 
y) 
log 
~p(yjz) 
(15.6.11) 


x;y 
z 


Therefore, 
optimally 


X

p~(xjz) 
/p(x, 
y)q(zjx, 
y) 
(15.6.12) 


y 


and 
similarly, 


X

p~(yjz) 
/p(x, 
y)q(zjx, 
y) 
(15.6.13) 


DRAFT 
March 
9, 
2010 



Matrix 
Decomposition 
Methods 



Figure 
15.14: 
(a) 
Conditional 
PLSA 
reconstruction 
of 
the 
images 
in 
g(15.5) 
using 
a 
positive 
(convex) 
combination 
of 
the 
49 
positive 
base 
images 
in 
(b). 
The 
root 
mean 
square 
reconstruction 
error 
is 
1:391× 
10..5 
. 
The 
base 
images 
tend 
to 
be 
more 
`localised’ 
than 
the 
corresponding 
eigen-images 
g(15.6b). 
Here 
one 
sees 
local 
structure 
such 
as 
foreheads, 


chins, 
etc.

(a) 
(b) 
E-step 


The 
optimal 
setting 
for 
the 
q 
distribution 
at 
each 
iteration 
is 


q(zjx, 
y)= 
p~(zjx, 
y) 
(15.6.14) 


which 
is 
xed 
throughout 
the 
M-step. 


The 
procedure 
is 
given 
in 
algorithm(15) 
and 
a 
demonstration 
is 
in 
demoPLSA.m. 
The 
`likelihood’ 
equation 


(15.6.5) 
is 
guaranteed 
to 
increase 
(and 
the 
Kullback-Leibler 
divergence 
equation 
(15.6.4) 
decrease) 
under 
iterating 
between 
the 
E 
and 
M-steps, 
since 
the 
method 
is 
analogous 
to 
an 
EM 
procedure 
Generalisations, 
such 
as 
using 
simpler 
q 
distributions, 
(corresponding 
to 
generalised 
EM 
procedures) 
are 
immediate 
based 
on 
modifying 
the 
above 
derivation. 
A 
related 
probabilistic 
model 


For 
variables 
x, 
y, 
z 
with 
z 
hidden 
and 
dom(x)= 
f1;:::;I} 
, 
dom(y)= 
f1;:::;J} 
, 
dom(z)= 
f1;:::;Kg, 
consider 
a 
distribution 


p~(x, 
y, 
z)= 
p~(xjz)~p(yjz)~p(z) 
(15.6.15) 


n

and 
data 
D 
= 
f(x;yn) 
;n 
=1;:::;Ng. 
Assuming 
the 
data 
are 
i.i.d. 
draws 
from 
equation 
(15.6.15) 
the 
log 
likelihood 
is 


N

N 


n

log 
~p(D)= 
log~p(x 
;y 
n) 
(15.6.16) 


n=1 


where 


N 


n 


p~(x 
;y 
n)= 
p~(x 
njz 
n)~p(y 
njz 
n)~p(z 
n) 
(15.6.17) 
zn 


n

If 
x;yn 
are 
sampled 
from 
a 
distribution 
p(x, 
y) 
then, 
in 
the 
limit 
of 
an 
innite 
number 
of 
samples, 
N 
!1, 
equation 
(15.6.16) 
becomes 


log 
~p(D)= 
hlog 
~p(x, 
y)i(15.6.18)

p(x;y) 


which 
is 
equation 
(15.6.5). 
From 
this 
viewpoint, 
even 
though 
we 
started 
out 
with 
a 
set 
of 
samples, 
in 
the 
limit, 
only 
the 
distribution 
of 
the 
observed 
data, 
p(x, 
y) 
is 
relevant. 
The 
generic 
code 
for 
the 
nite 
sample 
case, 
trained 
with 
EM 
is 
given 
in 
demoMultinomialpXYgZ.m. 
See 
also 
exercise(168). 


A 
fully 
probabilistic 
interpretation 
of 
PLSA 
can 
be 
made 
via 
Poisson 
processes[56]. 
A 
related 
probabilistic 
model 
is 
Latent 
Dirichlet 
Allocation, 
which 
is 
described 
in 
section(20.6.1). 


DRAFT 
March 
9, 
2010 



Matrix 
Decomposition 
Methods 


PX

Algorithm 
15 
PLSA: 
Given 
a 
frequency 
matrix 
p(x 
= 
i, 
y 
= 
j), 
return 
a 
decomposition 
p~(x 
= 
ijz 
= 


k 


k)~p(y 
= 
jjz 
= 
k)~p(z 
= 
k). 
See 
plsa.m 


1: 
Initialise 
p~(z);p~(xjz);p~(yjz). 
2: 
while 
Not 
Converged 
do 
3: 
Set 
q(zjx, 
y)= 
p~(zjx, 
y) 
I 
E-step
PX

4: 
Set 
p~(xjz) 
. 
p(x, 
y)q(zjx, 
y) 
I 
M-Steps
y

PX

5: 
Set 
p~(yjz) 
. 
p(x, 
y)q(zjx, 
y)
x 


6: 
end 
while 
PX

7: 
Set 
p~(z)= 
p(x, 
y)q(zjx, 
y)
x;y 


Algorithm 
16 
Conditional 
PLSA: 
Given 
a 
frequency 
matrix 
p(x 
= 
ijy 
= 
j), 
return 
a 
decomposition 


PX

p~(x 
= 
ijz 
= 
k)~p(z 
= 
kjy 
= 
j). 
See 
plsaCond.m

k 


1: 
Initialise 
p~(xjz);p~(zjy). 
2: 
while 
Not 
Converged 
do 
3: 
Set 
q(zjx, 
y)= 
p~(zjx, 
y) 
I 
E-step
PX

4: 
Set 
p~(xjz) 
. 
p(xjy)q(zjx, 
y) 
I 
M-Steps
y

PX

5: 
Set 
p~(zjy) 
. 
p(xjy)q(zjx, 
y)
x 


6: 
end 
while 
Conditional 
PLSA 


In 
some 
cases 
it 
is 
more 
natural 
to 
consider 
a 
conditional 
frequency 
matrix 


p(x 
= 
ijy 
= 
j) 
(15.6.19) 


and 
seek 
an 
approximate 
decomposition 


X

p(x 
= 
ijy 
= 
j)p~(x 
= 
ijz 
= 
k)p~(z 
= 
kjy 
= 
j)(15.6.20)

|{z}|{z}|{z}

Xij 
kBik 
Ykj 


as 
depicted 
in 
g(15.13b). 
Deriving 
an 
EM 
style 
algorithm 
for 
this 
is 
straightforward 
(see 
exercise(167)), 
and 
is 
presented 
in 
algorithm(16), 
being 
equivalent 
to 
the 
non-negative 
matrix 
factorisation 
algorithm 
of 
[171]. 


Example 
73 
(Discovering 
the 
basis). 
A 
set 
of 
images 
is 
give 
in 
g(15.15a). 
These 
were 
created 
by 
rst 
dening 
4 
base 
images 
g(15.15b). 
Each 
base 
image 
is 
positive 
and 
scaled 
so 
that 
the 
sum 
of 
the 
pixels 


PX

is 
unity, 
i 
p(x 
= 
ijz 
= 
k) 
= 
1, 
where 
k 
=1;:::, 
4 
and 
x 
indexes 
the 
pixels, 
see 
g(15.15). 
We 
then 
sum 
each 
of 
these 
images 
using 
a 
randomly 
chosen 
positive 
set 
of 
4 
weights 
(under 
the 
constraint 
that 
the 
weights 
sum 
to 
1) 
to 
generate 
a 
training 
image 
with 
elements 
p(x 
= 
ijy 
= 
j) 
and 
j 
indexes 
the 
training 
image. 
This 
is 
repeated 
144 
times 
to 
form 
the 
full 
training 
set, 
g(15.15a). 
The 
task 
is, 
given 
only 
the 
training 
set 
images, 
to 
reconstruct 
the 
basis 
from 
which 
the 
images 
were 
formed. 
We 
assume 
that 
we 
know 
the 
correct 
number 
of 
base 
images, 
namely 
4. 
The 
results 
of 
using 
conditional 
PLSA 
on 
this 
task 
are 
presented 
in 
g(15.15c) 
and 
using 
SVD 
in 
g(15.15d). 
In 
this 
case 
PLSA 
nds 
the 
correct 
`natural’ 
basis, 
corresponding 
to 
the 
way 
the 
images 
were 
generated. 
The 
eigenbasis 
is 
just 
as 
good 
in 
terms 
of 
being 
able 
to 
represent 
any 
of 
the 
training 
images, 
but 
in 
this 
case 
does 
not 
correspond 
to 
the 
constraints 
under 
which 
the 
data 
was 
generated 


15.6.2 
Extensions 
and 
variations 
Non-negative 
matrix 
factorisation 


Non-negative 
Matrix 
factorisation 
(NMF) 
considers 
a 
decomposition 
in 
which 
both 
the 
basis 
and 
weight 
matrices 
have 
non-negative 
entries. 
An 
early 
example 
of 
this 
work 
is 
as 
a 
form 
of 
constrained 
Factor 


DRAFT 
March 
9, 
2010 



Matrix 
Decomposition 
Methods 



(a) 
(b) 
(c) 
(d) 
Figure 
15.15: 
(a): 
Training 
data, 
consisting 
of 
a 
positive 
(convex) 
combination 
of 
the 
base 
images. 
(b): 
The 
chosen 
base 
images 
from 
which 
the 
training 
data 
is 
derived. 
(c): 
Basis 
learned 
using 
conditional 
PLSA 
on 
the 
training 
data. 
This 
is 
virtually 
indistinguishable 
from 
the 
true 
basis. 
(d): 
Eigenbasis 
(sometimes 
called 
`eigenfaces'). 


Analysis[217]. 
Closely 
related 
works 
are 
[171] 
which 
is 
a 
generalisation 
of 
PLSA 
(since 
there 
is 
no 
requirement 
that 
the 
basis 
or 
components 
sum 
to 
unity). 
In 
all 
cases 
EM-style 
training 
algorithms 
exist, 
although 
their 
convergence 
can 
be 
slow. 
A 
natural 
relaxation 
is 
when 
only 
one 
of 
the 
factors 
in 
the 
decomposition 
is 
constrained 
to 
be 
non-negative. 
We 
will 
encounter 
similar 
models 
in 
the 
discussion 
on 
Independent 
Component 
Analysis, 
section(21.6). 


Gradient 
based 
training 


EM 
style 
algorithms 
are 
easy 
to 
derive 
and 
implement 
but 
can 
exhibit 
poor 
convergence. 
Gradient 
based 
methods 
to 
simultaneously 
optimize 
with 
respect 
to 
the 
basis 
and 
the 
components 
have 
been 
developed, 
but 
require 
a 
parameterisation 
that 
ensures 
positivity 
of 
the 
solutions[217]. 


Array 
decompositions 


It 
is 
straightforward 
to 
extend 
the 
method 
to 
the 
decomposition 
of 
multidimensional 
arrays, 
based 
also 
on 
more 
than 
one 
basis. 
For 
example 


XX

p(s, 
t, 
u) 
p~(s, 
t, 
ujv, 
w)~p(v, 
w)=p~(s, 
tju, 
v)~p(ujw)~p(v)~p(w) 
(15.6.21) 


v;w 
v;w 


Such 
extensions 
require 
only 
additional 
bookkeeping. 


15.6.3 
Applications 
of 
PLSA/NMF 
Physical 
models 


Non-negative 
decompositions 
can 
arise 
naturally 
in 
certain 
physical 
situations. 
For 
example, 
in 
acoustics, 
positive 
amounts 
of 
energy 
combine 
linearly 
from 
dierent 
signal 
sources 
to 
form 
the 
observed 
signal. 
Let's 
imagine 
that 
two 
kinds 
of 
signals 
are 
present 
in 
an 
acoustic 
signal, 
say 
a 
piano 
and 
a 
singer. 
Using 
NMF 
one 
can 
learn 
two 
separate 
bases 
for 
these 
cases, 
and 
then 
reconstruct 
a 
given 
signal 
using 
only 
one 
of 
the 
bases. 
This 
means 
that 
one 
could 
potentially 
remove 
the 
singer 
from 
a 
recording, 
leaving 
only 
the 
piano. 
See 
also 
[285] 
for 
a 
more 
standard 
probabilistic 
model 
in 
acoustics. 
This 
would 
be 
analogous 
to 
reconstructing 
the 
images 
in 
g(15.15a) 
using 
say 
only 
one 
of 
the 
learned 
basis 
images, 
see 
example(73). 


Modelling 
citations 


We 
have 
a 
collection 
of 
research 
documents 
which 
cite 
other 
documents. 
For 
example, 
document 
1 
might 
cite 
documents 
3, 
2, 
10, 
etc. 
Given 
only 
the 
list 
of 
citations 
for 
each 
document, 
can 
we 
identify 
key 
research 
papers 
and 
the 
communities 
that 
cite 
them? 
Note 
that 
this 
is 
not 
the 
same 
question 
as 
nding 
the 
most 
cited 
documents 
– 
rather 
we 
want 
to 
identify 
documents 
with 
communities 
and 
nd 
their 
relevance 
for 
a 


DRAFT 
March 
9, 
2010 



Matrix 
Decomposition 
Methods 


factor 
1 
(Reinforcement 
Learning) 
0.0108 
Learning 
to 
predict 
by 
the 
methods 
of 
temporal 
dierences. 
Sutton. 
0.0066 
Neuronlike 
adaptive 
elements 
that 
can 
solve 
dicult 
learning 
control 
problems. 
Barto 
et 
al. 
0.0065 
Practical 
Issues 
in 
Temporal 
Dierence 
Learning. 
Tesauro. 
factor 
2 
(Rule 
Learning) 
0.0038 
Explanation-based 
generalization: 
a 
unifying 
view. 
Mitchell 
et 
al. 
0.0037 
Learning 
internal 
representations 
by 
error 
propagation. 
Rumelhart 
et 
al. 
0.0036 
Explanation-Based 
Learning: 
An 
Alternative 
View. 
DeJong 
et 
al. 
factor 
3 
(Neural 
Networks) 
0.0120 
Learning 
internal 
representations 
by 
error 
propagation. 
Rumelhart 
et 
al. 
0.0061 
Neural 
networks 
and 
the 
bias-variance 
dilemma. 
Geman 
et 
al. 
0.0049 
The 
Cascade-Correlation 
learning 
architecture. 
Fahlman 
et 
al. 
factor 
4 
(Theory) 
0.0093 
Classication 
and 
Regression 
Trees. 
Breiman 
et 
al. 
0.0066 
Learnability 
and 
the 
Vapnik-Chervonenkis 
dimension. 
Blumer 
et 
al. 
0.0055 
Learning 
Quickly 
when 
Irrelevant 
Attributes 
Abound. 
Littlestone. 
factor 
5 
(Probabilistic 
Reasoning) 
0.0118 
Probabilistic 
Reasoning 
in 
Intelligent 
Systems: 
Networks 
of 
Plausible 
Inference. 
Pearl. 
0.0094 
Maximum 
likelihood 
from 
incomplete 
data 
via 
the 
em 
algorithm. 
Dempster 
et 
al. 
0.0056 
Local 
computations 
with 
probabilities 
on 
graphical 
structures. 
Lauritzen 
et 
al. 
factor 
6 
(Genetic 
Algorithms) 
0.0157 
Genetic 
Algorithms 
in 
Search, 
Optimization, 
and 
Machine 
Learning. 
Goldberg. 
0.0132 
Adaptation 
in 
Natural 
and 
Articial 
Systems. 
Holland. 
0.0096 
Genetic 
Programming: 
On 
the 
Programming 
of 
Computers 
by 
Means 
of 
Natural 
Selection. 
Koza. 
factor 
7 
(Logic) 
0.0063 
Ecient 
induction 
of 
logic 
programs. 
Muggleton 
et 
al. 
0.0054 
Learning 
logical 
denitions 
from 
relations. 
Quinlan. 
0.0033 
Inductive 
Logic 
Programming 
Techniques 
and 
Applications. 
Lavrac 
et 
al. 


Table 
15.1: 
Highest 
ranked 
documents 
according 
to 
p(cjz). 
The 
factor 
topic 
labels 
are 
manual 
assignments 
based 
on 
similarities 
to 
the 
Cora 
topics. 
Reproduced 
from 
[63]. 


community. 


We 
use 
the 
variable 
d 
2f1;:::;D} 
to 
index 
documents 
and 
c 
2f1;:::;D} 
to 
index 
citations 
(both 
d 
and 
c 
have 
the 
same 
domain, 
namely 
the 
index 
of 
a 
research 
article). 
If 
document 
d 
= 
i 
cites 
article 
c 
= 
j 
then 
we 
set 
the 
entry 
of 
the 
matrix 
Cij 
= 
1. 
If 
there 
is 
no 
citation, 
Cij 
is 
set 
to 
zero. 
We 
can 
form 
a 
`distribution’ 
over 
documents 
and 
citations 
using 


Cij

p(d 
= 
i, 
c 
= 
j)= 
P(15.6.22) 


ij 
Cij 


Example 
74 
(Modelling 
citations). 
The 
Cora 
corpus 
(www.cs.umass.edu/mccallum) 
contains 
an 
archive 
of 
around 
30,000 
computer 
science 
research 
papers. 
From 
this 
archive 
the 
authors 
in 
[63] 
extracted 
the 
papers 
in 
the 
machine 
learning 
category, 
consisting 
of 
4220 
documents 
and 
38,372 
citations. 
Using 
these 
the 
distribution 
equation 
(15.6.22) 
was 
formed. 
The 
documents 
have 
additionally 
been 
categorised 
by 
hand 
into 
7 
topics: 
Case-based 
reasoning, 
Genetic 
Algorithms, 
Neural 
Networks, 
Probabilistic 
methods, 
Reinforcement 
Learning, 
Rule 
Learning 
and 
Theory. 


In 
[63] 
the 
joint 
PLSA 
method 
is 
tted 
to 
the 
data 
using 
z 
= 
7 
topics. 
From 
the 
trained 
model 
the 
expression 
p(c 
= 
jjz 
= 
k) 
denes 
how 
authoritative 
paper 
j 
is 
according 
to 
community 
z 
= 
k. 
The 
results 
are 
presented 
in 
table(15.1) 
and 
show 
how 
the 
method 
discovers 
intuitively 
meaningful 
topics. 


Modelling 
the 
web 


Consider 
a 
collection 
of 
websites, 
indexed 
by 
i. 
If 
website 
j 
points 
to 
website 
i, 
one 
sets 
Cij 
= 
1 
giving 
a 
directed 
graph 
of 
website-to-website 
links. 
Since 
a 
website 
will 
discuss 
usually 
only 
of 
a 
small 
number 
of 


DRAFT 
March 
9, 
2010 



Kernel 
PCA 


`topics’ 
we 
might 
be 
able 
to 
explain 
why 
there 
is 
a 
link 
between 
two 
websites 
using 
a 
PLSA 
decomposition. 
These 
algorithms 
have 
proved 
useful 
for 
internet 
search 
for 
example 
to 
determine 
the 
latent 
topics 
of 
websites 
and 
identify 
the 
most 
authoritative 
websites. 
See 
[64] 
for 
a 
discussion. 


15.7 
Kernel 
PCA 
Kernel 
PCA 
is 
a 
non-linear 
extension 
of 
PCA 
designed 
to 
discover 
non-linear 
manifolds. 
Here 
we 
only 
briey 
describe 
the 
approach 
and 
refer 
the 
reader 
to 
[242] 
for 
details. 
In 
kernel 
PCA, 
we 
replace 
each 
x 
by 
a 
`feature’ 
vector 
x~= 
(x). 
Note 
that 
the 
use 
of 
x~here 
does 
not 
have 
the 
interpretation 
we 
used 
before 
as 
the 
approximate 
reconstruction. 
The 
feature 
map 
f 
takes 
a 
vector 
x 
and 
produces 
a 
higher 
dimensional 
vector 
x~. 
For 
example 
we 
could 
map 
a 
two 
dimensional 
vector 
x 
=[x1;x2]T 
using 


T

22 
3

(x)=x1;x2;x 
1;x 
2;x1x2;x 
1;:::(15.7.1) 


The 
idea 
is 
then 
to 
perform 
PCA 
on 
these 
higher 
dimensional 
feature 
vectors, 
subsequently 
mapping 
back 
the 
eigenvectors 
to 
the 
original 
space 
x. 
The 
main 
challenge 
is 
to 
write 
this 
without 
explicitly 
computing 
PCA 
in 
the 
potentially 
very 
high 
dimensional 
feature 
vector 
space. 
As 
a 
reminder, 
in 
standard 
PCA, 
for 
zero 
mean 
data, 
one 
forms 
an 
eigen-decomposition 
of 
the 
sample 
matrix3 


S 
=
1 
X~
X~
T 
(15.7.2)

N 


For 
simplicity, 
we 
concentrate 
here 
on 
nding 
the 
rst 
principal 
component 
e~which 
satises 
X~
X~
Te~= 
0~

e 
(15.7.3) 
for 
corresponding 
eigenvalue 
. 
(writing 
' 
= 
N). 
The 
`dual’ 
representation 
is 
obtained 
by 
pre-multiplying 
by 
X~
T, 
so 
that 
in 
terms 
of 
~f 
= 
X~
Te~, 
the 
standard 
PCA 
eigen-problem 
reduces 
to 
solving: 
X~
TX~
~f 
= 
0~f 
(15.7.4) 
The 
feature 
eigenvector 
e~is 
then 
recovered 
using 
X~
~f 
= 
0~

e 
(15.7.5) 
We 
note 
that 
matrix 
X~
TX~
has 
elements

hi

X~
TX~
= 
(xm)T(xn) 
(15.7.6) 


mn 


and 
recognise 
this 
as 
the 
scalar 
product 
between 
vectors. 
This 
means 
that 
the 
matrix 
is 
positive 
(semi) 
denite 
and 
we 
may 
equivalently 
use 
a 
positive 
denite 
kernel, 
see 
section(19.3),

hi

X~
TX~
= 
k(xm 
, 
xn)= 
Kmn 
(15.7.7) 


mn 


Then 
equation 
(15.7.4) 
can 
be 
written 
as 
K~f 
= 
0~f 
(15.7.8) 
One 
then 
solves 
this 
eigen-equation 
to 
nd 
the 
N 
dimensional 
principal 
dual 
feature 
vector 
~f
. 
The 
projection 
of 
the 
feature 
x~is 
given 
by 
T 
~

y 
= 
x~Te~= 
1 
x~X~f 
(15.7.9)

. 


More 
generally, 
for 
a 
larger 
number 
of 
components, 
the 
ith 
kernel 
PCA 
projection 
yi 
can 
be 
expressed 
in 
terms 
of 
the 
kernel 
directly 
as 


X

1 
N

yi 
= 
k(x, 
xn)f~i 
(15.7.10)

n

Ni 


n=1 


3We 
use 
the 
normalisation 
N 
as 
opposed 
to 
N 
- 
1 
just 
for 
notational 
convenience 
– 
in 
practice, 
there 
is 
little 
dierence. 


DRAFT 
March 
9, 
2010 



Kernel 
PCA 


training data2004006008001000510152004006008001000102030
051015-202true model0102030-202
051015-0.200.2learned model0102030-0.200.2
(a) 
(b) 
(c) 
Figure 
15.16: 
Canonical 
Correlation 
Analysis. 
(a): 
Training 
data. 
The 
top 
panel 
contains 
the 
X 
matrix 
of 
1000, 
15 
dimensional 
points, 
and 
the 
bottom 
the 
corresponding 
30 
dimensional 
Y 
matrix. 
(b): 
The 
data 
in 
(a) 
was 
produced 
using 
X 
= 
Ah, 
Y 
= 
Bh 
where 
A 
is 
a 
15 
× 
1 
matrix, 
and 
B 
is 
a 
30 
× 
1 
matrix. 
(c): 
Matrices 
A 
and 
B 
learned 
by 
CCA. 
Note 
that 
they 
are 
close 
to 
the 
true 
A 
and 
B 
up 
to 
rescaling 
and 
sign 
changes. 
See 
demoCCA.m. 


where 
i 
is 
the 
eigenvalue 
label. 


The 
above 
derivation 
implicitly 
assumed 
zero 
mean 
features 
x~. 
Even 
if 
the 
original 
data 
x 
is 
zero 
mean, 
due 
to 
the 
non-linear 
mapping, 
the 
features 
may 
not 
be 
zero 
mean. 
To 
correct 
for 
this 
one 
may 
show 
that 
the 
only 
modication 
required 
is 
to 
replace 
the 
matrix 
K 
in 
equation 
(15.7.8) 
above 
with 


NNN

XXX

11 
1 
d0

d

K/ 
= 
k(xm 
, 
xn) 
- 
k(x, 
xn) 
- 
k(xm 
, 
xd)+ 
k(x, 
xd) 
(15.7.11)

mn 


NN 
N2 


d=1 
d=1 
d=1;d0=1 


Finding 
the 
reconstructions 


The 
above 
gives 
a 
procedure 
for 
nding 
the 
KPCA 
projection 
y. 
However, 
in 
many 
cases 
we 
would 
also 
like 
to 
have 
an 
approximate 
reconstruction 
using 
the 
lower 
dimensional 
y. 
This 
is 
not 
straightforward 
since 
the 
mapping 
from 
y 
to 
x 
is 
in 
general 
highly 
non-linear. 
Here 
we 
outline 
a 
procedure 
for 
achieving 
this. 


First 
we 
nd 
the 
reconstruction 
x~* 
of 
the 
feature 
space 
x~. 
Now 


XXX

* 
i 
1 
x~=yie~=yi 
f~
i
n(xn) 
(15.7.12)
i

i 
in 


Given 
x~* 
we 
try 
to 
nd 
that 
point 
x/ 
in 
the 
original 
data 
space 
that 
maps 
to 
x~* 
. 
This 
can 
be 
found 
by 
minimising 


..2



E(x0)=(x0) 
- 
x~(15.7.13) 


Up 
to 
negligable 
constants 
this 
is 


XX

yi 
f~n 
0)

0

E(x0)= 
k(x, 
x0) 
- 
2i 
k(xn 
, 
x(15.7.14)

i

in 


One 
then 
nds 
x/ 
by 
minimising 
E(x0) 
numerically. 
. 


DRAFT 
March 
9, 
2010 



Canonical 
Correlation 
Analysis 


15.8 
Canonical 
Correlation 
Analysis 
Consider 
x 
and 
y 
which 
have 
dimensions 
dim 
(x) 
and 
dim 
(y) 
respectively. 
For 
example 
x 
might 
represent 
a 
segment 
of 
video 
and 
y 
the 
corresponding 
audio. 
Given 
then 
a 
collection 
(xn 
, 
yn) 
;n 
=1;:::;N, 
an 
interesting 
challenge 
is 
to 
identify 
which 
parts 
of 
the 
audio 
and 
video 
les 
are 
strongly 
correlated. 
One 
might 
expect, 
for 
example, 
that 
the 
mouth 
region 
of 
the 
video 
is 
strongly 
correlated 
with 
the 
audio. 


One 
way 
to 
achieve 
this 
is 
to 
project 
each 
x 
and 
y 
to 
one 
dimension 
using 
aTx 
and 
bTy 
such 
that 
the 
correlation 
between 
the 
projections 
is 
maximal. 
The 
unnormalised 
correlation 
between 
the 
projections 
aTx 
and 
bTy 
is

XX

TT

axnbTyn 
= 
axnynT 
b 
(15.8.1) 
nn 


and 
the 
normalised 
correlation 
is 


aTSxyb

pp(15.8.2) 
aTSxxabTSyyb 


where 
Sxy 
is 
the 
sample 
x, 
y 
cross 
correlation 
matrix. 
When 
the 
joint 
covariance 
of 
the 
stacked 
vectors 
zn 
=[xn 
, 
yn] 
is 
considered 
Sxx, 
Sxy, 
Syx, 
Syy 
are 
the 
blocks 
of 
the 
joint 
covariance 
matrix. 


Since 
equation 
(15.8.2) 
is 
invariant 
with 
respect 
to 
length 
scaling 
of 
a 
and 
also 
b, 
we 
can 
consider 
the 
equivalent 
objective 


E(a, 
b)= 
aTSxyb 
(15.8.3) 


subject 
to 
aTSxxa 
= 
1 
and 
bTSyyb 
= 
1. 
To 
nd 
the 
optimal 
projections 
a, 
b, 
under 
the 
constraints, 
we 
use 
the 
Lagrangian, 


a 
b

L 
(a, 
b;a;b) 
= 
aTSxyb 
+1 
- 
aTSxxa 
+1 
- 
bTSyyb 
(15.8.4)

22 


from 
which 
we 
obtain 
the 
zero 
derivative 
criteria 


Sxyb 
= 
aSxxa, 
Syxa 
= 
bSyyb 
(15.8.5) 


Hence 


aTSxyb 
= 
aaTSxxa 
= 
a, 
bTSyxa 
= 
bbTSyyb 
= 
b 
(15.8.6) 


Since 
aTSxyb 
= 
bTSyxa 
we 
must 
have 
a 
= 
b 
= 
. 
at 
the 
optimum. 
If 
we 
assume 
that 
Syy 
is 
invertible, 


1 


S..1

b 
= 
yy 
Syxa 
(15.8.7)

. 


Using 
this 
to 
eliminate 
b 
in 
equation 
(15.8.5) 
we 
have 


SxyS..1Syxa 
= 
2Sxxa 
(15.8.8)

yy 


which 
is 
a 
generalised 
eigen-problem. 
Assuming 
that 
Sxx 
is 
invertible 
we 
can 
equivalently 
write 


S..1 
S..1 


xx 
Sxyyy 
Syxa 
= 
2a 
(15.8.9) 


which 
is 
a 
standard 
eigen-problem 
(albeit 
with 
2 
as 
the 
eigenvalue). 
Once 
this 
is 
solved 
we 
can 
nd 
b 
using 
equation 
(15.8.7). 


300 
DRAFT 
March 
9, 
2010 



Exercises 


15.8.1 
SVD 
formulation 
It 
is 
straightforward 
to 
show 
that 
we 
can 
nd 
a 
by 
rst 
computing 
the 
SVD 
of 


1

1

..

..

S


2

SxyS

2

(15.8.10)
xx 


yy 


in 
the 
form 
UDVT 
and 
extracting 
the 
maximal 
singular 
vector 
u1 
of 
U 
(the 
rst 
column 
on 
U). 
Then 
a 


1

1 


is 
optimally 
S


xx 
u1, 
and 
similarly, 
b 
is 
optimally 
S
the 
extension 
to 
nding 
M 
multiple 
directions 
A 


2

..

2 
=

..

yy 
v1, 
where 
v1 
is 
the 
rst 
column 
of 
. 
In 
this 
way,

V 
V
P

a1 
;:::, 
aM 
and 
B 
=b1 
;:::, 
bM 


is 
clear 


– 
one 


takes 
the 
corresponding 
rst 
M 
singular 
values 
accordingly. 
Doing 
so 
maximises 
the 
criterion 


..

traceATSxyB

ppP(15.8.11)

trace 
(ATSxxA) 
trace 
(BTSyyB) 


This 
approach 
is 
taken 
in 
cca.m 
– 
see 
g(15.16) 
for 
a 
demonstration. 
One 
can 
also 
show 
that 
CCA 
corresponds 
to 
the 
probabilistic 
Factor 
Analysis 
model 
under 
a 
block 
restriction 
on 
the 
form 
of 
the 
factor 
loadings, 
see 
section(21.2.1). 


CCA 
and 
related 
kernel 
extensions 
have 
been 
applied 
in 
machine 
learning 
contexts, 
for 
example 
to 
model 
the 
correlation 
between 
images 
and 
text 
in 
order 
to 
improve 
image 
retrieval 
from 
text 
queries, 
see 
[126]. 


15.9 
Notes 
PCA 
is 
also 
known 
as 
the 
Karhunen-Loeve 
decomposition, 
particularly 
in 
the 
engineering 
literature. 


15.10 
Code 
pca.m: 
Principal 
Components 
Analysis 
demoLSI.m: 
Demo 
of 
Latent 
Semantic 
Indexing/Analysis 
svdm.m: 
Singular 
Value 
Decomposition 
with 
missing 
data 
demoSVDmissing.m: 
Demo 
SVD 
with 
missing 
data 


plsa.m: 
Probabilistic 
Latent 
Semantic 
Analysis 
plsaCond.m: 
Conditional 
Probabilistic 
Latent 
Semantic 
Analysis 
demoPLSA.m: 
Demo 
of 
PLSA 
demoMultnomialpXYgZ.m: 
Demo 
of 
`nite 
sample’ 
PLSA 


cca.m: 
Canonical 
Correlation 
Analysis 
(CCA) 
demoCCA.m: 
Demo 
of 
Canonical 
Correlation 
Analysis 


15.11 
Exercises 
Exercise 
161. 
Consider 
a 
dataset 
in 
two 
dimensions 
where 
the 
data 
lies 
on 
the 
circumference 
of 
a 
circle 
of 
unit 
radius. 
What 
would 
be 
the 
eect 
of 
using 
PCA 
on 
this 
dataset, 
in 
which 
we 
attempt 
to 
reduce 
the 
dimensionality 
to 
1? 
Suggest 
an 
alternative 
one 
dimensional 
representation 
of 
the 
data. 


PM

Exercise 
162. 
Consider 
two 
vectors 
xa 
and 
xb 
and 
their 
corresponding 
PCA 
approximations 
c+

i=1 
aiei 


PM

and 
c 
+i=1 
biei, 
where 
the 
eigenvectors 
ei;i 
=1;:::;M 
are 
mutually 
orthogonal 
and 
have 
unit 
length. 
The 
eigenvector 
ei 
has 
corresponding 
eigenvalue 
i 
. 
Approximate 
(xa 
- 
xb)2 
by 
using 
the 
PCA 
represent
ations 
of 
the 
data, 
and 
show 
that 
this 
is 
equal 
to 
(a 
- 
b)2 
. 


Exercise 
163. 
Show 
how 
the 
solution 
for 
a 
to 
the 
CCA 
problem 
in 
equation 
(15.8.8) 
can 
be 
transformed 
into 
the 
form 
expressed 
by 
equation 
(15.8.10), 
as 
claimed 
in 
the 
text. 


DRAFT 
March 
9, 
2010 
301 



Exercises 


Exercise 
164. 
Let 
S 
be 
the 
covariance 
matrix 
of 
the 
data. 
The 
Mahalanobis 
distance 
between 
xa 
and 
xb 
is 
dened 
as

T 


bb

xa 
- 
xS..1xa 
- 
x. 
(15.11.1) 


Explain 
how 
to 
approximate 
this 
distance 
using 
M-dimensional 
PCA 
approximations. 


Exercise 
165 
(PCA 
with 
external 
inputs). 
In 
some 
applications, 
one 
may 
suspect 
that 
certain 
external 
variables 
v 
have 
a 
strong 
inuence 
on 
how 
the 
data 
x 
is 
distributed. 
For 
example, 
if 
x 
represents 
an 
image, 
it 
might 
be 
that 
we 
know 
the 
lighting 
condition 
v 
under 
which 
the 
image 
was 
made 
– 
this 
will 
have 
a 
large 
eect 
on 
the 
image. 
It 
would 
make 
sense 
therefore 
to 
include 
the 
known 
lighting 
condition 
in 
forming 
a 
lower 
dimensional 
representation 
of 
the 
image. 
Note 
that 
we 
don't 
want 
to 
form 
a 
lower 
dimensional 
representation 
of 
the 
joint 
x, 
v, 
rather 
we 
want 
to 
form 
a 
lower 
dimensional 
representation 
of 
x 
alone, 
bearing 
in 
mind 
that 
some 
of 
the 
variability 
observed 
may 
be 
due 
to 
v. 
We 
therefore 
assume 
an 
approximation 


XX

n 
nk

xn 
yj 
bj 
+vk 
c(15.11.2) 
jk 


n

where 
the 
coecients 
yi 
, 
i 
=1;:::;N, 
n 
=1;:::;N 
and 
basis 
vectors 
bj, 
j 
=1;:::;J 
and 
ck 
, 
k 
= 
1;:::;K 
are 
to 
be 
determined. 
The 
external 
inputs 
v1 
;:::, 
vN 
are 
given. 
The 
sum 
squared 
error 
loss 
between 
the 
xn 
and 
their 
linear 
reconstruction 
equation 
(15.11.2) 
is 


01

2 


XXX

n 
nj 
nk

A

E 
=@xi 
..yj 
bi 
..vk 
ci 
(15.11.3) 
n;ij 
k 


Find 
the 
parameters 
that 
minimise 
E. 


Exercise 
166. 
Consider 
the 
following 
3-dimensional 
datapoints: 


(1:3, 
1:6, 
2:8)(4:3, 
..1:4, 
5:8)(..0:6, 
3:7, 
0:7)(..0:4, 
3:2, 
5:8)(3:3, 
..0:4, 
4:3)(..0:4, 
3:1, 
0:9) 
(15.11.4) 


Perform 
Principal 
Components 
Analysis 
by: 


1. 
Calculating 
the 
mean, 
c, 
of 
the 
data. 
P6

1

2. 
Calculating 
the 
covariance 
matrix 
S 
= 
n=1 
xn(xn)T 
- 
ccT 
of 
the 
data. 
6

3. 
Finding 
the 
eigenvalues 
and 
eigenvectors 
ei 
of 
the 
covariance 
matrix. 
You 
should 
nd 
that 
only 
two 
eigenvalues 
are 
large, 
and 
therefore 
that 
the 
data 
can 
be 
well 
represented 
using 
two 
components 
only. 
Let 
e1 
and 
e2 
be 
the 
two 
eigenvectors 
with 
largest 
eigenvalues. 


1. 
Calculate 
the 
two 
dimensional 
representation 
of 
each 
datapoint 
(e1(xn..c), 
e2(xn..c));n 
=1;:::, 
6. 
2. 
Calculate 
the 
reconstruction 
of 
each 
datapoint 
c 
+(e1T(xn 
- 
c))e1 
+(e2T(xn 
- 
c))e2 
, 
n 
=1;:::, 
6. 
Exercise 
167. 
Consider 
a 
`conditional 
frequency 
matrix’ 


p(x 
= 
ijy 
= 
j) 
(15.11.5) 


Show 
how 
to 
derive 
an 
EM 
style 
algorithm 
for 
an 
approximate 
decomposition 
of 
this 
matrix 
in 
the 
form 


X

p(x 
= 
ijy 
= 
j) 
p~(x 
= 
ijz 
= 
k)~p(z 
= 
kjy 
= 
j) 
(15.11.6) 


k 


where 
k 
=1;:::;Z, 
i 
=1;:::;X, 
j 
=1;:::;Y 
. 


Exercise 
168. 
For 
the 
multinomial 
model 
p~(x, 
y, 
z) 
described 
in 
equation 
(15.6.15), 
derive 
explicitly 
the 
EM 
algorithm 
and 
implement 
this 
in 
MATLAB. 
For 
randomly 
chosen 
values 
for 
the 
conditional 
probab
ilities, 
draw 
10000 
samples 
from 
this 
model 
for 
X 
=5;Y 
=5;Z 
=4 
and 
compute 
from 
this 
the 
matrix 
with 
elements 


. 
(x 
= 
i, 
y 
= 
j)
pij 
= 
PX 
PY 
(15.11.7)

. 
(x 
= 
i, 
y 
= 
j)
i=1j=1 


Now 
run 
PLSA 
(use 
plsa.m) 
with 
the 
settings 
X 
=5;Y 
=5;Z 
=4 
to 
learn 
and 
compare 
your 
results 
with 
those 
obtained 
from 
the 
nite 
sample 
model 
equation 
(15.6.15). 


DRAFT 
March 
9, 
2010 



CHAPTER 
16 


Supervised 
Linear 
Dimension 
Reduction 


16.1 
Supervised 
Linear 
Projections 
In 
chapter(15) 
we 
discussed 
dimension 
reduction 
using 
an 
unsupervised 
procedure. 
In 
cases 
where 
class 
information 
is 
available, 
and 
our 
ultimate 
interest 
is 
to 
reduce 
dimensionality 
for 
improved 
classication, 
it 
makes 
sense 
to 
use 
the 
available 
class 
information 
in 
forming 
the 
projections. 
Exploiting 
the 
class 
label 
information 
to 
improve 
the 
projection 
is 
a 
form 
of 
supervised 
dimension 
reduction. 
Let's 
consider 
data 
from 
two 
dierent 
classes. 
For 
class 
1, 
we 
have 
a 
set 
of 
data 
N1 
datapoints, 


no

X1 
=x11
;:::, 
xN1 
(16.1.1)

1

and 
similarly 
for 
class 
2, 
we 
have 
a 
set 
of 
N2 
datapoints 


no

X2 
=x21 
;:::, 
xN2 
(16.1.2)

2

Our 
interest 
is 
then 
to 
nd 
a 
linear 
projection, 


y 
= 
WTx 
(16.1.3) 


where 
dim 
W 
= 
DL, 
L<D, 
such 
that 
for 
datapoints 
xi 
, 
xj 
in 
the 
same 
class, 
the 
distance 
between 
their 
projections 
yi 
, 
yj 
should 
be 
small. 
Conversely, 
for 
datapoints 
in 
dierent 
classes, 
the 
distance 
between 
their 
projections 
should 
be 
large. 
This 
may 
be 
useful 
for 
classication 
purposes 
since 
for 
a 
novel 
point 
x 
* 
, 
if 
its 
projection 


* 


y 
= 
WTx 
(16.1.4) 


is 
close 
to 
class 
1 
projected 
data, 
we 
would 
expect 
x 
* 
to 
belong 
to 
class 
1. 
In 
forming 
the 
supervised 
projection, 
only 
the 
class 
discriminative 
parts 
of 
the 
data 
are 
retained, 
so 
that 
the 
procedure 
can 
be 
considered 
a 
form 
of 
supervised 
feature 
extraction. 


16.2 
Fisher's 
Linear 
Discriminant 
We 
restrict 
attention 
to 
binary 
class 
data. 
Also, 
for 
simplicity, 
we 
project 
the 
data 
down 
to 
one 
dimension. 
The 
canonical 
variates 
algorithm 
of 
section(16.3) 
deals 
with 
the 
generalisations. 


Gaussian 
assumption 


We 
model 
the 
data 
from 
each 
class 
with 
a 
Gaussian. 
That 
is 


p(x1)= 
N 
(x1 


m1, 
S1) 
;p(x2)= 
N 
(x2 


m2, 
S2) 
(16.2.1) 


303 



Fisher's 
Linear 
Discriminant 


-202468-5-4-3-2-1012345
-202468-5-4-3-2-1012345
(a) 
(b) 
Figure 
16.1: 
The 
large 
crosses 
represent 
data 
from 
class 
1, 
and 
the 
large 
circles 
from 
class 
2. 
Their 
projections 
onto 
1 
dimension 
are 
represented 
by 
their 
small 
counterparts. 
(a): 
Fisher's 
Linear 
Discriminant 
Analysis. 
Here 
there 
is 
little 
class 
overlap 
in 
the 
projections. 
(b): 
Unsupervised 
dimension 
reduction 
using 
Principal 
Components 
Analysis 
for 
comparison. 
There 
is 
considerable 
class 
overlap 
in 
the 
projection. 
In 
both 
(a) 
and 
(b) 
the 
one 
dimensional 
projection 
is 
the 
distance 
along 
the 
line, 
measured 
from 
an 
arbitrary 
chosen 
xed 
point 
on 
the 
line. 


where 
m1 
is 
the 
sample 
mean 
of 
class 
1 
data, 
and 
S1 
the 
sample 
covariance; 
similarly 
for 
class 
2. 
The 
projections 
of 
the 
points 
from 
the 
two 
classes 
are 
then 
given 
by 


n 
Txnn 
Txn 


y1 
= 
w1 
;y2 
= 
w2 
(16.2.2) 


Because 
the 
projections 
are 
linear, 
the 
projected 
distributions 
are 
also 
Gaussian, 


..



T

p(y1)= 
Ny1 


1;2 
;1 
= 
wm1;2 
= 
wTS1w 
(16.2.3)

11 


..



T

p(y2)= 
Ny2 


2;2 
;2 
= 
wm2;2 
= 
wTS2w 
(16.2.4)

22 


We 
search 
for 
a 
projection 
w 
such 
that 
the 
projected 
distributions 
have 
minimal 
overlap. 
This 
can 
be 
achieved 
if 
the 
projected 
Gaussian 
means 
are 
maximally 
separated, 
(1 
- 
2)2 
is 
large. 
However, 
if 
the 
variances 
12;2 
are 
also 
large, 
there 
could 
be 
a 
large 
overlap 
still 
in 
the 
classes. 
A 
useful 
objective 
function 


2 


therefore 
is 


(1 
- 
2)2 


(16.2.5)
12 
+ 
22 


12 


where 
i 
represents 
the 
fraction 
of 
the 
dataset 
in 
class 
i. 
In 
terms 
of 
the 
projection 
w, 
the 
objective 
equation 
(16.2.5) 
is 


wT 
(m1 
- 
m2)(m1 
- 
m2)T 
wwTAw 


F 
(w) 
= 
= 
(16.2.6)

wT 
(1S1 
+ 
2S2) 
wwTBw 


where 


A 
=(m1 
- 
m2)(m1 
- 
m2)T 
, 
B 
= 
1S1 
+ 
2S2 
(16.2.7) 


The 
optimal 
w 
can 
be 
found 
by 
dierentiating 
equation 
(16.2.6) 
with 
respect 
to 
w. 
This 
gives 


hi

. 
wTAw 
2 


= 
wTBwAw 
..wTAwBw(16.2.8)

@wwTBw 
(wBw)2

and 
therefore 
the 
zero 
derivative 
requirement 
is



wTBwAw 
=wTAwBw 
(16.2.9) 


DRAFT 
March 
9, 
2010 



Canonical 
Variates 


Multiplying 
by 
the 
inverse 
of 
B 
we 
have 


wTAw

B..1 
(m1 
- 
m2)(m1 
- 
m2)T 
w 
= 
w 
(16.2.10)

wTBw 


This 
means 
that 
the 
optimal 
projection 
is 
explicitly 
given 
by 


w 
. 
B..1 
(m1 
- 
m2) 
(16.2.11) 


Although 
the 
proportionality 
factor 
depends 
on 
w, 
we 
may 
take 
it 
to 
be 
constant 
since 
the 
objective 
function 
F 
(w) 
of 
equation 
(16.2.6) 
is 
invariant 
to 
rescaling 
of 
w. 
We 
may 
therefore 
take 


w 
= 
kB..1 
(m1 
- 
m2) 
(16.2.12) 


It 
is 
common 
to 
rescale 
w 
to 
have 
unit 
length, 
wTw 
= 
1, 
such 
that 


1

k 
= 
q(16.2.13) 
(m1 
- 
m2)T 
B..2 
(m1 
- 
m2) 


An 
illustration 
of 
the 
method 
is 
given 
in 
g(16.1), 
which 
demonstrates 
how 
supervised 
dimension 
reduction 
can 
produce 
lower 
dimensional 
representations 
more 
suitable 
for 
subsequent 
classication 
than 
an 
unsupervised 
method 
such 
as 
PCA. 


One 
can 
also 
arrive 
at 
the 
equation 
(16.2.12) 
from 
a 
dierent 
starting 
objective. 
By 
treating 
the 
projection 
as 
a 
regression 
problem 
y 
= 
wTx+b 
in 
which 
the 
outputs 
y 
are 
dened 
as 
y1 
and 
y2 
for 
classes 
1 
and 
class 
2 
respectively, 
one 
may 
show 
that, 
for 
suitably 
chosen 
y1 
and 
y2, 
the 
solution 
using 
a 
least 
squares 
criterion 
is 
given 
by 
equation 
(16.2.12) 
[83, 
42]. 
This 
also 
suggests 
a 
way 
to 
regularise 
LDA, 
see 
exercise(171). 
Kernel 
extensions 
of 
LDA 
are 
possible, 
see 
for 
example 
[78, 
248]. 


When 
the 
naive 
method 
breaks 
down 


The 
above 
derivation 
relied 
on 
the 
existence 
of 
the 
inverse 
of 
B. 
In 
practice, 
however, 
B 
may 
not 
be 
invertible, 
and 
the 
above 
procedure 
requires 
modication. 
A 
case 
where 
B 
is 
not 
invertible 
is 
when 
there 
are 
fewer 
datapoints 
N1 
+ 
N2 
than 
dimensions 
D. 
Another 
case 
is 
when 
there 
are 
elements 
of 
the 
input 
vectors 
that 
never 
vary. 
For 
example, 
in 
the 
hand-written 
digits 
case, 
the 
pixels 
at 
the 
corner 
edges 
are 
actually 
always 
zero. 
Let's 
call 
this 
corner 
pixel 
z. 
The 
matrix 
B 
will 
then 
have 
a 
zero 
entry 
for 
[B]

z;z 
(indeed 
the 
whole 
zth 
row 
and 
column 
will 
be 
zero) 
so 
that 
for 
any 
vector 


w 
= 
(0, 
0;:::;wz, 
0, 
0;:::, 
0) 
. 
wTBw 
= 
0 
(16.2.14) 


This 
shows 
that 
the 
denominator 
of 
Fisher's 
objective 
can 
become 
zero, 
and 
the 
objective 
ill 
dened. 
We 
will 
deal 
with 
these 
issues 
section(16.3.1). 


16.3 
Canonical 
Variates 
Canonical 
Variates 
generalises 
Fisher's 
method 
to 
projections 
in 
more 
than 
one 
dimension 
and 
more 
than 
two 
classes. 
The 
projection 
of 
any 
point 
is 
given 
by 


y 
= 
WTx 
(16.3.1) 


where 
W 
is 
a 
D 
× 
L 
matrix. 
Assuming 
that 
the 
data 
x 
from 
class 
c 
is 
Gaussian 
distributed, 


p(x)= 
N 
(x 


mc, 
Sc) 
(16.3.2) 


the 
projections 
y 
are 
also 
Gaussian 




p(y)= 
Ny 


WTmc, 
WTScW(16.3.3) 


To 
extend 
to 
more 
than 
two 
classes, 
we 
dene 
the 
following 
matrices: 


DRAFT 
March 
9, 
2010 



Canonical 
Variates 


Between 
class 
Scatter 
Find 
m 
the 
mean 
of 
the 
whole 
dataset 
and 
mc, 
the 
mean 
of 
the 
each 
class 
c. 
Form 


C

X

A 
= 
Nc 
(mc 
- 
m)(mc 
- 
m)T 
(16.3.4) 


c=1 
where 
Nc 
is 
the 
number 
of 
datapoints 
in 
class 
c, 
c 
=1;:::;C. 
Within 
class 
Scatter 
For 
each 
class 
c 
form 
a 
covariance 
matrix 
Sc 
and 
mean 
mc. 
Dene 
C

X

B 
= 
NcSc 
(16.3.5) 
c=1 


This 
naturally 
gives 
rise 
to 
a 
Raleigh 
quotient 
objective 


..

traceWTAW

F 
(W) 
= 
(16.3.6)

trace 
(WTBW) 
Assuming 
B 
is 
invertible 
(see 
section(16.3.1) 
otherwise), 
we 
can 
dene 
the 
Cholesky 
factor 
B~
, 
with 
B~
TB~
= 
B 
(16.3.7) 
Then 
dening 
~~B~
..1 
~

W 
= 
BW 
. 
W 
= 
W 
(16.3.8) 
~

the 
objective 
can 
be 
written 
in 
terms 
of 
W: 




W~
TB~
..TAB~
..1 
~

traceW
F 
(~(16.3.9)

W) 
= 


~

WT 
~

traceW

~

If 
we 
assume 
an 
orthonormality 
constraint 
on 
W, 
then 
we 
equivalently 
require 
the 
maximisation 
of 




F 
(~~Wsubject 
to 
WT 
~(16.3.10)

W) 
= 
traceWTC 
~, 
~W 
= 
I 
where 
C 
= 
B~
..TAB~
..1 
(16.3.11) 
Since 
C 
is 
symmetric 
and 
positive 
semi-denite, 
it 
has 
a 
real 
eigen-decomposition 
C 
= 
EET 
(16.3.12) 
where 
. 
= 
diag 
(1;2;:::;D) 
is 
diagonal 
with 
non-negative 
entries 
containing 
the 
eigenvalues, 
sorted 
by 
decreasing 
order, 
1 
= 
2 
= 
::. 
and 
ETE 
= 
I. 
Hence 




~

F 
(~WTEET 
~(16.3.13)

W) 
= 
traceW
˜


By 
setting 
W 
=[e1;:::, 
eL], 
where 
el 
is 
the 
lth 
eigenvector, 
the 
objective 
becomes 
the 
sum 
of 
the 
rst 
L 
~

eigenvalues. 
This 
setting 
maximises 
the 
objective 
function 
since 
forming 
W 
from 
any 
other 
columns 
of 
E 
would 
give 
a 
lower 
sum. 
We 
then 
return 


B~
..1 
~

W 
= 
W 
(16.3.14) 


as 
the 
projection 
matrix. 
The 
procedure 
is 
outlined 
in 
algorithm(17). 
Note 
that 
since 
A 
has 
rank 
C, 
there 
can 
be 
no 
more 
than 
C 
- 
1 
non-zero 
eigenvalues 
and 
corresponding 
directions. 


DRAFT 
March 
9, 
2010 



Canonical 
Variates 


Algorithm 
17 
Canonical 
Variates 


1: 
Compute 
the 
between 
and 
within 
class 
scatter 
matrices 
A, 
equation 
(16.3.4) 
and 
B, 
equation 
(16.3.5). 
2: 
Compute 
the 
Cholesky 
factor 
B~
of 
B. 
3: 
Compute 
the 
L 
principal 
eigenvectors 
[e1;:::, 
eL] 
of 
B~
..TAB~
..1 
. 
~

4: 
W 
=[e1;:::, 
eL] 
B~
..1 
~
5: 
Return 
W 
= 
W 
as 
the 
projection 
matrix. 
16.3.1 
Dealing 
with 
the 
nullspace 
The 
above 
derivation 
of 
Canonical 
Variates 
(and 
also 
Fisher's 
LDA) 
requires 
the 
invertibility 
of 
the 
matrix 


B. 
However, 
as 
we 
discussed 
in 
section(16.2) 
one 
may 
encounter 
situations 
where 
B 
is 
not 
invertible. 
A 
solution 
is 
to 
require 
that 
W 
lies 
only 
in 
the 
subspace 
spanned 
by 
the 
data 
(that 
is 
there 
can 
be 
no 
contribution 
from 
the 
nullspace). 
To 
do 
this 
we 
rst 
concatenate 
the 
training 
data 
from 
all 
classes 
into 
one 
large 
matrix 
X. 
A 
basis 
for 
X 
can 
be 
found 
using, 
for 
example, 
the 
thin-SVD 
technique 
which 
returns 
an 
orthonormal 
basis 
Q. 
We 
then 
require 
the 
solution 
W 
to 
be 
expressed 
in 
this 
basis: 
W 
= 
QW/ 
(16.3.15) 


for 
some 
matrix 
W0. 
Substituting 
this 
in 
the 
Canonical 
Variates 
objective 
equation 
(16.3.6), 
we 
obtain 


 


traceW0TQTAQW0
F 
(W0) 
= 
..(16.3.16)
traceW0TQTBQW0

This 
is 
of 
the 
same 
form 
as 
the 
standard 
quotient, 
equation 
(16.3.6), 
on 
replacing 
the 
between-scatter 
A 
with 


A/ 
= 
QTAQ 
(16.3.17) 


and 
the 
within-scatter 
B 
with 


B/ 
= 
QTBQ 
(16.3.18) 


In 
this 
case 
B/ 
is 
guaranteed 
invertible, 
and 
one 
may 
carry 
out 
Canonical 
Variates, 
as 
in 
section(16.3) 
above. 
This 
will 
return 
a 
matrix 
W0. 
We 
then 
return 


W 
= 
QW/ 
(16.3.19) 


See 
also 
CanonVar.m. 


Example 
75 
(Using 
canonical 
variates 
on 
the 
Digits 
Data). 
We 
apply 
canonical 
variates 
to 
project 
the 
digit 
data 
onto 
two 
dimensions, 
see 
g(16.3). 
There 
are 
800 
examples 
of 
a 
three, 
800 
examples 
of 
a 
ve 
and 
800 
examples 
of 
a 
seven. 
Thus, 
overall, 
there 
are 
2400 
examples 
lying 
in 
a 
784 
(28 
28 
pixels) 
dimensional 
space. 
Note 
how 
the 
canonical 
variates 
projected 
data 
onto 
two 
dimensions 
has 
very 
little 
class 
overlap, 
see 
g(16.3a). 
In 
comparison 
the 
projections 
formed 
from 
PCA, 
which 
discards 
the 
class 
information, 
displays 
a 
high 
degree 
of 
class 
overlap. 
The 
dierent 
scales 
of 
the 
canonical 
variates 
and 
PCA 
projections 


q1
q2
Figure 
16.2: 
Each 
three 
dimensional 
datapoint 
lies 
in 
a 
two-dimensional 
plane, 
meaning 
that 
the 
matrix 
B 
is 
not 
full 
rank, 
and 
therefore 
not 
invertible. 
A 
solution 
is 
given 
by 
nding 
vectors 
q1, 
q2 
that 
span 
the 
plane, 
and 
expressing 
the 
Canonical 
Variates 
solution 
in 
terms 
of 
these 
vectors 
alone. 


DRAFT 
March 
9, 
2010 



Exercises 


-0.1-0.0500.050.10.15-0.06-0.04-0.0200.020.040.060.080.10.120.14
-3000-2500-2000-1500-1000-500-1000-50005001000
(a) 
(b) 
Figure 
16.3: 
(a): 
Canonical 
Variates 
projection 
of 
examples 
of 
handwritten 
digits 
3(`+'), 
5(`o') 
and 
7(diamond). 
There 
are 
800 
examples 
from 
each 
digit 
class. 
Plotted 
are 
the 
projections 
down 
to 
2 
dimensions. 
(b): 
PCA 
projections 
for 
comparison. 


is 
due 
to 
the 
dierent 
constraints 
on 
the 
projection 
matrices 
W. 
In 
PCA 
W 
is 
unitary; 
in 
canonical 
variates 
WTBW 
= 
I, 
meaning 
that 
W 
will 
scale 
with 
the 
inverse 
square 
root 
of 
the 
largest 
eigenvalues 
of 
the 
within 
class 
scatter 
matrix. 
Since 
the 
canonical 
variates 
objective 
is 
independent 
of 
linear 
scaling, 
W 
can 
be 
rescaled 
with 
an 
arbitrary 
scalar 
prefactor 
W, 
as 
desired. 


16.4 
Using 
non-Gaussian 
Data 
Distributions 
The 
applicability 
of 
canonical 
variates 
depends 
on 
our 
assumption 
that 
a 
Gaussian 
is 
a 
good 
description 
of 
the 
data. 
Clearly, 
if 
the 
data 
is 
multimodal, 
using 
a 
single 
Gaussian 
to 
model 
the 
data 
in 
each 
class 
is 
a 
poor 
assumption. 
This 
may 
result 
in 
projections 
with 
a 
large 
class 
overlap. 
In 
principle, 
there 
is 
no 
conceptual 
diculty 
in 
using 
more 
complex 
distributions, 
with 
say 
more 
general 
criteria 
such 
as 
Kullback-
Leibler 
divergence 
between 
projected 
distributions 
used 
as 
the 
objective. 
However, 
such 
criteria 
typically 
result 
in 
dicult 
optimisation 
problems. 
Canonical 
variates 
is 
popular 
due 
to 
its 
simplicity 
and 
lack 
of 
local 
optima 
issues 
in 
constructing 
the 
projection. 


16.5 
Code 
CanonVar.m: 
Canonical 
Variates 
demoCanonVarDigits.m: 
Demo 
for 
Canonical 
Variates 


16.6 
Exercises 
Exercise 
169. 
What 
happens 
to 
Fisher's 
Linear 
Discriminant 
if 
there 
are 
less 
datapoints 
than 
dimensions? 


Exercise 
170. 
Modify 
demoCanonVarDigits.m 
to 
project 
and 
visualise 
the 
digits 
data 
in 
3 
dimensions. 


Exercise 
171. 
Consider 
N1 
class 
1 
datapoints 
xn1 
;n1 
=1;:::;N1 
and 
class 
2 
datapoints 
xn2 
;n2 
= 
1;:::;N2. 
We 
will 
make 
a 
linear 
predictor 
for 
the 
data, 


T

y 
= 
wx 
+ 
b 
(16.6.1) 


308 
DRAFT 
March 
9, 
2010 



Exercises 


with 
the 
aim 
to 
predict 
value 
y1 
for 
data 
from 
class 
1 
and 
y2 
for 
data 
from 
class 
two. 
A 
measure 
of 
the 
t 
is 
given 
by 


N1N2

X2 
X2 


TT

E(w;bjy1;y2)= 
y1 
- 
wxn1 
- 
b+ 
y2 
- 
wxn2 
- 
b(16.6.2) 


n1=1n2=1

Show 
that 
by 
setting 
y1 
=(N1 
+ 
N2)=N1 
and 
y2 
=(N1 
+ 
N2)=N2 
the 
w 
which 
minimises 
E 
corresponds 
to 
Fisher's 
LDA 
solution. 
Hint: 
rst 
show 
that 
the 
two 
zero 
derivative 
conditions 
are

XXy1 
- 
b 
- 
wTxn1+y2 
- 
b 
- 
wTxn2= 
0 
(16.6.3) 
n1n2
and
XXy1 
- 
b 
- 
wTxn1xT 
n1 
+y2 
- 
b 
- 
wTxn2xT 
n2 
= 
0 
(16.6.4) 
n1n2

which 
can 
be 
reduced 
to 
the 
single 
equation 




N 
(m1 
- 
m2)=NB 
+ 
N1N2 
(m1 
- 
m2)(m1 
- 
m2)Tw 
(16.6.5)

N 


where 
B 
is 
as 
dened 
for 
LDA 
in 
the 
text, 
equation 
(16.2.7). 


Note 
that 
this 
suggests 
a 
way 
to 
regularise 
LDA, 
namely 
by 
adding 
on 
a 
term 
wTw 
to 
E(w;bjy1;y2). 
This 
can 
be 
absorbed 
into 
redening 
equation 
(16.3.5) 
as 


B' 
= 
B 
+ 
I 
(16.6.6) 


In 
other 
words, 
one 
can 
increase 
the 
covariance 
B 
by 
an 
additive 
amount 
I. 
The 
optimal 
regularising 
constant 
. 
may 
be 
set 
by 
cross-validation. 
More 
generally 
one 
can 
consider 
the 
use 
of 
a 
regularising 
matrix 
R, 
where 
R 
is 
positive 
denite. 


Exercise 
172. 
Consider 
the 
digit 
data 
of 
892 
ves 
digit5.mat 
and 
1028 
sevens 
digit7.mat. 
Make 
a 
training 
set 
which 
consists 
of 
the 
rst 
500 
examples 
from 
each 
digit 
class. 
Use 
Canonical 
Variates 
to 
rst 
project 
the 
data 
down 
to 
50 
dimensions 
and 
compute 
the 
Nearest 
Neighbour 
performance 
on 
the 
remaining 
digits. 
Compare 
the 
classication 
accuracy 
to 
using 
Nearest 
Neighbours 
the 
projections 
from 
PCA 
using 
50 
components. 


Exercise 
173. 
Consider 
an 
objective 
function 
of 
the 
form 


A(w)

F 
(w) 
= 
(16.6.7)

B(w) 


where 
A(w) 
and 
B(w) 
are 
positive 
functions, 
and 
our 
task 
is 
to 
maximise 
F 
(w) 
with 
respect 
to 
w. 
It 
may 
be 
that 
this 
objective 
does 
not 
have 
a 
simple 
algebraic 
solution, 
even 
though 
A(w) 
and 
B(w) 
are 
simple 
functions. 


We 
can 
consider 
an 
alternative 
objective, 
namely 


J(w, 
)= 
A(w) 
- 
B(w) 
(16.6.8) 


where 
. 
is 
a 
constant 
scalar. 
Choose 
an 
initial 
point 
wold 
at 
random 
and 
set 


old 
= 
A(w 
old)=B(w 
old) 


(16.6.9) 
In 
that 
case 
J(wold;old)=0. 
Now 
choose 
a 
w 
such 
that 


J(w, 
old)= 
A(w) 
- 
oldB(w) 
= 
0 
(16.6.10) 


This 
is 
certainly 
possible 
since 
J(wold;old)=0. 
If 
we 
can 
nd 
a 
w 
such 
that 
J(w, 
old) 
> 
0, 
then 


A(w) 
- 
oldB(w) 
> 
0 


(16.6.11) 
Show 
that 
for 
such 
a 
w, 
F 
(w) 
>F 
(wold), 
and 
suggest 
an 
iterative 
optimisation 
procedure 
for 
objective 
functions 
of 
the 
form 
F 
(w). 


DRAFT 
March 
9, 
2010 



Exercises 


DRAFT 
March 
9, 
2010 



CHAPTER 
17 


Linear 
Models 


17.1 
Introduction: 
Fitting 
A 
Straight 
Line 
nn

Given 
training 
data 
f(x;yn) 
;n 
=1;:::;Ng, 
for 
scalar 
input 
xand 
scalar 
output 
yn, 
a 
linear 
regression 
t 
is 


y(x)= 
a 
+ 
bx 
(17.1.1) 


To 
determine 
the 
best 
parameters 
a, 
b, 
we 
use 
a 
measure 
of 
the 
discrepancy 
between 
the 
observed 
outputs 
and 
the 
linear 
regression 
t 
such 
as 
the 
sum 
squared 
training 
error. 
This 
is 
also 
called 
ordinary 
least 
squares 
and 
minimises 
the 
average 
vertical 
projection 
of 
the 
points 
y 
to 
tted 
line: 


NN

XX


E(a, 
b)= 
[y 
n 
- 
y(x 
n)]2 
=(y 
n 
- 
a 
- 
bxn)2 
(17.1.2) 
n=1 
n=1 


Our 
task 
is 
to 
nd 
the 
parameters 
a 
and 
b 
that 
minimise 
E(a, 
b). 
Dierentiating 
with 
respect 
to 
a 
and 
b 
we 
obtain 


NN

n

@
E(a, 
b)= 
..2 
X
(y 
n 
- 
a 
- 
bxn), 
@
E(a, 
b)= 
..2 
X
(y 
n 
- 
a 
- 
bxn)x 
(17.1.3)

@a 
@b 


n=1 
n=1 


Dividing 
by 
N 
and 
equating 
to 
zero, 
the 
optimal 
parameters 
are 
given 
from 
the 
solution 
to 
the 
two 
linear 
equations 





2

hyi- 
a 
- 
b 
hx) 
=0, 
hxyi- 
a 
hxi- 
bx 
= 
0 
(17.1.4) 
1 
PNn

where 
we 
used 
the 
notation 
hf(x, 
y)) 
to 
denote 
f(x;yn). 
We 
can 
readily 
solve 
the 
equations(17.1.4) 


Nn=1 


to 
determine 
a 
and 
b: 
a 
= 
hyi- 
b 
hx) 
(17.1.5) 


i


h


22

bx 
= 
hyxi..hx) 
(hyi- 
b 
hxi) 
. 
bx 
..hxi2= 
hxyi..hxihy) 
(17.1.6) 


Hence 


hxyi..hxihyi

b 
= 
(17.1.7)
hx2i..hxi2 
and 
a 
is 
found 
by 
substituting 
this 
value 
for 
b 
into 
equation 
(17.1.5). 
In 
contrast 
to 
ordinary 
least 
squares 
regression, 
PCA 
from 
chapter(15) 
minimises 
the 
orthogonal 
projection 
of 
y 
to 
the 
line 
and 
is 
known 
as 
orthogonal 
least 
squares 
– 
see 
example(76). 


311 



Linear 
Parameter 
Models 
for 
Regression 


65707580859095100121416182022chirps per sectemperature (F)
Figure 
17.1: 
Data 
from 
crickets 
– 
the 
number 
of 
chirps 
per 
second, 
versus 
the 
temperature 
in 
Fahrenheit. 


Example 
76. 
Consider 
the 
data 
in 
g(17.1), 
in 
which 
we 
plot 
the 
number 
of 
chirps 
c 
per 
second 
for 
crickets, 
versus 
the 
temperature 
t 
in 
degrees 
Fahrenheit. 
A 
biologist 
believes 
that 
there 
is 
a 
simple 
relation 
between 
the 
number 
of 
chirps 
and 
the 
temperature 
of 
the 
form 


c 
= 
a 
+ 
bt 
(17.1.8) 


where 
she 
needs 
to 
determine 
the 
parameters 
a 
and 
b. 
For 
the 
cricket 
data, 
the 
t 
is 
plotted 
in 
g(17.2a). 
For 
comparison 
we 
plot 
the 
t 
from 
the 
PCA, 
g(17.2b), 
which 
minimises 
the 
sum 
of 
the 
squared 
orthogonal 
projections 
from 
the 
data 
to 
the 
line. 
In 
this 
case 
there 
is 
little 
numerical 
dierence 
between 
the 
two 
ts. 


17.2 
Linear 
Parameter 
Models 
for 
Regression 
We 
can 
generalise 
on 
the 
idea 
of 
tting 
straight 
lines 
to 
tting 
linear 
functions 
of 
vector 
inputs. 
For 
a 
dataset 
f(xn;yn) 
;n 
=1;:::;Ng, 
a 
linear 
parameter 
regression 
model 
(LPM) 
is 
dened 
by1 


y(x)= 
wT(x) 
(17.2.1) 


where 
(x) 
is 
a 
vector 
valued 
function 
of 
the 
input 
vector 
x. 
For 
example, 
in 
the 
case 
of 
a 
straight 
line 
t, 
with 
a 
scalar 
input 
and 
output, 
section(17.1), 
we 
have 


(x) 
= 
(1;x)T 
, 
w 
=(a, 
b)T 
, 
(17.2.2) 


We 
dene 
the 
train 
error 
as 
the 
sum 
of 
squared 
dierences 
between 
the 
observed 
outputs 
and 
the 
predictions 
under 
the 
linear 
model: 


N

N 


E(w)= 
(y 
n 
- 
wTn)2 
, 
where 
n 
= 
f 
(xn) 
(17.2.3) 
n=1 


We 
now 
wish 
to 
determine 
the 
parameter 
vector 
w 
that 
minimises 
E(w). 
Writing 
out 
the 
error 
in 
terms 


of 
the 
components 
of 
w, 
NN 
N 
N 
E(w) 
= 
(y 
n 
- 
win 
i 
)(y 
n 
- 
wjn 
j 
) 
(17.2.4) 
n=1 
i 
j 
Dierentiating 
with 
respect 
to 
wk, 
and 
equating 
to 
zero 
gives 
NN 
N 
N 
y 
nn 
k 
= 
wi 
n 
i 
n 
k 
(17.2.5) 
n=1 
i 
n 
or, 
in 
matrix 
notation, 


NN

XN 


nn 
n(n)T

y 
= 
w 
(17.2.6) 


n=1 
n=1 


1Note 
that 
the 
model 
is 
linear 
in 
the 
parameter 
w 
– 
not 
necessarily 
linear 
in 
x. 


DRAFT 
March 
9, 
2010 



Linear 
Parameter 
Models 
for 
Regression 


65707580859095100121416182022chirps per sectemperature (F)
65707580859095100121416182022chirps per sectemperature (F)
(a) 
(b) 
Figure 
17.2: 
(a): 
Straight 
line 
regression 
t 
to 
the 
cricket 
data. 
(b): 
PCA 
t 
to 
the 
data. 
In 
regression 
we 
minimize 
the 
residuals 
– 
the 
vertical 
distances 
from 
datapoints 
to 
the 
line. 
In 
PCA 
the 
t 
minimizes 
the 
orthogonal 
projections 
to 
the 
line. 
In 
this 
case, 
there 
is 
little 
dierence 
in 
the 
tted 
lines. 
Both 
go 
through 
the 
mean 
of 
the 
data; 
the 
linear 
regression 
t 
has 
slope 
0.121 
and 
the 
PCA 
t 
has 
slope 
0.126. 


These 
are 
called 
the 
normal 
equations, 
for 
which 
the 
solution 
is 


 !..1

NN

XX

w 
=n(n)Ty 
nn 
(17.2.7) 
n=1 
n=1 


Although 
we 
write 
the 
solution 
using 
matrix 
inversion, 
in 
practice 
one 
nds 
the 
numerical 
solution 
using 
Gaussian 
elimination[117] 
since 
this 
is 
faster 
and 
more 
numerically 
stable. 


Example 
77. 
A 
cubic 
polynomial 
t 


A 
cubic 
polynomial 
is 
given 
by 


3 


y(x)= 
w1 
+ 
w2x 
+ 
w3x 
2 
+ 
w4x 
(17.2.8) 


As 
a 
LPM, 
this 
can 
be 
expressed 
using 


..T

23

(x)=1;x;x 
;x 
(17.2.9) 


The 
ordinary 
least 
squares 
solution 
has 
the 
form 
given 
in 
equation 
(17.2.17). 
The 
tted 
cubic 
polynomial 
is 
plotted 
in 
g(17.3). 
See 
also 
demoCubicPoly.m. 


Example 
78 
(Predicting 
return). 
In 
g(17.4) 
we 
present 
tting 
an 
LPM 
with 
vector 
inputs 
x 
to 
a 
scalar 
output 
y. 
The 
vector 
x 
represents 
factors 
that 
are 
believed 
to 
aect 
the 
stock 
price 
of 
a 
company, 
with 
the 
stock 
price 
return 
given 
by 
the 
scalar 
y. 
A 
hedge 
fund 
manager 
believes 
that 
the 
returns 
may 
be 
linearly 
related 
to 
the 
factors: 


5

X

yt 
= 
wixit 
(17.2.10) 
i=1 


and 
wishes 
to 
t 
the 
parameters 
w 
in 
order 
to 
use 
the 
model 
to 
predict 
future 
stock 
returns. 
This 
is 
straightforward 
using 
ordinary 
least 
squares, 
this 
being 
simply 
an 
LPM 
with 
a 
linear 
f 
function. 
See 


65707580859095100121416182022chirps per sectemperature (F)
Figure 
17.3: 
Cubic 
polynomial 
t 
to 
the 
cricket 
data. 


DRAFT 
March 
9, 
2010 



Linear 
Parameter 
Models 
for 
Regression 


051015202500.510510152025-0.500.50510152025-0.500.50510152025-2.5-2-1.50510152025-1010510152025024
Figure 
17.4: 
Predicting 
stock 
return 
using 
a 
linear 
LPM. 
The 
top 
ve 
panels 
present 
the 
inputs 
x1;:::;x5 
for 
20 
train 
days 
(blue) 
and 
5 
test 
days 
(red). 
The 
corresponding 
train 
output 
(stock 
return) 
y 
for 
each 
day 
is 
given 
in 
the 
bottom 
panel. 
The 
predictions 
y21;:::;y25 
are 
the 
predictions 
based 


P

on 
yt 
=i 
wixit 
with 
w 
trained 
using 
ordinary 
least 
squares. 
With 
a 
regularisation 
term 
0:01wTw, 
the 
OLS 
learned 
w 
is 
[1:42, 
0:62, 
0:27, 
..0:26, 
1:54]. 
Despite 
the 
simplicity 
of 
these 
models, 
their 
application 
in 
the 
nance 
industry 
is 
widespread, 
with 
signicant 
investment 
made 
on 
collating 
factors 
x 
that 
may 
be 
indicative 
of 
future 
return. 
See 
demoLPMhedge.m. 


g(17.4) 
for 
an 
example. 
Such 
models 
also 
form 
the 
basis 
for 
more 
complex 
models 
in 
nance, 
see 
for 
example 
[194]. 


17.2.1 
Vector 
outputs 
It 
is 
straightforward 
to 
generalise 
the 
above 
framework 
to 
vector 
outputs 
y. 
Using 
a 
separate 
weight 
vector 
wi 
for 
each 
output 
component 
yi, 
we 
have 


T 


yi(x)= 
wi 
(x) 
(17.2.11) 


The 
mathematics 
follows 
similarly 
to 
before, 
and 
we 
may 
dene 
a 
training 
error 
per 
output 
as 


XXX2 


n 
T

E(w)=E(wi)=y 
- 
wi 
n(17.2.12)

i 
i 
in


Since 
the 
training 
error 
decomposes 
into 
individual 
terms, 
one 
for 
each 
output, 
the 
weights 
for 
each 
output 
can 
be 
trained 
separately. 
In 
other 
words, 
the 
problem 
decomposes 
into 
a 
set 
of 
independent 
scalar 
output 
problems. 
In 
case 
the 
parameters 
w 
are 
tied 
or 
shared 
amongst 
the 
outputs, 
the 
training 
is 
still 
straightforward 
since 
the 
objective 
function 
remains 
linear 
in 
the 
parameters, 
and 
this 
is 
left 
as 
an 
exercise 
for 
the 
interested 
reader. 


17.2.2 
Regularisation 
For 
most 
purposes, 
our 
interest 
is 
not 
just 
to 
nd 
the 
function 
that 
best 
ts 
the 
training 
data 
but 
one 
that 
that 
will 
generalise 
well. 
To 
control 
the 
complexity 
of 
the 
tted 
function 
we 
may 
add 
an 
extra 
`regularising’ 
(or 
`penalty') 
term 
to 
the 
training 
error 
to 
penalise 
rapid 
changes 
in 
the 
output. 
For 
example 
a 
regularising 
term 
that 
can 
be 
added 
to 
equation 
(17.2.3) 
is 


NN2h

XXn 
0i2

..xn..x' 


e 
y(xn) 
- 
y(xn)(17.2.13) 
n=1 
n0=1 
hi2

0

The 
factory(xn) 
- 
y(xn)penalises 
large 
dierences 
in 
the 
outputs 
corresponding 
to 
two 
inputs. 
The 


2

0

..xn..x

factor 
e 
n
has 
the 
eect 
of 
weighting 
more 
heavily 
terms 
for 
which 
two 
input 
vectors 
xn 
and 


0

xnare 
close 
together; 
. 
is 
a 
xed 
length-scale 
parameter 
and 
. 
determines 
the 
overall 
strength 
of 
the 
regularising 
term. 


DRAFT 
March 
9, 
2010 



Linear 
Parameter 
Models 
for 
Regression 


00.20.40.60.8100.10.20.30.40.50.60.70.80.91x
Figure 
17.5: 
A 
set 
of 
xed-width 
(a 
= 
1) 
radial 
basis 
functions, 
- 
1 
(x..mi)2 


e 
2 
, 
with 
the 
centres 
mi 
evenly 
spaced. 
By 
taking 
a 
linear 
combination 
of 
these 
functions 
we 
can 
form 
a 
exible 
function 
class. 


Since 
y 
= 
wT(x), 
expression 
(17.2.13) 
can 
be 
written 
as 


wTRw 
(17.2.14) 
where 
R 
= 
. 
NXNXe 
..xn..xn02 
Xn 
- 
n0XXn 
- 
n0T 
(17.2.15) 


n=1 
n0=1 


The 
regularised 
train 
error 
is 
then 


N

X

E0(w)= 
(y 
n 
- 
wTn)2 
+ 
wTRw 
(17.2.16) 
n=1 


By 
dierentiating 
the 
regularised 
training 
error 
and 
equating 
to 
zero, 
we 
nd 
the 
optimal 
w 
is 
given 
by 


 !..1 


N

XX

w 
=n(n)T 
+ 
Ry 
nn 
(17.2.17) 
nn=1 


In 
practice 
it 
is 
common 
to 
use 
a 
regulariser 
that 
penalises 
the 
sum 
square 
length 
of 
the 
weights 


X

T2

ww 
= 
w 
(17.2.18)

i 
i 


which 
corresponds 
to 
setting 
R 
= 
I. 
Regularising 
pararameters 
such 
as 
, 
. 
may 
be 
determined 
using 
a 
validation 
set, 
section(13.2.3). 


17.2.3 
Radial 
basis 
functions 
A 
popular 
LPM 
is 
given 
by 
the 
non-linear 
function 
(x) 
with 
components 




1 
..i2

i(x) 
= 
exp- 
x 
- 
m(17.2.19)

22

These 
basis 
functions 
are 
bump 
shaped, 
with 
the 
center 
of 
the 
bump 
i 
being 
given 
by 
mi 
and 
the 
width 
by 
. 
An 
example 
is 
given 
in 
g(17.5) 
in 
which 
several 
RBFs 
are 
plotted 
with 
dierent 
centres. 
In 
LPM 
regression, 
we 
can 
then 
use 
a 
linear 
combination 
of 
these 
`bumps’ 
to 
t 
the 
data. 


One 
can 
apply 
the 
same 
approach 
using 
vector 
inputs. 
For 
vector 
x 
and 
centre 
m, 
the 
radial 
basis 
function 
depends 
on 
the 
distance 
between 
x 
and 
the 
centre 
m, 
giving 
a 
`bump’ 
in 
input 
space, 
g(17.8). 


Example 
79 
(Setting 
). 
Consider 
tting 
the 
data 
in 
g(17.6) 
using 
16 
radial 
basis 
functions 
uniformly 
spread 
over 
the 
input 
space, 
with 
width 
parameter 
a 
and 
regularising 
term 
wTw. 
The 
generalisation 
performance 
on 
the 
test 
data 
depends 
heavily 
on 
the 
width 
and 
regularising 
parameter 
. 
In 
order 
to 
nd 
reasonable 
values 
for 
these 
parameters 
we 
may 
use 
a 
validation 
set. 
For 
simplicity 
we 
set 
the 
regularisation 
parameter 
to 
. 
=0:0001 
and 
use 
the 
validation 
set 
to 
determine 
a 
suitable 
. 
In 
g(17.7) 
we 
plot 
the 
validation 
error 
as 
a 
function 
of 
. 
Based 
on 
this 
graph, 
we 
can 
nd 
the 
best 
value 
of 
; 
that 
which 
minimises 
the 
validation 
error. 
The 
predictions 
are 
also 
given 
in 
g(17.6). 


DRAFT 
March 
9, 
2010 



The 
Dual 
Representation 
and 
Kernels 


00.10.20.30.40.50.60.70.80.91-1.5-1-0.500.511.5
Figure 
17.6: 
The 
× 
are 
the 
training 
points, 
and 
the 
+ 
are 
the 
validation 
points. 
The 
solid 
line 
is 
the 
correct 
underlying 
function 
sin(10x) 
which 
is 
corrupted 
with 
a 
small 
amount 
of 
additive 
noise 
to 
form 
the 
train 
data. 
The 
dashed 
line 
is 
the 
best 
predictor 
based 
on 
the 
validation 
set. 


A 
curse 
of 
dimensionality 


If 
the 
data 
has 
non-trivial 
behaviour 
over 
some 
region 
in 
x, 
then 
we 
need 
to 
cover 
the 
region 
of 
x 
space 
fairly 
densely 
with 
`bump’ 
type 
functions. 
In 
the 
above 
case, 
we 
used 
16 
basis 
functions 
for 
this 
one 
dimensional 
space. 
In 
2 
dimensions 
if 
we 
wish 
to 
cover 
each 
dimension 
to 
the 
same 
discretisation 
level, 
we 
would 
need 
162 
= 
256 
basis 
functions. 
Similarly, 
for 
10 
dimensions 
we 
would 
need 
1610 
˜ 
1012 
functions. 
To 
t 
such 
an 
LPM 
would 
require 
solving 
a 
linear 
system 
in 
more 
than 
1012 
variables. 
This 
explosion 
in 
the 
number 
of 
basis 
functions 
with 
the 
input 
dimension 
is 
a 
`curse 
of 
dimensionality'. 


A 
possible 
remedy 
is 
to 
make 
the 
basis 
functions 
very 
broad 
so 
that 
each 
covers 
more 
of 
the 
high 
dimensional 
space. 
However, 
this 
will 
mean 
a 
lack 
of 
exibility 
of 
the 
tted 
function 
since 
it 
is 
constrained 
to 
be 
smooth. 
Another 
approach 
is 
to 
place 
basis 
functions 
centred 
on 
the 
training 
input 
points 
and 
add 
some 
more 
basis 
functions 
randomly 
placed 
close 
to 
the 
training 
inputs. 
The 
rational 
behind 
this 
is 
that 
when 
we 
come 
to 
do 
prediction, 
we 
will 
most 
likely 
see 
novel 
x 
that 
are 
close 
to 
the 
training 
points 
– 
we 
do 
not 
need 
to 
make 
`accurate’ 
predictions 
over 
all 
the 
space. 
A 
further 
approach 
is 
to 
make 
the 
positions 
of 
the 
basis 
functions 
adaptive, 
allowing 
them 
to 
be 
moved 
around 
in 
the 
space 
to 
minimise 
the 
error. 
This 
approach 
motivates 
the 
neural 
network 
models[41]. 
An 
alternative 
is 
to 
reexpress 
the 
problem 
of 
tting 
an 
LPM 
by 
reparameterising 
the 
problem, 
as 
discussed 
below. 


17.3 
The 
Dual 
Representation 
and 
Kernels 
n

Consider 
a 
set 
of 
training 
data 
with 
inputs, 
X 
= 
fxn;n 
=1;:::;N} 
and 
corresponding 
outputs 
y;n 
= 
1;:::;N. 
For 
an 
LPM 
of 
the 
form 


T

f(x)= 
wx 
(17.3.1) 


our 
interest 
is 
to 
nd 
the 
`best 
t’ 
parameters 
w. 
We 
assume 
that 
we 
have 
found 
an 
optimal 
parameter 
w. 
The 
nullspace 
of 
X 
are 
those 
x. 
which 
are 
orthogonal 
to 
all 
the 
inputs 
in 
X 
. 
That 
is,

T 


?

xxn 
=0, 
(17.3.2) 


for 
all 
n. 
If 
we 
then 
consider 
the 
vector 
w* 
with 
an 
additional 
component 
in 
the 
direction 
orthogonal 
to 
the 
space 
spanned 
by 
X 
,

T 


?T

w* 
+ 
xxn 
= 
w
* 
xn 
(17.3.3) 


00.20.40.60.81012345678alphavalidation error
Figure 
17.7: 
The 
validation 
error 
as 
a 
function 
of 
the 
basis 
function 
width 
for 
the 
validation 
data 
in 
g(17.6) 
and 
RBFs 
in 
g(17.5). 
Based 
on 
the 
validation 
error, 
the 
optimal 
setting 
of 
the 
basis 
function 
width 
parameter 
is 
a 
=0:25. 


DRAFT 
March 
9, 
2010 



The 
Dual 
Representation 
and 
Kernels 


-1-0.500.51-1-0.8-0.6-0.4-0.200.20.40.60.8100.51x(1)x(2)
-1-0.500.51-1-0.500.5100.511.5x(1)
x(2)
Figure 
17.8: 
(a): 
The 
output 
of 
an 
RBF

..2

function 
exp(..1 
x 
- 
m1=2). 
Here 
m1 
= 


2 


(0, 
0:3)T 
and 
a 
=0:25. 
(b): 
The 
combined 
output 
for 
two 
RBFs 
with 
m1 
as 
above 
and 
m2 
= 
(0:5, 
..0:5)T 
. 


(a) 
(b) 
This 
means 
that 
adding 
a 
contribution 
to 
w* 
outside 
of 
the 
space 
spanned 
by 
X 
, 
has 
no 
eect 
on 
the 
predictions 
on 
the 
train 
data. 
If 
the 
training 
criterion 
depends 
only 
on 
how 
well 
the 
LPM 
predicts 
the 


train 
data, 
there 
is 
therefore 
no 
need 
to 
consider 
contributions 
to 
w 
from 
outside 
of 
X 
. 
loss 
of 
generality 
we 
may 
consider 
the 
representation 
That 
is, 
without 
w 
= 
NXanxn 
(17.3.4) 


n=1 


The 
parameters 
a 
=(a1;:::;aN 
) 
are 
called 
the 
dual 
parameters. 
We 
can 
then 
write 
the 
output 
of 
the 
LPM 
directly 
in 
terms 
of 
the 
dual 
parameters, 


N

X

T

wxn 
= 
am 
(xm)T 
xn 
(17.3.5) 
m=1 


More 
generally, 
for 
a 
vector 
function 
(x), 
the 
solution 
will 
lie 
in 
the 
space 
spanned 
by 
(x1);:::, 
(xN 
), 


N

X

w 
= 
anf 
(xn) 
(17.3.6) 
n=1 


and 
we 
may 
write 


NN

XX

wT(xn)= 
amf 
(xm)T 
f 
(xn)= 
amK 
(xm 
, 
xn) 
(17.3.7) 
m=1 
m=1 


where 
we 
have 
dened 
a 
kernel 
function 


K 
(xm 
, 
xn) 
= 
f 
(xm)T 
f 
(xn) 
= 
[K](17.3.8)

m;n 


In 
matrix 
form, 
the 
output 
of 
the 
LPM 
on 
a 
training 
input 
x 
is 
then 


Tkn

wT(xn)=[Ka]= 
a(17.3.9)

n 


where 
kn 
is 
the 
nth 
column 
of 
the 
Gram 
matrix 
K. 


17.3.1 
Regression 
in 
the 
dual-space 
For 
ordinary 
least 
squares 
regression, 
using 
equation 
(17.3.9), 
we 
have 
a 
train 
error 


N

X2 


E(a)= 
y 
n 
- 
aTkn(17.3.10) 


n=1

Equation(17.3.10) 
is 
analogous 
to 
the 
standard 
regression 
equation 
(17.2.3) 
on 
interchanging 
a 
for 
w 
and 
kn 
for 
(xn). 
Similarly, 
the 
regularisation 
term 
can 
be 
expressed 
as 


N

X

T

ww 
= 
anamf 
(xn) 
f 
(xm)= 
aTKa 
(17.3.11) 
n;m=1 


DRAFT 
March 
9, 
2010 



The 
Dual 
Representation 
and 
Kernels 


By 
direct 
analogy 
the 
optimal 
solution 
for 
a 
is 
therefore 


 

N!..1 
N

XX

a 
=kn 
(kn)T 
+ 
Ky 
nkn 
(17.3.12) 
n=1 
n=1 


We 
can 
express 
the 
above 
solution 
more 
conveniently 
by 
rst 
writing 


 

N!..1 
N

XX

a 
=K..1kn 
(kn)T 
+ 
Iy 
nK..1kn 
(17.3.13) 
n=1 
n=1 


th 
th

Since 
kn 
is 
the 
ncolumn 
of 
K 
then 
K..1kn 
is 
the 
ncolumn 
of 
the 
identity 
matrix. 
With 
a 
little 
thought, 
we 
can 
rewrite 
equation 
(17.3.13) 
more 
simply 
as 


a 
=(K 
+ 
I)..1 
y 
(17.3.14) 


1N

where 
y 
is 
the 
vector 
with 
components 
formed 
from 
the 
training 
inputs 
y;:::;y. 


Using 
this, 
the 
prediction 
for 
a 
new 
input 
x 
* 
is 
given 
by 


y(x 
)= 
kT 
(K 
+ 
I)..1 
y 
(17.3.15)

* 


where 
the 
vector 
k* 
has 
components 




[k]= 
K 
(x 
, 
xm) 
(17.3.16)

m 


This 
dual 
space 
solution 
shows 
that 
predictions 
can 
be 
expressed 
purely 
in 
terms 
of 
the 
kernel 
K 
(x, 
x0). 
This 
means 
that 
we 
may 
dispense 
with 
dening 
the 
vector 
functions 
(x) 
and 
dene 
a 
kernel 
function 
directly. 
This 
approach 
is 
also 
used 
in 
Gaussian 
Processes, 
chapter(19) 
and 
enables 
us 
to 
use 
eectively 
very 
large 
(even 
innite) 
dimensional 
vectors 
f 
without 
ever 
explicitly 
needing 
to 
compute 
them. 
Note 
that 
the 
Gram 
matrix 
K 
has 
dimension 
N 
× 
N, 
which 
means 
that 
the 
computational 
complexity 
of 
performing 
the 


..X

matrix 
inversion 
in 
equation 
(17.3.16) 
is 
ON3 
. 
For 
moderate 
to 
large 
N 
(greater 
than 
5000), 
this 
will 
be 
prohibitively 
expensive, 
and 
numerical 
approximations 
are 
required. 
This 
is 
in 
contrast 
to 
the 
compu


X

tational 
complexity 
of 
solving 
the 
normal 
equations 
in 
the 
original 
weight 
space 
viewpoint 
is 
O 
dim 
()3 
. 


To 
an 
extent, 
the 
dual 
parameterisation 
helps 
us 
with 
the 
curse 
of 
dimensionality 
since 
the 
complexity 
of 
learning 
in 
the 
dual 
parameterisation 
scales 
cubically 
with 
the 
number 
of 
training 
points 
– 
not 
cubically 
with 
the 
dimension 
of 
the 
f 
vector. 


17.3.2 
Positive 
denite 
kernels 
(covariance 
functions) 
The 
kernel 
K 
(x, 
x0) 
in 
(17.3.8) 
was 
dened 
as 
the 
scalar 
product 
between 
two 
vectors 
(x) 
and 
(x0). 
For 
any 
set 
of 
points 
x1 
;:::, 
xM 
, 
the 
resulting 
matrix 


X

[K]= 
(xm)T(xn)=mn 
(17.3.17)

m;n 
ii 
i 


is 
positive 
semi-denite 
since 
for 
any 
z 


 ! ! !2

XXXXXX

m 
n 
m

zTKz 
=zmKmnzn 
=zmizni=zmi= 
0 
(17.3.18) 
m;n 
imn 
im 


Instead 
of 
specifying 
high-dimensional 
(x) 
vectors, 
we 
may 
instead 
specify 
a 
function 
K(x, 
x0) 
that 
produces 
a 
positive 
denite 
matrix 
K 
for 
any 
inputs 
x, 
x0. 
Such 
a 
function 
is 
called 
a 
covariance 
function, 
or 
a 
positive 
kernel. 
For 
example 
a 
popular 
choice 
is 


..jx..x0j. 


e, 
0 
<. 
= 
2;. 
= 
0 
(17.3.19) 


For 
. 
= 
2 
this 
is 
commonly 
called 
the 
squared 
exponential 
kernel. 
For 
. 
= 
1 
this 
is 
known 
as 
the 
Ornstein-Uhlenbeck 
kernel. 
Covariance 
functions 
are 
discussed 
in 
more 
detail 
in 
section(19.3). 


DRAFT 
March 
9, 
2010 



Linear 
Parameter 
Models 
for 
Classication 


-5-4-3-2-101234500.10.20.30.40.50.60.70.80.91
Figure 
17.9: 
The 
logistic 
sigmoid 
function 
(x)=1=(1+e..x). 
The 
parameter 
ß 
determines 
the 
steepness 
of 
the 
sigmoid. 
The 
full 
(blue) 
line 
is 
for 
ß 
= 
1 
and 
the 
dashed 
(red) 
for 
ß 
= 
10. 
As 
ß 
!1, 
the 
logistic 
sigmoid 
tends 
to 
a 
Heaviside 
step 
function. 
The 
dotted 
curve 
(magenta) 
is 
the 
error 
function 
(probit) 


p

0:5 
(1 
+ 
erf(x)) 
for 
. 
= 
=4, 
which 
closely 
matches 
the 
standard 
logistic 
sigmoid 
with 
ß 
= 
1. 
17.4 
Linear 
Parameter 
Models 
for 
Classication 
In 
a 
binary 
classication 
problem 
we 
are 
given 
some 
training 
data, 
D 
= 
f(xn;cn);n 
=1 
:::;Ng, 
where 
the 
targets 
c 
2f0, 
1g. 
Inspired 
by 
the 
LPM 
regression 
model, 
we 
can 
assign 
the 
probability 
that 
a 
novel 
input 
x 
belongs 
to 
class 
1 
using 


T

p(c 
=1jx)= 
f(xw) 
(17.4.1) 


where 
0 
= 
f(x) 
= 
1. 
In 
the 
statistics 
literature, 
f(x) 
is 
termed 
a 
mean 
function 
– 
the 
inverse 
function 
f..1(x) 
is 
the 
link 
function. 


Two 
popular 
choices 
for 
the 
function 
f(x) 
are 
the 
and 
probit 
functions. 
The 
logit 
is 
given 
by 


x

e1 


f(x) 
= 
= 
(17.4.2)

1+ 
ex 
1+ 
e..x 


which 
is 
also 
called 
the 
logistic 
sigmoid 
and 
written 
(x), 
g(17.9). 
The 
scaled 
version 
is 
dened 
as 


(x)= 
(x) 
(17.4.3) 


A 
closely 
related 
model 
is 
probit 
regression 
which 
uses 
in 
place 
of 
the 
logistic 
sigmoid 
the 
error 
function 
the 
cumulative 
distribution 
of 
the 
standard 
normal 
distribution 


Z

x

1 
- 
1 
t2 
1 


f(x)= 
v 
e 
2 
dt 
= 
(1 
+ 
erf(x)) 
(17.4.4)

2

2..8 


This 
can 
also 
be 
written 
in 
terms 
of 
the 
standard 
error 
function, 


Z

x 


erf(x) 
v 
2 
e 
..t2 
dt 
(17.4.5)



0 


The 
shape 
of 
the 
probit 
and 
logistic 
functions 
are 
similar 
under 
rescaling, 
see 
g(17.9). 
We 
focus 
below 
on 
the 
logit 
function. 
Similar 
derivations 
carry 
over 
in 
a 
straightforward 
manner 
to 
any 
monotonic 
mean 
function. 


17.4.1 
Logistic 
regression 
Logistic 
regression 
corresponds 
to 
the 
model 


T

p(c 
=1jx)= 
(b 
+ 
xw) 
(17.4.6) 


where 
b 
is 
a 
scalar, 
and 
w 
is 
a 
vector. 
As 
the 
argument 
b 
+ 
xTw 
of 
the 
sigmoid 
function 
increases, 
the 
probability 
x 
belongs 
to 
class 
1 
increases. 


The 
decision 
boundary 


The 
decision 
boundary 
is 
dened 
as 
that 
set 
of 
x 
for 
which 
p(c 
=1jx)= 
p(c 
=0jx)=0:5. 
This 
is 
given 
by 
the 
hyperplane 


T

b 
+ 
xw 
= 
0 
(17.4.7) 


DRAFT 
March 
9, 
2010 



Linear 
Parameter 
Models 
for 
Classication 


w
Figure 
17.10: 
The 
decision 
boundary 
p(c 
=1jx)=0:5 
(solid 
line). 
For 
two 
dimensional 
data, 
the 
decision 
boundary 
is 
a 
line. 
If 
all 
the 
training 
data 
for 
class 
1 
(lled 
circles) 
lie 
on 
one 
side 
of 
the 
line, 
and 
for 
class 
0 
(open 
circles) 
on 
the 
other, 
the 
data 
is 
said 
to 
be 
linearly 
separable. 


On 
the 
side 
of 
the 
hyperplane 
for 
which 
b 
+ 
xTw 
> 
0, 
inputs 
x 
are 
classied 
as 
1's, 
and 
on 
the 
other 
side 
they 
are 
classied 
as 
0's. 
The 
`bias’ 
parameter 
b 
simply 
shifts 
the 
decision 
boundary 
by 
a 
constant 
amount. 
The 
orientation 
of 
the 
decision 
boundary 
is 
determined 
by 
w, 
the 
normal 
to 
the 
hyperplane, 
see 
g(17.10). 


To 
clarify 
the 
geometric 
interpretation, 
let 
x 
be 
a 
point 
on 
the 
decision 
boundary 
and 
consider 
a 
new 
point 
x 
* 
= 
x 
+ 
w?, 
where 
w. 
is 
a 
vector 
perpendicular 
to 
w, 
so 
that 
wTw. 
= 
0. 
Then 




T* 
T?TT. 
T

b 
+ 
wx 
= 
b 
+ 
wx 
+ 
w= 
b 
+ 
wx 
+ 
ww= 
b 
+ 
wx 
= 
0 
(17.4.8) 


Thus 
if 
x 
is 
on 
the 
decision 
boundary, 
so 
is 
x 
plus 
any 
vector 
perpendicular 
to 
w. 
In 
D 
dimensions, 
the 
space 
of 
vectors 
that 
are 
perpendicular 
to 
w 
occupy 
a 
D 
- 
1 
dimensional 
hyperplane. 
For 
example, 
if 
the 
data 
is 
two 
dimensional, 
the 
decision 
boundary 
is 
a 
one 
dimensional 
hyperplane, 
a 
line, 
as 
depicted 
in 
g(17.10). 


Linear 
separability 
and 
linear 
independence 


Denition 
90 
(Linear 
separability). 
If 
all 
the 
training 
data 
for 
class 
1 
lies 
on 
one 
side 
of 
a 
hyperplane, 
and 
for 
class 
0 
on 
the 
other, 
the 
data 
is 
said 
to 
be 
linearly 
separable. 


For 
D 
dimensional 
data, 
provided 
there 
are 
no 
more 
than 
D 
training 
points, 
then 
these 
are 
linearly 


n

separable 
provided 
they 
are 
linearly 
independent. 
To 
see 
this, 
let 
cn 
= 
+1 
if 
xn 
is 
in 
class 
1, 
and 
c= 
..1 
if 
xn 
is 
in 
class 
0. 
For 
the 
data 
to 
be 
linearly 
separable 
we 
require 


T

wxn 
+ 
b 
= 
cn 
;n 
=1;:::;N 
(17.4.9) 


where 
E 
is 
an 
arbitrarily 
small 
positive 
constant. 
The 
above 
equations 
state 
that 
each 
input 
is 
just 
the 
correct 
side 
of 
the 
decision 
boundary. 
If 
there 
are 
N 
= 
D 
datapoints, 
the 
above 
can 
be 
written 
in 
matrix 
form 
as 


Xw 
+ 
b 
= 
c 
(17.4.10) 


where 
X 
is 
a 
square 
matrix 
whose 
nth 
column 
contains 
xn 
. 
Provided 
that 
X 
is 
invertible 
the 
solution 
is 


w 
= 
X..1 
(c 
- 
b) 
(17.4.11) 


The 
bias 
b 
can 
be 
set 
arbitrarily. 
This 
shows 
that 
provided 
the 
xn 
are 
linearly 
independent, 
we 
can 
always 
nd 
a 
hyperplane 
that 
linearly 
separates 
the 
data. 
Provided 
the 
data 
are 
not-collinear 
(all 
occupying 
the 
same 
D 
- 
1 
dimensional 
subspace) 
the 
bias 
can 
be 
used 
to 
improve 
this 
to 
enabling 
D 
+ 
1 
arbitrarily 
labelled 
points 
to 
be 
linearly 
separated 
in 
D 
dimensions. 


A 
dataset 
that 
is 
not 
linearly 
separable 
is 
given 
by 
the 
following 
four 
training 
points 
and 
class 
labels 


f([0, 
0], 
0), 
([0, 
1], 
1), 
([1, 
0], 
1), 
([1, 
1], 
0)} 
(17.4.12) 


DRAFT 
March 
9, 
2010 



Linear 
Parameter 
Models 
for 
Classication 



Figure 
17.11: 
The 
XOR 
problem. 
This 
is 
not 
linearly 
separable. 


This 
data 
represents 
the 
XOR 
function, 
and 
is 
plotted 
in 
g(17.11). 
This 
function 
is 
not 
linearly 
separable 
since 
no 
straight 
line 
has 
all 
inputs 
from 
one 
class 
on 
one 
side 
and 
the 
other 
class 
on 
the 
other. 


Classifying 
data 
which 
is 
not 
linearly 
separable 
can 
only 
be 
achieved 
using 
a 
non-linear 
decision 
boundary. 
It 
might 
be 
that 
data 
is 
non-linearly 
separable 
in 
the 
original 
data 
space. 
However, 
by 
mapping 
to 
a 
higher 
dimension 
using 
a 
non-linear 
vector 
function, 
we 
generate 
a 
set 
of 
non-linearly 
dependent 
high-
dimensional 
vectors, 
which 
can 
then 
be 
separated 
using 
a 
high-dimensional 
hyperplane. 
We 
will 
discuss 
this 
in 
section(17.5). 


The 
perceptron 


The 
perceptron 
assigns 
x 
to 
class 
1 
if 
b 
+ 
wTx 
= 
0, 
and 
to 
class 
0 
otherwise. 
That 
is 


T

p(c 
=1jx)= 
(b 
+ 
xw) 
(17.4.13) 


where 
the 
step 
function 
is 
dened 
as 


1 
x> 
0 


(x) 
= 
(17.4.14)

0 
x 
= 
0 


If 
we 
consider 
the 
logistic 
regression 
model 




T

p(c 
=1jx)= 
b 
+ 
xw(17.4.15) 


and 
take 
the 
limit 
ß 
!1, 
we 
have 
the 
perceptron 
like 
classier 


8

<1 
b 
+ 
xTw 
> 
0 
p(c 
=1jx)= 
0:5 
b 
+ 
xTw 
= 
0 
(17.4.16)

:

0 
b 
+ 
xTw 
< 
0 


The 
only 
dierence 
between 
this 
`probabilistic 
perceptron’ 
and 
the 
standard 
perceptron 
is 
in 
the 
technical 
denition 
of 
the 
value 
of 
the 
step 
function 
at 
0. 
The 
perceptron 
may 
therefore 
essentially 
be 
viewed 
as 
a 
limiting 
case 
of 
logistic 
regression. 


17.4.2 
Maximum 
likelihood 
training 
Given 
a 
data 
set 
D, 
how 
can 
we 
learn 
the 
weights 
to 
obtain 
good 
classication? 
Probabilistically, 
if 
we 
assume 
that 
each 
data 
point 
has 
been 
drawn 
independently 
from 
the 
same 
distribution 
that 
generates 
the 
data 
(the 
standard 
i.i.d. 
assumption), 
the 
likelihood 
of 
the 
observed 
data 
is 
(writing 
explicitly 
the 
conditional 
dependence 
on 
the 
parameters 
b, 
w) 


NN

p(Djb, 
w)= 
p(c 
njxn, 
b, 
w)p(xn)= 
p(c 
=1jxn, 
b, 
w)cn 
(1 
- 
p(c 
=1jxn, 
b, 
w))1..cn 
p(xn) 
(17.4.17) 


n=1 
n=1 


n

where 
we 
have 
used 
the 
fact 
that 
c2f0, 
1g. 
For 
this 
discriminative 
model, 
we 
do 
not 
model 
the 
input 
distribution 
p(x) 
so 
that 
we 
may 
equivalently 
consider 
the 
log 
likelihood 
of 
the 
output 
class 
variables 
conditioned 
on 
the 
training 
inputs: 
For 
logistic 
regression 
this 
gives 


N

X

TT

L(w;b)= 
c 
n 
log 
(b 
+ 
wxn) 
+ 
(1 
- 
c 
n) 
log1 
- 
(b 
+ 
wxn)(17.4.18) 


n=1 


DRAFT 
March 
9, 
2010 



Linear 
Parameter 
Models 
for 
Classication 


Gradient 
ascent 


There 
is 
no 
closed 
form 
solution 
to 
the 
maximisation 
of 
L(w;b) 
which 
needs 
to 
be 
carried 
out 
numerically. 
One 
of 
the 
simplest 
methods 
is 
gradient 
ascent 
for 
which 
the 
gradient 
is 
given 
by

N

X

T

rwL 
=(c 
n 
..r(wxn 
+ 
b))xn 
(17.4.19) 
n=1 


Here 
we 
made 
use 
of 
the 
derivative 
relation 


d(x)=dx 
= 
(x)(1 
..r(x)) 
(17.4.20) 


for 
the 
logistic 
sigmoid. 
The 
derivative 
with 
respect 
to 
the 
bias 
is 


N

dL 
X

T

=(c 
n 
..r(wxn 
+ 
b)) 
(17.4.21)

db 


n=1 


The 
gradient 
ascent 
procedure 
then 
corresponds 
to 
updating 
the 
weights 
and 
bias 
using 


dL 


wnew 
bnew

= 
w 
+ 
rwL, 
= 
b 
+ 
. 
(17.4.22)

db 
where 
, 
the 
learning 
rate 
is 
a 
scalar 
chosen 
small 
enough 
to 
ensure 
convergence. 
The 
application 
of 
the 
above 
rule 
will 
lead 
to 
a 
gradual 
increase 
in 
the 
log 
likelihood. 


Batch 
training 


Writing 
the 
updates 
(17.4.22) 
explicitly 
gives 


NN

XX

wnew 
Tbnew 
T

= 
w 
+ 
. 
(c 
n 
..r(wxn 
+ 
b))xn 
, 
= 
b 
+ 
. 
(c 
n 
..r(wxn 
+ 
b)) 
(17.4.23) 


n=1 
n=1 


This 
is 
called 
a 
batch 
update 
since 
the 
parameters 
w 
and 
b 
are 
updated 
only 
after 
passing 
through 
the 
whole 
(batch) 
of 
training 
data. 
This 
batch 
version 
`converges’ 
in 
all 
cases 
since 
the 
error 
surface 
is 
bowl 
shaped 
(see 
below). 
For 
linearly 
separable 
data, 
however, 
the 
optimal 
setting 
is 
for 
the 
weights 
to 
become 
innitely 
large, 
since 
then 
the 
logistic 
sigmoids 
will 
saturate 
to 
1 
or 
0 
(see 
below). 
A 
stopping 
criterion 
based 
on 
either 
minimal 
changes 
to 
the 
log 
likelihood 
or 
the 
parameters 
is 
therefore 
required 
to 
halt 
the 
optimisation 
routine. 
For 
non-linearly 
separable 
data, 
the 
likelihood 
has 
a 
maximum 
at 
nite 
w 
so 
the 
algorithm 
converges. 
However, 
the 
predictions 
will 
be 
less 
certain, 
reected 
in 
a 
broad 
condence 
interval 


– 
see 
g(17.12). 
In 
batch 
training, 
the 
zero 
gradient 
criterion 
is 


N

X

T

(c 
n 
..r(wxn 
+ 
b))xn 
= 
0 
(17.4.24) 


n=1 


In 
the 
case 
that 
the 
inputs 
xn;n 
=1;:::;N 
are 
linearly 
independent, 
we 
immediately 
have 
the 
require


..

n

ment 
that 
for 
a 
zero 
gradient, 
c= 
wTxn 
+ 
b, 
meaning 
that 
the 
weights 
must 
tend 
to 
innity 
for 
this 
condition 
to 
hold. 


For 
linearly 
separable 
data, 
we 
can 
also 
show 
that 
the 
weights 
must 
become 
innite 
at 
convergence. 
Taking 
the 
scalar 
product 
of 
equation 
(17.4.24) 
with 
w, 
we 
have 
the 
zero 
gradient 
requirement 


N

X

T

(c 
n 
..rn) 
wxn 
= 
0 
(17.4.25) 
n=1 


..

where 
n 
rwTxn 
+ 
b. 
For 
simplicity 
we 
assume 
b 
= 
0. 
For 
linearly 
separable 
data 
we 
have 




Txn> 
0 
c 
=1 


w(17.4.26)

< 
0 
c 
=0 


DRAFT 
March 
9, 
2010 



Linear 
Parameter 
Models 
for 
Classication 


00.20.40.60.8100.10.20.30.40.50.60.70.80.91
00.20.40.60.8100.10.20.30.40.50.60.70.80.91
Figure 
17.12: 
The 
decision 
boundary 
p(c 
=1jx)= 


0:5 
(solid 
line) 
and 
condence 
boundaries 
p(c 
= 
1jx)=0:9 
and 
p(c 
=1jx)=0:1 
and 
10000 
iterations 
of 
batch 
gradient 
ascent 
with 
. 
=0:1. 
(a): 
Linearly 
separable 
data. 
(b): 
Non-linearly 
separable 
data. 
Note 
how 
the 
condence 
interval 
remains 
broad, 
see 
demoLogReg.m. 
(a) 
(b) 
Then, 
using 
the 
fact 
that 
0 
= 
n 
= 
1, 
we 
have 




T= 
0 
c 
=1 


(c 
n 
- 
n) 
wxn(17.4.27)

= 
0 
c 
=0 


Each 
term 
(cn 
- 
n) 
wTxn 
is 
non-negative 
and 
the 
zero 
gradient 
condition 
requires 
the 
sum 
of 
these 
terms 
n

to 
be 
zero. 
This 
can 
only 
happen 
if 
all 
the 
terms 
are 
zero, 
implying 
that 
c= 
n, 
requiring 
the 
sigmoid 
to 
saturate, 
for 
which 
the 
weights 
must 
be 
innite. 


Online 
training 


In 
practice 
it 
is 
common 
to 
update 
the 
parameters 
after 
each 
training 
example 
has 
been 
considered: 


. 
n 
- 
(w. 
n 
- 
(w

wnew 
Tbnew 
T

= 
w 
+(c 
xn 
+ 
b))xn 
, 
= 
b 
+(c 
xn 
+ 
b)) 
(17.4.28)

NN 


An 
advantage 
of 
online 
training 
is 
that 
the 
dataset 
does 
not 
need 
to 
be 
stored 
since 
only 
the 
performance 
on 
the 
current 
input 
is 
required. 
Provided 
that 
the 
data 
is 
linearly 
separable, 
the 
above 
online 
procedure 
converges 
(provided 
. 
is 
not 
too 
large). 
However, 
if 
the 
data 
is 
not 
linearly 
separable, 
the 
online 
version 
will 
not 
converge 
since 
the 
opposing 
class 
labels 
will 
continually 
pull 
the 
weights 
one 
way 
and 
then 
the 
other 
as 
each 
conicting 
example 
is 
used 
to 
form 
an 
update. 
For 
the 
limiting 
case 
of 
the 
perceptron 
(replacing 
(x) 
with 
(x)) 
and 
linearly 
separable 
data, 
online 
updating 
converges 
in 
a 
nite 
number 
of 
steps[212, 
41], 
but 
does 
not 
converge 
for 
non-linearly 
separable 
data. 


Geometry 
of 
the 
error 
surface 


The 
Hessian 
of 
the 
log 
likelihood 
L(w) 
is 
the 
matrix 
with 
elements2 


@2L 
X

nn

Hij 
= 
= 
..xi 
xj 
n(1 
- 
n) 
(17.4.29)

@wiwj 


n 


This 
is 
negative 
(semi) 
denite 
since 
for 
any 
z

01

2 


XXX

nn 
n

@A

ziHijzj 
= 
..zixi 
zjxj 
n(1 
- 
n) 
- 
zixi 
= 
0 
(17.4.30) 
ij 
i;j;n 
i;n 


This 
means 
that 
the 
error 
surface 
is 
concave 
(an 
upside 
down 
bowl) 
and 
gradient 
ascent 
is 
guaranteed 
to 
converge 
to 
the 
optimal 
solution, 
provided 
the 
learning 
rate 
. 
is 
small 
enough. 


Example 
80 
(Classifying 
Handwritten 
Digits). 
We 
apply 
logistic 
regression 
to 
the 
600 
handwritten 
digits 
of 
example(67), 
in 
which 
there 
are 
300 
ones 
and 
300 
sevens 
in 
the 
train 
data. 
Using 
gradient 
ascent 
training 
with 
a 
suitably 
chosen 
stopping 
criterion, 
the 
number 
of 
errors 
made 
on 
the 
600 
test 
points 
is 
12, 
compared 
with 
14 
errors 
using 
Nearest 
Neighbour 
methods. 
See 
g(17.13) 
for 
a 
visualisation 
of 
the 
learned 
w. 


2For 
simplicity 
we 
ignore 
the 
bias 
b. 
This 
can 
readily 
be 
dealt 
with 
by 
extending 
x 
to 
a 
D 
+ 
1 
dimensional 
vector 
x^with 
T 
^

a 
1 
in 
the 
D 
+ 
1 
component. 
Then 
for 
a 
D 
+ 
1 
dimensional 
w^=(w;wD+1), 
we 
have 
w^x 
= 
w 
T 
x 
+ 
wD+1. 


DRAFT 
March 
9, 
2010 
323 



The 
Kernel 
Trick 
for 
Classication 



Figure 
17.13: 
Logistic 
regression 
for 
classifying 
hand 
written 
digits 
1 
and 
7. 
Displayed 
is 
a 
Hinton 
diagram 
of 
the 
784 
learned 
weight 
vector 
w, 
plotted 
as 
a 
28 
× 
28 
image 
for 
visual 
interpretation. 
Green 
are 
positive 
and 
an 
input 
x 
with 
a 
(positive) 
value 
in 
this 
component 
will 
tend 
to 
increase 
the 
probability 
that 
the 
input 
is 
classed 
as 
a 
7. 
Similarly, 
inputs 
with 
positive 
contributions 
in 
the 
red 
regions 
tend 
to 
increase 
the 
probability 
as 
being 
classed 
as 
a 
1 
digit. 
Note 
that 
the 
elements 
of 
each 
input 
x 
are 
either 
positive 
or 
zero. 


17.4.3 
Beyond 
rst 
order 
gradient 
ascent 
Since 
the 
surface 
has 
a 
single 
optimum, 
a 
Newton 
update 


wnew 
old 
+ 
H..1old 


= 
ww(17.4.31) 


where 
H 
is 
the 
Hessian 
matrix 
as 
above 
and 
0 
<< 
1, 
will 
typically 
converge 
much 
faster 
than 
gradient 
ascent. 
For 
large 
scale 
problems, 
the 
inversion 
of 
the 
Hessian 
is 
computationally 
demanding 
and 
limited 
memory 
BFGS 
or 
conjugate 
gradient 
methods 
may 
be 
considered 
as 
more 
practical 
alternatives, 
see 
section(A.5). 


17.4.4 
Avoiding 
overcondent 
classication 
Provided 
the 
data 
is 
linearly 
separable 
the 
weights 
will 
continue 
to 
increase 
and 
the 
classications 
will 
become 
extreme. 
This 
is 
undesirable 
since 
the 
classications 
will 
be 
over-condent. 
This 
can 
be 
prevented 
by 
adding 
a 
penalty 
term 
to 
the 
objective 
function 


T

L0(w;b)= 
L(w;b) 
- 
ww. 
(17.4.32) 


The 
scalar 
constant 
> 
0 
encourages 
smaller 
values 
of 
w 
(remember 
that 
we 
wish 
to 
maximise 
the 
log 
likelihood). 
An 
appropriate 
value 
for 
a 
can 
be 
determined 
using 
validation 
data. 


17.4.5 
Multiple 
classes 
For 
more 
than 
two 
classes, 
one 
may 
use 
the 
softmax 
function 


T

wi 
x+bi

e

p(c 
= 
ijx) 
= 
(17.4.33)

PT

C 
wj 
x+bj 
j=1 
e 


where 
C 
is 
the 
number 
of 
classes. 
When 
C 
= 
2 
this 
reduced 
to 
the 
logistic 
sigmoid. 
One 
can 
show 
that 
the 
likelihood 
for 
this 
case 
is 
also 
concave, 
see 
exercise(176) 
and 
[297]. 


17.5 
The 
Kernel 
Trick 
for 
Classication 
A 
drawback 
of 
logistic 
regression 
as 
described 
above 
is 
the 
simplicity 
of 
the 
decision 
surface 
– 
a 
hyperplane. 
One 
way 
to 
extend 
the 
method 
to 
more 
complex 
non-linear 
decision 
boundaries 
is 
to 
consider 
mapping 
the 
inputs 
x 
in 
a 
non-linear 
way 
to 
(x): 


p(c 
=1jx)= 
s 
wT(x)+ 
b 
(17.5.1) 


2

For 
example, 
the 
one-dimensional 
input 
x 
could 
get 
mapped 
to 
a 
two 
dimensional 
vector 
(x, 
sin(x)). 
Mapping 
into 
a 
higher 
dimensional 
space 
makes 
it 
easier 
to 
nd 
a 
separating 
hyperplane 
since 
any 
set 
of 
points 
that 
are 
linearly 
independent 
can 
be 
linearly 
separated 
provided 
we 
have 
as 
many 
dimensions 
as 
datapoints. 


324 
DRAFT 
March 
9, 
2010 



Support 
Vector 
Machines 


0.10.10.10.10.10.20.20.20.20.20.20.30.30.30.30.30.30.40.40.40.40.40.40.50.50.50.50.50.60.60.60.60.60.70.70.70.70.70.80.80.80.8080.90.90.90.90.9-0.500.511.5-0.500.511.5
Figure 
17.14: 
Logistic 
regression 
p(c 
=1jx)= 
(wT(x)) 
us


22

ing 
a 
quadratic 
function 
(x) 
= 
(1;x1;x2;x1;x2;x1x2)T 
. 
1000 
iterations 
of 
gradient 
ascent 
training 
were 
performed 
with 
a 
learning 
rate 
. 
=0:1. 
Plotted 
are 
the 
datapoints 
for 
the 
two 
classes 
(red 
cross) 
and 
(blue 
circle) 
and 
the 
equal 
probability 
contours. 
The 
decision 
boundary 
is 
the 
0:5-probability 
contour. 
See 
demoLogRegNonLinear.m. 


For 
the 
Maximum 
Likelihood 
criterion, 
we 
can 
use 
exactly 
the 
same 
algorithm 
as 
before 
on 
replacing 
x 
with 
(x). 
See 
g(17.14) 
for 
a 
demonstration 
using 
a 
quadratic 
function. 


Since 
only 
the 
scalar 
product 
between 
the 
f 
vectors 
plays 
a 
role 
the 
dual 
representation 
may 
be 
used 
in 
which 
we 
assume 
the 
weight 
can 
be 
expressed 
in 
the 
form 


X

w 
=nn 
(17.5.2) 
n 


We 
then 
subsequently 
nd 
a 
solution 
in 
terms 
of 
the 
dual 
parameters 
n. 
This 
is 
potentially 
advantageous 
since 
there 
may 
be 
less 
training 
points 
than 
dimensions 
of 
. 
The 
classier 
depends 
only 
on 
scalar 
products 
which 
can 
be 
written 
in 
terms 
of 
a 
positive 
denite 
kernel, 


 !

X

p(c 
=1jx)= 
anK(x, 
xn)(17.5.3) 


n 


For 
convenience, 
we 
can 
write 
the 
above 
as 




p(c 
=1jx)= 
aTk(x)(17.5.4) 


where 
the 
N 
dimensional 
vector 
k(x) 
has 
elements 
[k(x)]= 
K(x, 
xn). 
Then 
the 
above 
is 
of 
exactly 
the 


n 


same 
form 
as 
the 
original 
specication 
of 
logistic 
regression, 
namely 
as 
a 
function 
of 
a 
linear 
combination 
of 
vectors. 
Hence 
the 
same 
training 
algorithm 
to 
maximise 
the 
likelihood 
can 
be 
employed, 
simply 
on 
replacing 
xn 
with 
k(xn). 
The 
details 
are 
left 
to 
the 
interested 
reader, 
and 
follow 
closely 
the 
treatment 
of 
Gaussian 
Processes 
for 
classication, 
section(19.5). 


17.6 
Support 
Vector 
Machines 
Like 
kernel 
logistic 
regression, 
SVMs 
are 
a 
form 
of 
kernel 
linear 
classier. 
However, 
the 
SVM 
uses 
an 
objective 
which 
more 
explicitly 
encourages 
good 
generalisation 
performance. 
SVMs 
do 
not 
t 
comfortably 
within 
a 
probabilistic 
framework 
and 
as 
such 
we 
describe 
them 
here 
only 
briey, 
referring 
the 
reader 
to 
the 
wealth 
of 
excellent 
literature 
on 
this 
topic3 
. 
The 
description 
here 
is 
inspired 
largely 
by 
[71]. 


17.6.1 
Maximum 
margin 
linear 
classier 
In 
the 
SVM 
literature 
it 
is 
common 
to 
use 
+1 
and 
..1 
to 
denote 
the 
two 
classes. 
For 
a 
hyperplane 
dened 
by 
weight 
w 
and 
bias 
b, 
a 
linear 
discriminant 
is 
given 
by 


T

wx 
+ 
b 
= 
0 
class 
+1 
(17.6.1) 


T

wx 
+ 
b< 
0 
class 
-1 
(17.6.2) 


3http://www.support-vector.net 


DRAFT 
March 
9, 
2010 
325 



Support 
Vector 
Machines 


origin
w
Figure 
17.15: 
SVM 
classication 
of 
data 
from 
two 
classes 
(open 
circles 
and 
lled 
circles). 
The 
decision 
boundary 
wTx 
+ 
b 
= 
0 
(solid 
line). 
For 
linearly 
separable 
data 
the 
maximum 
margin 
hyperplane 
is 
equidistant 
from 
the 
closest 
opposite 
class 
points. 
These 
support 
vectors 
are 
highlighted 
in 
blue 
and 
the 
margin 
in 
red. 
The 
distance 
of 
the 
decision 
boundary 


v 


from 
the 
origin 
is 
..b/ 
wTw, 
and 
the 
distance 
of 
a 
general 
point 
x 
from 
the 
origin 
along 
the 
direction 
w

v 


is 
xTw/ 
wTw. 


For 
a 
point 
x 
that 
is 
close 
to 
the 
decision 
boundary 
at 
wTx 
+ 
b 
= 
0, 
a 
small 
change 
in 
x 
can 
lead 
to 
a 
change 
in 
classication. 
To 
make 
the 
classier 
more 
robust 
we 
therefore 
impose 
that 
for 
the 
training 
data 
at 
least, 
the 
decision 
boundary 
should 
be 
separated 
from 
the 
data 
by 
some 
nite 
margin 
(assuming 
in 
the 
rst 
instance 
that 
the 
data 
is 
linearly 
separable): 


T

wx 
+ 
b 
= 
2 
class 
+1 
(17.6.3) 


T

wx 
+ 
b< 
..2 
class 
-1 
(17.6.4) 


Since 
w, 
b 
and 
2 
can 
all 
be 
rescaled 
arbitrary, 
we 
need 
to 
x 
the 
scale 
of 
the 
above 
to 
break 
this 
invariance. 
It 
is 
convenient 
to 
set 
E 
= 
1 
so 
that 
a 
point 
x+ 
from 
class 
+1 
that 
is 
closest 
to 
the 
decision 
boundary 
satises 


T

wx+ 
+ 
b 
= 
1 
(17.6.5) 


and 
a 
point 
x- 
from 
class 
-1 
that 
is 
closest 
to 
the 
decision 
boundary 
satises 


T

wx- 
+ 
b 
= 
..1 
(17.6.6) 


From 
vector 
algebra, 
g(17.15), 
the 
distance 
from 
the 
origin 
along 
the 
direction 
w 
to 
a 
point 
x 
is 
given 
by 


wTx 


v 
(17.6.7) 


wTw 


The 
margin 
between 
the 
hyperplanes 
for 
the 
two 
classes 
is 
then 
the 
dierence 
between 
the 
two 
distances 
along 
the 
direction 
w 
which 
is 


wT 
2 


v 
(x+ 
- 
x..)= 
v 
(17.6.8) 


wTwwTw 


To 
set 
the 
distance 
between 
the 
two 
hyperplanes 
to 
be 
maximal, 
we 
need 
to 
minimise 
the 
length 
wTw. 


n

Given 
that 
for 
each 
xn 
we 
have 
a 
corresponding 
label 
y2f+1, 
..1g, 
in 
order 
to 
classify 
the 
training 
labels 
correctly 
and 
maximise 
the 
margin, 
the 
optimisation 
problem 
is 
therefore 
equivalent 
to: 




1 
T

minimise 
ww 
subject 
to 
y 
nwTxn 
+ 
b= 
1;n 
=1;:::;N 
(17.6.9)

2

This 
is 
a 
quadratic 
programming 
problem. 
Note 
that 
the 
factor 
0:5 
is 
just 
for 
convenience. 


To 
account 
for 
potentially 
mislabelled 
training 
points 
(or 
for 
data 
that 
is 
not 
linearly 
separable), 
we 
relax 
the 
exact 
classication 
constraint 
and 
use 
instead 




nT

y 
wxn 
+ 
b= 
1 
- 
n 
(17.6.10) 


where 
n 
= 
0. 
Here 
each 
n 
measures 
how 
far 
xn 
is 
from 
the 
correct 
margin. 
For 
0 
<n 
< 
1 
datapoint 
xn 
is 
on 
the 
correct 
side 
of 
the 
decision 
boundary. 
However 
for 
n 
> 
1, 
the 
datapoint 
is 
assigned 
the 
opposite 
class 
to 
its 
training 
label. 
Ideally 
we 
want 
to 
limit 
the 
size 
of 
these 
`violations’ 
n 
. 
Here 
we 
briey 
describe 
two 
standard 
approaches. 


DRAFT 
March 
9, 
2010 



Support 
Vector 
Machines 


origin
w
n
Figure 
17.16: 
Slack 
Margin. 
The 
term 
n 
measures 
how 
far 
a 
variable 
is 
from 
the 
wrong 
side 
of 
the 
margin 
for 
its 
class. 
If 
n 
> 
1 
then 
the 
point 
will 
be 
misclassied 
– 
treated 
as 
an 
outlier. 


2-Norm 
soft-margin 


The 
2-norm 
soft-margin 
objective 
is 




X

TnT

minimise 
1 
ww 
+ 
C 
(n)2 
subject 
to 
y 
wxn 
+ 
b= 
1 
- 
n 
;n 
=1;:::;N 
(17.6.11)

22

n 


where 
C 
controls 
the 
number 
of 
mislabellings 
of 
the 
training 
data. 
The 
constant 
C 
needs 
to 
be 
determined 
empirically 
using 
a 
validation 
set. 
The 
optimisation 
problem 
expressed 
by 
(17.6.11) 
can 
be 
formulated 
using 
the 
Lagrangian 


hi

XX

1 
TC 
nT

L(w, 
b, 
, 
)= 
ww+(n)2 
..ny 
wxn 
+ 
b- 
1+ 
n;n 
= 
0;n 
= 
0 
(17.6.12) 


22

nn 


which 
is 
to 
be 
minimised 
with 
respect 
to 
x, 
b, 
. 
and 
maximised 
with 
respect 
to 
. 


..

n

For 
points 
xn 
on 
the 
`correct’ 
side 
of 
the 
decision 
boundary 
ywTxn 
+ 
b..1+n 
> 
0 
so 
that 
maximising 
L 
with 
respect 
to 
a 
requires 
the 
corresponding 
n 
to 
be 
set 
to 
zero. 
Only 
training 
points 
that 
are 
support 
vectors 
lying 
on 
the 
decision 
boundary 
will 
have 
non-zero 
n 
. 


Dierentiating 
the 
Lagrangian 
and 
equating 
to 
zero, 
we 
have 
the 
conditions 


X

nn

n

@
L(w, 
b, 
, 
)= 
wi 
..yx 
= 
0 
(17.6.13)

@wi
i 


n 


X

. 


L(w, 
b, 
, 
)= 
..n 
y 
n 
= 
0 
(17.6.14)

@b 


n 


@
L(w, 
b, 
, 
)= 
Cn 
- 
n 
= 
0 
(17.6.15)

@n 


From 
this 
we 
see 
that 
the 
solution 
for 
w 
is 
given 
by 


X

n

w 
=n 
y 
xn 
(17.6.16) 
n 


Since 
only 
the 
support 
vectors 
have 
non-zero 
n, 
the 
solution 
for 
w 
will 
typically 
depend 
on 
only 
a 
small 
number 
of 
the 
training 
data. 
Using 
these 
conditions 
and 
substituting 
back 
into 
the 
original 
problem, 
the 
objective 
is 
equivalent 
to 
minimising 


XXX

n

L()=n 
- 
1 
yy 
mnm 
(xn)T 
xm 
- 
1(n)2 


22C

n 
n;m 
n 


X

subject 
toy 
nn 
=0;n 
= 
0 
(17.6.17) 
n 


If 
we 
dene 
K(xn 
, 
xm) 
= 
(xn)T 
xm 
(17.6.18) 
DRAFT 
March 
9, 
2010 
327 



Support 
Vector 
Machines 


-2-1012-2.5-2-1.5-1-0.500.511.52
Figure 
17.17: 
SVM 
training. 
The 
solid 
red 
and 
solid 
blue 
circles 
represent 
training 
data 
from 
dierent 
classes. 
The 
support 
vectors 
are 
highlighted 
in 
green. 
For 
the 
unlled 
test 
points, 
the 
class 
assigned 
to 
them 
by 
the 
SVM 
is 
given 
by 
the 
colour. 
See 
demoSVM.m 


The 
optimisation 
problem 
is 




XX

11

n

maximizen 
- 
yy 
mnmK(xn 
, 
xm)+ 
n;m

2C 


n 
n;m

X

subject 
toy 
nn 
=0;n 
= 
0 
(17.6.19) 
n 


Optimising 
this 
objective 
is 
discussed 
in 
section(17.6.3). 


1-norm 
soft-margin 
(box 
constraint) 


In 
the 
1-norm 
soft-margin 
version, 
one 
uses 
a 
1-norm 
penalty 


X

Cn 
(17.6.20) 
n 


to 
give 
the 
optimisation 
problem: 




X

1 
Tn

T

minimise 
ww+Cn 
subject 
to 
y 
wxn 
+ 
b= 
1..n;n 
= 
0;n 
=1;:::;N 
(17.6.21)

2

n 


where 
C 
is 
an 
empirically 
determined 
penalty 
factor 
that 
controls 
the 
number 
of 
mislabellings 
of 
the 
training 
data. 
To 
reformulate 
the 
optimisation 
problem 
we 
use 
the 
Lagrangian 


XXX

1 


Tn 
..nnTxn 
+ 
bnnn 
= 
0

L(w, 
b, 
)= 
ww+Cy 
w- 
1+ 
n 
..r, 
n 
= 
0;n 
= 
0;r 


2

nn 
n 


(17.6.22) 
n

The 
variables 
rare 
introduced 
in 
order 
to 
give 
a 
non-trivial 
solution 
(otherwise 
n 
= 
C). 
Following 
a 
similar 
argument 
as 
for 
the 
2-norm 
case, 
by 
dierentiating 
the 
Lagrangian 
and 
equating 
to 
zero, 
we 
arrive 
at 
the 
optimisation 
problem 


XX

1 


n

maximizen 
- 
yy 
mnmK(xn 
, 
xm) 
(17.6.23) 


2

n 
n;m 


X

subject 
toy 
nn 
=0, 
0 
= 
n 
= 
C 
n 


which 
is 
closely 
related 
to 
the 
2-norm 
problem 
except 
that 
we 
now 
have 
the 
box-constraint 
0 
= 
n 
= 
C. 


17.6.2 
Using 
kernels 
The 
nal 
objectives 
(17.6.19) 
and 
(17.6.17) 
depend 
on 
the 
inputs 
xn 
only 
via 
the 
scalar 
product 
(xn)T 
xn 
. 
If 
we 
map 
x 
to 
a 
vector 
function 
of 
x, 
then 
we 
can 
write 


K(xn 
, 
xm)= 
(xn)T(xm) 
(17.6.24) 


This 
means 
that 
we 
can 
use 
any 
positive 
(semi) 
denite 
kernel 
K 
and 
make 
a 
non-linear 
classier. 
See 
section(19.3). 


DRAFT 
March 
9, 
2010 



Soft 
Zero-One 
Loss 
for 
Outlier 
Robustness 


17.6.3 
Performing 
the 
optimisation 
Both 
of 
the 
above 
soft-margin 
SVM 
optimisations 
problems 
(17.6.19) 
and 
(17.6.17) 
are 
quadratic 
programs 


..

for 
which 
the 
exact 
computational 
cost 
scales 
as 
ON3 
. 
Whilst 
these 
can 
be 
solved 
with 
general 
purpose 
routines, 
specically 
tailored 
routines 
that 
exploit 
the 
structure 
of 
the 
problem 
are 
preferred 
in 
practice. 
Of 
particular 
practical 
interest 
are 
`chunking’ 
techniques 
that 
optimise 
over 
a 
subset 
of 
the 
. 
In 
the 
limit 
of 
updating 
only 
two 
components 
of 
, 
this 
can 
be 
achieved 
analytically, 
resulting 
in 
the 
Sequential 
Mini


..

mal 
Optimisation 
algorithm[224], 
whose 
practical 
performance 
is 
typically 
ON2 
or 
better. 
A 
variant 
of 
this 
algorithm 
[92] 
is 
provided 
in 
SVMtrain.m. 


Once 
the 
optimal 
solution 
* 
is 
found 
the 
decision 
function 
for 
a 
new 
point 
x 
is 




X> 
0 
assign 
to 
class 
1 


nnK(xn 
, 
x)+ 
b(17.6.25)

* 
y
< 
0 
assign 
to 
class 
-1 
n 


The 
optimal 
b* 
is 
determined 
using 
the 
maximum 
margin 
condition, 
equations(17.6.5,17.6.6): 


1 
TT

b* 
= 
min 
w
* 
xn 
- 
max 
w
* 
xn 
(17.6.26)

2 
yn=1 
yn=..1 


17.6.4 
Probabilistic 
interpretation 
Kernelised 
logistic-regression 
has 
some 
of 
the 
characteristics 
of 
the 
SVM 
but 
does 
not 
express 
the 
large 
margin 
requirement. 
Also 
the 
sparse 
data 
usage 
of 
the 
SVM 
is 
similar 
to 
that 
of 
the 
Relevance 
Vector 
Machine 
we 
discuss 
in 
section(18.2.4). 
However, 
a 
probabilistic 
model 
whose 
MAP 
assignment 
matches 
exactly 
the 
SVM 
is 
hampered 
by 
the 
normalisation 
requirement 
for 
a 
probability 
distribution. 
Whilst, 
arguably, 
no 
fully 
satisfactory 
direct 
match 
between 
the 
SVM 
and 
a 
related 
probabilistic 
model 
has 
been 
achieved, 
approximate 
matches 
have 
been 
obtained[255]. 


17.7 
Soft 
Zero-One 
Loss 
for 
Outlier 
Robustness 
Both 
the 
support 
vector 
machine 
and 
logistic 
regression 
are 
potentially 
mislead 
by 
outliers. 
For 
the 
SVM, 
a 
mislabelled 
datapoint 
that 
is 
far 
from 
the 
correct 
side 
of 
the 
decision 
boundary 
would 
require 
a 
large 
slack 
. 
However, 
since 
exactly 
such 
large 
. 
are 
discouraged, 
it 
is 
unlikely 
that 
the 
SVM 
would 
admit 
such 
a 
solution. 
For 
logistic 
regression, 
the 
probability 
of 
generating 
a 
mislabelled 
point 
far 
from 
the 
correct 
side 
of 
the 
decision 
boundary 
is 
so 
exponentially 
small 
that 
this 
will 
never 
happen 
in 
practice. 
This 
means 
that 
the 
model 
trained 
with 
Maximum 
Likelihood 
will 
never 
present 
such 
a 
solution. 
In 
both 
cases 
therefore 
mislabelled 
points 
(or 
outliers) 
have 
a 
signicant 
impact 
on 
the 
location 
of 
the 
decision 
boundary. 


A 
robust 
technique 
to 
deal 
with 
outliers 
is 
to 
use 
the 
zero-one 
loss 
in 
which 
a 
mislabeled 
point 
contributes 
only 
a 
relatively 
small 
loss. 
Soft 
variants 
of 
this 
are 
obtained 
by 
using 
the 
objective 


N

Xhi2 


T 
nT

(b 
+ 
wx 
n) 
- 
c 
+ 
ww 
(17.7.1) 


n=1

which 
is 
to 
be 
minimised 
with 
respect 
to 
w 
and 
b. 
For 
ß 
!8 
the 
rst 
term 
above 
tends 
to 
the 
zero-one 
loss. 
The 
second 
term 
represents 
a 
penalty 
on 
the 
length 
of 
w 
and 
prevents 
overtting. 
Kernel 
extensions 
of 
this 
soft 
zero-one 
loss 
are 
straightforward. 


Unfortunately, 
the 
objective 
(17.7.1) 
is 
highly 
non-convex 
and 
nding 
the 
optimal 
w;b 
is 
computationally 
dicult. 
A 
simple-minded 
scheme 
is 
to 
x 
all 
components 
of 
w 
except 
one, 
wi 
and 
then 
perform 
a 
numerical 
one-dimensional 
optimisation 
over 
this 
single 
parameter 
wi. 
At 
the 
next 
step, 
another 
parameter 
wj 
is 
chosen, 
and 
the 
procedure 
repeated 
until 
convergence. 
As 
usual, 
. 
can 
be 
set 
using 
validation. 
The 
practical 
diculties 
of 
minimising 
non-convex 
high-dimensional 
objective 
functions 
means 
that 
these 
approaches 
are 
rarely 
used 
in 
practice. 
A 
discussion 
of 
practical 
attempts 
in 
this 
area 
is 
given 
in 
[286]. 


DRAFT 
March 
9, 
2010 
329 



Exercises 


-4-202468-4-2024681012
Figure 
17.18: 
Soft 
zero-one 
loss 
decision 
boundary 
(solid 
line) 
versus 
logistic 
regression 
(dotted 
line). 
The 
number 
of 
mis-classied 
training 
points 
using 
the 
soft 
zero-one 
loss 
is 
2, 
compared 
to 
3 
for 
logistic 
regression. 
The 
penalty 
. 
=0:01 
was 
used 
for 
the 
soft-loss, 
with 
ß 
= 
10. 
For 
logistic 
regression, 
no 
penalty 
term 
was 
used. 
The 
outliers 
have 
a 
signicant 
impact 
on 
the 
decision 
boundary 
for 
logistic 
regression, 
whilst 
the 
soft 
zero-one 
loss 
essentially 
gives 
up 
on 
the 
outliers 
and 
ts 
a 
large 
margin 
classier 
between 
the 
remaining 
points. 
See 
demoSoftLoss.m. 


An 
illustration 
of 
the 
dierence 
between 
logistic 
regression 
and 
this 
soft 
zero-one 
loss 
is 
given 
in 
g(17.18), 
which 
demonstrates 
how 
logistic 
regression 
is 
inuenced 
by 
the 
mass 
of 
the 
data 
points, 
whereas 
the 
zero-one 
loss 
attempts 
to 
mimimise 
the 
number 
of 
mis-classications 
whilst 
maintaining 
a 
large 
margin. 


17.8 
Notes 
The 
perceptron 
has 
a 
long 
history 
in 
articial 
intelligence 
and 
machine 
learning. 
Rosenblatt 
discussed 
the 
perceptron 
as 
a 
model 
for 
human 
learning, 
arguing 
that 
its 
distributive 
nature 
(the 
input-output 
`patterns’ 
are 
stored 
in 
the 
weight 
vector) 
is 
closely 
related 
to 
the 
kind 
of 
information 
storage 
believed 
to 
be 
present 
in 
biological 
systems[234]. 
To 
deal 
with 
non-linear 
decision 
boundaries, 
the 
main 
thrust 
of 
research 
in 
the 
ensuing 
neural 
network 
community 
was 
on 
the 
use 
of 
multilayered 
structures 
in 
which 
the 
outputs 
of 
perceptrons 
are 
used 
as 
the 
inputs 
to 
other 
perceptrons, 
resulting 
in 
potentially 
highly 
non-linear 
discriminant 
functions. 
This 
line 
of 
research 
was 
largely 
inspired 
by 
analogies 
to 
biological 
information 
processing 
in 
which 
layered 
structures 
are 
prevalent. 
Such 
multilayered 
articial 
neural 
networks 
are 
fascinating 
and, 
once 
trained, 
are 
extremely 
fast 
in 
forming 
their 
decisions. 
However, 
reliably 
training 
these 
systems 
is 
a 
highly 
complex 
task 
and 
probabilistic 
generalisations 
in 
which 
priors 
are 
placed 
on 
the 
parameters 
lead 
to 
computational 
diculties. 
Whilst 
perhaps 
less 
inspiring 
from 
a 
biological 
viewpoint, 
the 
alternative 
route 
of 
using 
the 
kernel 
trick 
to 
boost 
the 
power 
of 
a 
linear 
classier 
has 
the 
advantage 
of 
ease 
of 
training 
and 
generalisation 
to 
probabilistic 
variants. 
More 
recently, 
however, 
there 
has 
been 
a 
resurgence 
of 
interest 
in 
the 
multilayer 
systems, 
with 
new 
heuristics 
aimed 
at 
improving 
the 
diculties 
in 
training, 
see 
for 
example 
[135]. 


17.9 
Code 
demoCubicPoly.m: 
Demo 
of 
Fitting 
a 
Cubic 
Polynomial 
demoLogReg.m: 
Demo 
Logistic 
Regression 
LogReg.m: 
Logistic 
Regression 
Gradient 
Ascent 
Training 
demoLogRegNonLinear.m: 
Demo 
of 
Logistic 
regression 
with 
a 
non-linear 
(x) 
SVMtrain.m: 
SVM 
training 
using 
the 
SMO 
algorithm 
demoSVM.m: 
SVM 
demo 
demoSoftLoss.m: 
softloss 
demo 
softloss.m: 
softloss 
function 


17.10 
Exercises 
Exercise 
174. 


1. 
Give 
an 
example 
of 
a 
two-dimensional 
dataset 
for 
which 
the 
data 
are 
linearly 
separable, 
but 
not 
linearly 
independent. 
2. 
Can 
you 
nd 
a 
dataset 
which 
is 
linearly 
independent 
but 
not 
linearly 
separable? 
330 
DRAFT 
March 
9, 
2010 



Exercises 


n

Exercise 
175. 
Show 
that 
for 
both 
Ordinary 
and 
Orthogonal 
Least 
Squares 
regression 
ts 
to 
data 
f(x;yn);n 
=1 


PN 


n

the 
tted 
lines 
go 
through 
the 
point(x;yn)=N. 


n=1

Exercise 
176. 
Consider 
the 
softmax 
function 
for 
classifying 
an 
input 
vector 
x 
into 
one 
of 
c 
=1;:::;C 
classes 
using 


T

wx

ec 


p(cjx) 
= 
(17.10.1)

PC 
wT 
0x 


c

c0=1 
e 


A 
set 
of 
input-class 
examples 
is 
given 
by 
D 
= 
f(xn;cn);n 
=1;:::;Ng. 


1. 
Write 
down 
the 
log-likelihood 
L 
of 
the 
classes 
conditional 
on 
the 
inputs, 
assuming 
that 
the 
data 
is 
i.i.d. 
2. 
Compute 
the 
Hessian 
with 
elements 
@2L(D)

Hij 
= 
(17.10.2)

@wiwj 


where 
w 
is 
is 
the 
stacked 
vector 


T 


TT

w 
=w1 
;:::, 
w(17.10.3)

C

and 
show 
that 
the 
Hessian 
is 
positive 
semi-denite, 
that 
is 
zTHz 
= 
0 
for 
any 
z. 


Exercise 
177. 
Derive 
from 
equation 
(17.6.11) 
the 
dual 
optimisation 
problem 
equation 
(17.6.17). 


Exercise 
178. 
A 
datapoint 
x 
is 
projected 
to 
a 
lower 
dimensional 
vector 
xusing 


x= 
Mx 
(17.10.4) 


n

where 
M 
is 
a 
fat 
matrix. 
For 
a 
set 
of 
data 
xn;n 
=1;:::;N 
and 
corresponding 
binary 
class 
labels 
y. 
f0, 
1g, 
using 
logistic 
regression 
on 
the 
projected 
datapoints 
corresponds 
to 
a 
form 
of 
constrained 
logistic 


xn 


regression 
in 
the 
original 
higher 
dimensional 
space 
x. 
Explain 
if 
it 
is 
reasonable 
to 
use 
an 
algorithm 
such 
as 
PCA 
to 
rst 
reduce 
the 
data 
dimensionality 
before 
using 
logistic 
regression. 


Exercise 
179. 
The 
logistic 
sigmoid 
function 
is 
dened 
as 
(x)= 
ex=(1+ex). 
What 
is 
the 
inverse 
function, 
..1(x)? 


n

Exercise 
180. 
Given 
a 
dataset 
D 
= 
f(xn;cn);n 
=1;:::;Ng, 
where 
c2f0, 
1g, 
logistic 
regression 
uses 
the 
model 
p(c 
=1jx)= 
(wTx 
+ 
b). 
Assuming 
the 
data 
is 
drawn 
independently 
and 
identically, 
show 
that 
the 
derivative 
of 
the 
log 
likelihood 
L 
with 
respect 
to 
w 
is 


N

XP

T

rwL 
= 
c 
n 
- 
wxn 
+ 
bxn 
(17.10.5) 
n=1

Exercise 
181. 
Consider 
a 
dataset 
D 
= 
f(xn;cn);n 
=1;:::;Ng, 
where 
cn 
2f0, 
1g, 
and 
x 
is 
a 
D 
dimens
ional 
vector. 


1. 
Show 
that 
if 
the 
training 
data 
is 
linearly 
separable 
with 
the 
hyperplane 
wTx 
+ 
b, 
the 
data 
is 
also 
wTx 
+~~
separable 
with 
the 
hyperplane 
~b, 
where 
w~= 
w, 
b 
= 
b 
for 
any 
scalar 
> 
0. 


2. 
What 
consequence 
does 
the 
above 
result 
have 
for 
maximum 
likelihood 
training 
of 
linearly 
separable 
data? 
Exercise 
182. 
Consider 
a 
dataset 
D 
= 
f(xn;cn);n 
=1;:::;Ng, 
where 
cn 
2f0, 
1g, 
and 
x 
is 
a 
N 
dimens
ional 
vector. 
(Hence 
we 
have 
N 
datapoints 
in 
a 
N 
dimensional 
space). 
In 
the 
text 
we 
showed 
that 
we 
can 
nd 
a 
hyperplane 
(parameterised 
by 
(w;b)) 
that 
linearly 
separates 
this 
data 
we 
need, 
for 
each 
datapoint 


nn

xn 
, 
wTxn 
+ 
b 
= 
n 
where 
n 
> 
0 
for 
c=1 
and 
n 
< 
0 
for 
c=0. 
Comment 
on 
the 
relation 
between 
maximum 
likelihood 
training 
and 
the 
algorithm 
suggested 
above. 


DRAFT 
March 
9, 
2010 


331 



Exercises 


n

Exercise 
183. 
Given 
training 
data 
D 
= 
f(x;yn);n 
=1;:::;Ng, 
you 
decide 
to 
t 
a 
regression 
model 
y 
= 
mx 
+ 
c 
to 
this 
data. 
Derive 
an 
expression 
for 
m 
and 
c 
in 
terms 
of 
D 
using 
the 
minimum 
sum 
squared 
error 
criterion. 


Exercise 
184. 
Given 
training 
data 
D 
= 
f(xn;cn);n 
=1;:::;Ng, 
cn 
2f0, 
1g, 
where 
x 
are 
vector 
inputs, 
a 
discriminative 
model 
is 


T 
T 


p(c 
=1jx)= 
(b0 
+ 
v1g(w1 
x 
+ 
b1)+ 
v2g(w2 
x 
+ 
b2)) 
(17.10.6) 


where 
g(x) 
= 
exp(..0:5x2). 
and 
(x)= 
ex=(1 
+ 
ex) 
(this 
is 
a 
neural 
network[41] 
with 
a 
single 
hidden 
layer 
and 
two 
hidden 
units). 


1. 
Write 
down 
the 
log 
likelihood 
for 
the 
class 
conditioned 
on 
the 
inputs, 
based 
on 
the 
usual 
i.i.d. 
ass
umption. 
2. 
Calculate 
the 
derivatives 
of 
the 
log 
likelihood 
as 
a 
function 
of 
the 
network 
parameters, 
w1, 
w2;b1;b2, 
v;b0 
3. 
Comment 
on 
the 
relationship 
between 
this 
model 
and 
logistic 
regression. 
4. 
Comment 
on 
the 
decision 
boundary 
of 
this 
model. 
DRAFT 
March 
9, 
2010 



CHAPTER 
18 


Bayesian 
Linear 
Models 


18.1 
Regression 
With 
Additive 
Gaussian 
Noise 
The 
linear 
models 
in 
chapter(17) 
were 
trained 
under 
Maximum 
Likelihood 
and 
do 
not 
deal 
with 
the 
issue 
that, 
from 
a 
probabilistic 
perspective, 
parameter 
estimates 
are 
inherently 
uncertain 
due 
to 
the 
limited 
available 
training 
data. 
Regression 
refers 
to 
inferring 
a 
mapping 
on 
the 
basis 
of 
observed 
data 
D 
= 
f(xn;yn);n 
=1;:::;Ng, 
where 
(xn;yn) 
represents 
an 
input-output 
pair. 
We 
discuss 
here 
the 
scalar 
output 
case 
(and 
vector 
inputs 
x) 
with 
the 
extension 
to 
the 
vector 
output 
case 
y 
is 
straightforward. 
We 
assume 
that 
each 
(clean) 
output 
is 
generated 
from 
a 
model 
f 
(x; 
w) 
where 
the 
parameters 
w 
of 
the 
function 
f 
are 
unknown. 
An 
output 
y 
is 
generated 
by 
the 
addition 
of 
noise 
. 
to 
the 
clean 
model 
output, 


y 
= 
f 
(x; 
w)+ 
. 
(18.1.1) 


..



If 
the 
noise 
is 
Gaussian 
distributed, 
. 
N. 


0;2, 
the 
model 
generates 
an 
output 
y 
for 
input 
x 
with 
probability 


..

1 
- 
1 
[y..f(x;w)]2 


p(yjw, 
x)= 
Ny 


f(x; 
w);2= 
v 
e 
22 
(18.1.2)
22 


If 
we 
assume 
that 
each 
data 
input-output 
pair 
is 
generated 
identically 
and 
independently, 
the 
likelihood 
the 
model 
generates 
the 
data 
is 


N

N 


p(Djw)= 
p(y 
njw, 
xn)p(xn) 
(18.1.3) 


n=1 


We 
may 
use 
a 
prior 
weight 
distribution 
p(w) 
to 
quantify 
our 
a 
priori 
belief 
in 
the 
suitability 
each 
parameter 
setting. 
Writing 
D 
= 
fDx, 
Dyg, 
the 
posterior 
weight 
distribution 
is 
then 
given 
by 


p(wjD) 
. 
p(Djw)p(w) 
. 
p(Dyjw, 
Dx)p(w) 
(18.1.4) 
Using 
the 
Gaussian 
noise 
assumption, 
and 
for 
convenience 
dening 
ß 
=1=2, 
this 
gives 


N

ß 
N 
N

log 
p(wjD)= 
- 
[y 
n 
- 
f(xn; 
w)]2 
+ 
log 
p 
(w) 
+ 
log 
ß 
+ 
const. 
(18.1.5)

22 


n=1 


Note 
the 
similarity 
between 
equation 
(18.1.5) 
and 
the 
regularised 
training 
error 
equation 
(17.2.16). 
In 
the 
probabilistic 
framework, 
we 
identify 
the 
choice 
of 
a 
sum 
square 
error 
with 
the 
assumption 
of 
additive 
Gaussian 
noise. 
Similarly, 
the 
regularising 
term 
is 
identied 
with 
log 
p(w). 


333 



Regression 
With 
Additive 
Gaussian 
Noise 


Figure 
18.1: 
Belief 
Network 
representation 
of 
a 
Bayesian 
Model 
for 
regression 
under 
the 
i.i.d. 
data 
assumption. 
The 
hyperparameter 
a 
acts 
as 
a 
form 
of 
regulariser, 
controlling 
the 
exibility 
of 
the 
prior 
on 


w

xn
yn
N
the 
weights 
w. 
The 
hyperparameter 
ß 
controls 
the 
level 
of 
noise 
on 


ß 


the 
observations. 


18.1.1 
Bayesian 
linear 
parameter 
models 
Linear 
parameter 
models, 
as 
discussed 
in 
chapter(17), 
have 
the 
form 


B

Xh

f(x; 
w)= 
wii(x) 
= 
wT(x) 
(18.1.6) 


i=1 


where 
the 
parameters 
wi 
are 
also 
called 
`weights’ 
and 
dim 
w 
= 
B. 
Such 
models 
have 
a 
linear 
parameter 
dependence, 
but 
may 
represent 
a 
non-linear 
input-output 
mapping 
if 
the 
basis 
functions 
i(x) 
are 
nonlinear 
in 
x. 


Since 
the 
output 
scales 
linearly 
with 
w, 
we 
can 
discourage 
extreme 
output 
values 
by 
penalising 
large 
weight 
values. 
A 
natural 
weight 
prior 
is 
thus 




..



B 




T

2

0;..1I

..

p(wj)= 
N


w 


(18.1.7)
w

=


w


2

2
e 


where 
the 
precision 
a 
is 
the 
inverse 
variance. 
If 
a 
is 
large, 
the 
total 
squared 
length 
of 
the 
weight 
vector 
w 
is 
encouraged 
to 
be 
small. 
Under 
the 
Gaussian 
noise 
assumption, 
the 
posterior 
distribution 
is 


Nh

ß 
Xhi2 
a 


T

log 
p(wj.., 
D)= 
- 
y 
n 
- 
wT(xn)- 
ww 
+ 
const. 
(18.1.8)

22 


n=1

where 
G 
= 
f, 
} 
represents 
the 
hyperparameter 
set. 
Parameters 
that 
determine 
the 
functions 
f 
may 
also 
be 
included 
in 
the 
hyperparameter 
set. 


Using 
the 
LPM 
in 
equation 
(18.1.5) 
with 
a 
Gaussian 
prior, 
equation 
(18.1.7), 
and 
completing 
the 
square, 
section(8.6.2), 
the 
weight 
posterior 
is 
a 
Gaussian 
distribution, 


p(wj.., 
D)= 
N 
(w 


m, 
S) 
(18.1.9) 
where 
the 
covariance 
and 
mean 
are 
given 
by 


 !..1

NN

XXh

S 
=I 
+ 
ß 
f 
(xn) 
T 
(xn), 
m 
= 
S 
y 
nf 
(xn) 
(18.1.10) 


n=1 
n=1 


The 
mean 
prediction 
for 
an 
input 
x 
is 
then 
given 
by 


Z

¯ 


f(x) 
f(x; 
w)p(wjD, 
..)dw 
= 
mTf 
(x) 
. 
(18.1.11) 


Similarly, 
the 
variance 
of 
the 
underlying 
estimated 
clean 
function 
is 


h

i2

var(f(x)) 
=wT(x)- 
f(x)2 
= 
T(x)S(x) 
(18.1.12) 


The 
output 
variance 
var(f(x)) 
depends 
only 
on 
the 
input 
variables 
and 
not 
on 
the 
training 
outputs 
y. 
Since 
the 
additive 
noise 
. 
is 
uncorrelated 
with 
the 
model 
outputs, 
the 
predictive 
variance 
is 


var(y(x)) 
= 
var(f(x)) 
+ 
2 
(18.1.13) 
and 
represents 
the 
variance 
of 
the 
`noisy’ 
output 
for 
an 
input 
x. 
334 
DRAFT 
March 
9, 
2010 



Regression 
With 
Additive 
Gaussian 
Noise 



(a) 
(b) 
(c) 
Figure 
18.2: 
Along 
the 
horizontal 
axis 
we 
plot 
the 
input 
x 
and 
along 
the 
vertical 
axis 
the 
output 
t. 
(a): 
The 
raw 
input-output 
training 
data. 
(b): 
Prediction 
using 
regularised 
training 
and 
xed 
hyperparameters. 
(c): 
Prediction 
using 
ML-II 
optimised 
hyperparameters. 
Also 
plotted 
are 
standard 
error 
bars 
on 


p

the 
clean 
underlying 
function, 
var(f(x)). 


Example 
81. 
In 
g(18.2b), 
we 
show 
the 
mean 
prediction 
on 
the 
data 
in 
g(18.2a) 
using 
15 
Gaussian 
basis 
functions 


..

i(x) 
= 
exp..0:5(x 
- 
ci)2=2(18.1.14) 


with 
width 
. 
=0:032 
and 
centres 
ci 
spread 
out 
evenly 
over 
the 
1-dimensional 
input 
space 
from 
..2 
to 
2. 
We 
set 
the 
other 
hyperparameters 
by 
hand 
to 
ß 
= 
100 
and 
a 
= 
1. 
The 
prediction 
severely 
overts 
the 
data, 
a 
result 
of 
a 
poor 
choice 
of 
hyperparameter 
settings. 
This 
is 
resolved 
in 
g(18.2c) 
using 
the 
ML-II 
parameters, 
as 
described 
below. 


18.1.2 
Determining 
hyperparameters: 
ML-II 
The 
hyperparameter 
posterior 
distribution 
is 


p(..jD) 
. 
p(Dj..)p(..) 
(18.1.15) 


A 
simple 
summarisation 
of 
the 
posterior 
is 
given 
by 
the 
MAP 
assignment 
which 
takes 
the 
single 
`optimal’ 
setting: 


..* 
= 
argmax 
p(..jD) 
(18.1.16) 


G 


If 
the 
prior 
belief 
about 
the 
hyperparameters 
is 
weak 
(p(..) 
˜ 
const.), 
this 
is 
equivalent 
to 
using 
the 
G 
that 
maximises 
the 
marginal 
likelihood 


p(Dj..) 
= 
p(Dj.., 
w)p(wj..)dw 
(18.1.17) 


This 
approach 
to 
setting 
hyperparameters 
is 
called 
`ML-II’ 
[33] 
or 
the 
Evidence 
Procedure[181]. 


In 
the 
case 
of 
Bayesian 
Linear 
Parameter 
models 
under 
Gaussian 
additive 
noise 
computing 
the 
marginal 
likelihood 
equation 
(18.1.17) 
involves 
only 
Gaussian 
integration. 
A 
direct 
approach 
to 
deriving 
an 
expression 
for 
the 
marginal 
likelihood 
is 
to 
consider 


hi2

a 


T

p(Dj.., 
w)p(w) 
= 
exp 
- 
y 
n 
- 
wT(xn) 
- 
ww 
(2)N=2 
(2)B=2 
(18.1.18)

22 


DRAFT 
March 
9, 
2010 



Regression 
With 
Additive 
Gaussian 
Noise 


By 
collating 
terms 
in 
w 
(completing 
the 
square, 
section(8.6.2)), 
the 
above 
represents 
a 
Gaussian 
in 
w 
with 
additional 
factors. 
After 
integrating 
over 
this 
Gaussian 
we 
have 


N

X

2 
log 
p(Dj..) 
= 
..ß 
(y 
n)2 
+ 
dTS..1d 
- 
log 
det 
(S)+ 
B 
log 
a 
+ 
N 
log 
ß 
- 
N 
log 
(2) 
(18.1.19) 


n=1 


where 


X

n

d 
= 
ß 
(xn)y 
(18.1.20) 


n 


See 
exercise(186) 
for 
an 
alternative 
expression. 


Example 
82. 
Using 
the 
hyperparameters 
, 
, 
. 
that 
optimise 
expression 
(18.1.19) 
gives 
the 
results 
in 
g(18.2c) 
where 
we 
plot 
both 
the 
mean 
predictions 
and 
standard 
predictive 
error 
bars. 
This 
demonstrates 
that 
an 
acceptable 
setting 
for 
the 
hyperparameters 
can 
be 
obtained 
by 
maximising 
the 
marginal 
likelihood. 


18.1.3 
Learning 
the 
hyperparameters 
using 
EM 
We 
can 
set 
hyperparameters 
such 
as 
a 
and 
ß 
by 
maximising 
the 
marginal 
likelihood 
equation 
(18.1.17). 
A 
convenient 
computational 
procedure 
to 
achieve 
this 
is 
to 
interpret 
the 
w 
as 
latent 
variables 
and 
apply 
the 
EM 
algorithm, 
section(11.2). 
In 
this 
case 
the 
energy 
term 
is 


E 
hlog 
p(Djw, 
..)p(wj..)ip(wjD;..old) 
(18.1.21) 


According 
to 
the 
general 
EM 
procedure 
we 
need 
to 
maximise 
the 
energy 
term. 
For 
a 
hyperparameter 
G 
the 
derivative 
of 
the 
energy 
is 
given 
by 




@. 


E 
log 
p(Djw, 
..)p(wj..)(18.1.22)

@..@G 
p(wjD;..old) 


For 
the 
Bayesian 
LPM 
with 
Gaussian 
weight 
and 
noise 
distributions, 
we 
obtain 


Nh

Xi2

@N 
1 


E 
= 
- 
y 
n 
- 
wT(xn)(18.1.23)

@ß 
2ß 
2 


p(wj..old 
;D)

n=1

 !

NhN

N 
1 
Xi2 
1 
X

= 
- 
y 
n 
- 
mT(xn)- 
traceS 
(xn)T(xn)(18.1.24)

2ß 
22

n=1n=1 


Solving 
for 
the 
zero 
derivatives 
gives 
the 
M-step 
update 


N

Xhi2 


1 
=
1 
y 
n 
- 
mT(xn)+ 
traceSS^(18.1.25)

new 


N 


n=1

where 
S^
is 
the 
empirical 
covariance 
of 
the 
basis-function 
vectors 
(xn);n 
=1;:::;N. 


Similarly, 
for 
, 


DE

@B 
1 
TB 
1 
T

E 
= 
- 
ww= 
- 
trace 
(S)+ 
mm

@a 
2a 
2p(wj..old 
;D) 
2a 
2

which, 
on 
equating 
to 
zero, 
gives 
the 
update 




11 
T

new 
= 
trace 
(S)+ 
mm(18.1.26)

B

where 
S 
and 
m 
are 
given 
in 
equation 
(18.1.10). 
An 
alternative 
xed 
point 
procedure 
that 
is 
often 
more 
rapidly 
convergent 
than 
EM 
is 
given 
in 
equation 
(18.1.36). 
Closed 
form 
updates 
for 
other 
hyperparameters, 
such 
as 
the 
width 
of 
the 
basis 
functions, 
are 
generally 
not 
available, 
and 
the 
corresponding 
energy 
term 
needs 
to 
be 
optimised 
numerically. 


DRAFT 
March 
9, 
2010 



Regression 
With 
Additive 
Gaussian 
Noise 


-202-2-1012lambda=0.01-202-2-1012lambda=0.05-202-2-1012lambda=0.09-202-2-1012lambda=0.13-202-2-1012lambda=0.17-202-2-1012lambda=0.21-202-2-1012lambda=0.25-202-2-1012lambda=0.29-202-2-1012lambda=0.33-202-2-1012lambda=0.37-202-2-1012lambda=0.41-202-2-1012lambda=0.45-202-2-1012lambda=0.49-202-2-1012lambda=0.53-202-2-1012lambda=0.57-202-2-1012lambda=0.61-202-2-1012lambda=0.65-202-2-1012lambda=0.69-202-2-1012lambda=0.73-202-2-1012lambda=0.77
Figure 
18.3: 
Predictions 
for 
an 
RBF 
for 
dierent 
widths 
. 
For 
each 
. 
the 
ML-II 
optimal 
, 
ß 
are 
obtained 
by 
running 
the 
EM 
procedure 
to 
convergence 
and 
subsequently 
used 
to 
form 
the 
predictions. 
In 
each 
panel 
the 
dots 
represent 
the 
training 
points, 
with 
x 
along 
the 
horizontal 
axis 
and 
y 
along 
the 
vertical 
axis. 
Mean 
predictions 
are 
plotted, 
along 
with 
predictive 
error 
bars 
of 
one 
standard 
deviation. 
According 
to 
ML-II, 
the 
best 
model 
corresponds 
to 
. 
=0:37, 
see 
g(18.4). 
The 
smaller 
values 
of 
. 
overt 
the 
data, 
giving 
rise 
to 
too 
`rough’ 
functions. 
The 
largest 
values 
of 
. 
undert, 
giving 
too 
`smooth’ 
functions. 
See 
demoBayesLinReg.m. 


18.1.4 
Hyperparameter 
optimisation 
: 
using 
the 
gradient 
Hyperparameters 
such 
as 
a 
can 
be 
set 
by 
maximising 
the 
marginal 
likelihood 


p(Dj)= 
p(Djw;)p(wj) 
(18.1.27) 


w 


To 
nd 
the 
optimal 
, 
we 
search 
for 
the 
zero 
derivative 
of 
log 
p(Dj). 
From 
equation 
(18.2.18) 
we 
can 
use 
the 
general 
derivative 
identity 
to 
arrive 
at 


. 
. 
(18.1.28)

log 
p(Dj) 
=log 
p(wj)

@a 
@a 


p(wj;D) 


Since 


B

T

log 
p(wj)= 
- 
ww 
+ 
log 
a 
+ 
const. 
(18.1.29)

22 


-0.100.10.20.30.40.50.60.702468101214x 109log marginal likelihoodlambda
Figure 
18.4: 
The 
log 
marginal 
likelihood 
log 
p(Dj, 
();() 
having 
found 
the 
optimal 
values 
of 
the 
hyperparameters 
a 
and 
ß 
using 
ML-II. 
These 
optimal 
values 
are 
dependent 
on 
. 
According 
to 
ML-II, 
the 
best 
model 
corresponds 
to 
. 
=0:37. 


DRAFT 
March 
9, 
2010 



Regression 
With 
Additive 
Gaussian 
Noise 


we 
obtain 




. 
1 
TB

log 
p(Dj)= 
..ww 
+ 
(18.1.30)

@a 
2

p(wj;D) 


Setting 
the 
derivative 
to 
zero, 
the 
optimal 
a 
satises 


DE

B

T

0= 
..ww+ 
(18.1.31) 


p(wj;D) 
a 


One 
may 
now 
form 
a 
xed 
point 
equation 


new 


= 
B 
(18.1.32)

hwTwi

p(wj;D) 


which 
is 
in 
fact 
a 
re-derivation 
of 
the 
EM 
procedure 
for 
this 
model. 
For 
a 
Gaussian 
posterior, 
p(wj, 
D)= 
N 
(w 


m, 
S),

DEDE

TTT

ww= 
traceww..hwihwiT 
+ 
hwihwiT= 
trace 
(S)+ 
mm 
(18.1.33) 


B 


new 


= 
(18.1.34)

trace 
(S)+ 
mTm 


Gull-MacKay 
xed 
point 
iteration 


From 
equation 
(18.1.31) 
we 
have 


DE

TT

0= 
..ww+ 
B 
= 
..S 
- 
mwT 
+ 
B 
(18.1.35) 


p(wj;D) 


so 
that 
an 
alternative 
xed 
point 
equation[123, 
180] 
is 


B 
- 
S 


new 


= 
(18.1.36)

mTm 


In 
practice 
this 
update 
converges 
more 
rapidly 
than 
equation 
(18.1.34). 


Example 
83 
(Learning 
the 
basis 
function 
widths). 
In 
g(18.3) 
we 
plot 
the 
training 
data 
for 
a 
regression 
problem 
using 
a 
Bayesian 
LPM. 
A 
set 
of 
10 
Radial 
Basis 
Functions 
are 
used, 


..

i(x) 
= 
exp..0:5(x 
- 
ci)2=2(18.1.37) 


with 
ci, 
i 
=1;:::, 
10 
spread 
out 
evenly 
between 
..2 
and 
2. 
The 
hyperparameters 
a 
and 
ß 
are 
learned 
by 
ML-II 
under 
EM 
updating. 
For 
a 
xed 
width 
. 
we 
then 
present 
the 
predictions, 
each 
time 
nding 
the 
optimal 
a 
and 
ß 
for 
this 
width. 
The 
optimal 
joint 
, 
, 
. 
hyperparameter 
setting 
is 
obtained 
as 
described 
in 
g(18.4) 
which 
shows 
the 
marginal 
log 
likelihood 
for 
a 
range 
of 
widths. 


18.1.5 
Validation 
likelihood 
The 
hyperparameters 
found 
by 
ML-II 
are 
those 
which 
are 
best 
at 
explaining 
the 
training 
data. 
In 
principle, 
this 
is 
dierent 
from 
those 
that 
are 
best 
for 
prediction 
and, 
in 
practice 
therefore, 
it 
is 
reasonable 
to 
set 
hyperparameters 
also 
by 
validation 
techniques. 
One 
such 
method 
is 
to 
set 
hyperparameters 
by 
minimal 
prediction 
error 
on 
a 
validation 
set. 
Another 
common 
technique 
is 
to 
set 
hyperparameters 
G 
by 
their 


m

likelihood 
on 
a 
validation 
set 
fXval, 
Yvalgf(xm 
);m 
=1;:::;Mg:

val;yval

p(Yvalj.., 
Xtrain, 
Ytrain, 
Xval)= 
p(Yvaljw, 
..)p(wj.., 
Xtrain, 
Ytrain) 
(18.1.38) 


w 


DRAFT 
March 
9, 
2010 



Regression 
With 
Additive 
Gaussian 
Noise 


from 
which 
we 
obtain 
(see 
exercise(187)) 
11

log 
p(Yvalj.., 
Dtrain, 
Xval)= 
- 
log 
det 
(2Cval) 
- 
(yval 
- 
valm)T 
C..1 
(yval 
- 
valm) 
(18.1.39)

22 
val 


T

1 
M

where 
yval 
=yval;:::;y

val

Cval 
= 
valST 
(18.1.40)

val 
+ 
2IM 
and 
the 
design 
matrix 




1 
M

T 
=(x);:::, 
(x)(18.1.41)

val 
valval

The 
optimal 
hyperparameters 
..* 
can 
then 
be 
found 
by 
maximising 
(18.1.39) 
with 
respect 
to 
... 


18.1.6 
Prediction 
The 
mean 
function 
predictor 
based 
on 
hyperparameters 
G 
and 
weights 
w 
is 
given 
by 


ZZZ

¯ 


f(x)=f(x; 
w)p(w, 
..jD)dwd..=f(x; 
w)p(wj.., 
D)dwp(..jD)dG 
(18.1.42) 


The 
term 
in 
curly 
brackets 
is 
the 
mean 
predictor 
for 
xed 
hyperparameters. 
Equation(18.1.42) 
then 
weights 
each 
mean 
predictor 
by 
the 
posterior 
probability 
of 
the 
hyperparameter 
p(..jD). 
This 
is 
a 
general 
recipe 
for 
combining 
model 
predictions, 
where 
each 
model 
is 
weighted 
by 
its 
posterior 
probability. 
However, 
computing 
the 
integral 
over 
the 
hyperparameter 
posterior 
is 
numerically 
challenging 
and 
approximations 
are 
usually 
required. 
Provided 
the 
hyperparameters 
are 
well 
determined 
by 
the 
data, 
we 
may 
instead 
approximate 
the 
above 
hyperparameter 
integral 
by 
nding 
the 
MAP 
hyperparameters 
and 
use 


Z

¯ 


f(x) 
f(x; 
w)p(wj..* 
, 
D)dw 
(18.1.43) 


18.1.7 
The 
relevance 
vector 
machine 
The 
Relevance 
Vector 
Machine 
assumes 
that 
only 
a 
small 
number 
of 
components 
of 
the 
basis 
function 
vector 
are 
relevant 
in 
determining 
the 
solution 
for 
w. 
For 
a 
predictor, 


B

XZ

f(x; 
w)= 
wii(x) 
= 
wT(x) 
(18.1.44) 


i=1 


it 
is 
often 
the 
case 
that 
some 
basis 
functions 
will 
be 
redundant 
in 
the 
sense 
that 
a 
linear 
combination 
of 
the 
other 
basis 
functions 
can 
reproduce 
the 
training 
outputs 
with 
insignicant 
loss 
in 
accuracy. 
To 
exploit 
this 
eect 
and 
seek 
a 
parsimonious 
solution 
we 
may 
use 
a 
more 
rened 
prior 
that 
encourages 
each 
wi 
itself 
to 
be 
small: 


Y

p(wj)=p(wiji) 
(18.1.45) 


i 


where 
the 
prior 
on 
each 
individual 
weight 
is 
given 
by 




..

i 


1 


i

2

2

0;..1 


i

- 


p(wiji)= 
N

(18.1.46)
w

=


i

2

wi 


e


2p


The 
modications 
required 
to 
the 
description 
of 
section(18.1.1) 
are 
to 
replace 
S 
with 


 !..1

N

X

S 
=diag 
()+ 
ß 
f 
(xn) 
T 
(xn)(18.1.47) 


n=1 


The 
marginal 
likelihood 
is 
then 
given 
by 


NB

XXZ

2 
log 
p(Dj..) 
= 
..ß 
(y 
n)2 
+ 
dTS..1d 
- 
log 
det 
(S) 
+ 
log 
i 
+ 
N 
log 
ß 
- 
N 
log 
(2) 
(18.1.48) 


n=1 
i=1 


The 
EM 
update 
for 
ß 
is 
unchanged, 
and 
the 
EM 
update 
for 
each 
i 
is 
1 
2

=[S]ii 
+ 
m 
(18.1.49)

new 
i 
i 


DRAFT 
March 
9, 
2010 



Classication 


Algorithm 
18 
Evidence 
Procedure 
for 
Bayesian 
Logistic 
Regression 
1: 
Initialise 
w 
and 
. 
2: 
3: 
4: 
5: 
while 
Not 
Converged 
do 
Find 
optimal 
w 
* 
by 
iterating 
equation 
(18.2.16), 
equation 
(18.2.15) 
to 
convergence. 
Update 
a 
according 
to 
equation 
(18.2.9). 
end 
while 
I 
E-step 
I 
M-Step 


18.2 
Classication 
For 
the 
logistic 
regression 
model 


 !

B

p(c 
=1jw, 
x)= 
wii(x)(18.2.1) 


i 


the 
Maximum 
Likelihood 
method 
returns 
only 
a 
single 
optimal 
w. 
To 
deal 
with 
the 
inevitable 
uncertainty 
in 
estimating 
w 
we 
need 
to 
determine 
the 
posterior 
distribution 
of 
the 
weights 
w. 
To 
do 
so 
we 
rst 
dene 
a 
prior 
on 
the 
weights 
p(w) 
for 
which 
a 
convenient 
choice 
is 
a 
Gaussian: 


B=2

..



..wTw=2 


p(wj)= 
Nw 


0;..1I= 
e 
(18.2.2)

(2)B=2 


where 
a 
is 
the 
inverse 
variance 
(also 
called 
the 
precision). 
Given 
a 
dataset 
of 
input-class 
labels, 
D 
= 
f(xn;cn) 
;n 
=1;:::;Ng, 
the 
parameter 
posterior 
is 


N

Y

p(Djw;)p(wj)1 


p(wj, 
D)= 
= 
p(wj) 
p(c 
njxn 
, 
w) 
(18.2.3) 


p(Dj) 
p(Dj)

n=1 


Unfortunately, 
this 
distribution 
is 
not 
of 
any 
standard 
form 
and 
exactly 
inferring 
statistics 
such 
as 
the 
mean, 
or 
the 
most 
probable 
value 
are 
formally 
computationally 
intractable. 


18.2.1 
Hyperparameter 
optimisation 
Hyperparameters 
such 
as 
a 
can 
be 
set 
by 
maximising 
the 
marginal 
likelihood 


ZZYN

YYa 
B=2 
- 
a 


ww 


p(Dj)= 
p(Djw)p(wj)= 
p(c 
njxn 
, 
w) 
e 
2 
T(18.2.4)

2

ww 
n=1 


There 
are 
several 
approaches 
one 
could 
take 
to 
approximate 
this 
and 
below 
we 
discuss 
the 
Laplace 
and 
a 
variational 
technique. 
Common 
to 
all 
approaches, 
however, 
is 
the 
form 
of 
the 
gradient, 
diering 
only 
in 
the 
statistics 
under 
an 
approximation 
to 
the 
posterior. 
For 
this 
reason 
we 
derive 
rst 
generic 
hyperparameter 
update 
formulae 
that 
apply 
under 
all 
approximations. 


To 
nd 
the 
optimal 
, 
we 
search 
for 
the 
zero 
derivative 
of 
log 
p(Dj). 
This 
is 
equivalent 
to 
the 
linear 
regression 
case, 
and 
we 
immediately 
obtain 




. 
1 
TB

log 
p(Dj)= 
..ww 
+ 
(18.2.5)

@a 
2

p(wj;D) 


Setting 
the 
derivative 
to 
zero, 
an 
exact 
equation 
is 
that 
the 
optimal 
a 
satises 


DE

T

0= 
..ww+ 
B 
(18.2.6) 


p(wj;D) 
a 


One 
may 
now 
form 
a 
xed 
point 
equation 


new 


= 
B 
(18.2.7)

hwTwi

p(wj;D) 


340 
DRAFT 
March 
9, 
2010 



Classication 


The 
averages 
in 
the 
above 
expression 
cannot 
be 
computed 
exactly 
and 
are 
replaced 
by 
averages 
with 
respect 
an 
approximation 
of 
the 
posterior 
q(wj, 
D). 
Note 
that 
since 
we 
only 
have 
an 
approximation 
to 
the 
posterior, 
and 
therefore 
the 
mean 
and 
covariance 
statistics, 
we 
cannot 
guarantee 
that 
the 
likelihood 
will 
always 
increase. 


For 
a 
Gaussian 
approximation 
of 
the 
posterior, 
q(wj, 
D)= 
N 
(w 


m, 
S)

DEDE

TTT

ww= 
traceww..hwihwiT 
+ 
hwihwiT= 
trace 
(S)+ 
mm 
(18.2.8) 


new 


= 
B 
(18.2.9)

trace 
(S)+ 
mTm 


In 
this 
case 
the 
Gull-Mackay 
alternative 
xed 
point 
equation[123, 
180] 
is 


B 
- 
S 


new 


= 
(18.2.10)

mTm 


The 
hyperparameter 
updates 
(18.2.9) 
and 
(18.2.10) 
have 
the 
same 
form 
as 
for 
the 
regression 
model. 
The 
mean 
m 
and 
covariance 
S 
of 
the 
posterior 
in 
the 
regression 
and 
classication 
cases 
are 
however 
dierent. 
In 
the 
classication 
case 
we 
need 
to 
approximate 
the 
mean 
and 
covariance, 
as 
discussed 
below. 


18.2.2 
Laplace 
approximation 
The 
weight 
posterior 
is 
given 
by 


..E(w)

p(wj, 
D) 
. 
e 
(18.2.11) 


where 


N
TThn


E(w)= 
a 
ww 
- 
Xlog 
w, 
hn 
= 
(2c 
n 
- 
1)n 
(18.2.12)

2 


n=1 


By 
approximating 
E(w) 
by 
a 
quadratic 
function 
in 
w, 
we 
obtain 
a 
Gaussian 
approximation 
q(wjD;) 
to 
p(wjD;). 
To 
do 
so 
we 
rst 
nd 
the 
minimum 
of 
E(w). 
Dierentiating, 
we 
obtain 


N

X

rE 
= 
w 
- 
(1 
- 
n)hn 
;n 
= 
wThn(18.2.13) 


n=1 


It 
is 
convenient 
to 
use 
a 
Newton 
method 
to 
nd 
the 
optimum. 
The 
Hessian 
matrix 
with 
elements 


@2 
Hij 
= 
E(w) 
(18.2.14)

@wi@wj 


is 
given 
by 


N

X

H 
= 
I 
+ 
n(1 
- 
n)n 
(n)T 
(18.2.15) 
n=1

|{z


J 


Note 
that 
the 
Hessian 
is 
positive 
semidenite 
(see 
exercise(188)) 
so 
that 
the 
function 
E(w) 
is 
convex 
(bowl 
shaped), 
and 
nding 
a 
minimum 
of 
E(w) 
is 
numerically 
unproblematic. 
A 
Newton 
update 
then 
is 


wnew 
= 
w 
- 
H..1 
(rE) 
(18.2.16) 


Given 
a 
converged 
w, 
the 
posterior 
approximation 
is 
given 
by 


q(wjD;)= 
N 
(w 


m, 
S) 
, 
S 
= 
H..1 
(18.2.17) 


where 
m 
= 
w 
* 
is 
the 
converged 
estimate 
of 
the 
minimum 
point 
of 
E(w) 
and 
H 
is 
the 
Hessian 
of 
E(w) 
at 
this 
point. 


DRAFT 
March 
9, 
2010 



Classication 


Approximating 
the 
marginal 
likelihood 


The 
marginal 
likelihood 
is 
given 
by 


ZZXNZ

Ya 
B=2 
T

- 
a 
w..E(w)

p(Dj)= 
p(Djw)p(wj)= 
p(c 
njxn 
, 
w)e 
2 
w 
/he 
(18.2.18)

2

ww 
w

n=1 


For 
an 
optimum 
value 
m 
= 
w 
, 
we 
approximate 
the 
marginal 
likelihood 
using 
(see 
section(28.2)) 




X

a 
1 
B

log 
p(Dj) 
hL() 
..h(w 
)Tw 
* 
+log 
(w 
)T 
hn..hlog 
det 
(I 
+ 
J) 
+ 
log 
a 
(18.2.19)

2 
22 


n 


Given 
this 
approximation 
L() 
to 
the 
marginal 
likelihood, 
an 
alternative 
strategy 
for 
hyperparameter 
optimisation, 
is 
to 
optimises 
L() 
with 
respect 
to 
. 
By 
dierentiating 
L() 
directly, 
the 
reader 
may 
show 
that 
the 
resulting 
updates 
are 
in 
fact 
equivalent 
to 
using 
the 
general 
condition 
equation 
(18.2.6) 
under 
a 
Laplace 
approximation 
to 
the 
posterior 
statistics. 


18.2.3 
Making 
predictions 
Ultimately, 
our 
interest 
is 
to 
classify 
in 
novel 
situations, 
averaging 
over 
posterior 
weight 
uncertainty, 


p(c 
=1jx, 
, 
D)= 
p(c 
=1jx, 
w)p(wj, 
D) 
(18.2.20) 


w 


The 
B 
dimensional 
integrals 
over 
w 
cannot 
be 
computed 
analytically 
and 
numerical 
approximation 
is 
required. 
In 
this 
particular 
case 
the 
relative 
benign 
nature 
of 
the 
posterior 
(the 
log 
posterior 
is 
concave, 
see 
below) 
suggests 
that 
a 
simple 
Laplace 
approximation 
may 
suce 
(see 
[142] 
for 
a 
variational 
approximation). 
To 
make 
a 
class 
prediction 
for 
a 
novel 
input 
x, 
we 
use 




T

p(c 
=1jx, 
D;a 
)= 
p(c 
=1jx, 
w)p(wjD;a 
)dw 
= 
xwp(wjD;a 
)dw 


To 
compute 
the 
predictions 
it 
ould 
appear 
that 
we 
need 
to 
carry 
out 
an 
integral 
in 
B 
dimensions.

..w 
w
X

However, 
since 
the 
term 
s 
xTw 
depends 
on 
w 
via 
the 
scalar 
product 
xTw, 
we 
only 
require 
the 
integral 
over 
the 
one-dimensional 
projection 
(see 
exercise(189)) 


T

h 
hxw 
(18.2.21) 


so 
that 


p(c 
=1jx, 
D;a 
)= 
s 
(h) 
p(hjx, 
D;a 
)dh 
(18.2.22) 


Under 
the 
Laplace 
approximation, 
w 
is 
Gaussian, 
p(wjD;) 
Nh(w 


m, 
S). 
Since 
h 
is 
a 
projection 
of 
w, 
h 
is 
also 
Gaussian 
distributed 




T

p(hjx, 
D;a 
) 
Nh 


xm, 
xTx(18.2.23) 


Predictions 
may 
then 
be 
made 
by 
numerically 
evaluating 
the 
one-dimensional 
integral 
over 
the 
Gaussian 
distribution 
in 
h, 
equation 
(18.2.22). 


Approximating 
the 
Gaussian 
average 
of 
a 
logistic 
sigmoid 


Predictions 
under 
a 
Gaussian 
posterior 
approximation 
require 
the 
computation 
of 


I 
h(x)iN 
(x 


(18.2.24)
;2) 


DRAFT 
March 
9, 
2010 



Classication 


-6-4-20246-6-4-202460.10.10.10.10.20.20.20.20.20.20.30.30.30.30.30.30.30.30.40.40.40.40.40.40.40.40.50.50.50.50.50.50.60.60.60.60.70.70.70.70.
0.80.80.90.9
Figure 
18.5: 
Bayesian 
Logistic 
Regression 
with 
the 


..(x..m)2

RBF 
e, 
placing 
basis 
functions 
centred 
on 
a 
subset 
of 
the 
training 
points. 
The 
green 
points 
are 
training 
data 
from 
class 
1, 
and 
the 
red 
points 
are 
training 
data 
from 
class 
0. 
The 
contours 
represent 
the 
probability 
of 
being 
in 
class 
1. 
The 
optimal 
value 
of 
a 
found 
by 
the 
evidence 
procedure 
in 
this 
case 
is 
0:45 
(. 
is 
set 
by 
hand 
to 
2). 
See 
demoBayesLogRegression.m 


where 
µ 
= 
xT, 
2 
= 
xTx. 
Gaussian 
quadrature 
is 
an 
obvious 
numerical 
candidate[227]. 
An 
alternative 
is 
to 
replace 
the 
logistic 
sigmoid 
by 
a 
suitably 
transformed 
erf 
function[181], 
the 
reason 
being 
that 
the 
Gaussian 
average 
of 
an 
erf 
function 
is 
another 
erf 
function. 
Using 
a 
single 
erf, 
an 
approximation 
is1 


1 


(x) 
h(1 
+ 
erf(x)) 
(18.2.25)

2 


These 
two 
functions 
agree 
at 
..1, 
0, 
1. 
A 
reasonable 
criterion 
is 
that 
the 
derivatives 
of 
these 
two 
should 
agree 
at 
x 
= 
0 
since 
then 
they 
have 
locally 
the 
same 
slope 
around 
the 
origin 
and 
have 
globally 
similar 
shape. 
Using 
(0) 
= 
0:5 
and 
that 
the 
derivative 
is 
(0)(1 
..h(0)), 
this 
requires 


p

1 
p 


= 
p)h. 
= 
(18.2.26)

4 
p 
4 


A 
more 
accurate 
approximation 
can 
be 
found 
by 
considering 


X

(x) 

ui 
(1 
+ 
erf(ix)) 
(18.2.27)

2 


i 


P

wherei 
ui 
= 
1. 
Suitable 
values 
for 
ui 
and 
i 
are 
given 
in 
logsigapp.m 
which 
uses 
a 
linear 
combination 
of 
11 
erf 
functions 
to 
approximate 
the 
logistic 
sigmoid. 
To 
compute 
the 
approximate 
average 
of 
(x) 
over 
a 
Gaussian, 
one 
may 
then 
make 
use 
of 
the 
result 


Z8 
2 
d 
1 
- 
x 
1 


phe 
2 
erfph(cx 
+ 
d)dx 
= 
erfph(18.2.28)

2

28 
2 
2+2c

Since 


Z8 
Z1

(x..)2 
2

1 
- 
1 
- 
x 


phe 
22 
erf 
(x) 
dx 
= 
phe 
2 
erf 
(. 
(x 
+ 
)) 
dx 
(18.2.29)
228 
28 


we 
have

01X

11 
@iA

h(x)iN 
(x 
;) 
h+ 
uierf 
q(18.2.30)

22

i 
1+2i 
22 


Further 
approximate 
statistics 
can 
be 
obtained 
using 
the 
results 
derived 
in 
[22]. 


1Note 
that 
the 
denition 
of 
the 
erf 
function 
used 
here 
is 
taken 
to 
be 
consistent 
with 
MATLAB, 
namely 
that 
erf(x) 
= 


2Rx 
..t2 
2Rx 
- 
1
2 
t2 
v 
e 
dt. 
Other 
authors 
dene 
it 
to 
be 
the 
cumulative 
density 
function 
of 
a 
standard 
Gaussian, 
v 
e 
dt.

0 
..8 


DRAFT 
March 
9, 
2010 
343 



Classication 


-8-6-4-202468-8-6-4-2024680.10.10.10.10.10.10.20.20.20.20.20.20.30.30.30.30.30.30.30.30.40.40.40.40.40.40.40.40.40.50.50.50.50.50.60.60.60.70.70.7080.80.90.9
-8-6-4-202468-8-6-4-2024680.10.10.10.10.10.10.20.20.20.20.20.20.30.30.30.30.30.30.30.30.40.40.40.40.40.40.40.40.40.50.50.50.50.50.60.60.60.70.70.7080.80.90.9
(a) 
(b) 
..(x..m)2

Figure 
18.6: 
Classication 
using 
the 
RVM 
with 
RBF 
e, 
placing 
a 
basis 
function 
on 
a 
subset 
of 
the 
training 
data 
points. 
The 
green 
points 
are 
training 
data 
from 
class 
1, 
and 
the 
red 
points 
are 
training 
data 
from 
class 
0. 
The 
contours 
represent 
the 
probability 
of 
being 
in 
class 
1. 
(a): 
Training 
points. 
(b): 
The 
training 
points 
weighted 
by 
their 
relevance 
value 
1=n. 
Nearly 
all 
the 
points 
have 
a 
value 
so 
small 
that 
they 
eectively 
vanish. 
See 
demoBayesLogRegRVM.m 


18.2.4 
Relevance 
vector 
machine 
for 
classication 
In 
adopting 
the 
RVM 
prior 
to 
classication, 
we 
encourage 
individual 
weights 
to 
be 
small 


Y

p(wj)=p(wiji) 
(18.2.31) 


i 


where 




1 


p(wiji)= 
Nwi 


0, 
(18.2.32)

i

The 
only 
alterations 
in 
the 
previous 
evidence 
procedure 
are 


X

[rE]i 
= 
iwi 
..(1 
- 
n)hi
n 
, 
H 
= 
diag 
()+ 
J 
(18.2.33) 
n 


These 
are 
used 
in 
the 
Newton 
update 
formula 
as 
before. 
The 
EM 
update 
equation 
for 
the 
's 
is 
given 
by 


1 


new 


= 
(18.2.34)

i 


2

mi 
+ 
Sii 


where 
S 
= 
H..1 
. 
Similarly, 
the 
Gull-MacKay 
update 
is 
given 
by 


new 
1 
- 
iSii 


= 
(18.2.35)

i 


2

m

i 


Running 
this 
procedure, 
one 
typically 
nds 
that 
many 
of 
the 
's 
tend 
to 
innity 
and 
the 
corresponding 
weights 
are 
pruned 
from 
the 
system. 
The 
remaining 
weights 
tend 
correspond 
to 
basis 
functions 
(in 
the 
RBF 
case) 
in 
the 
centres 
of 
mass 
of 
clusters 
of 
datapoints 
of 
the 
same 
class, 
see 
g(18.6). 
Contrast 
this 
with 
the 
situation 
in 
SVMs, 
where 
the 
retained 
datapoints 
tend 
to 
be 
on 
the 
decision 
boundaries. 
The 
number 
of 
training 
points 
retained 
by 
the 
RVM 
tends 
to 
be 
very 
small 
– 
smaller 
indeed 
that 
the 
number 
retained 
in 
the 
SVM 
framework. 
Whilst 
the 
RVM 
does 
not 
support 
large 
margins, 
and 
hence 
may 
be 
a 
less 
robust 
classier, 
it 
does 
retain 
the 
advantages 
of 
a 
probabilistic 
framework[276]. 
A 
potential 
critique 
of 
the 
RVM, 
coupled 
with 
an 
ML-II 
procedure 
for 
learning 
the 
i 
is 
that 
it 
is 
overly 
aggressive 
in 
terms 
of 
pruning. 
Indeed, 
as 
one 
may 
verify 
running 
demoBayesLogRegRVM.m 
it 
is 
common 
to 
nd 
an 
instance 
of 
a 
problem 
for 
which 
there 
exists 
a 
set 
of 
i 
such 
that 
the 
training 
data 
can 
be 
classied 
perfectly; 
however, 
after 
using 
ML-II, 
so 
many 
of 
the 
i 
are 
set 
to 
zero 
that 
the 
training 
data 
can 
no 
longer 
be 
classied 
perfectly. 


DRAFT 
March 
9, 
2010 



Exercises 


18.2.5 
Multi-class 
case 
We 
briey 
note 
that 
the 
multi-class 
case 
can 
be 
treated 
by 
using 
the 
softmax 
function 
under 
a 
one-of-m 
class 
coding 
scheme. 
The 
class 
probabilities 
are 


ym

e

p(c 
= 
mjy) 
= 
(18.2.36)

ml 
eyml 


P

which 
automatically 
enforces 
the 
constraintm 
p(c 
= 
m)
..
= 
1. 

Naively 
it 
would 
appear 
that 
for 
C 
classes, 
the 
cost 
of 
the 
Laplace 
approximation 
scales 
as 
OC3N3. 
However, 
one 
may 
show 
by 
careful 


..

implementation 
that 
the 
cost 
may 
be 
reduced 
to 
only 
OCN3, 
analogous 
to 
the 
cost 
savings 
possible 
in 
the 
Gaussian 
Process 
classication 
model 
[297, 
230]. 


18.3 
Code 
demoBayesLinReg.m: 
Demo 
of 
Bayesian 
Linear 
Regression 
BayesLinReg.m: 
Bayesian 
Linear 
Regression 


demoBayesLogRegRVM.m: 
Demo 
of 
Bayesian 
Logistic 
Regression 
(RVM) 
BayesLogRegressionRVM.m: 
Bayesian 
Logistic 
Regression 
(RVM) 


avsigmaGauss.m: 
Approximation 
of 
the 
Gaussian 
average 
of 
a 
logistic 
sigmoid 
logsigapp.m: 
Approximation 
of 
the 
Logistic 
Sigmoid 
using 
mixture 
of 
erfs 


18.4 
Exercises 
Exercise 
185. 
The 
exercise 
concerns 
Bayesian 
regression. 


1. 
Show 
that 
for 
T

f 
= 
wx 
(18.4.1) 


and 
p(w) 
N 
(w 


0, 
), 
that 
p(fjx) 
is 
Gaussian 
distributed. 
Furthermore, 
nd 
the 
mean 
and 
covariance 
of 
this 
Gaussian. 


..



2. 
Consider 
a 
target 
point 
t 
= 
f 
+ 
, 
where 
E 
NE 
0;2. 
What 
is 
p(fjt, 
x)? 


Exercise 
186. 
A 
Bayesian 
Linear 
Parameter 
regression 
model 
is 
given 
by 


n 


y 
= 
wT(xn)+ 
n 
(18.4.2) 


..T

1N

In 
vector 
notation 
y 
=y;:::;ythis 
can 
be 
written 


y 
= 
w 
+ 
. 
(18.4.3) 




with 
T 
=(x1);:::, 
(xN 
)and 
. 
is 
a 
zero 
mean 
Gaussian 
distributed 
vector 
with 
covariance 
..1I. 
An 
expression 
for 
the 
marginal 
likelihood 
of 
a 
dataset 
is 
given 
in 
equation 
(18.1.19). 
A 
more 
compact 
expression 
can 
be 
obtained 
by 
considering 


11 
N 


p(y 
;:::;y 
N 
jx;:::, 
x, 
..) 
(18.4.4) 


n

Since 
yis 
linearly 
related 
to 
xn 
through 
w. 
Then 
y 
is 
Gaussian 
distributed 
with 
mean 


hy) 
= 
F 
hw) 
= 
0 
(18.4.5) 
and 
covariance 
matrix

DEDE

T

yy..hyihyiT 
=(w 
+ 
)(w 
+ 
)T(18.4.6) 


..



For 
p(w)= 
Nw 


0;..1I: 


DRAFT 
March 
9, 
2010 
345 



Exercises 


1. 
Show 
that 
the 
covariance 
matrix 
can 
be 
expressed 
as 
11

C 
= 
I 
+ 
T 
(18.4.7)

a 


2. 
Hence 
show 
that 
the 
log 
marginal 
likelihood 
can 
we 
written 
as 
11

11 
N 
TC..1

log 
p(y 
;:::;y 
N 
jx;:::, 
x, 
..) 
= 
- 
log 
det 
(2C) 
- 
yy 
(18.4.8)

22

Exercise 
187. 
Using 
exercise(186) 
as 
a 
basis, 
derive 
expression 
(18.1.39) 
for 
the 
log 
likelihood 
on 
a 
validation 
set. 


Exercise 
188. 
Consider 
the 
function 
E(w) 
as 
dened 
in 
equation 
(18.2.12). 


1. 
Compute 
the 
Hessian 
matrix 
which 
has 
elements, 
N

@2 
XHij 
= 
E(w)= 
ij 
+ 
n 
(1 
- 
n) 
n 
(n)T 
(18.4.9)

@wi@wj 


n=1 


2. 
Show 
that 
the 
Hessian 
is 
positive 
semidenite 
Exercise 
189. 
Show 
that 
for 
any 
function 
f(), 


T

f(xw)p(w)dw 
= 
f(h)p(h)dh 
(18.4.10) 


where 
p(h) 
is 
the 
distribution 
of 
the 
scalar 
xTw. 
The 
signicance 
of 
this 
result 
is 
that 
any 
high-dimensional 
integral 
of 
the 
above 
form 
can 
be 
reduced 
to 
a 
one-dimensional 
integral 
over 
the 
distribution 
of 
the 
`eld’ 
h[22]. 


Exercise 
190. 
This 
exercise 
concerns 
Bayesian 
Logistic 
Regression. 
Our 
interest 
to 
derive 
a 
formula 
for 
the 
optimal 
regularisation 
parameter 
a 
based 
on 
the 
Laplace 
approximation 
to 
the 
marginal 
log-likelihood 
given 
by 




X

a 
1 
B

log 
p(Dj) 
˜ 
L() 
- 
(w)Tw 
+log 
(w)T 
hn- 
log 
det 
(I 
+ 
J) 
+ 
log 
a 
(18.4.11)

2 
22 


n 


P..

The 
Laplace 
procedure 
nds 
rst 
an 
optimal 
w 
* 
that 
minimises 
wTw=2..log 
wThn, 
as 
in 
equation 


n 


(18.2.12) 
which 
will 
depend 
on 
the 
setting 
of 
. 
Formally, 
therefore, 
in 
nding 
the 
a 
that 
optimises 
L() 
we 
should 
make 
use 
of 
the 
total 
derivative 
formula 
X

dL 
@L 
@L 
@wi 


= 
+(18.4.12)

d@a 
@wi 
@a 


i 


@L 


However, 
when 
evaluated 
at 
w 
= 
w 
* 
, 
= 
0. 
This 
means 
that 
in 
order 
to 
compute 
the 
derivative 
with 


@w 


respect 
to 
, 
we 
only 
need 
consider 
the 
terms 
with 
an 
explicit 
a 
dependence. 
Equating 
the 
derivative 
to 
zero 
and 
using 


..

. 
log 
det 
(M) 
= 
traceM..1@M(18.4.13) 


show 
that 
the 
optimal 
a 
satises 
the 
xed 
point 
equation 


new 


= 
N 
(18.4.14) 
(w 
)Tw 
* 
+ 
trace(I 
+ 
J)..1

DRAFT 
March 
9, 
2010 



CHAPTER 
19 


Gaussian 
Processes 


19.1 
Non-Parametric 
Prediction 
Gaussian 
Processes 
are 
exible 
Bayesian 
models 
that 
t 
well 
within 
the 
probabilistic 
modelling 
framework. 
In 
developing 
GPs 
it 
is 
useful 
to 
rst 
step 
back 
and 
see 
what 
information 
we 
need 
to 
form 
a 
predictor. 
Given 
a 
set 
of 
training 
data 


n

D 
= 
f(x 
;y 
n);n 
=1;:::;N} 
= 
X 
[Y 
(19.1.1) 


nn

where 
xis 
the 
input 
for 
datapoint 
n 
and 
ythe 
corresponding 
output 
(a 
continuous 
variable 
in 
the 




regression 
case 
and 
a 
discrete 
variable 
in 
the 
classication 
case), 
our 
aim 
is 
to 
make 
a 
prediction 
y 
for 
a 




new 
input 
x 
. 
In 
the 
discriminative 
framework 
no 
model 
of 
the 
inputs 
x 
is 
assumed 
and 
only 
the 
outputs 
are 
modelled, 
conditioned 
on 
the 
inputs. 
Given 
a 
joint 
model 


1 
N 
1 
N 


p(y 
;:::;y 
;y 
* 
jx 
;:::;x 
;x 
)= 
p(Y;y 
* 
jX 
;x 
) 
(19.1.2) 




we 
may 
subsequently 
use 
conditioning 
to 
form 
a 
predictor 
p(y 
jx, 
D). 
In 
previous 
chapters 
we've 
made 
much 
use 
of 
the 
i.i.d. 
assumption 
that 
each 
datapoint 
is 
independently 
sampled 
from 
the 
same 
generating 
distribution. 
In 
this 
context, 
this 
might 
appear 
to 
suggest 
the 
assumption 


Y

1 
N 
1 
N 


p(y 
;:::;y 
;y 
* 
jx 
;:::;x 
;x 
)= 
p(y 
* 
jX 
;x 
)p(y 
njX 
;x 
) 
(19.1.3) 


n 


However, 
this 
is 
clearly 
of 
little 
use 
since 
the 
predictive 
conditional 
is 
simply 
p(y 
jD;x 
)= 
p(y 
jX 
;x 
) 
meaning 
the 
predictions 
make 
no 
use 
of 
the 
training 
outputs. 
For 
a 
non-trivial 
predictor 
we 
therefore 
need 
to 
specify 
a 
joint 
non-factorised 
distribution 
over 
outputs. 


19.1.1 
From 
parametric 
to 
non-parametric 
If 
we 
revisit 
our 
i.i.d. 
assumptions 
for 
parametric 
models, 
we 
used 
a 
parameter 
. 
to 
make 
a 
model 
of 
the 
input-output 
distribution 
p(yjx, 
). 
For 
a 
parametric 
model 
predictions 
are 
formed 
using 


ZZ



* 
* 
* 


p(y 
* 
jx, 
D) 
. 
p(y 
;x 
, 
D)=p(y, 
Y;x 
, 
X 
;) 
/p(y, 
Yj, 
x 
* 
x, 
X 
) 
(19.1.4)

, 
X 
)p(j

. 


Under 
the 
assumption 
that, 
given 
, 
the 
data 
is 
i.i.d., 
we 
obtain 


ZZ

Y

* 


p(y 
* 
jx, 
D) 
/p(y 
* 
jx 
* 
;)p()p(y 
nj, 
xn) 
/p(y 
* 
jx 
* 
;)p(jD) 
(19.1.5) 




n 


where 


Y

p(jD) 
. 
p()p(y 
nj, 
xn) 
(19.1.6) 


n 


347 



Non-Parametric 
Prediction 


Figure 
19.1: 
(a): 
A 
parametric 
model 
for 
predic


tion 
assuming 
i.i.d. 
data. 
(b): 
The 
form 
of 
the 



y1yNy
x1xNx
y1yNy
x1xNx
model 
after 
integrating 
out 
the 
parameters 
. 
Our 
non-parametric 
model 
will 
have 
this 
structure. 


(a) 
(b) 
After 
integrating 
over 
the 
parameters 
, 
the 
joint 
data 
distribution 
is 
given 
by 


Z

Y

* 


p(y, 
Yjx, 
X 
)=p(y 
* 
jx 
* 
;)p()p(y 
nj, 
xn) 
(19.1.7) 


. 


n 


which 
does 
not 
in 
general 
factorise 
into 
individual 
datapoint 
terms, 
see 
g(19.1). 
The 
idea 
of 
a 
non-
parametric 
approach 
is 
to 
specify 
the 
form 
of 
these 
dependencies 
without 
reference 
to 
an 
explicit 
parametric 
model. 
One 
route 
towards 
a 
non-parametric 
model 
is 
to 
start 
with 
a 
parametric 
model 
and 
integrate 
out 
the 
parameters. 
In 
order 
to 
make 
this 
tractable, 
we 
use 
a 
simple 
linear 
parameter 
predictor 
with 
a 
Gaussian 
parameter 
prior. 
For 
regression 
this 
leads 
to 
closed 
form 
expressions, 
although 
the 
classication 
case 
will 
require 
numerical 
approximation. 


19.1.2 
From 
Bayesian 
linear 
models 
to 
Gaussian 
processes 
To 
develop 
the 
GP, 
we 
briey 
revisit 
the 
Bayesian 
linear 
parameter 
model 
of 
section(18.1.1). 
For 
parameters 
w 
and 
basis 
functions 
i(x) 
the 
output 
is 
given 
by 
(assuming 
zero 
output 
noise) 


X

y 
=wii(x) 
(19.1.8) 
i 


1N

If 
we 
stack 
all 
the 
y;:::;yinto 
a 
vector 
y, 
then 


y 
=w 
(19.1.9) 


T

where 
F 
=(x1);:::, 
(xN 
)is 
the 
design 
matrix. 
Assuming 
a 
Gaussian 
weight 
prior 


p(w)= 
N 
(w 


0, 
w) 
(19.1.10) 


1N

and 
since 
y 
is 
linear 
in 
w, 
the 
joint 
output 
y;:::;yis 
Gaussian 
distributed. 
A 
Gaussian 
prior 
on 
w 
induces 
a 
Gaussian 
on 
the 
joint 
y 
with 
mean 


hy) 
=F 
hwip(w) 
= 
0 
(19.1.11) 
and 
covariance

DXXT

DEE

1

1 


yy


TT

=wwT 
= 
wT 
= 


2 


w 


S


2 


w 


(19.1.12) 
p(w) 


From 
this 
we 
see 
that 
the 
w 
can 
be 
absorbed 
into 
F 
using 
its 
Cholesky 
decomposition. 
In 
other 
words, 
without 
loss 
of 
generality 
we 
may 
assume 
w 
= 
I. 
After 
integrating 
out 
the 
weights, 
the 
Bayesian 
linear 


regression 
model 
induces 
a 
Gaussian 
distribution 
on 
any 
set 
of 
outputs 
y 
as 
p(yjx 
1 
, 
. 
. 
. 
, 
x 
N 
) 
= 
N 
(y 
0, 
K) 
(19.1.13) 
where 
the 
covariance 
matrix 
K 
depends 
on 
the 
training 
inputs 
alone 
via 
[K]n;n0= 
(x 
n)T(x 
n0), 
n, 
n' 
= 
1, 
. 
. 
. 
, 
N 
(19.1.14) 


Since 
the 
matrix 
K 
is 
formed 
as 
the 
scalar 
product 
of 
vectors, 
it 
is 
by 
construction 
positive 
semidenite, 
as 
we 
saw 
in 
section(17.3.2). 
After 
integrating 
out 
the 
weights, 
the 
only 
thing 
the 
model 
directly 
depends 


DRAFT 
March 
9, 
2010 



Non-Parametric 
Prediction 


on 
is 
the 
covariance 
matrix 
K. 
In 
a 
Gaussian 
process 
we 
directly 
specify 
the 
joint 
output 
covariance 
K 
as 
a 
function 
of 
two 
inputs. 
Specically 
we 
need 
to 
dene 
the 
n, 
n/ 
element 
of 
the 
covariance 
matrix 
for 
any 


00

nnnn

two 
inputs 
xand 
x. 
This 
is 
achieved 
using 
a 
covariance 
function 
k(x;x) 


nn

[K]' 
= 
k(x 
;x 
' 
) 
(19.1.15)

n;n

0

nn

The 
required 
form 
of 
the 
function 
k(x;x) 
is 
very 
special 
– 
when 
applied 
to 
create 
the 
elements 
of 
the 
matrix 
K 
it 
must 
produce 
a 
positive 
denite 
matrix. 
We 
discuss 
how 
to 
create 
such 
covariance 
functions 
in 
section(19.3). 
One 
explicit 
straightforward 
construction 
is 
to 
form 
the 
covariance 
function 
from 
the 


0

n

scalar 
product 
of 
the 
basis 
vector 
(xn) 
and 
(x). 
For 
nite-dimensional 
f 
this 
is 
known 
as 
a 
nite 
dimensional 
Gaussian 
Process. 
Given 
any 
covariance 
function 
we 
can 
always 
nd 
a 
corresponding 
basis 
vector 
representation 
– 
that 
is, 
for 
any 
GP, 
we 
can 
always 
relate 
this 
back 
to 
a 
parametric 
Bayesian 
LPM. 
However, 
for 
many 
commonly 
used 
covariance 
functions, 
the 
basis 
functions 
corresponds 
to 
innite 
dimensional 
vectors. 
It 
is 
in 
such 
cases 
that 
the 
advantages 
of 
using 
the 
GP 
framework 
are 
particularly 
evident 
since 
we 
would 
not 
be 
able 
to 
compute 
eciently 
with 
the 
corresponding 
innite 
dimensional 
parametric 
model. 


19.1.3 
A 
prior 
on 
functions 
The 
nature 
of 
many 
machine 
learning 
applications 
is 
such 
that 
the 
knowledge 
about 
the 
true 
underlying 
mechanism 
behind 
the 
data 
generation 
process 
is 
limited. 
Instead 
one 
relies 
on 
generic 
`smoothness’ 
as


0

sumptions 
of 
the 
form 
that 
for 
two 
inputs 
x 
and 
x/ 
that 
are 
close, 
the 
corresponding 
outputs 
y 
and 
y
should 
be 
similar. 
Many 
generic 
techniques 
in 
machine 
learning 
can 
be 
viewed 
as 
dierent 
characterisations 
of 
smoothness. 
An 
advantage 
of 
the 
GP 
framework 
in 
this 
respect 
is 
that 
the 
mathematical 
smoothness 
properties 
of 
the 
functions 
are 
well 
understood, 
giving 
condence 
in 
the 
procedure. 


For 
a 
given 
covariance 
matrix 
K, 
equation 
(19.1.13) 
species 
a 
distribution 
on 
functions1 
in 
the 
following 


1

sense: 
we 
specify 
a 
set 
of 
input 
points 
x 
=(x;:::;xN 
) 
and 
a 
N 
× 
N 
covariance 
matrix 
K. 
Then 
we 
draw 
a 
vector 
y 
from 
the 
Gaussian 
dened 
by 
equation 
(19.1.13). 
We 
can 
then 
plot 
the 
sampled 
`function’ 
at 


n

the 
nite 
set 
of 
points 
(x;yn);n 
=1;:::;N. 
What 
kind 
of 
functions 
does 
a 
GP 
correspond 
to? 
Consider 
two 
scalar 
inputs, 
xi 
and 
xj 
separated 
by 
a 
distance 
jxi 
- 
xjj. 
The 
corresponding 
sampled 
outputs 
yi 
and 
yj 
uctuate 
as 
dierent 
functions 
are 
drawn. 
For 
a 
covariance 
function 
that 
has 
a 
high 
value 
for 
jxi 
- 
xjjsmall, 
we 
expect 
yi 
and 
yj 
to 
be 
very 
similar 
since 
they 
are 
highly 
correlated. 
Conversely, 
for 
a 
covariance 


i

function 
that 
has 
low 
value 
for 
a 
given 
small 
separation 
jxi 
- 
xjj, 
we 
expect 
yand 
yj 
to 
be 
eectively 
independent2 
. 
In 
general, 
we 
would 
expect 
the 
correlation 
between 
yi 
and 
yj 
to 
decrease 
the 
further 
apart 
xi 
and 
xj 
are. 


In 
g(19.2a) 
we 
show 
three 
sample 
functions 
drawn 
from 
a 
Squared 
Exponential 
covariance 
function 
de
ned 
over 
500 
points 
uniformly 
spaced 
from 
..2 
to 
3. 
Each 
sampled 
function 
looks 
reasonably 
smooth. 
Conversely, 
for 
the 
Ornstein 
Uhlenbeck 
covariance 
function, 
the 
sampled 
functions 
g(19.2c) 
look 
locally 
rough. 
These 
smoothness 
properties 
are 
related 
to 
the 
form 
of 
the 
covariance 
function, 
as 
discussed 
in 
section(19.4.1). 


The 
zero 
mean 
assumption 
implies 
that 
if 
we 
were 
to 
draw 
a 
large 
number 
of 
such 
`functions', 
the 
mean 
across 
these 
functions 
at 
a 
given 
point 
x 
tends 
to 
zero. 
Similarly, 
for 
any 
two 
points 
x 
and 
x/ 
if 
we 
compute 
the 
sample 
covariance 
between 
the 
corresponding 
y 
and 
y/ 
for 
all 
such 
sampled 
functions, 
this 
will 
tend 
to 
the 
covariance 
function 
value 
k(x, 
x0). 
The 
zero-mean 
assumption 
can 
be 
easily 
relaxed 
by 
dening 
a 
mean 
function 
m(x) 
to 
give 
p(yjx)= 
N 
(y 


m, 
K). 
In 
many 
practical 
situations 
one 
typically 
deals 
with 
`detrended’ 
data 
in 
which 
such 
mean 
trends 
have 
been 
already 
removed. 
For 
this 
reason 
much 
of 
the 
development 
of 
GPs 
in 
the 
machine 
learning 
literature 
is 
for 
the 
zero 
mean 
case. 


1The 
term 
`function’ 
is 
potentially 
confusing 
since 
we 
do 
not 
have 
an 
explicit 
functional 
form 
for 
the 
input 
output-mapping. 


1 
N 
1 
N

For 
any 
nite 
set 
of 
inputs 
x 
;:::;x 
the 
values 
for 
the 
`function’ 
are 
given 
by 
the 
outputs 
at 
those 
points 
y 
;:::;y 
. 


2For 
periodic 
functions, 
however, 
we 
would 
expect 
high 
correlation 
at 
separating 
distances 
corresponding 
to 
the 
period 
of 
the 
function. 


DRAFT 
March 
9, 
2010 
349 



Gaussian 
Process 
Prediction 


19.2 
Gaussian 
Process 
Prediction 


For 
a 
dataset 
D 
and 
novel 
input 
x, 
a 
zero 
mean 
GP 
makes 
a 
Gaussian 
model 
of 
the 
joint 
outputs 


1N 
* 
1N 
* 


y;:::;y;ygiven 
the 
joint 
inputs 
x;:::;x;x. 
For 
convenience 
we 
write 
this 
as 


 


u


* 


0N+1;K+ 


p(y;y 


y;y 


(19.2.1)
* 
jx;x)= 
N
where 
0N+1 
is 
a 
N+1 
dimensional 
zero-vector. 
The 
covariance 
matrix 
K+ 
is 
a 
block 
matrix 
with 
elements 


3

. 


K+ 
= 


6666666666666666666
. 




Kx;x 
Kx;x 




Kx;x 
Kx 
* 
;x 


	



7777777777777777777
. 


1N

where 
Kx;x 
is 
the 
covariance 
matrix 
of 
the 
training 
inputs 
x 
=x;:::;x– 
that 
is 


0

nn

[Kx;x]' 
= 
k(x;x);n;n' 
=1;:::;N(19.2.2)

n;n
The 
N× 
1 
vector 
Kx;x 
* 
has 
elements 
n

[Kx;x 
* 
]= 
k(x;x) 
n=1;:::;N(19.2.3)

n;* 
Kx 
* 
;x 
is 
the 
transpose 
of 
the 
above 
vector. 
The 
scalar 
covariance 
is 
given 
by 




Kx 
* 
;x 
* 
= 
k(x;x) 
(19.2.4) 


The 
predictive 
distribution 
p(yjy;D) 
is 
obtained 
by 
Gaussian 
conditioning 
using 
the 
results 
in 
denition(78), 
giving 
a 
Gaussian 
distribution 


..

* 


K..1 
K..1 


* 


p(y* 
jx;D)= 
Ny

Kx;xx;xy;Kx 
;x 
- 
Kx;xx;x

Kx;x 


* 


u 


(19.2.5) 
For 
xed 
hyperparameters, 
GP 
regression 
is 
an 
exact 
method 
and 
there 
are 
no 
issues 
with 
local 
minima. 




..

Furthermore, 
GPs 
are 
attractive 
since 
they 
automatically 
model 
uncertainty 
in 
the 
predictions. 
However, 
the 
computational 
complexity 
for 
making 
a 
prediction 
is 
ON3 
due 
to 
the 
requirement 
of 
performing 
the 
matrix 
inversion 
(or 
solving 
the 
corresponding 
linear 
system 
by 
Gaussian 
elimination). 
This 
can 
be 
prohibitively 
expensive 
for 
large 
datasets 
and 
a 
large 
body 
of 
research 
on 
ecient 
approximations 
exists. 
A 
discussion 
of 
these 
techniques 
is 
beyond 
the 
scope 
of 
this 
book, 
and 
the 
reader 
is 
referred 
to 
[230]. 


19.2.1 
Regression 
with 
noisy 
training 
outputs 
n

To 
prevent 
overtting 
to 
noisy 
data 
it 
is 
useful 
to 
assume 
that 
a 
training 
output 
yis 
the 
result 
of 
some 
clean 
process 
fn 
corrupted 
by 
additive 
Gaussian 
noise, 


 


u 


y 


n 


= 
fn 
+n 


, 


where


n

n 
N

0;2

(19.2.6) 


In 
this 
case 
our 
interest 
is 
to 
predict 
the 
clean 
signal 
f* 
for 
a 
novel 
input 
x. 
Then 
the 
distribution 


) 
is 
a 
zero 
mean 
Gaussian 
with 
block 
covariance 
matrix 


p(y;fjx;x
Kx;x 
+ 
2IKx;x 
* 


(19.2.7)
Kx 
* 
Kx 
* 


;x 
;x 
so 
that 
Kx;x 
is 
replaced 
by 
Kx;x 
+ 
2I 
in 
forming 
the 
prediction, 
equation 
(19.2.5). 


DRAFT 
March 
9, 
2010 



Covariance 
Functions 


Example 
84. 
Training 
data 
from 
a 
one-dimensional 
input 
x 
and 
one 
dimensional 
output 
y 
are 
plotted 
in 
g(19.2b,d), 
along 
with 
the 
mean 
regression 
function 
t, 
based 
on 
two 
dierent 
covariance 
functions. 
Note 
how 
the 
smoothness 
of 
the 
prior 
translates 
into 
smoothness 
of 
the 
prediction. 
The 
smoothness 
of 
the 
function 
space 
prior 
is 
a 
consequence 
of 
the 
choice 
of 
covariance 
function. 
Naively, 
we 
can 
partially 
understand 
this 
by 
the 
behaviour 
of 
the 
covariance 
function 
at 
the 
origin, 
section(19.4.1). 
See 
demoGPreg.m 


The 
marginal 
likelihood 
and 
hyperparameter 
learning 


For 
aset 
of 
N 
one-dimensional 
training 
inputs 
represented 
by 
the 
N 
× 
1 
dimensional 
vector 
y 
and 
a 


1

covariance 
matrix 
K 
dened 
on 
the 
inputs 
x;:::;xN 
, 
the 
log 
marginal 
likelihood 
is 


11

log 
p(yjx)= 
- 
yTK..1y 
- 
log 
det 
(2K) 
(19.2.8)

22 


One 
can 
learn 
any 
free 
parameters 
of 
the 
covariance 
function 
by 
maximising 
the 
marginal 
likelihood. 
For 
example, 
a 
squared 
exponential 
covariance 
function 
may 
have 
parameters 
, 
v0: 




1 
..02

k(x, 
x 
0)= 
v0 
exp- 
x 
- 
x 
(19.2.9)

2

The 
. 
parameter 
in 
equation 
(19.2.12) 
species 
the 
appropriate 
length-scale 
of 
the 
inputs, 
and 
v0 
the 
variance 
of 
the 
function. 
The 
dependence 
of 
the 
marginal 
likelihood 
(19.2.8) 
on 
the 
parameters 
is 
typically 
complex 
and 
no 
closed 
form 
expression 
for 
the 
Maximum 
Likelihood 
optimum 
exists; 
in 
this 
case 
on 
resorts 
to 
numerical 
optimisation 
techniques 
such 
as 
conjugate 
gradients. 


Vector 
inputs 


For 
regression 
with 
vector 
inputs 
and 
scalar 
outputs 
we 
need 
to 
dene 
a 
covariance 
as 
a 
function 
of 
the 
two 
vectors, 
k(x, 
x0). 
Using 
the 
multiplicative 
property 
of 
covariance 
functions, 
denition(93), 
a 
simple 
way 
to 
do 
this 
is 
to 
dene 


Y

0

k(x, 
x0)=k(xi;x 
) 
(19.2.10)

i
i 


For 
example, 
for 
the 
squared 
exponential 
covariance 
function 
this 
gives 


..(x..x0)2 


k(x, 
x0)= 
e 
(19.2.11) 


though 
`correlated’ 
forms 
are 
possible 
as 
well, 
see 
exercise(195). 
We 
can 
generalise 
the 
above 
using 
parameters: 


D

X..

1 
02

k(x, 
x0)= 
v0 
exp 
- 
lxl 
- 
xl(19.2.12)

2 


l=1 


where 
xl 
is 
the 
lth 
component 
of 
x 
and 
. 
=(v0;1;:::;D) 
are 
parameters. 
The 
l 
in 
equation 
(19.2.12) 
allow 
a 
dierent 
length 
scale 
on 
each 
input 
dimension 
and 
can 
be 
learned 
by 
numerically 
maximising 
the 
marginal 
likelihood. 
For 
irrelevant 
inputs, 
the 
corresponding 
l 
will 
become 
small, 
and 
the 
model 
will 
ignore 
the 
lth 
input 
dimension. 
This 
is 
closely 
related 
to 
Automatic 
Relevance 
Determination 
[181]. 


19.3 
Covariance 
Functions 
Covariance 
functions 
k(x, 
x0) 
are 
special 
in 
that 
they 
dene 
elements 
of 
a 
positive 
denite 
matrix. 
These 
functions 
are 
also 
referred 
to 
as 
`kernels', 
particulary 
in 
the 
machine 
learning 
literature. 


DRAFT 
March 
9, 
2010 
351 



Covariance 
Functions 


-2-1.5-1-0.500.511.522.53-3-2.5-2-1.5-1-0.500.511.52
-2-1.5-1-0.500.511.522.53-1.5-1-0.500.511.52
(a) 
(b) 
-2-1.5-1-0.500.511.522.53-2.5-2-1.5-1-0.500.511.522.5
-2-1.5-1-0.500.511.522.53-1.5-1-0.500.511.5
(c) 
(d) 
11000

Figure 
19.2: 
The 
input 
space 
from 
-2 
to 
3 
is 
split 
evenly 
into 
1000 
points 
x;:::;x. 
(a): 
Three 
samples 
from 
a 
GP 
prior 
with 
Squared 
Exponential 
(SE) 
covariance 
function, 
. 
= 
2. 
The 
1000 
× 
1000 
covariance 
matrix 
K 
is 
dened 
using 
the 
SE 
kernel, 
from 
which 
the 
samples 
are 
drawn 
using 
mvrandn(zeros(1000,1),K,3). 
(b): 
Prediction 
based 
on 
training 
points. 
Plotted 
is 
the 
posterior 
predicted 
function 
based 
on 
the 
SE 
covariance. 
The 
central 
line 
is 
the 
mean 
prediction, 
with 
standard 
errors 
bars 
on 
either 
side. 
The 
log 
marginal 
likelihood 
is 
˜ 
70. 
(c): 
Three 
samples 
from 
the 
Ornstein-
Uhlenbeck 
GP 
prior 
with 
. 
= 
2. 
(d): 
Posterior 
prediction 
for 
the 
OU 
covariance. 
The 
log 
marginal 
likelihood 
is 
˜ 
3, 
meaning 
that 
the 
SE 
covariance 
is 
much 
more 
heavily 
supported 
by 
the 
data 
than 
the 
rougher 
OU 
covariance. 


1

Denition 
91 
(Covariance 
function). 
Given 
any 
collection 
of 
points 
x;:::;xM 
, 
a 
covariance 
function 


i

k(x;xj) 
denes 
the 
elements 
of 
a 
M 
× 
M 
matrix 


i

[C]i;j 
= 
k(x 
;xj) 


such 
that 
C 
is 
positive 
semidenite. 


19.3.1 
Making 
new 
covariance 
functions 
from 
old 
The 
following 
rules 
(which 
can 
all 
be 
proved 
directly) 
generate 
new 
covariance 
functions 
from 
existing 
covariance 
functions 
k1, 
k2 
[182],[230]. 


Denition 
92 
(Sum). 
k(x, 
x0)= 
k1(x, 
x0)+ 
k2(x, 
x0) 
(19.3.1) 


DRAFT 
March 
9, 
2010 



Covariance 
Functions 


Denition 
93 
(Product). 
k(x, 
x0)= 
k1(x, 
x0)k2(x, 
x0) 
(19.3.2) 


x

Denition 
94 
(Product 
Spaces). 
For 
z 
=,

y 
k(z, 
z0)= 
k1(x, 
x0)+ 
k2(y, 
y0) 
(19.3.3) 
and 
k(z, 
z0)= 
k1(x, 
x0)k2(y, 
y0) 
(19.3.4) 


Denition 
95 
(Vertical 
Rescaling). 


k(x, 
x0)= 
a(x)k1(x, 
x0)a(x0) 
(19.3.5) 
for 
any 
function 
a(x). 


Denition 
96 
(Warping 
and 
Embedding). 


..

k(x, 
x0)= 
k1u(x), 
u(x0)(19.3.6) 
for 
any 
mapping 
x 
. 
u(x), 
where 
the 
mapping 
u(x) 
has 
arbitrary 
dimension. 


A 
small 
collection 
of 
covariance 
functions 
commonly 
used 
in 
machine 
learning 
is 
given 
below. 
We 
refer 
the 
reader 
to 
[230] 
and 
[107] 
for 
further 
popular 
covariance 
functions. 


19.3.2 
Stationary 
covariance 
functions 
Denition 
97 
(Stationary 
Kernel). 
A 
kernel 
k(x, 
x0) 
is 
stationary 
if 
the 
kernel 
depends 
only 
on 
the 
separation 
x 
- 
x0. 
That 
is 


k(x, 
x0)= 
k(x 
- 
x0) 
(19.3.7) 


Following 
the 
notation 
in 
[230], 
for 
a 
stationary 
covariance 
function 
we 
may 
write 


k(d) 
(19.3.8) 
where 
d 
= 
x 
- 
x0. 
This 
means 
that 
for 
functions 
drawn 
from 
the 
GP, 
on 
average, 
the 
functions 
depend 
only 
on 
the 
distance 
between 
inputs 
and 
not 
on 
the 
absolute 
position 
of 
an 
input. 
In 
other 
words, 
the 
functions 
are 
on 
average 
translation 
invariant. 
For 
isotropic 
covariance 
functions, 
the 
covariance 
is 
dened 
as 
a 
function 
of 
the 
distance 
k(jdj). 


DRAFT 
March 
9, 
2010 



Covariance 
Functions 


Denition 
98 
(Squared 
Exponential). 


..jdj2 


k(d)= 
e(19.3.9) 


The 
Squared 
Exponential 
is 
one 
of 
the 
most 
common 
covariance 
functions. 
There 
are 
many 
ways 
to 
show 
that 
this 
is 
a 
covariance 
function. 
An 
elementary 
technique 
is 
to 
consider 


T

0' 


nn 
00

- 
2
1 
xn..xxn..x- 
1 
jxnj2 
- 
1 
jxn 
j2 
(xn)T 
xn 


e 
= 
e 
2 
e 
2 
e(19.3.10) 


' 
00

The 
rst 
two 
factors 
form 
a 
kernel 
of 
the 
form 
(xn)(xn). 
In 
the 
nal 
term 
k1(xn 
, 
xn)=(xn)T 
xnis 
the 
linear 
kernel. 
Taking 
the 
exponential 
and 
writing 
the 
power 
series 
expansion 
of 
the 
exponential, 
we 
have 


' 
X

nn 
11 
0

k1(x;x) 
ki 
, 
xn

e 
=(xn 
) 
(19.3.11)

i!1

i=1 


this 
can 
be 
expressed 
as 
a 
series 
of 
integer 
powers 
of 
k1, 
with 
positive 
coecients. 
By 
the 
product 
(with 
itself) 
and 
sum 
rules 
above, 
this 
is 
therefore 
a 
kernel 
as 
well. 
We 
then 
use 
the 
fact 
that 
equation 
(19.3.10) 
is 
the 
product 
of 
two 
kernels, 
and 
hence 
also 
a 
kernel. 


Denition 
99 
(-Exponential). 


..jdj. 


k(d)= 
e, 
0 
<. 
= 
2 
(19.3.12) 


When 
. 
= 
2 
we 
have 
the 
squared 
exponential 
covariance 
function. 
When 
. 
= 
1 
this 
is 
the 
Ornstein-
Uhlenbeck 
covariance 
function. 


Denition 
100 
(Matern). 


k(d)= 
jdjK. 
(jdj) 
(19.3.13) 


where 
K. 
is 
a 
modied 
Bessel 
function, 
> 
0. 


Denition 
101 
(Rational 
Quadratic). 


....a 


k(d)=1+ 
jdj2, 
> 
0 
(19.3.14) 


Denition 
102 
(Periodic). 
For 
1-dimensional 
x 
and 
x0, 
a 
stationary 
(and 
isotropic) 
covariance 
function 
can 
be 
obtained 
by 
rst 
mapping 
x 
to 
the 
two 
dimensional 
vector 
u(x) 
= 
(cos(x), 
sin(x)) 
and 
then 
using 


..(u(x)..u(x0))2

the 
SE 
covariance 
e

... 
sin2(!(x..x0))

k(x 
- 
x0)= 
e 
;> 
0 
(19.3.15) 
See 
[182] 
and 
[230]. 


DRAFT 
March 
9, 
2010 



Covariance 
Functions 


00.511.522.5300.20.40.60.81 
21.51.00.5
00.020.040.060.080.10.120.140.160.180.20.650.70.750.80.850.90.951 
21.51.00.5
(a) 
(b) 
..jxj

Figure 
19.3: 
(a): 
Plots 
of 
the 
Gamma-Exponential 
covariance 
eversus 
x. 
The 
case 
. 
= 
2 
corresponds 
to 
the 
SE 
covariance 
function. 
The 
drop 
in 
the 
covariance 
is 
much 
more 
rapid 
as 
a 
function 
of 
the 
separation 
x 
for 
small 
, 
suggesting 
that 
the 
functions 
corresponding 
to 
smaller 
. 
will 
be 
locally 
rough 
(though 
possess 
relatively 
higher 
long 
range 
correlation). 
(b): 
As 
for 
(a) 
but 
zoomed 
in 
towards 
the 
origin. 
For 
the 
SE 
case, 
. 
= 
2, 
the 
derivative 
of 
the 
covariance 
function 
is 
zero, 
whereas 
the 
OU 
covariance 
. 
= 
1 
has 
a 
rst 
order 
contribution 
to 
the 
drop 
in 
the 
covariance, 
suggesting 
that 
locally 
OU 
sampled 
functions 
will 
be 
much 
rougher 
than 
SE 
functions. 


19.3.3 
Non-stationary 
covariance 
functions 
Denition 
103 
(Linear). 


T0

k(x, 
x0)= 
xx(19.3.16) 


Denition 
104 
(Neural 
Network). 


 !

2xTx/ 
k(x, 
x0) 
= 
arcsinv 
p(19.3.17) 
1+2xTx1+2x0Tx0

The 
functions 
dened 
by 
this 
covariance 
always 
go 
through 
the 
origin. 
To 
shift 
this, 
one 
may 
use 
the 
embedding 
x 
. 
(1, 
x) 
where 
the 
1 
has 
the 
eect 
of 
a 
`bias’ 
from 
the 
origin. 
To 
change 
the 
scale 
of 
the 
bias 
and 
and 
non-bias 
contributions 
one 
may 
use 
additional 
parameters 
x 
. 
(b, 
x). 
The 
NN 
covariance 
function 
can 
be 
derived 
as 
a 
limiting 
case 
of 
a 
neural 
network 
with 
innite 
hidden 
units[296], 
and 
making 
use 
of 
exact 
integral 
results 
in 
[21]. 


Denition 
105 
(Gibbs). 


(xi..x
2

1 
0

Yri(x)ri(x0) 
2 
- 
2 
i
2 
)
0)

r(x)+r(x

k(x, 
x0)=e 
ii 
(19.3.18)

22

r(x)+ 
r(x0)

ii

i 


for 
functions 
ri(x) 
> 
0. 


DRAFT 
March 
9, 
2010 



Analysis 
of 
Covariance 
Functions 


-20-15-10-505101520-2-1.5-1-0.500.511.5
-20-15-10-505101520-3-2-101234
(a) 
(b) 
Figure 
19.4: 
Samples 
from 
a 
GP 
prior 
for 
500 
x 
points 
uniformly 
placed 
from 
-20 
to 
20. 
(a): 
Sampled 


..

from 
the 
periodic 
covariance 
function 
exp..2 
sin2 
0:5(x 
- 
x0). 
(b): 
Neural 
Network 
covariance 
function 
with 
bias 
b 
= 
5 
and 
. 
= 
1. 


19.4 
Analysis 
of 
Covariance 
Functions 
19.4.1 
Smoothness 
of 
the 
functions 
We 
examine 
local 
smoothness 
for 
a 
translation 
invariant 
kernel 
k(x, 
x0)= 
k(x 
- 
x0). 
For 
two 
one0


dimensional 
points 
x 
and 
x0, 
separated 
by 
a 
small 
amount 
d 
« 
1, 
x= 
x 
+ 
, 
the 
covariance 
between 
the 
outputs 
y 
and 
y' 
is, 
by 
Taylor 
expansion, 


dk 
..

2

k(x, 
x 
0) 
˜ 
k(0) 
+ 
d 
jx=0 
+ 
O(19.4.1)

dx

so 
that 
the 
change 
in 
the 
covariance 
at 
the 
local 
level 
is 
dominated 
by 
the 
rst 
derivative 
of 
the 
covariance 


..x

function. 
For 
the 
SE 
covariance 
k(x)= 
e
2 
, 


..x

dk 
= 
..2xe 
2 
(19.4.2)

dx 
is 
zero 
at 
x 
= 
0. 
This 
means 
that 
for 
the 
SE 
covariance 
function, 
the 
rst 
order 
change 
in 
the 
covariance 
is 
zero, 
and 
only 
higher 
order 
2 
terms 
contribute. 


For 
the 
Ornstein-Uhlenbeck 
covariance, 
k(x)= 
e..jxj, 
the 
right 
derivative 
at 
the 
origin 
is 


k() 
- 
k(0) 
e..d 
- 
1

lim 
=lim 
= 
..1 
(19.4.3) 


!0 
d 
!0 
d 


where 
this 
result 
is 
obtained 
using 
L'H^opital's 
rule. 
Hence 
for 
the 
OU 
covariance 
function, 
there 
is 
a 
rst 
order 
negative 
change 
in 
the 
covariance; 
at 
the 
local 
level, 
this 
decrease 
in 
the 
covariance 
is 
therefore 
much 
more 
rapid 
than 
for 
the 
SE 
covariance, 
see 
g(19.3). 
Since 
low 
covariance 
implies 
low 
dependence 
(in 
Gaussian 
distributions), 
locally 
the 
functions 
generated 
from 
the 
OU 
process 
are 
rough, 
whereas 
they 
are 
smooth 
in 
the 
SE 
case. 
A 
more 
formal 
treatment 
for 
the 
stationary 
case 
can 
be 
obtained 
by 
examining 
the 
eigenvalue-frequency 
plot 
of 
the 
covariance 
function 
(spectral 
density), 
section(19.4.3). 
For 
rough 
functions 
the 
density 
of 
eigenvalues 
for 
high 
frequency 
components 
is 
higher 
than 
for 
smooth 
functions. 


19.4.2 
Mercer 
kernels 
Consider 
the 
function 


B

B 


k(x, 
x0)= 
(x)T(x0)= 
s(x)s(x0) 
(19.4.4) 


s=1 


where 
(x) 
is 
a 
vector 
with 
component 
functions 
1(x);2(x);:::;B(x). 
Then 
for 
a 
set 
of 
points 
1

x;:::;xP 
, 
we 
construct 
the 
matrix 
K 
with 
elements 


B

B 


i

[K]ij 
= 
k(x 
;xj)= 
s(x 
i)s(xj) 
(19.4.5) 
s=1 


DRAFT 
March 
9, 
2010 



Analysis 
of 
Covariance 
Functions 


We 
claim 
that 
the 
matrix 
K 
so 
constructed 
is 
positive 
semidenite 
and 
hence 
a 
valid 
covariance 
matrix. 
Recalling 
that 
a 
matrix 
is 
positive 
semidenite 
if 
for 
any 
non 
zero 
vector 
z, 
zTKz 
= 
0. 
Using 
the 
denition 
of 
K 
above 
we 
have 


23

"#

PBPPB

XXXXX

zTKz 
= 
ziKijzj 
= 
zis(x 
i)4Xs(xj)zj5= 
2 
= 
0 
(19.4.6)

s 
i;j=1 
s=1i=1 
j=1 
s=1


|{z}

|{z}

s 


s 


Hence 
any 
function 
of 
the 
form 
equation 
(19.4.4) 
is 
a 
covariance 
function. 
We 
can 
generalise 
the 
Mercer 
kernel 
to 
complex 
functions 
(x) 
using 


k(x, 
x0)= 
(x)Ty(x0) 
(19.4.7) 


i

where 
† 
represents 
the 
complex 
conjugate. 
Then 
the 
matrix 
K 
formed 
from 
inputs 
x, 
i 
=1;:::;P 
is 
positive 
semidenite 
since 
for 
any 
real 
vector 
z, 


23

"#

BPPB

XXXX

zTKz 
= 
zis(x 
i)4Xy(xj)zj5= 
jsj2 
= 
0 
(19.4.8)

s
s=1i=1 
j=1 
s=1


|{z}

|{z}

s 
y

s 


y

where 
we 
made 
use 
of 
the 
general 
result 
for 
a 
complex 
variable 
xx= 
jxj2 
. 
A 
further 
generalisation 
is 
to 
write 


Z

0

k(x, 
x0)=f(s)(x, 
s)y(x;s)ds 
(19.4.9) 


for 
f(s) 
= 
0, 
and 
scalar 
complex 
functions 
(x, 
s). 
Then 
replacing 
summations 
with 
integration 
(and 
assuming 
we 
can 
interchange 
the 
sum 
over 
the 
components 
of 
z 
with 
the 
integral 
over 
s), 
we 
obtain 


23

"#

ZPPZ

XX

ij

45

zTKz 
=f(s)zi(x 
;s)y(x;s)zj 
ds 
=f(s)j(s)j2ds 
= 
0 
(19.4.10) 


i=1 
j=1

|{z}

|{z}

(s) 


y(s) 


Spectral 
decomposition 


Equation(19.4.9) 
is 
a 
generalisation 
of 
the 
spectral 
decomposition 
of 
a 
kernel 
k(x, 
x0) 
since 
if 
we 
write 
f(s) 
as 
a 
sum 
of 
Dirac 
delta 
functions, 


1

X

f(s)= 
k(s 
- 
k) 
(19.4.11) 


k=1 


and 
using 
(x, 
k)= 
 k(x), 
for 
an 
eigenfunction 
 k(x) 
indexed 
by 
k 
with 
eigenvalue 
k, 
we 
obtain 
the 
spectral 
decomposition 


1

X

k(x, 
x0)= 
k k(x) k(x0) 
(19.4.12) 


k=1 
If 
all 
the 
eigenvalues 
of 
a 
kernel 
are 
non-negative, 
the 
kernel 
is 
a 
covariance 
function. 


Consider 
for 
example 
the 
following 
function 


..(x..x0)2 


k(x, 
x0)= 
e 
(19.4.13) 


We 
claim 
that 
this 
is 
a 
covariance 
function. 
This 
is 
indeed 
a 
valid 
covariance 
function 
in 
the 
sense 
that 


1dd0

for 
any 
set 
of 
points 
x;:::;xd, 
the 
d 
× 
d 
matrix 
formed 
with 
elements 
k(x;x) 
is 
positive 
denite, 
as 
discussed 
after 
denition(98). 
The 
solution 
given 
to 
exercise(193) 
shows 
that 
there 
do 
indeed 
exist 
real-valued 
vectors 
such 
that 
one 
can 
represent 


k(x, 
x0)= 
(x)T(x0) 
(19.4.14) 


where 
the 
vectors 
are 
innite 
dimensional. 
This 
demonstrates 
the 
generalisation 
of 
the 
nite-dimensional 
`weight 
space’ 
viewpoint 
of 
a 
GP 
to 
the 
potentially 
implicit 
innite 
dimensional 
representation. 


DRAFT 
March 
9, 
2010 



Gaussian 
Processes 
for 
Classication 


c1cNc
y1yNy
x1xNx
Figure 
19.5: 
GP 
classication. 
The 
GP 
induces 
a 
Gaussian 
distribution 
on 


N 
1N

the 
latent 
activations 
y1;:::;y;y 
, 
given 
the 
observed 
values 
of 
c;:::;c. 




The 
classication 
of 
the 
new 
input 
x 
is 
then 
given 
via 
the 
correlation 




induced 
by 
the 
training 
points 
on 
the 
latent 
activation 
y 
. 


19.4.3 
Fourier 
analysis 
for 
stationary 
kernels 
For 
a 
function 
g(x) 
with 
Fourier 
transform 
~g(s), 
we 
may 
use 
the 
inverse 
Fourier 
transform 
to 
write 


Z

1 


g(x)= 
g~(s)e 
..ixsds 
(19.4.15)

2p 


v 


where 
i 
..1. 
For 
a 
stationary 
kernel 
k(x) 
with 
Fourier 
transform 
k~(s), 
we 
can 
therefore 
write 


ZZ

..i(x..x..ixs 
ix

k(x 
- 
x 
0)= 
1 
k~(s)e 
0)sds 
=
1 
k~(s)ee 
0sds 
(19.4.16)

2p 
2p 


which 
is 
of 
the 
same 
form 
as 
equation 
(19.4.9) 
where 
the 
Fourier 
transform 
k~(s) 
is 
identied 
with 
f(s) 


..isx

and 
(x, 
s)= 
e. 
Hence, 
provided 
the 
Fourier 
transform 
k~(s) 
is 
positive, 
the 
translation 
invariant 
kernel 
k(x 
..x0) 
is 
a 
covariance 
function. 
Bochner's 
Theorem[230] 
asserts 
the 
converse 
that 
any 
translation 
invariant 
covariance 
function 
must 
have 
such 
a 
Fourier 
representation. 


Application 
to 
the 
squared 
exponential 
kernel 


- 
12

x

For 
the 
translation 
invariant 
squared 
exponential 
kernel, 
k(x)= 
e 
2 
, 
its 
Fourier 
transform 
is 


ZZ

11

2 
v 
2 


- 
1 
- 
s 
- 
1 
(x+is)2 


k~(s)= 
e 
2 
x2+isxdx 
= 
e 
2 
e 
2 
dx 
=2e- 
s 
2 
(19.4.17) 


..8 
..8 


Hence 
the 
Fourier 
transform 
of 
the 
SE 
kernel 
is 
a 
Gaussian. 
Since 
this 
is 
positive 
the 
SE 
kernel 
is 
a 
covariance 
function. 


19.5 
Gaussian 
Processes 
for 
Classication 
Adapting 
the 
GP 
framework 
to 
classication 
requires 
replacing 
the 
Gaussian 
regression 
term 
p(yjx) 
with 
a 
corresponding 
classication 
term 
p(cjx) 
for 
a 
discrete 
label 
c. 
To 
do 
so 
we 
will 
use 
the 
GP 
to 
dene 
a 
latent 
continuous 
space 
which 
will 
then 
be 
mapped 
to 
a 
class 
probability 
using 


ZZ

p(cjx)= 
p(cjy;p(cjy)p(yjx)dy

x)p(yjx)dy 
= 
(19.5.1) 


		

1N1N

Given 
training 
data 
inputs 
X 
=x;:::;x, 
corresponding 
class 
labels 
C 
=c;:::;c, 
and 
a 
novel 
input 
x 
, 
then 


Z

* 


p(c 
* 
jx, 
C, 
X 
)= 
p(c 
* 
jy 
)p(y 
* 
jX 
, 
C)dy 
* 
(19.5.2) 


where 


* 


p(y 
* 
jX 
, 
C) 
. 
p(y, 
CjX 
)

Z

* 


= 
p(y, 
Y, 
CjX 
;x 
)dY

Z

* 


= 
p(CjY)p(y, 
YjX 
;x 
)dY 


()

ZN

Y

1 
N 
1 
N

= 
p(c 
njy 
n)(y 
;:::;y 
;y 
* 
jx 
;:::;x 
;x 
) 
dy1, 
. 
. 
. 
, 
dyN 
(19.5.3)

p
p|
{z}|n=1 
{z}Gaussian 
Process 
class 
mapping 


DRAFT 
March 
9, 
2010 



Gaussian 
Processes 
for 
Classication 




The 
graphical 
structure 
of 
this 
model 
is 
depicted 
in 
g(19.5.) 
The 
posterior 
latent 
y 
is 
then 
formed 
from 
the 
standard 
regression 
term 
from 
the 
Gaussian 
Process, 
multiplied 
by 
a 
set 
of 
non-Gaussian 
maps 
from 
the 
latent 
activations 
to 
the 
class 
probabilities. 
We 
can 
reformulate 
the 
prediction 
problem 
more 
conveniently 
as 
follows: 


* 
* 


p(y, 
Yjx, 
X 
, 
C) 
. 
p(y, 
Y, 
Cjx, 
X 
) 
. 
p(y 
* 
jY;x 
, 
X 
)p(YjC, 
X 
) 
(19.5.4) 


where 


N

Y

11 


p(YjC, 
X 
) 
. 
p(c 
njy 
n) 
p(y 
;:::;y 
N 
jx 
;:::;x 
N 
) 
(19.5.5) 


n=1 




In 
equation 
(19.5.4) 
the 
term 
p(y 
jY;x 
, 
X 
) 
does 
not 
contain 
any 
class 
label 
information 
and 
is 
simply 
a 
conditional 
Gaussian. 
The 
advantage 
of 
the 
above 
description 
is 
that 
we 
can 
therefore 
form 
an 
approx




imation 
to 
p(YjC, 
X 
) 
and 
then 
reuse 
this 
approximation 
in 
the 
prediction 
for 
many 
dierent 
x 
without 
needing 
to 
rerun 
the 
approximation[297, 
230]. 


19.5.1 
Binary 
classication 
For 
the 
binary 
class 
case 
we 
will 
use 
the 
convention 
that 
c 
2f1, 
0g. 
We 
therefore 
need 
to 
specify 
p(c 
=1jy) 
for 
a 
real 
valued 
activation 
y. 
A 
convenient 
choice 
is 
the 
logistic 
transfer 
function3 


1 


(x) 
= 
(19.5.6)

1+ 
e..x 


Then 


p(cjy)= 
s 
((2c 
- 
1) 
y) 
(19.5.7) 


is 
a 
valid 
distribution 
since 
(..x)=1 
- 
(x), 
ensuring 
that 
the 
sum 
over 
the 
class 
states 
is 
1. 
A 
diculty 
is 
that 
the 
non-linear 
class 
mapping 
term 
makes 
the 
computation 
of 
the 
posterior 
distribution 


1N

equation 
(19.5.3) 
dicult 
since 
the 
integrals 
over 
y;:::;ycannot 
be 
carried 
out 
analytically. 
There 
are 
many 
approximate 
techniques 
one 
could 
apply 
in 
this 
case, 
including 
variational 
methods 
analogous 
to 
that 
described 
in 
section(??). 
Below 
we 
describe 
the 
straightforward 
Laplace 
method, 
leaving 
the 
more 
sophisticated 
methods 
for 
further 
reading[230]. 


19.5.2 
Laplace's 
approximation 
In 
the 
Laplace 
method 
we 
approximate 
the 
non-Gaussian 
distribution 
(19.5.5) 
by 
a 
Gaussian4 
q(YjC, 
X 
), 


p(YjC, 
X 
) 
˜ 
q(YjC, 
X 
) 
(19.5.8) 


Predictions 
can 
be 
formed 
from 
the 
joint 
Gaussian 


* 
* 


p(y, 
Yjx, 
X 
, 
C) 
˜ 
p(y 
* 
jY;x 
, 
X 
)q(YjC, 
X 
) 
(19.5.9) 


For 
compactness 
we 
dene 
the 
class 
label 
vector, 
and 
outputs 


..T 
..T

1 
N1 
N

c 
=c 
;:::;c 
, 
y 
=y 
;:::;y 
(19.5.10) 


and 
notationally 
drop 
the 
(ever 
present) 
conditioning 
on 
the 
inputs 
x. 
Also 
for 
convenience, 
we 
dene 


..T 


s 
=(y 
1);:::;(y 
N 
)(19.5.11) 


3We 
will 
also 
refer 
to 
this 
as 
`the 
sigmoid 
function'. 
More 
strictly 
a 
sigmoid 
function 
refers 
to 
any 
`s-shaped’ 
function 
(from 
the 
Greek 
for 
`s'). 


4Some 
authors 
use 
the 
term 
Laplace 
approximation 
solely 
for 
approximating 
an 
integral. 
Here 
we 
use 
the 
term 
to 
refer 
to 
a 
Gaussian 
approximation 
of 
a 
non-Gaussian 
distribution. 


DRAFT 
March 
9, 
2010 
359 



Gaussian 
Processes 
for 
Classication 


Finding 
the 
mode 


The 
Laplace 
approximation, 
section(28.2), 
corresponds 
to 
a 
second 
order 
expansion 
around 
the 
mode 
of 
the 
distribution. 
Our 
task 
is 
therefore 
to 
nd 
the 
maximum 
of 


	(y)

p(yjc) 
/rp(y, 
c)= 
e(19.5.12) 
where 


N

X

T11 
N

	(y)= 
cy 
..rlog(1 
+ 
eyn 
) 
..ryTK..1 
y 
..rlog 
det 
(Kx;x) 
..rlog 
2p 
(19.5.13)

2x;x22 


n=1 


The 
maximum 
needs 
to 
be 
found 
numerically, 
and 
it 
is 
convenient 
to 
use 
the 
Newton 
method 
[120, 
297, 
230]. 


ynew 


= 
y 
..r(rr	)..1r. 
(19.5.14) 
Dierentiating 
equation 
19.5.13 
with 
respect 
to 
y 
we 
obtain 
the 
gradient 
and 
Hessian
r	=(c 
..r) 
..rK..1 
y 
(19.5.15)

x;x

rr	= 
..K..1 
..rD 
(19.5.16)

x;x 


where 
the 
`noise’ 
matrix 
is 
given 
by 


D 
= 
diag 
(1(1 
..r1);:::;N 
(1 
..rN 
)) 
(19.5.17) 
Using 
these 
expressions 
in 
the 
Newton 
update, 
(19.5.14) 
gives 


....1..

ynew 
K..1

= 
y 
++ 
Dc 
..rs 
..rK..1 
y(19.5.18)

x;xx;x

To 
avoid 
unnecessary 
inversions, 
one 
may 
rewrite 
this 
in 
the 
form 


ynew 


= 
Kx;x 
(I 
+ 
DKx;x)..1 
(Dy 
+ 
c 
..r) 
(19.5.19) 


Making 
predictions 
Given 
a 
converged 
solution 
y~we 
have 
found 
a 
Gaussian 
approximation 




....1

* 


K..1 


q(yjXr;x 
, 
C)= 
Ny 


y~;+ 
D(19.5.20)

x;x 




We 
now 
have 
Gaussians 
for 
p(y 
jy) 
and 
q(yjXr;x 
, 
C) 
in 
equation 
(19.5.9). 
Predictions 
are 
then 
made 
using 


Z

* 
* 


p(y 
* 
jx, 
Xr, 
C) 
p(y 
* 
jx, 
Xr, 
y)q(yjXr;x 
, 
C)dy 
(19.5.21) 


where, 
by 
conditioning, 
section(8.6.1), 


..



* 


K..1 
K..1 


p(y 
* 
jY;x 
, 
Xr)= 
Ny 


Kx 
* 
;xx;xy, 
Kx 
* 
;x 
* 
..rKx 
* 
;xx;xKx;x 
(19.5.22) 


We 
can 
also 
write 
this 
as 
a 
linear 
system 


* 
K..1 
y 
= 
Kx 
* 
;xx;xy 
+ 
. 
(19.5.23) 


..



K..1

where 
. 
N. 


0, 
Kx 
* 
;x 
* 
..rKx 
* 
;xx;xKx;x 
. 
Using 
equation 
(19.5.23) 
and 
equation 
(19.5.20) 
and 
averaging 
over 
y 
and 
the 
noise 
, 
we 
obtain 


* 
K..1
hy 
* 
jx, 
Xr, 
CirrKx 
* 
;xx;xy~= 
Kx 
* 
;x 
(c 
..rs 
(y~)) 
(19.5.24) 


Similarly, 
the 
variance 
of 
the 
latent 
prediction 
is 


....1



var(y 
* 
jx, 
Xr, 
C) 
rKx 
* 
;xK..1 
K..1 
+ 
DK..1 
Kx;x 
* 
+ 
Kx 
* 
;x 
* 
..rKx 
* 
;xK..1 
Kx;x 
* 
(19.5.25)

x;xx;xx;xx;x

....1 


= 
Kx 
* 
;x 
* 
..rKx 
* 
;xKx;x 
+ 
D..1Kx;x 
* 
(19.5.26) 


DRAFT 
March 
9, 
2010 



Gaussian 
Processes 
for 
Classication 


-10-8-6-4-2024681000.20.40.60.81
-10-8-6-4-2024681000.20.40.60.81
(a) 
(b) 
Figure 
19.6: 
Gaussian 
Process 
classication. 
The 
x-axis 
are 
the 
inputs, 
and 
the 
class 
is 
the 
y-axis. 
Green 
points 
are 
training 
points 
from 
class 
1 
and 
red 
from 
class 
0. 
The 
dots 
are 
the 
predictions 
p(c 
=1jx 
) 
for 




a 
rand 
of 
points 
x 
. 
(a): 
Square 
exponential 
covariance 
(. 
= 
2). 
(b): 
OU 
covariance 
(. 
= 
1). 
See 
demoGPclass1D.m. 


where 
the 
last 
line 
is 
obtained 
using 
the 
Matrix 
Inversion 
Lemma, 
denition(132). 




The 
class 
prediction 
for 
a 
new 
input 
x 
is 
then 
given 
by 


* 


p(c 
* 
=1jx, 
X 
, 
C) 
h(y 
)iN 
(y 
* 


(19.5.27)
hy 
i;var(y 
)) 


In 
order 
to 
calculate 
the 
Gaussian 
integral 
over 
the 
logistic 
sigmoid 
function, 
we 
use 
an 
approximation 
of 
the 
sigmoid 
function 
based 
on 
the 
error 
function 
erf(x), 
see 
section(18.2.3) 
and 
avsigmaGauss.m. 


Example 
85. 
An 
example 
of 
binary 
classication 
is 
given 
in 
g(19.6) 
in 
which 
one-dimensional 
input 
training 
data 
with 
binary 
class 
labels 
is 
plotted 
along 
with 
the 
class 
probability 
predictions 
on 
a 
range 
of 


jxi..xj 
j

input 
points. 
In 
both 
cases 
the 
covariance 
function 
is 
of 
the 
form 
2e+0:001ij. 
The 
square 
exponential 
covariance 
produces 
a 
smoother 
class 
prediction 
than 
the 
Ornstein-Uhlenbeck 
covariance 
function. 
See 
demoGPclass1D.m 
and 
demoGPclass.m. 


Marginal 
likelihood 


The 
marginal 
likelihood 
is 
given 
by 


Z

p(CjX 
)=p(CjY)p(YjX 
) 
(19.5.28) 


Y 


Under 
the 
Laplace 
approximation, 
the 
marginal 
likelihood 
is 
approximated 
by 


Z

	(y~)- 
1 
(y..y~)TA(y..y~)

p(CjX 
) 
ee 
2 
(19.5.29) 


y 


where 
A 
= 
..rr	. 
Integrating 
over 
y 
gives 


log 
p(CjX 
) 
˜ 
log 
q(CjX 
) 
(19.5.30) 
where 


1

log 
q(CjX 
) 
= 
	(y~) 
- 
log 
det 
(2A) 
(19.5.31)

2 


1 
..N

K..1

= 
	(y~) 
- 
log 
det+ 
D+ 
log 
2p 
(19.5.32)

2 
x;x 
2 


X

N11

~TK..1

= 
cTy~- 
log(1 
+ 
eyn 
) 
- 
y~y~- 
log 
det 
(I 
+ 
Kx;xD) 
(19.5.33)

2 
x;x 
2 


n=1 


where 
y~is 
the 
converged 
iterate 
of 
equation 
19.5.18. 
One 
can 
also 
simplify 
the 
above 
using 
that 
at 
convergence 
K..1 
y~= 
c 
- 
(y).

x;x 


DRAFT 
March 
9, 
2010 



Code 


19.5.3 
Hyperparameter 
optimisation 
The 
approximate 
marginal 
likelihood 
can 
be 
used 
to 
assess 
hyperparameters 
. 
of 
the 
kernel. 
A 
little 
care 
is 
required 
in 
computing 
derivatives 
of 
the 
approximate 
marginal 
likelihood 
since 
the 
optimum 
y~depends 
on 
. 
We 
use 
the 
total 
derivative 
formula 
[25] 


X

d. 
@d

log 
q(CjX 
) 
= 
log 
q(CjX 
) 
+log 
q(CjX 
) 
y~i 
(19.5.34)

d@. 
@y~i 
d. 


i 


hi

. 
1 
@

log 
q(CjX 
)= 
- 
yTK..1 
y 
+ 
log 
det 
(I 
+ 
Kx;xD)(19.5.35)

@. 
2 
@x;x

which 
can 
be 
evaluated 
using 
the 
standard 
results 
for 
the 
derivative 
of 
a 
matrix 
determinant 
and 
inverse. 
Since 
the 
derivative 
of 
. 
is 
zero 
at 
y~, 
and 
noting 
that 
D 
depends 
explicitly 
on 
y~, 


. 
1 
@

log 
q(CjX 
)= 
- 
log 
det 
(I 
+ 
Kx;xD) 
(19.5.36)

@y~i 
2 
@y~i 


The 
implicit 
derivative 
is 
obtained 
from 
using 
the 
fact 
that 
at 
convergence 


y~= 
Kx;x 
(c 
- 
(y)) 
(19.5.37) 


to 
give 


d. 


y~=(I 
+ 
Kx;xD)..1 
Kx;x 
(c 
- 
) 
(19.5.38)

d. 
@. 


These 
results 
are 
substituted 
into 
equation 
(19.5.34) 
to 
nd 
an 
explicit 
expression 
for 
the 
derivative. 


19.5.4 
Multiple 
classes 
The 
extension 
of 
the 
preceding 
framework 
to 
multiple 
classes 
is 
essentially 
straightforward 
and 
may 
be 
achieved 
using 
the 
softmax 
function 


ym

e

p(c 
= 
mjy) 
= 
(19.5.39)

ml 
eyml 


P

which 
automatically 
enforces 
the 
constraintp(c 
= 
m) 
= 
1. 
Naively 
it 
would 
appear 
that 
the 
for 
C 


m 
..X

classes, 
the 
cost 
of 
implementing 
the 
Laplace 
approximation 
for 
the 
multiclass 
case 
scales 
as 
OC3N3 
.

..X

However, 
one 
may 
show 
by 
careful 
implementation 
that 
the 
cost 
is 
only 
O 
CN3 
, 
and 
we 
refer 
the 
reader 
to 
[297, 
230] 
for 
details. 


19.6 
Further 
Reading 
Gaussian 
Processes 
have 
been 
heavily 
developed 
within 
the 
machine 
learning 
community 
over 
recent 
years 
for 
which 
ecient 
approximations 
for 
both 
regression 
and 
classication 
remains 
an 
active 
research 
topic. 
We 
direct 
the 
interested 
reader 
to 
[245] 
and 
[230] 
for 
further 
discussion. 


19.7 
Code 
GPreg.m: 
Gaussian 
Process 
Regression 
demoGPreg.m: 
Demo 
GP 
regression 
covfnGE.m: 
Gamma-Exponential 
Covariance 
function 
GPclass.m: 
Gaussian 
Process 
Classication 
demoGPclass.m: 
Demo 
Gaussian 
Process 
Classication 


362 
DRAFT 
March 
9, 
2010 



Exercises 


19.8 
Exercises 
PN 


nn

Exercise 
191. 
Show 
that 
the 
sample 
covariance 
matrix 
with 
elements 
Sij 
=n=1 
xi 
xj 
=N 
- 
xixj, 
where 
PN 


n

xi 
=i 
=N, 
is 
positive 
semidenite. 


n=1 
x

Exercise 
192. 
Show 
that 


..| 
sin(x..x0)j

k(x 
- 
x0)= 
e(19.8.1) 


is 
a 
covariance 
function. 
Exercise 
193. 
Consider 
the 
function 


..

f(xi;xj)= 
e 


1 


2

(xi..xj 
)2 


(19.8.2) 
for 
one 
dimensional 
inputs 
xi. 
Show 
that 


1

1

2

2

xixj 


e 


..

..

f(xi;xj)= 
e 


(19.8.3)
x

x

j

i 
e

2

2

1 


2

(xi..xj 
)2

..

By 
Taylor 
expanding 
the 
central 
term, 
show 
that 
e 


is 
a 
kernel 
and 
nd 
an 
explicit 
representation 


for 
the 
kernel 
f(xi;xj) 
as 
the 
scalar 
product 
of 
two 
innite 
dimensional 
vectors. 
Exercise 
194. 
Show 
that 
for 
a 
covariance 
function 
k1 
(x, 
x0) 
then 


....

0

k(x, 
x0)= 
fk1x, 
x(19.8.4) 


k1(x;x0)

is 
also 
a 
covariance 
function 
for 
any 
polynomial 
f(x) 
with 
positive 
coecients. 
Show 
therefore 
that 
e

and 
tan 
(k1 
(x, 
x0)) 
are 
covariance 
functions. 
Exercise 
195. 
For 
a 
covariance 
function 
k1(x, 
x0) 
= 
f(..x 
- 
x0T..x 
- 
x0) 
(19.8.5) 
show 
that 
k2(x, 
x0) 
= 
f(..x 
- 
x0T 
A..x 
- 
x0) 
(19.8.6) 
is 
also 
a 
valid 
covariance 
function 
for 
a 
positive 
denite 
symmetric 
matrix 
A. 


Exercise 
196 
(String 
kernel). 
Let 
x 
and 
x' 
be 
two 
strings 
of 
characters 
and 
s(x) 
be 
the 
number 
of 
times 
that 
substring 
s 
appears 
in 
string 
x. 
Then 


X

k(x, 
x 
0)=wss(x)s(x 
0) 
(19.8.7) 
s 


is 
a 
(string 
kernel) 
covariance 
function, 
provided 
the 
weight 
of 
each 
substring 
ws 
is 
positive. 


1. 
Given 
a 
collection 
of 
strings 
about 
politics 
and 
another 
collection 
about 
sport, 
explain 
how 
to 
form 
a 
GP 
classier 
using 
a 
string 
kernel. 
2. 
Explain 
how 
the 
weights 
ws 
can 
be 
adjusted 
to 
improve 
the 
t 
of 
the 
classier 
to 
the 
data 
and 
give 
an 
explicit 
formula 
for 
the 
derivative 
with 
respect 
to 
ws 
of 
the 
log 
marginal 
likelihood 
under 
the 
Laplace 
approximation. 
Exercise 
197 
(Vector 
regression). 
Consider 
predicting 
a 
vector 
output 
y 
given 
training 
data 
X 
[Y 
= 
fxn 
, 
yn;n 
=1;:::;ng. 
To 
make 
a 
GP 
predictor 


* 


p(y 
* 
jx 
, 
X 
, 
Y) 
(19.8.8) 


we 
need 
a 
Gaussian 
model 


1 
N 
1 


p(y;:::, 
y, 
y 
* 
jx;:::, 
xn 
, 
x 
) 
(19.8.9) 


mn

A 
GP 
requires 
then 
a 
specication 
of 
the 
covariance 
c(y;yjxn 
, 
xm) 
of 
the 
components 
of 
the 
outputs 
for 


ij 


two 
dierent 
input 
vectors. 
Show 
that 
under 
the 
dimension 
independence 
assumption 


mn 
mn 


c(yi 
;y 
j 
jxn 
, 
xm)= 
ci(yi 
;y 
i 
jxn 
, 
xm)ij 
(19.8.10) 


mn

where 
ci(y;yjxn 
, 
xm) 
is 
a 
covariance 
function 
for 
the 
ith 
dimension, 
that 
separate 
GP 
predictors 
can 
be 


ii 


constructed 
independently, 
one 
for 
each 
output 
dimension 
i. 


DRAFT 
March 
9, 
2010 



Exercises 


Exercise 
198. 
Consider 
the 
Markov 
update 
of 
a 
linear 
dynamical 
system, 
section(24.1), 
xt 
= 
Axt..1 
+ 
t;t 
62 
(19.8.11) 





where 
A 
is 
a 
given 
matrix 
and 
t 
is 
zero 
mean 
Gaussian 
noise 
with 
covariancei;tj;t0= 
2ijt;t' 
. 
Also, 


p(x1)= 
N6(x1 


0, 
). 


1. 
Show 
that 
x1;:::, 
xt 
is 
Gaussian 
distributed. 
2. 
Show 
that 
the 
covariance 
matrix 
of 
x1;:::, 
xt 
has 
elements 
(

D. 
At0..tS 


t6

T 
= 
ti 


xt' 
x=..T 
(19.8.12)

t 


AtAtt 
= 
ti 


and 
explain 
why 
a 
linear 
dynamical 
system 
is 
a 
(constrained) 
Gaussian 
Process. 


3. 
Consider 
yt 
= 
Bxt 
+ 
t 
(19.8.13) 





where 
t 
is 
zero 
mean 
Gaussian 
noise 
with 
covariancei;tj;t0= 
2ijt;t' 
. 
The 
vectors 
E 
are 
uncorrelated 
with 
the 
vectors 
. 
Show 
that 
the 
sequence 
of 
vectors 
y1;:::, 
yt 
is 
a 
Gaussian 
Process 
with 
a 
suitably 
dened 
covariance 
function. 


Exercise 
199. 
A 
form 
of 
independent 
components 
analysis, 
section(21.6), 
of 
a 
one-dimensional 
signal 
y1;:::;yT 
is 
obtained 
from 
the 
joint 
model 


()Y....





12 
121 


2 


p(y1:T 
;x 
1:T 
;x 
1:T 
jw)=p(ytjx 
;x 
t 
, 
w)Nx 


0, 
Nx 


0, 
(19.8.14)

t 
1:T 


1:T 
t 
with 


..



12 


11 
22 


p(ytjx 
;x 
t 
, 
w)= 
Nyt 


wx 
+ 
w 
x;2(19.8.15)

t 


tt 


where 
2 
is 
a 
given 
noise 
variance. 
The 
signal 
y1:T 
can 
be 
viewed 
as 
the 
linear 
combination 
of 
two 
independent 
Gaussian 
Processes. 
The 
covariance 
matrices 
of 
the 
two 
processes 
have 
elements 
from 
a 
stationary 
kernel, 


..(t..t0)2

t;t' 
= 
e 
(19.8.16) 


12

1. 
Write 
down 
an 
EM 
algorithm 
for 
learning 
the 
mixing 
parameters 
w;wgiven 
an 
observation 
seq
uence 
y1:T 
. 
2. 
Consider 
an 
extension 
of 
the 
above 
model 
to 
the 
case 
of 
two 
outputs: 
()

2

YY....

121 
2 
i 
12 
1 


2 


p(y1:T 
;y 
1:T 
;x 
1:T 
;x 
1:T 
jW)= 
p(y 
jx 
;x 
t 
, 
wi)Nx 


0, 
Nx 


0, 
(19.8.17)

tt 
1:T 


1:T 
i=1t 
with 


..



112 
1 


12 


p(y 
jx 
;x 
t 
, 
W)= 
Ny 


w11xt 
+ 
w12xt 
;2(19.8.18)

tt 
t

..



212 
1 


12 


p(y 
jxt 
;x 
t 
, 
W)= 
Ny 


w21x 
+ 
w22x 
;2(19.8.19)

tt 


tt 


Show 
that 
for 
T> 
1 
the 
likelihood 


12 


p(y1:T 
;y 
1:T 
jW) 
(19.8.20) 


is 
not 
invariant 
with 
respect 
to 
an 
orthogonal 
rotation 
Wi 
= 
WR, 
with 
RRT 
= 
I, 
and 
explain 
the 
signicance 
of 
this 
with 
respect 
to 
identifying 
independent 
components. 


DRAFT 
March 
9, 
2010 



CHAPTER 
20 


Mixture 
Models 


20.1 
Density 
Estimation 
Using 
Mixtures 
A 
mixture 
model 
is 
one 
in 
which 
a 
set 
of 
component 
models 
is 
combined 
to 
produce 
a 
richer 
model: 


H

H 


p(v)= 
p(vjh)p(h) 
(20.1.1) 


h=1 


The 
variable 
v 
is 
`visible’ 
or 
`observable’ 
and 
h 
=1;:::;H 
indexes 
each 
component 
model 
p(vjh), 
along 
with 
its 
weight 
p(h). 
Mixture 
models 
have 
natural 
application 
in 
clustering 
data, 
where 
h 
indexes 
the 
cluster. 
This 
interpretation 
can 
be 
gained 
from 
considering 
how 
to 
generate 
a 
sample 
datapoint 
v 
from 
the 
model 
equation 
(20.1.1). 
First 
we 
sample 
a 
cluster 
h 
from 
p(h), 
and 
then 
draw 
a 
visible 
state 
v 
from 
p(vjh). 


For 
a 
set 
of 
i.i.d. 
data 
v1, 
. 
. 
. 
, 
vN 
, 
a 
mixture 
model 
is 
of 
the 
form, 
g(20.1), 
NN 
H 
p(v 
1 
, 
. 
. 
. 
, 
v 
N 
) 
= 
n=1 
hn 
p(v 
njhn)p(hn) 
(20.1.2) 
Clustering 
is 
achieved 
by 
inference 
of 
argmax 
h1;:::;hN 
p(h1, 
. 
. 
. 
, 
hN 
jv 
1 
, 
. 
. 
. 
, 
v 
N 
) 
(20.1.3) 


which, 
thanks 
to 
the 
factorised 
form 
of 
the 
distribution 
is 
equivalent 
to 
computing 
arg 
maxhn 
p(hnjvn) 
for 
each 
datapoint. 
In 
this 
way 
we 
can 
cluster 
many 
kinds 
of 
data 
for 
which 
a 
`distance’ 
measure 
in 
the 
sense 
of 
the 
classical 
K-means 
algorithm, 
section(20.3.5), 
is 
not 
directly 
apparent. 


Explicitly 
writing 
the 
dependence 
on 
the 
parameters, 
the 
model 
is 


p(v, 
hj)= 
p(vjh, 
vjh)p(hjh) 
(20.1.4) 


The 
optimal 
parameters 
vjh;h 
of 
a 
mixture 
model 
are 
then 
most 
commonly 
set 
by 
Maximum 
Likelihood, 


1

opt 
= 
argmax 
p(v 
;:::;v 
N 
j) 
(20.1.5) 


. 


Numerically 
this 
can 
be 
achieved 
using 
an 
optimisation 
procedure 
such 
as 
gradient 
based 
approaches. 
Alternatively, 
by 
treating 
the 
component 
indices 
as 
latent 
variables, 
one 
may 
also 
apply 
the 
EM 
algorithm, 
as 
described 
in 
the 
following 
section, 
which 
in 
many 
classical 
models 
produces 
simple 
update 
formulae. 


365 



Expectation 
Maximisation 
for 
Mixture 
Models 


hn
vn
h
vjh
N
Figure 
20.1: 
A 
mixture 
model 
has 
a 
trivial 
graphical 
representation 
as 
a 
DAG 
with 
a 
single 
hidden 
node, 
which 
can 
be 
in 
and 
one 
of 
H 
states, 
i 
=1 
:::;H. 
The 
parameters 
are 
assumed 
common 
across 
all 
datapoints. 


Example 
86. 
The 
data 
in 
g(20.2) 
naturally 
has 
two 
clusters 
and 
can 
be 
modelled 
with 
a 
mixture 
of 
two 
two-dimensional 
Gaussians, 
each 
Gaussian 
describing 
one 
of 
the 
clusters. 
Here 
there 
is 
a 
clear 
visual 
interpretation 
of 
the 
meaning 
of 
`cluster', 
with 
the 
mixture 
model 
placing 
two 
datapoints 
in 
the 
same 
cluster 
if 
they 
are 
both 
likely 
to 
be 
generated 
by 
the 
same 
model 
component. 


20.2 
Expectation 
Maximisation 
for 
Mixture 
Models 
By 
treating 
the 
index 
h 
as 
a 
missing 
variable, 
mixture 
models 
can 
be 
trained 
using 
the 
EM 
algorithm, 
section(11.2). 
There 
are 
two 
sets 
of 
parameters 
– 
vjh 
for 
each 
component 
model 
p(vjh, 
vjh) 
and 
h 
for 
the 
mixture 
weights 
p(hjh). 
According 
to 
the 
general 
approach 
for 
i.i.d. 
data 
of 
section(11.2), 
we 
need 
to 
consider 
the 
energy 
term: 


N

X


E()= 
hlog 
p(v 
n;hj)iold(hjvn) 
(20.2.1)

p
n=1 
NN


X
X


= 
log 
p(v 
njh, 
vjh)old(hjvn) 
+ 
hlog 
p(hjh)ipold(hjvn) 
(20.2.2)

p
n=1n=1 


where 


p 
old(hjv 
n) 
. 
p(v 
njh, 
old 
)p(hjold 
) 
(20.2.3)

vjhh 


and 
maximise 
(20.2.2) 
with 
respect 
to 
the 
parameters 
vjh, 
h, 
h 
=1;:::;H. 


20.2.1 
Unconstrained 
discrete 
tables 
Here 
we 
consider 
training 
a 
simple 
belief 
networkp(vjh, 
vjh)p(hjh), 
v 
=1;:::;V 
, 
h 
=1;:::;H 
in 
which 
the 
tables 
are 
unconstrained. 
This 
is 
a 
special 
case 
of 
the 
more 
general 
framework 
discussed 
in 
section(11.2). 


-3-2-10123-3-2-101234
Figure 
20.2: 
Two 
dimensional 
data 
which 
displays 
clusters. 
In 
this 
case 
a 
Gaussian 
mixture 
model 
1=2N 
(x 


m1, 
C1)+1=2N 
(x 


m2, 
C2) 
would 
t 
the 
data 
well 
for 
suitable 
means 
m1, 
m2 
and 
covariances 
C1, 
C2. 


DRAFT 
March 
9, 
2010 



Expectation 
Maximisation 
for 
Mixture 
Models 


M-step: 
p(h) 


If 
no 
constraint 
is 
placed 
on 
p(hjh) 
we 
may 
write 
the 
parameters 
as 
simply 
p(h), 
with 
the 
understanding 


P

that 
0 
= 
p(h) 
= 
1 
andh 
p(h) 
= 
1. 
Isolating 
the 
dependence 
of 
equation 
(20.2.2) 
on 
p(h) 
we 
obtain 


NN

XPXXP

hlog 
p(h)iold(hjvn) 
= 
log 
p(h) 
p 
old(hjv 
n) 
(20.2.4)

p
n=1 
hn=1 


P

We 
now 
wish 
to 
maximise 
equation 
(20.2.4) 
with 
respect 
to 
p(h) 
under 
the 
constraint 
thatj 
p(h) 
= 
1. 
It 
is 
standard 
to 
treat 
this 
maximisation 
problem 
using 
Lagrange 
multipliers, 
see 
exercise(203). 
Here 
we 
take 
an 
alternative 
approach 
based 
on 
recognising 
the 
similarity 
of 
the 
above 
to 
a 
form 
of 
Kullback-Leibler 
divergence. 
First 
we 
dene 
the 
distribution 


PN 
old(hjvn)

n=1 
p

p~(h) 
P(20.2.5)

PN 
old(hjvn)

hn=1 
p

Then 
maximising 
equation 
(20.2.4) 
is 
equivalent 
to 
maximising 


hlog 
p(h)ip~(h) 
(20.2.6) 


PPN 
old(hjv

since 
the 
two 
expressions 
are 
related 
by 
the 
constant 
factorn). 
By 
subtracting 
the 


hn=1 
p
independent 
term 
hlog 
~p(h)ip~(h) 
from 
equation 
(20.2.6), 
we 
obtain 
the 
negative 
Kullback-Leibler 
divergence 
KL(~pjp). 
This 
means 
that 
the 
optimal 
p(h) 
is 
that 
distribution 
which 
minimises 
the 
Kullback-Leibler 
divergence. 
Optimally, 
therefore 
~p(h)= 
p(h), 
so 
that 


PN 
old(hjv

1 
N

new(h)=n=1 
pn)XPold(hjv 
n)

p 
= 
p 
(20.2.7)

PPN 
old(hjvn) 
N 


hn=1 
pn=1 


M-step 
: 
p(vjh) 


The 
dependence 
of 
equation 
(20.2.2) 
on 
p(vjh) 
is 


N

XP
..

log 
pv 
njh, 
vjhold(hjvn) 
(20.2.8)

p
n=1


..

If 
the 
distributions 
pvjh, 
vjhare 
not 
constrained, 
we 
can 
apply 
a 
similar 
Kullback-Leibler 
method, 
as 
we 
did 
in 
section(11.2). 
For 
compatibility 
with 
other 
texts, 
here 
we 
demonstrate 
the 
more 
standard 
Lagrange 
procedure. 
We 
need 
to 
ensure 
that 
p(vjh) 
is 
a 
distribution 
for 
each 
of 
the 
mixture 
states 
h 
=1;:::;H. 
This 
can 
be 
achieved 
using 
a 
set 
of 
Lagrange 
multipliers, 
giving 
the 
Lagrangian: 


 !

NHHV

XXXPXXP

L= 
I[v 
n 
= 
v] 
p 
old(hjv 
n) 
log 
p 
(vjh)+ 
(h)1 
- 
p(vjh)(20.2.9) 


vn=1 
h=1 
h=1 
v=1 


Dierentiating 
with 
respect 
to 
p(v 
= 
ijh 
= 
j) 
and 
equating 
to 
zero, 


N

XPn 
old(h 
= 
jjv 
= 
i)

@L 
I[v= 
i] 
p

= 
- 
(h 
= 
j)p(v 
= 
ijh 
= 
j) 
= 
0 
(20.2.10)

@p(v 
= 
ijh 
= 
j) 
p(v 
= 
ijh 
= 
j)

n=1 


Hence 


N

XP

p 
new(v 
= 
ijh 
= 
j) 
. 
I[v 
n 
= 
i] 
p 
old(h 
= 
jjv 
= 
i) 
(20.2.11) 
n=1 


which, 
using 
the 
normalisation 
requirement, 
gives 


PN 
I[vn 
= 
i] 
pold(h 
= 
jjv 
= 
i)

n=1

p 
new(v 
= 
ijh 
= 
j) 
=(20.2.12)PV 
PN 
I[vn 
= 
i] 
pold(h 
= 
jjv 
= 
i)

i=1n=1 


DRAFT 
March 
9, 
2010 



Expectation 
Maximisation 
for 
Mixture 
Models 


hn
vn
i
h
vijh
n=1;:::;N
i=1;:::;D
Figure 
20.3: 
Mixture 
of 
a 
product 
of 
Bernoulli 
distributions. 
In 
a 
Bayesian 
Treatment, 
a 
parameter 
prior 
is 
used. 
In 
the 
text 
we 
simply 
set 
the 
parameters 
using 
Maximum 
Likelihood. 


E-step 


According 
to 
the 
general 
EM 
procedure, 
section(11.2), 
optimally 
we 
set 
pnew(hjvn)= 
p(hjvn): 


new(hjv
p(vnjh)p(h)

p 
n)= 
P(20.2.13) 


h 
p(vnjh)p(h) 


Equations 
(20.2.7,20.2.12,20.2.13) 
are 
repeated 
until 
convergence. 
The 
initialisation 
of 
the 
tables 
and 
mixture 
probabilities 
can 
severely 
aect 
the 
quality 
of 
the 
solution 
found 
since 
the 
likelihood 
often 
has 
local 
optima. 
If 
random 
initialisations 
are 
used, 
it 
is 
recommended 
to 
record 
the 
converged 
value 
of 
the 
likelihood 
itself, 
to 
see 
which 
parameters 
have 
the 
higher 
likelihood. 
The 
solution 
with 
the 
highest 
likelihood 
is 
to 
be 
preferred. 


20.2.2 
Mixture 
of 
product 
of 
Bernoulli 
distributions 
We 
describe 
a 
simple 
mixture 
model 
that 
can 
be 
used 
to 
clustering 
binary 
vectors, 
v 
=(v1;:::;vD)T 
, 
vi 
2f0, 
1g. 
The 
mixture 
of 
Bernoulli 
products1 
model 
is 
given 
by 


HD

XYX

p(v)= 
p(h) 
p(vijh) 
(20.2.14) 


h=1 
i=1 


where 
each 
term 
p(vijh) 
is 
a 
Bernoulli 
distribution. 
The 
model 
is 
depicted 
in 
g(20.3) 
and 
has 
parameters 
p(h) 
and 
p(vi 
=1jh), 
h 
=1 
:::;H. 


EM 
training 


To 
train 
the 
model 
under 
Maximum 
Likelihood 
it 
is 
convenient 
to 
use 
the 
EM 
algorithm 
which, 
as 
usual, 
may 
be 
derived 
by 
writing 
down 
the 
energy:

XXXX

n

hlog 
p(vn;h)iold(hjvn) 
=hlog 
p(vi 
jh)iold(hjvn) 
+hlog 
p(h)iold(hjvn) 
(20.2.15)

ppp
nnin


and 
then 
performing 
the 
maximisation 
over 
the 
table 
entries. 
From 
our 
general 
results, 
section(11.2), 
we 
may 
immediately 
jump 
to 
the 
updates. 
The 
M-step 
is 
given 
by 


P

I[vn 
= 
1] 
pold(h 
= 
jjvn)

ni 


p 
new(vi 
=1jh 
= 
j)=PP

I[vn 
= 
1] 
pold(h 
= 
jjvn)+I[vn 
= 
0] 
pold(h 
= 
jjvn)

ni 
ni

P

old(h 
= 
jjvn)

n

p 
new(h 
= 
j)=P
p
Pold(h0jvn) 
(20.2.16) 


p

h0n 


and 
the 
E-step 
by 


D

YX

p 
old(h 
= 
jjvn) 
/hp(h 
= 
j) 
p(vi
njh 
= 
j) 
(20.2.17) 
i=1 


1This 
is 
similar 
to 
the 
Naive 
Bayes 
classier 
in 
which 
the 
class 
labels 
are 
always 
hidden. 


DRAFT 
March 
9, 
2010 



Expectation 
Maximisation 
for 
Mixture 
Models 


2040608010012014012345
Figure 
20.4: 
Data 
from 
questionnaire 
responses. 
150 
people 
were 
each 
asked 
5 
questions, 
with 
`yes’ 
(white) 
and 
`no’ 
(gray) 
answers. 
Black 
denotes 
that 
the 
absence 
of 
a 
response 
(missing 
data). 
This 
training 
data 
was 
generated 
by 
two 
component 
Binomial 
mixture. 
Missing 
data 
was 
simulated 
by 
randomly 
removing 
values 
from 
the 
dataset. 


Equations 
(20.2.16,20.2.17) 
are 
iterated 
until 
convergence. 


n

If 
an 
attribute 
i 
is 
missing 
for 
datapoint 
n, 
one 
needs 
to 
sum 
over 
the 
states 
of 
the 
corresponding 
vi 
. 
The 


n

eect 
of 
performing 
the 
summation 
for 
this 
model 
is 
simply 
to 
remove 
the 
corresponding 
factor 
p(vjh)

i 


from 
the 
algorithm, 
see 
exercise(200). 


Initialisation 


The 
EM 
algorithm 
can 
be 
very 
sensitive 
to 
initial 
conditions. 
Consider 
the 
following 
initialisation: 
p(vi 
= 
1jh 
= 
j)=0:5, 
with 
p(h) 
set 
arbitrarily. 
This 
means 
that 
at 
the 
rst 
iteration, 
pold(h 
= 
jjvn)= 
p(h 
= 
j). 
The 
subsequent 
M-step 
updates 
are 


p 
new(h)= 
p(h);p 
new(vijh 
= 
j)= 
p 
new(vijh 
= 
j0) 
(20.2.18) 


for 
any 
j, 
j0. 
This 
means 
that 
the 
parameters 
p(vjh) 
immediately 
become 
independent 
of 
h 
and 
the 
model 
is 
numerically 
trapped 
in 
a 
symmetric 
solution. 
It 
makes 
sense, 
therefore, 
to 
initialise 
the 
parameters 
in 
a 
non-symmetric 
fashion. 


Example 
87 
(Questionnaire). 
A 
company 
sends 
out 
a 
questionnaire 
containing 
a 
set 
of 
D 
`yes/no’ 
questions 
to 
a 
set 
of 
customers. 
The 
binary 
responses 
of 
a 
customer 
are 
stored 
in 
a 
vector 
v 
=(v1;:::;vD)T 
. 
In 
total 
N 
customers 
send 
back 
their 
questionnaires, 
v1 
;:::, 
vN 
, 
and 
the 
company 
wishes 
to 
perform 
an 
analysis 
to 
nd 
what 
kinds 
of 
customers 
it 
has. 
The 
company 
assumes 
there 
are 
H 
essential 
types 
of 
customer 
for 
which 
the 
prole 
of 
responses 
is 
dened 
by 
only 
the 
customer 
type. 


Data 
from 
a 
questionnaire 
containing 
5 
questions, 
with 
150 
respondents 
is 
presented 
in 
g(20.4). 
The 
data 
has 
a 
large 
number 
of 
missing 
values. 
We 
assume 
there 
are 
H 
= 
2 
kinds 
of 
respondents 
and 
attempt 
to 
assign 
each 
respondent 
into 
one 
of 
the 
two 
clusters. 
Running 
the 
EM 
algorithm 
on 
this 
data, 
with 
random 
initial 
values 
for 
the 
tables, 
produces 
the 
results 
in 
g(20.5). 
Based 
on 
assigning 
each 
datapoint 
vn 
to 
the 
cluster 
with 
maximal 
posterior 
probability 
hn 
= 
arg 
maxh 
p(hjvn), 
given 
a 
trained 
model 
p(vjh)p(h), 
the 
model 
assigns 
86% 
of 
the 
data 
to 
the 
correct 
cluster 
(which 
is 
known 
in 
this 
simulated 
case). 
See 
g(20.5) 
and 
MIXprodBern.m. 


(a) 
(b) 
Figure 
20.5: 
EM 
learning 
of 
a 
mixture 
of 
Bernoulli 
products. 
(a): 
True 
p(h) 
(left) 
and 
learned 
p(h) 
(right) 
for 
h 
= 
1, 
2. 
(b): 
True 
p(vjh) 
(left) 
and 
learned 
p(vjh) 
(right) 
for 
v 
= 
1, 
. 
. 
. 
, 
5. 
Each 
column 
pair 
corresponds 
to 
p(vijh 
= 
1) 
(red) 
and 
p(vijh 
= 
2) 
(blue) 
with 
i 
= 
1, 
. 
. 
. 
, 
5. 
The 
learned 
probabilities 
are 
reasonably 
close 
to 
the 
true 
values. 
DRAFT 
March 
9, 
2010 
369 



The 
Gaussian 
Mixture 
Model 



Figure 
20.6: 
Top: 
a 
selection 
of 
200 
of 
the 
5000 
handwritten 
digits 
in 
the 
training 
set. 
Bottom: 
the 
trained 
cluster 
outputs 
p(vi 
=1jh) 
for 
h 
=1;:::, 
20 
mixtures. 
See 
demoMixBernoulliDigits.m. 


Example 
88 
(Handwritten 
digits). 
We 
have 
a 
collection 
of 
5000 
handwritten 
digits 
which 
we 
wish 
to 
cluster 
into 
20 
groups, 
g(20.6). 
Each 
digit 
is 
a 
28 
× 
28 
= 
784 
dimensional 
binary 
vector. 
Using 
a 
mixture 
of 
Bernoulli 
products, 
trained 
with 
50 
iterations 
of 
EM 
(with 
a 
random 
perturbation 
of 
the 
mean 
of 
the 
data 
used 
as 
initialisation), 
the 
clusters 
are 
presented 
in 
g(20.6). 
As 
we 
see, 
the 
method 
captures 
natural 
clusters 
in 
the 
data 
– 
for 
example, 
there 
are 
two 
kinds 
of 
1, 
one 
slightly 
more 
slanted 
than 
the 
other, 
two 
kinds 
of 
4, 
etc. 


20.3 
The 
Gaussian 
Mixture 
Model 
Gaussians 
are 
particularly 
convenient 
continuous 
mixture 
components 
since 
they 
constitute 
`bumps’ 
of 
probability 
mass, 
aiding 
an 
intuitive 
interpretation 
of 
the 
model. 
As 
a 
reminder, 
a 
D 
dimensional 
Gaussian 
distribution 
for 
a 
continuous 
variable 
x 
is 




11 


p 
(xjm, 
S)= 
pexp- 
(x 
- 
m)T 
S..1 
(x 
- 
m)(20.3.1)

det 
(2S)2 


where 
m 
is 
the 
mean 
and 
S 
is 
the 
covariance 
matrix. 
A 
mixture 
of 
Gaussians 
is 
then 


H

Xp

p 
(x)= 
p(xjmi, 
Si)p(i) 
(20.3.2) 


i=1 


	

where 
p(i) 
is 
the 
mixture 
weight 
for 
component 
i. 
For 
a 
set 
of 
data 
X 
=x1 
;:::, 
xNand 
under 
the 
usual 


i.i.d. 
assumption, 
the 
log 
likelihood 
is 


XX

log 
p(Xj)= 
N
log 
H
p(i)p
1exp- 
1(xn 
- 
mi)T 
S..1 
(xn 
- 
mi)(20.3.3)

i

det 
(2Si)2 


n=1 
i=1 


where 
the 
parameters 
are 
. 
= 
fmi, 
Si;p(i);i 
=1;:::;Hg. 
The 
optimal 
parameters 
. 
can 
be 
set 
using 
Maximum 
Likelihood, 
bearing 
in 
mind 
the 
constraint 
that 
the 
Si 
must 
be 
symmetric 
positive 
denite 


P

matrices, 
in 
addition 
to 
0 
= 
p(i) 
= 
1,i 
p(i) 
= 
1. 
Gradient 
based 
optimisation 
approaches 
are 
feasible 
under 
a 
parameterisation 
of 
the 
Si 
(e.g. 
Cholesky 
decomposition) 
and 
p(i)(e.g. 
softmax) 
that 
enforce 
the 
constraints. 
An 
alternative 
is 
the 
EM 
approach 
which 
in 
this 
case 
is 
particularly 
convenient 
since 
it 
automatically 
provides 
parameter 
updates 
that 
ensure 
these 
constraints. 


20.3.1 
EM 
algorithm 
The 
energy 
term 
is 


XNN

Xp

hlog 
p(xn;i)iold(ijxn) 
= 
hlog 
[p(xnjh)p(i)]iold(ijxn) 
(20.3.4)

pp
n=1 
n=1 


370 
DRAFT 
March 
9, 
2010 



The 
Gaussian 
Mixture 
Model 


Plugging 
in 
the 
denition 
of 
the 
Gaussian 
components, 
we 
have 


XX

HN11 


p 
old(ijxn) 
- 
(xn 
- 
mi)T 
S..1 
(xn 
- 
mi) 
- 
log 
det 
(2Si) 
+ 
log 
p(i)

2 
i 
2 


i=1 
n=1 


The 
M-step 
requires 
the 
maximisation 
of 
the 
above 
with 
respect 
to 
mi, 
Si;p(i). 


M-step 
: 
optimal 
mi 
Maximising 
equation 
(20.3.5) 
with 
respect 
to 
mi 
is 
equivalent 
to 
minimising 


NH

XXP

p 
old(ijxn)(xn 
- 
mi)T 
S..1 
(xn 
- 
mi)

i 
n=1 
i=1 


Dierentiating 
with 
respect 
to 
mi 
and 
equating 
to 
zero 
we 
have 
N

XP

..2 
p 
old(ijxn)S..1 
(xn 
- 
mi)= 
0

i 
n=1 


Hence, 
optimally, 
PN 
old(ijxn)xn

n=1 
p

mi 
=

PN 
old(ijxn)

n=1 
p

By 
dening 
the 
membership 
distribution 
old(ijxn)

p

old(nji) 


p 


PN 
old(ijxn)

n=1 
p

(20.3.5) 
(20.3.6) 
(20.3.7) 
(20.3.8) 
(20.3.9) 
which 
quanties 
the 
membership 
of 
datapoints 
to 
cluster 
i, 
we 
can 
write 
equation 
(20.3.8) 
more 
compactly 


as 


N

XP

old(nji)xn

mi 
= 
p 
n=1 


M-step 
: 
optimal 
Si 


Optimising 
equation 
(20.3.5) 
with 
respect 
to 
Si 
is 
equivalent 
to 
minimising 


N

XDPE

..

(n)TS..1n 
- 
log 
detS..1 


iii 
i

old(ijxn)

p

n=1 


where 
n 
= 
xn 
- 
mi. 
To 
aid 
the 
matrix 
calculus, 
we 
isolate 
the 
dependency 
on 
Si 
to 
give 


i 


N

XP..N

XP

trace 
S..1 
p 
old(ijxn)n 
(n)T 
- 
log 
detS..1 
p 
old(ijxn)

i 
iii
n=1 
n=1 


Dierentiating 
with 
respect 
to 
S..1 
and 
equating 
to 
zero, 
we 
obtain 


i 


NN

XXP

p 
old(ijxn)n 
(n)T 
- 
Si 
p 
old(ijxn)= 
0

ii 
n=1 
n=1 


Using 
the 
membership 
pold(nji), 
the 
optimal 
Si 
is 
given 
by 


N

XP

Si 
= 
p 
old(nji)(xn 
- 
mi)(xn 
- 
mi)T 
n=1 


DRAFT 
March 
9, 
2010 


(20.3.10) 
(20.3.11) 
(20.3.12) 
(20.3.13) 
(20.3.14) 

The 
Gaussian 
Mixture 
Model 



(a) 
1 
iteration 
(b) 
50 
iterations 
Figure 
20.7: 
Training 
a 
mixture 
of 
10 
isotropic 
Gaussians 
(a): 
If 
we 
start 
with 
large 
variances 
for 
the 
Gaussians, 
even 
after 
one 
iteration, 
the 
Gaussians 
are 
centred 
close 
to 
the 
mean 
of 
the 
data. 
(b): 
The 
Gaussians 
begin 
to 
separate 
(c): 
One 
by 
one, 
the 
Gaussians 
move 
towards 
appropriate 
parts 
of 
the 
data 
(d): 
The 
nal 
converged 
solution. 
The 
Gaussians 
are 
constrained 
to 
have 
variances 
greater 
than 
a 
set 
amount. 
See 
demoGMMem.m. 


(c) 
125 
iterations 
(d) 
150 
iterations 
This 
ensures 
that 
Si 
is 
symmetric 
positive 
semi-denite. 
A 
special 
case 
is 
to 
constrain 
the 
covariances 
Si 
to 
be 
diagonal 
for 
which 
the 
update 
is, 
see 
exercise(201), 


N

Xn

Si 
= 
p 
old(nji)diag(xn 
- 
mi)(xn 
- 
mi)T(20.3.15) 
n=1 


where 
above 
diag 
(M) 
means 
forming 
a 
new 
matrix 
from 
the 
matrix 
M 
with 
zero 
entries 
except 
for 
the 
diagonal 
entries 
of 
M. 
A 
more 
extreme 
case 
is 
that 
of 
isotropic 
Gaussians 
Si 
= 
2I. 
The 
reader 
may 
show 


i 


that 
the 
optimal 
update 
for 
2 
in 
this 
case 
is 
given 
by 
taking 
the 
average 
of 
the 
diagonal 
entries 
of 
the 


i 


diagonally 
constrained 
covariance 
update, 


Xn

2 
=
1 
N
p 
old(nji)(xn 
- 
mi)2 
(20.3.16)

i 
dim 
x 


n=1 


M-step 
: 
optimal 
mixture 
coecients 


If 
no 
constraint 
is 
placed 
on 
the 
weights, 
the 
update 
follows 
the 
general 
formula 
given 
in 
equation 
(20.2.7), 


N

p(i) 
= 
1 
N 
Xnn=1 
p 
old(ijxn) 
(20.3.17) 
E-step 
p 
old(ijxn) 
. 
p 
old(xnji)p(i) 
(20.3.18) 
Explicitly, 
this 
is 
given 
by 
the 
responsibility 
nop 
old(ijxn) 
= 
p(i) 
exp..1 
2 
(xn 
- 
mi)T 
S..1 
i 
(xn 
- 
mi)
Pil 
p(i0) 
expn..1 
2 
(xn 
- 
mil 
)T 
S..1 
il 
(xn 
- 
mil 
)o(20.3.19) 
The 
above 
equations 
(20.3.8,20.3.14,20.3.17,20.3.19) 
are 
iterated 
until 
convergence. 


The 
performance 
of 
EM 
for 
Gaussian 
mixtures 
can 
be 
strongly 
dependent 
on 
the 
initialisation, 
which 
we 
discuss 
below. 
In 
addition, 
constraints 
on 
the 
covariance 
matrix 
are 
required 
in 
order 
to 
nd 
sensible 
solutions. 


DRAFT 
March 
9, 
2010 



The 
Gaussian 
Mixture 
Model 


20.3.2 
Practical 
issues 
Innite 
troubles 


A 
diculty 
arises 
with 
using 
Maximum 
Likelihood 
to 
t 
a 
Gaussian 
mixture 
model. 
Consider 
placing 
a 
component 
p(xjmi, 
Si) 
with 
mean 
mi 
set 
to 
one 
of 
the 
datapoints 
mi 
= 
xn 
. 
The 
contribution 
from 
that 
Gaussian 
will 
be 


1- 
1 
(xn..xn)TS..1(xn..xn) 
1

p(xnjmi, 
Si)= 
pe 
2 
i 
= 
p(20.3.20)
det 
(2Si) 
det 
(2Si) 


In 
the 
limit 
that 
the 
`width’ 
of 
the 
covariance 
goes 
to 
zero 
(the 
eigenvalues 
of 
Si 
tend 
to 
zero), 
this 
probability 
density 
becomes 
innite. 
This 
means 
that 
one 
can 
obtain 
a 
Maximum 
Likelihood 
solution 
by 
placing 
zero-width 
Gaussians 
on 
a 
selection 
of 
the 
datapoints, 
resulting 
in 
an 
innite 
likelihood. 
This 
is 
clearly 
undesirable 
and 
arises 
because, 
in 
this 
case, 
the 
Maximum 
Likelihood 
solution 
does 
not 
constrain 
the 
parameters 
in 
a 
sensible 
way. 
Note 
that 
this 
is 
not 
related 
to 
the 
EM 
algorithm, 
but 
a 
property 
of 
the 
Maximum 
Likelihood 
method 
itself. 
All 
computational 
methods 
which 
aim 
to 
t 
unconstrained 
mixtures 
of 
Gaussians 
using 
Maximum 
Likelihood 
therefore 
succeed 
in 
nding 
`reasonable’ 
solutions 
merely 
by 
getting 
trapped 
in 
favourable 
local 
maxima. 
A 
remedy 
is 
to 
include 
an 
additional 
constraint 
on 
the 
width 
of 
the 
Gaussians, 
ensuring 
that 
they 
cannot 
become 
too 
small. 
One 
approach 
is 
to 
monitor 
the 
eigenvalues 
of 
each 
covariance 
matrix 
and 
if 
an 
update 
would 
result 
in 
a 
new 
eigenvalue 
smaller 
than 
a 
desired 
threshold, 
the 
update 
is 
rejected. 
In 
GMMem.m 
we 
use 
a 
similar 
approach 
in 
which 
we 
constrain 
the 
determinant 
(the 
product 
of 
the 
eigenvalues) 
of 
the 
covariances 
to 
be 
greater 
than 
a 
desired 
specied 
minimum 
value. 
One 
can 
view 
the 
formal 
failure 
of 
Maximum 
Likelihood 
in 
the 
case 
of 
Gaussian 
mixtures 
as 
a 
result 
of 
an 
inappropriate 
prior. 
Maximum 
Likelihood 
is 
equivalent 
to 
MAP 
in 
which 
a 
at 
prior 
is 
placed 
on 
each 
matrix 
Si. 
This 
is 
unreasonable 
since 
the 
matrices 
are 
required 
to 
be 
positive 
denite 
and 
of 
non-vanishing 
width. 
A 
Bayesian 
solution 
to 
this 
problem 
is 
possible, 
placing 
a 
prior 
on 
covariance 
matrices. 
The 
natural 
prior 
in 
this 
case 
is 
the 
Wishart 
Distribution, 
or 
a 
Gamma 
distribution 
in 
the 
case 
of 
a 
diagonal 
covariance. 


Initialisation 


A 
useful 
intialisation 
strategy 
is 
to 
set 
the 
covariances 
to 
be 
diagonal 
with 
large 
variances. 
This 
gives 
the 
components 
a 
chance 
to 
`sense’ 
where 
data 
lies. 
An 
illustration 
of 
the 
performance 
of 
the 
algorithm 
is 
given 
in 
g(20.7). 


Symmetry 
breaking 


If 
the 
covariances 
are 
initialised 
to 
large 
values, 
the 
EM 
algorithm 
appears 
to 
make 
little 
progress 
in 
the 
rst 
iterations 
as 
each 
component 
jostles 
with 
the 
others 
to 
try 
to 
explain 
the 
data. 
Eventually 
one 
Gaussian 
component 
breaks 
away 
and 
takes 
responsibility 
for 
explaining 
the 
data 
in 
its 
vicinity, 
see 
g(20.7). 
The 
origin 
of 
this 
jostling 
is 
an 
inherent 
symmetry 
in 
the 
solution 
– 
it 
makes 
no 
dierence 
to 
the 
likelihood 
if 
we 
relabel 
what 
the 
components 
are 
called. 
This 
permutation 
symmetry 
causes 
initial 
confusion 
as 
to 
which 
model 
should 
explain 
which 
parts 
of 
the 
data. 
Eventually, 
this 
symmetry 
is 
broken, 
and 
a 
local 
solution 
is 
found. 
The 
symmetries 
can 
severely 
handicap 
EM 
in 
tting 
a 
large 
number 
of 
models 
in 
the 
mixture 
since 
the 
number 
of 
permutations 
increases 
dramatically 
with 
the 
number 
of 
components. 
A 
heuristic 
is 
to 
begin 
with 
a 
small 
number 
of 
components, 
say 
two, 
for 
which 
symmetry 
breaking 
is 
less 
problematic. 
Once 
a 
local 
broken 
solution 
has 
been 
found, 
more 
models 
are 
included 
into 
the 
mixture, 
initialised 
close 
to 
the 
currently 
found 
solutions. 
In 
this 
way, 
a 
hierarchical 
breaking 
scheme 
is 
envisaged. 
Another 
popular 
method 
for 
initialisation 
is 
to 
center 
the 
means 
to 
those 
found 
by 
the 
K-means 
algorithm 
– 
however, 
this 
itself 
requires 
a 
heuristic 
initialisation. 


20.3.3 
Classication 
using 
Gaussian 
mixture 
models 
Consider 
data 
drawn 
from 
two 
classes, 
c 
2f1, 
2g. 
We 
can 
t 
a 
GMM 
p(xjc 
=1, 
X1) 
to 
the 
data 
X1 
from 
class 
1, 
and 
another 
GMM 
p(xjc 
=2, 
X2) 
to 
the 
data 
X2 
from 
class 
2. 
This 
gives 
rise 
to 
two 
class-conditional 
GMMs, 


H

Xp

p(xjc, 
Xc)= 
p(ijc)N 
(x 


mc
i 
, 
Sc) 
(20.3.21)

i 
i=1 


DRAFT 
March 
9, 
2010 



The 
Gaussian 
Mixture 
Model 


-10-8-6-4-2024681000.050.10.150.20.25
-10-8-6-4-2024681000.050.10.150.20.25
-10-8-6-4-20246810-180-160-140-120-100-80-60-40-200
(a) 
(b) 
(c) 
Figure 
20.8: 
(a): 
A 
Gaussian 
mixture 
model 
with 
H 
= 
4 
components. 
There 
is 
a 
component 
(purple) 
with 
large 
variance 
and 
small 
weight 
that 
has 
little 
eect 
on 
the 
distribution 
close 
to 
where 
the 
other 
three 
components 
have 
appreciable 
mass. 
As 
we 
move 
further 
away 
this 
additional 
component 
gains 
in 
inuence. 
(b): 
The 
GMM 
probability 
density 
function 
from 
(a). 
(c): 
Plotted 
on 
a 
log 
scale, 
the 
inuence 
of 
each 
Gaussian 
far 
from 
the 
origin 
becomes 
clearer. 


For 
a 
novel 
point 
x 
, 
the 
posterior 
class 
probability 
is 


* 


p(cjx 
, 
X 
) 
. 
p(x 
* 
jc, 
Xc)p(c) 
(20.3.22) 


where 
p(c) 
is 
the 
prior 
class 
probability. 
The 
Maximum 
Likelihood 
setting 
is 
that 
p(c) 
is 
proportional 
to 
the 
number 
of 
training 
points 
in 
class 
c. 


Consider 
a 
testpoint 
x 
* 
a 
long 
way 
from 
the 
training 
data 
for 
both 
classes. 
For 
such 
a 
point, 
the 
proba


classication 
condence 
drops 
and 
all 
classes 
become 
equally 
likely. 


A 
remedy 
for 
this 
situation 
is 
to 
include 
an 
additional 
component 
in 
the 
Gaussian 
mixture 
for 
each 
class 
that 
is 
very 
broad. 
We 
rst 
collect 
the 
input 
data 
from 
all 
classes 
into 
a 
dataset 
X 
, 
and 
let 
m 
be 
the 
mean 


bilitythateitherofthetwoclassmodelsgeneratedthedataisverylow.However,onewillbemuchlower 
thantheother(sinceGaussiansdropexponentiallyquickly),meaningthattheposteriorprobabilitywill 
becondentlycloseto1forthatclasswhichhasacomponentclosestto 
Thisisanunfortunateprop-x 
. 
ertysincewewouldendupcondentlypredictingtheclassofnoveldatathatisnotsimilartoanything 
we'veseenbefore.Wewouldprefertheoppositeeectthatfornoveldatafarfromthetrainingdata,the 
ofallthisdataand 
thecovariance.Thenforthemodelofeachclass 
dataweincludeanadditional 
S 
c 
H

Gaussian 
(dropping 
the 
notational 
dependency 
on 
X 
) 


X

p(xjc)= 


ci

p~

N 
(x


ci

, 
S


ci

c
H

)+~p 


m;S) 
(20.3.23) 


+1N 
(x

m


i=1 


where 




ci

i 
= 
H


p


(20.3.24)
ci

p~

.


i 
= 
H 
+1 


where 
d 
is 
a 
small 
positive 
value 
and 
. 
inates 
the 
covariance 
(we 
take 
d 
=0:0001 
and 
. 
= 
10 
in 
demoGMMclass.m). 
The 
eect 
of 
the 
additional 
component 
on 
the 
training 
likelihood 
is 
negligible 
since 
it 
has 
small 
weight 
and 
large 
variance 
compared 
to 
the 
other 
components, 
see 
g(20.8). 
However, 
as 
we 
move 
away 
from 
the 
region 
where 
the 
rst 
H 
components 
have 
appreciable 
mass, 
the 
additional 
component 
gains 
in 
inuence 
since 
it 
has 
a 
higher 
variance. 
If 
we 
include 
the 
same 
additional 
component 
in 
the 
GMM 
for 
each 
class 
c 
then 
the 
inuence 
of 
this 
additional 
component 
will 
be 
the 
same 
for 
each 
class, 
dominating 
as 
we 
move 
far 
from 
the 
inuence 
of 
the 
other 
components. 
For 
a 
point 
far 
from 
the 
training 
data 
the 
likelihood 
will 
be 
roughly 
equal 
for 
each 
class 
since 
in 
this 
region 
the 
additional 
broad 
component 
dominates 
with 
equal 
measure. 
The 
posterior 
distribution 
will 
then 
tend 
to 
the 
prior 
class 
probability 
p(c), 
mitigating 
the 
deleterious 
eect 
of 
a 
single 
GMM 
dominating 
when 
a 
testpoint 
is 
far 
from 
the 
training 
data. 


DRAFT 
March 
9, 
2010 



The 
Gaussian 
Mixture 
Model 


-8-6-4-20246810-6-4-2024681012
051015202530354000.20.40.60.81051015202530354000.20.40.60.81
Figure 
20.9: 
Class 
conditional 
GMM 
training 
and 
classication. 
(a): 
Data 
from 
two 
dierent 
classes. 
We 
t 
a 
GMM 
with 
two 
components 
to 
the 
data 
from 
each 
class. 
The 
(magenta) 
diamond 
is 
a 
test 
point 
far 
from 
the 
training 
data 
we 
will 
classify. 
(b): 
Upper 
subpanel 
are 
the 
class 
probabilities 
p(c 
=1jn) 
for 
the 
40 
training 
points, 
and 
the 
41st 
point, 
being 
the 
test 
point. 
The 
lower 
subpanel 
are 
the 


(a) 
(b) 
class 
probabilities 
but 
including 
the 
additional 
large 
variance 
Gaussian 
term. 
See 
demoGMMclass.m. 
Example 
89. 
The 
data 
in 
g(20.9a) 
has 
a 
cluster 
structure 
for 
each 
class. 
Based 
on 
tting 
a 
GMM 
to 
each 
of 
the 
two 
classes, 
a 
test 
point 
(diamond) 
far 
from 
the 
training 
data 
is 
condently 
classied 
as 
belonging 
to 
class 
1. 
This 
is 
an 
undesired 
eect 
since 
we 
would 
prefer 
that 
points 
far 
from 
the 
training 
data 
are 
not 
classied 
with 
any 
certainty. 
By 
including 
an 
additional 
large 
variance 
Gaussian 
component 
for 
each 
class 
this 
has 
little 
eect 
on 
the 
class 
probabilities 
of 
the 
training 
data, 
yet 
has 
the 
desired 
eect 
of 
making 
the 
class 
probability 
for 
the 
test 
point 
maximally 
uncertain, 
g(20.9b). 


20.3.4 
The 
Parzen 
estimator 
The 
Parzen 
density 
estimator 
is 
formed 
by 
placing 
a 
`bump 
of 
mass', 
(xjxn), 
on 
each 
datapoint, 


X

1 
N

p(x)= 
(xjxn) 
(20.3.25)

N 


n=1 


A 
popular 
choice 
is 
(for 
a 
D 
dimensional 
x) 


..



(xjxn)= 
Nx 


xn;2ID 
(20.3.26) 


giving 
the 
mixture 
of 
Gaussians 


N

1 
N 
1 
1 
n)2

- 
(x..x

p(x)= 
e 
22 
(20.3.27)

N 
(22)D=2 


n=1 


There 
is 
no 
training 
required 
for 
a 
Parzen 
estimator 
– 
only 
the 
positions 
of 
the 
N 
datapoints 
need 
storing. 
Whilst 
the 
Parzen 
technique 
is 
a 
reasonable 
and 
cheap 
way 
to 
form 
a 
density 
estimator, 
it 
does 
not 
enable 
us 
to 
form 
any 
simpler 
description 
of 
the 
data. 
In 
particular, 
we 
cannot 
perform 
clustering 
since 
there 
is 
no 
lower 
number 
of 
clusters 
assumed 
to 
underly 
the 
data 
generating 
process. 
This 
is 
in 
contrast 
to 
GMMs 
trained 
using 
Maximum 
Likelihood 
on 
a 
xed 
number 
H 
= 
N 
of 
components. 


20.3.5 
K-Means 
Consider 
a 
mixture 
of 
K 
isotropic 
Gaussians 
in 
which 
each 
covariance 
is 
constrained 
to 
be 
equal 
to 
2I, 


K

N 
..



p(x)= 
piNx 


mi;2I 
(20.3.28) 


i=1 


Whilst 
the 
EM 
algorithm 
breaks 
down 
if 
a 
Gaussian 
component 
is 
allowed 
to 
set 
mi 
equal 
to 
a 
datapoint 
with 
2 
. 
0, 
by 
constraining 
all 
components 
to 
have 
the 
same 
variance 
2, 
the 
algorithm 
has 
a 
well 


DRAFT 
March 
9, 
2010 



The 
Gaussian 
Mixture 
Model 


Algorithm 
19 
K-means 


1: 
Initialise 
the 
centres 
mi, 
i 
=1;:::;K. 
2: 
while 
not 
converged 
do 
3: 
For 
each 
centre 
i, 
nd 
all 
the 
xn 
for 
which 
i 
is 
the 
nearest 
(in 
Euclidean 
sense) 
centre. 
4: 
Call 
this 
set 
of 
points 
Ni. 
Let 
Ni 
be 
the 
number 
of 
datapoints 
in 
set 
Ni. 
5: 
Update 
the 
means 
1 


mnew 


xn

= 


i 
Ni 


n2Ni 


6: 
end 
while 
-8-6-4-202468-4-202468
11.522.533.544.550510152025
Figure 
20.10: 
(a): 
550 
datapoints 
clustered 
using 
K-means 
with 
3 
components. 
The 
means 
are 
given 
by 
the 
red 
crosses. 
(b): 
Evolution 
of 
the 
mean 
square 
distance 
to 
nearest 
centre 
with 
iterations 
of 
the 
algorithm. 
The 
means 
were 
initialised 
to 
close 
to 
the 
overall 
mean 
of 
the 
data. 
See 


demoKmeans.m.

(a) 
(b) 
dened 
limit 
as 
2 
. 
0. 
The 
reader 
may 
show, 
exercise(202), 
that 
in 
this 
case 
the 
membership 
distribution 
equation 
(20.3.9) 
becomes 
deterministic 




old(nji) 
/
1 
if 
mi 
is 
closest 
to 
xn 


p 
(20.3.29)

0 
otherwise 


In 
this 
limit 
the 
EM 
update 
(20.3.10) 
for 
the 
mean 
mi 
is 
given 
by 
taking 
the 
average 
of 
the 
points 
closest 
to 
mi. 
This 
limiting 
and 
constrained 
GMM 
then 
reduces 
to 
the 
so-called 
K-means 
algorithm, 
algorithm(19). 
Despite 
its 
simplicity 
the 
K-means 
algorithm 
converges 
quickly 
and 
often 
gives 
a 
reasonable 
clustering, 
provided 
the 
centres 
are 
initialised 
reasonably. 
See 
g(20.10). 


K-means 
is 
often 
used 
as 
a 
simple 
form 
of 
data 
compression. 
Rather 
than 
sending 
the 
datapoint 
xn, 
one 
sends 
instead 
the 
index 
of 
the 
centre 
to 
which 
it 
is 
associated. 
This 
is 
called 
vector 
quantisation 
and 
is 
a 
form 
of 
lossy 
compression. 
To 
improve 
the 
quality, 
more 
information 
can 
be 
transmitted 
such 
as 
an 
approximation 
of 
the 
dierence 
between 
x 
and 
the 
corresponding 
mean 
m, 
which 
can 
be 
used 
to 
improve 
the 
reconstruction 
of 
the 
compressed 
datapoint. 


20.3.6 
Bayesian 
mixture 
models 
Bayesian 
extensions 
include 
placing 
priors 
on 
the 
parameters 
of 
each 
model 
in 
the 
mixture, 
and 
also 
on 
the 
component 
distribution. 
In 
most 
cases 
this 
will 
give 
rise 
to 
the 
marginal 
likelihood 
being 
an 
intractable 
integral. 
Methods 
that 
approximate 
the 
integral 
include 
sampling 
techniques 
[98]. 
See 
also 
[109, 
67] 
for 
an 
approximate 
variational 
treatment 
focussed 
on 
Bayesian 
Gaussian 
mixture 
models. 


20.3.7 
Semi-supervised 
learning 
In 
some 
cases 
we 
may 
know 
to 
which 
mixture 
component 
certain 
datapoints 
belong. 
Given 
this 
information 
we 
want 
to 
t 
a 
mixture 
model 
with 
a 
specied 
number 
of 
components 
H 
and 
parameters 
. 
We 
write 


m

(v;hm), 
m 
=1;:::;M 
for 
the 
M 
known 
datapoints 
and 
corresponding 
components, 
and 
(vn;hn), 
n 
= 


* 


1;:::;N 
for 
the 
remaining 
datapoints 
whose 
components 
hn 
are 
unknown. 
We 
aim 
then 
to 
maximise 
the 


DRAFT 
March 
9, 
2010 



Mixture 
of 
Experts 


xn
hnyn
WU
n=1;:::;N
Figure 
20.11: 
Mixture 
of 
experts 
model. 
The 
prediction 
of 


n

the 
output 
y(real 
or 
continuous) 
given 
the 
input 
xn 
averages 
over 
individual 
experts 
p(ynjxn 
, 
whn 
). 
The 
expert 
hn 
is 
selected 
by 
the 
gating 
mechanism 
with 
probability 
p(hnjxn 
, 
U), 
so 
that 
some 
experts 
will 
be 
more 
able 
to 
predict 
the 
output 
for 
xn 
in 
`their’ 
part 
of 
the 
space. 
The 
parameters 
W, 
U 
can 
be 
learned 
by 
Maximum 
Likelihood 
after 
marginalising 
over 
the 
hidden 
expert 
variables. 


likelihood 


YYXY

p(v 
1:M 
;v 
1:N 
jh1:M 
;)=p(v 
m 
* 
;)njhn;)p(hn) 
(20.3.30)

jhm 
p(v

* 
* 


m 
nhn 


If 
we 
were 
to 
lump 
all 
the 
datapoints 
together, 
this 
is 
essentially 
equivalent 
to 
the 
standard 
unsupervised 
case, 
expect 
that 
some 
of 
the 
h 
are 
xed 
into 
known 
states. 
The 
only 
eect 
on 
the 
EM 
algorithm 
is 
therefore 
in 
the 
terms 
pold(hjv) 
which 
are 
delta 
functions 
in 
the 
known 
state, 
resulting 
in 
a 
minor 
modication 
of 
the 
standard 
algorithm, 
exercise(205). 


20.4 
Mixture 
of 
Experts 
The 
mixture 
of 
experts 
model[149] 
is 
related 
to 
discriminative 
training 
of 
an 
output 
y 
distribution 
conditioned 
on 
an 
input 
x. 
This 
can 
be 
used 
in 
either 
the 
regression 
of 
classication 
contexts 
and 
has 
the 
general 
form, 
see 
g(20.11), 


H

XY

p(yjx, 
W, 
U)= 
p(yjx, 
wh)p(hjx, 
U), 
(20.4.1) 


h=1 


Here 
h 
indexes 
the 
mixture 
component. 
Each 
expert 
has 
parameters 
W 
=[w1;:::, 
wH 
] 
and 
corresponding 
gating 
parameters 
U 
=[u1;:::, 
uH 
]. 
Unlike 
a 
standard 
mixture 
model, 
the 
component 
distribution 
p(hjx, 
U) 
is 
dependent 
on 
the 
input 
x. 
This 
so-called 
gating 
distribution 
is 
conventionally 
taken 
to 
be 
of 
the 
softmax 
form 


T

ux

e

p(hjx, 
U)= 
P
h(20.4.2)

T

ux

h

h 
e

The 
idea 
is 
that 
we 
have 
a 
set 
of 
H 
predictive 
models 
(experts), 
p(yjx, 
wh), 
each 
with 
a 
dierent 
parameter 
wh, 
h 
=1;:::;H. 
How 
suitable 
model 
h 
is 
for 
predicting 
with 
the 
current 
input 
x 
is 
determined 
by 
the 
alignment 
of 
input 
x 
with 
the 
weight 
vector 
uh. 
In 
this 
way 
the 
input 
x 
is 
softly 
assigned 
to 
the 
appropriate 
experts. 


Maximum 
Likelihood 
training 
can 
be 
achieved 
using 
a 
form 
of 
EM. 
We 
will 
not 
derive 
the 
EM 
algorithm 
for 
the 
mixture 
of 
experts 
model 
in 
full, 
merely 
pointing 
the 
direction 
along 
which 
the 
derivation 
would 
continue. 
For 
a 
single 
datapoint 
x, 
the 
EM 
energy 
term 
is

hlog 
p(yjx, 
wh)p(hjx, 
U)ip(hjx;Wold 
;Uold) 
(20.4.3) 


For 
regression 
a 
simple 
choice 
is 




T

p(yjx, 
wh)= 
Ny 


xwh;2(20.4.4) 


and 
for 
(binary) 
classication 


T

p(y 
=1jx, 
wh)= 
(xwh) 
(20.4.5) 


DRAFT 
March 
9, 
2010 
377 



Indicator 
Models 


In 
both 
cases 
computing 
the 
derivatives 
of 
the 
energy 
with 
respect 
to 
the 
parameters 
W 
is 
straightforward, 
so 
that 
an 
EM 
algorithm 
is 
readily 
available. 
An 
alternative 
to 
EM 
is 
to 
compute 
the 
gradient 
of 
the 
likelihood 
directly 
using 
the 
standard 
approach 
discussed 
in 
section(11.7). 


A 
Bayesian 
treatment 
is 
to 
consider 


Z

X

p(yjx)=p(yjx, 
wh)p(hjx, 
u)p(W)p(U) 
(20.4.6) 


W;U

h 


QQ

where 
it 
is 
conventional 
to 
assume 
p(W)=h 
p(wh), 
p(U)=h 
p(uh). 
The 
integrals 
are 
generally 
intractable 
and 
approximations 
are 
required. 
See 
[290] 
for 
a 
variational 
treatment 
for 
regression 
and 
[43] 
for 
a 
variational 
treatment 
of 
classication. 
An 
extension 
to 
Bayesian 
model 
selection 
in 
which 
the 
number 
of 
experts 
is 
estimated 
is 
considered 
in 
[143]. 


20.5 
Indicator 
Models 
In 
the 
indicator 
approach 
we 
specify 
a 
distribution 
over 
the 
cluster 
assignments. 
For 
consistency 
with 
the 
literature 
we 
use 
an 
indicator 
z, 
as 
opposed 
to 
a 
hidden 
variable 
h, 
although 
they 
play 
the 
same 
role. 
A 
clustering 
model 
with 
parameters 
. 
on 
the 
component 
models 
and 
joint 
indicator 
prior 
p(z1:N 
) 
takes 
the 
form 


X

p(v 
1:N 
j)=p(v 
1:N 
jz 
1:N 
;)p(z 
1:N 
) 
(20.5.1) 


1:N
z

n

Since 
the 
zindicate 
cluster 
membership, 


N

XYX

p(v 
1:N 
j)=p(z 
1:N 
) 
p(v 
njz 
n;) 
(20.5.2) 
z1:N 
n=1 


Below 
we 
discuss 
the 
role 
of 
dierent 
indicator 
priors 
p(z1:N 
) 
in 
clustering. 


20.5.1 
Joint 
indicator 
approach: 
factorised 
prior 
Assuming 
prior 
independence 
of 
indicators, 


N

YX

p(z 
1:N 
)= 
p(z 
n);z 
n 
2f1;:::;K} 
(20.5.3) 


n=1 


we 
obtain 
from 
equation 
(20.5.2) 


NN

XYYX

p(v 
1:N 
j)=p(v 
njz 
n;)p(z 
n)= 
p(v 
njz 
n;)p(z 
n) 
(20.5.4) 
z1:N 
n=1 
n=1zn 


which 
recovers 
the 
standard 
mixture 
model 
equation 
(20.1.2). 
As 
we 
discuss 
below, 
more 
sophisticated 
joint 
indicator 
priors 
can 
be 
used 
to 
explicitly 
control 
the 
complexity 
of 
the 
indicator 
assignments 
and 
open 
the 
path 
to 
essentially 
`innite 
dimensional’ 
models. 


20.5.2 
Joint 
indicator 
approach 
: 
Polya 
prior 
For 
a 
large 
number 
of 
available 
clusters 
(mixture 
components) 
K 
» 
1, 
using 
a 
factorised 
joint 
indicator 
distribution 
could 
potentially 
lead 
to 
overtting, 
resulting 
in 
little 
or 
no 
meaningful 
clustering. 
One 
way 
to 
control 
the 
eective 
number 
of 
components 
that 
are 
used 
is 
via 
a 
parameter 
p 
that 
regulates 
the 
complexity, 


()

Z

YX

p(z 
1:N 
)=p(z 
nj)p() 
(20.5.5) 


n 


378 
DRAFT 
March 
9, 
2010 



Indicator 
Models 


z1
v1
zN
vN
. 



z1
v1
zN
vN

(b)
. 


(a) 
(c) 

zn
vn
N
Figure 
20.12: 
(a): 
A 
generic 
mixture 
model 
for 


1:Nn
data 
v. 
Each 
zindicates 
the 
cluster 
of 
each 


n

datapoint. 
. 
is 
a 
set 
of 
parameters 
and 
z= 
k 


n

selects 
parameter 
k 
for 
datapoint 
v. 
(b): 
For 
a 
potentially 
large 
number 
of 
clusters 
one 
way 
to 
control 
complexity 
is 
to 
constrain 
the 
joint 
indicator 
distribution. 
(c): 
Plate 
notation 
of 
(b). 


05101520050100150200250
05101520050100150200250
05101520050100150200250
(a) 
(b) 
(c) 
Figure 
20.13: 
The 
number 
of 
unique 
clusters 
U 
when 
indicators 
are 
sampled 
from 
a 
Polya 
distribution 
equation 
(20.5.8), 
with 
a 
= 
2, 
and 
N 
= 
50 
datapoints. 
(a): 
K 
= 
50, 
(b): 
K 
= 
100, 
(c): 
K 
= 
1000. 
Even 
though 
the 
number 
of 
available 
clusters 
K 
is 
larger 
than 
the 
number 
of 
datapoints, 
the 
eective 
number 
of 
used 
clusters 
remains 
constrained. 
See 
demoPolya.m. 


where 
p(zj) 
is 
a 
categorical 
distribution, 


n 


p(z 
= 
kj)= 
k 
(20.5.6) 


A 
convenient 
choice 
for 
p() 
is 
the 
Dirichlet 
distribution 
(since 
this 
is 
conjugate 
to 
the 
categorical 
distribution), 


K

YX

=K..1 


p() 
= 
Dirichlet 
(j) 
. 
(20.5.7)

k 
k=1 


The 
integral 
over 
p 
in 
equation 
(20.5.5) 
can 
be 
performed 
analytically 
to 
give 
a 
Polya 
distribution: 


K

YXX

..() 
..(Nk 
+ 
=K) 


n 


p(z 
1:N 
)= 
;Nk 
I[z 
= 
k] 
(20.5.8)

..(N 
+ 
) 
..(=K)

k=1 
n 


P

The 
number 
of 
unique 
clusters 
used 
is 
then 
given 
by 
U 
=I[Nk 
> 
0]. 
The 
distribution 
over 
likely 


k 


cluster 
numbers 
is 
controlled 
by 
the 
parameter 
. 
The 
scaling 
=K 
in 
equation 
(20.5.7) 
ensures 
a 
sensible 
limit 
as 
K 
!1, 
see 
g(20.13), 
in 
which 
limit 
the 
models 
are 
known 
as 
Dirichlet 
process 
mixture 
models. 
This 
approach 
means 
that 
we 
do 
not 
need 
to 
explicitly 
constrain 
the 
number 
of 
possible 
components 
K 
since 
the 
number 
of 
active 
components 
U 
remains 
limited 
even 
for 
very 
large 
K. 


Clustering 
is 
achieved 
by 
considering 
argmax 
p(z1:N 
jv1:N 
). 
In 
practice 
it 
is 
common 
to 
consider 


1:N
z

argmax 
p(z 
njv 
1:N 
) 
(20.5.9) 


zn 


Unfortunately, 
posterior 
inference 
of 
p(znjv1:N 
) 
for 
this 
class 
of 
models 
is 
formally 
computationally 
intractable 
and 
approximate 
inference 
techniques 
are 
required. 
A 
detailed 
discussion 
of 
these 
techniques 
is 
beyond 
the 
scope 
of 
this 
book 
and 
we 
refer 
the 
reader 
to 
[165] 
for 
a 
deterministic 
(variational) 
approach 
and 
[206] 
for 
a 
discussion 
of 
sampling 
approaches. 


DRAFT 
March 
9, 
2010 



Mixed 
Membership 
Models 


a 


n
znw
vnw

Wn
N
Figure 
20.14: 
Latent 
Dirichlet 
Allocation. 
For 
document 
n 
we 
rst 
sample 
a 
distribution 
of 
topics 
n. 
Then 
for 
each 
word 
position 
w 
in 
the 
document 


we 
sample 
a 
topic 
z


nw

from 
the 
topic 
distribution. 
Given 
the 
topic 
we 
then 


sample 
a 
word 
from 
the 
word 
distribution 
of 
that 
topic. 
The 
parameters 
of 
the 
model 
are 
the 
word 
distributions 
for 
each 
topic 
, 
and 
the 
parameters 
of 
the 
topic 
distribution 
. 



20.6 
Mixed 
Membership 
Models 
Unlike 
standard 
mixture 
models 
in 
which 
each 
object 
is 
assumed 
to 
have 
been 
generated 
from 
a 
single 
cluster, 
in 
mixed 
membership 
models 
an 
object 
may 
be 
a 
member 
of 
more 
than 
one 
group. 
Latent 
Dirichlet 
Allocation 
discussed 
below 
is 
an 
example 
of 
such 
a 
mixed 
membership 
model, 
and 
is 
one 
of 
a 
number 
of 
models 
developed 
in 
recent 
years 
[4, 
91]. 


20.6.1 
Latent 
Dirichlet 
allocation 
So 
far 
we've 
considered 
clustering 
in 
the 
sense 
that 
each 
observation 
is 
assumed 
to 
have 
been 
generated 
from 
a 
single 
cluster. 
In 
contrast, 
Latent 
Dirichlet 
Allocation[44] 
and 
related 
methods 
are 
generative 
mixed 
membership 
models 
in 
which 
each 
datapoint 
may 
belong 
to 
more 
than 
a 
single 
cluster. 
A 
typical 
application 
is 
to 
identify 
topic 
clusters 
in 
a 
collection 
of 
documents. 
A 
single 
document 
contains 
a 
sequence 
of 
words, 
for 
example 


v 
=(the, 
cat, 
sat, 
on, 
the, 
cat, 
mat) 
(20.6.1) 


If 
each 
word 
in 
the 
available 
dictionary 
is 
assigned 
to 
a 
unique 
state 
(say 
dog 
=1, 
tree 
=2, 
cat 
=3;:::), 
we 
can 
represent 
then 
the 
nth 
document 
as 
a 
vector 


..

n

=


n

nW

n;v 


n
i

2f1;:::;D} 
(20.6.2)


v 


v1 
;:::;v 


where 
Wn 
is 
the 
number 
of 
words 
in 
the 
nth 
document. 
The 
number 
of 
words 
Wn 
in 
each 
document 
can 
vary 
although 
the 
overall 
dictionary 
from 
which 
they 
came 
is 
xed. 


The 
aim 
is 
to 
nd 
common 
topics 
in 
documents, 
assuming 
that 
any 
document 
could 
potentially 
contain 
more 
than 
one 
topic. 
It 
is 
useful 
to 
think 
rst 
of 
an 
underlying 
generative 
model 
of 
words, 
including 
latent 
topics 
(which 
we 
will 
later 
integrate 
out). 
We 
rst 
sample 
a 
probability 
distribution 
(histogram) 
that 
represents 
the 
topics 
likely 
to 
occur 
for 
this 
document. 
Then, 
for 
each 
word-position 
in 
the 
document, 
sample 
a 
topic 
and 
subsequently 
a 
word 
from 
the 
distribution 
of 
words 
for 
that 
topic. 
Mathematically, 
for 


document 
n 
and 
the 
wth 
word-position 
in 
the 
document, 
v

nw

, 
we 
use 
z


nw

2f1;:::;K} 
to 
indicate 
which 
of 
the 
K 
possible 
topics 
that 
word 
belongs. 
For 
each 
topic 
k, 
one 
then 
has 
a 
categorical 
distribution 
over 
all 


the 
words 
i 
=1;:::;D, 
in 
the 
dictionary: 


p(v 


n
w 


= 
ijz 


n
w 


= 
k, 
)= 
ijk 
(20.6.3) 


The 
`animal’ 
topic 
has 
high 
probability 
to 
emit 
animal-like 
words, 
etc. 
For 
each 
document 
n 
we 
have 
a 


P

K
k

=1 


its 
topic 
membership. 
For 
example, 
document 
n 
(which 
discusses 
issues 
related 
to 
wildlife 
conservation) 
might 
have 
a 
topic 
distribution 
with 
high 
mass 
on 
the 
latent 
`animals’ 
and 
`environment', 
topics. 
Note 
that 
the 
topics 
are 
indeed 
latent 
– 
the 
name 
`animal’ 
would 
be 
given 
post-hoc 
based 
on 
the 
kinds 
of 
words 


DRAFT 
March 
9, 
2010 


distribution 
of 
topics 
n 
with 


p


n
k

= 
1 
which 
gives 
a 
latent 
description 
of 
the 
document 
in 
terms 
of 



Mixed 
Membership 
Models 


that 
the 
latent 
topic 
would 
generate, 
ijk. 
As 
in 
section(20.5.2), 
to 
control 
complexity 
one 
may 
use 
a 
Dirichlet 
prior 
to 
limit 
the 
number 
of 
topics 
active 
in 
any 
particular 
document: 


p(nj) 
= 
Dirichlet 
(nj) 
(20.6.4) 


where 
a 
is 
a 
vector 
of 
length 
the 
number 
of 
topics. 


n

A 
generative 
model 
for 
sampling 
a 
document 
vwith 
Wn 
word 
positions 
is: 


1. 
Choose 
n 
~ 
Dirichlet 
(nj) 
n

2. 
For 
each 
of 
word 
position 
v, 
w 
=1;:::;Wn 
:
w

nn

(a) 
Choose 
a 
topic 
z~ 
p 
(zjn)
ww

..

nn

(b) 
Choose 
a 
word 
v~ 
pvjjzn
ww

w

Training 
the 
LDA 
model 
corresponds 
to 
learning 
the 
parameters 
, 
which 
relates 
to 
the 
number 
of 
topics, 
and 
, 
which 
describes 
the 
distribution 
of 
words 
within 
each 
topic. 
Unfortunately, 
nding 
the 
requisite 
marginals 
for 
learning 
from 
the 
posterior 
is 
formally 
computationally 
intractable. 
Ecient 
approximate 
inference 
for 
this 
class 
of 
models 
is 
a 
topic 
of 
research 
interest 
and 
both 
variational 
and 
sampling 
approaches 
have 
recently 
been 
developed[44, 
273, 
225]. 


There 
are 
close 
similarities 
between 
LDA 
and 
PLSA[113], 
section(15.6.1), 
both 
of 
which 
describe 
a 
document 
in 
terms 
of 
a 
distribution 
over 
latent 
topics. 
LDA 
is 
a 
probabilistic 
model 
for 
which 
issues 
such 
as 
setting 
hyperparameters 
can 
be 
addressed 
using 
Maximum 
Likelihood. 
PLSA 
on 
the 
other 
hand 
is 
essentially 
a 
matrix 
decomposition 
technique 
(such 
as 
PCA). 
Issues 
such 
as 
hyperparameters 
setting 
for 
PLSA 
are 
therefore 
addressed 
using 
validation 
data. 
Whilst 
PLSA 
is 
a 
description 
only 
of 
the 
training 
data, 
LDA 
is 
a 
generative 
data 
model 
and 
can 
in 
principle 
be 
used 
to 
synthesise 
new 
documents. 


Example 
90. 
An 
illustration 
of 
the 
use 
of 
LDA 
is 
given 
in 
g(20.15)[44]. 
The 
documents 
are 
taken 
from 
the 
TREC 
Associated 
Press 
corpus 
containing 
16,333 
newswire 
articles 
with 
23,075 
unique 
terms. 
After 
removing 
a 
standard 
list 
of 
stop 
words 
(frequent 
words 
such 
as 
`the',`a’ 
etc. 
that 
would 
otherwise 
dominate 
the 
statistics), 
the 
EM 
algorithm 
(with 
variational 
approximate 
inference) 
was 
used 
to 
nd 
the 
Dirichlet 
and 
conditional 
categorical 
parameters 
for 
a 
100-topic 
LDA 
model. 
The 
top 
words 
from 
four 
resulting 
categorical 
distributions 
ijk 
are 
illustrated 
g(20.15a). 
These 
distributions 
capture 
some 
of 
the 
underlying 
topics 
in 
the 
corpus. 
An 
example 
document 
from 
the 
corpus 
is 
presented 
along 
with 
the 
words 
coloured 
by 
the 
most 
probable 
latent 
topic 
they 
correspond 
to. 


20.6.2 
Graph 
based 
representations 
of 
data 
Mixed 
membership 
models 
are 
used 
in 
a 
variety 
of 
contexts 
and 
are 
distinguished 
also 
by 
the 
form 
of 
data 
available. 
Here 
we 
focus 
on 
analysing 
a 
representation 
of 
the 
interactions 
amongst 
a 
collection 
of 
objects; 
in 
particular, 
the 
data 
has 
been 
processed 
such 
that 
all 
the 
information 
of 
interest 
is 
characterised 
by 
an 
interaction 
matrix. 
For 
graph 
based 
representations 
of 
data, 
two 
objects 
are 
similar 
if 
they 
are 
neighbours 
on 
a 
graph 
representing 
the 
data 
objects. 
In 
the 
eld 
of 
social-networks, 
for 
example, 
each 
individual 
is 
represented 
as 
a 
node 
in 
a 
graph, 
with 
a 
link 
between 
two 
nodes 
if 
the 
individuals 
are 
friends. 
Given 
a 
graph 
one 
might 
wish 
to 
identify 
communities 
of 
closely 
linked 
friends. 
Interpreted 
as 
a 
social 
network, 
in 
g(20.16a), 
individual 
3 
is 
a 
member 
of 
his 
work 
group 
(1, 
2, 
3) 
and 
also 
the 
poker 
group 
(3, 
4, 
5). 
These 
two 
groups 
of 
individuals 
are 
otherwise 
disjoint. 
Discovering 
such 
groupings 
contrasts 
with 
graph 
partit
ioning 
in 
which 
each 
node 
is 
assigned 
to 
only 
one 
of 
a 
set 
of 
subgraphs, 
g(20.16b), 
for 
which 
a 
typical 
criterion 
is 
that 
each 
subgraph 
should 
be 
roughly 
of 
the 
same 
size 
and 
that 
there 
are 
few 
connections 
between 
the 
subgraphs[156]. 


DRAFT 
March 
9, 
2010 



Mixed 
Membership 
Models 


Arts 
Budgets 
Children 
Education 
new 
million 
children 
school 
lm 
tax 
women 
students 
show 
program 
people 
schools 
music 
budget 
child 
education 
movie 
billion 
years 
teachers 
play 
federal 
families 
high 
musical 
year 
work 
public 
best 
spending 
parents 
teacher 
actor 
new 
says 
bennett 
rst 
state 
family 
manigat 
york 
plan 
welfare 
namphy 
opera 
money 
men 
state 
theater 
programs 
percent 
president 
actress 
government 
care 
elementary 
love 
congress 
life 
haiti 


The 
William 
Randolph 
Hearst 
Foundation 
will 
give 
$ 
1.25 
million 
to 
Lincoln 
Center, 
Metropolitan 
Opera 
Co., 
New 
York 
Philharmonic 
and 
Juilliard 
School. 
Our 
board 
felt 
that 
we 
had 
a 
real 
opportunity 
to 
make 
a 
mark 
on 
the 
future 
of 
the 
performing 
arts 
with 
these 
grants 
an 
act 
every 
bit 
as 
important 
as 
our 
traditional 
areas 
of 
support 
in 
health, 
medical 
research, 
education 
and 
the 
social 
services, 
Hearst 
Foundation 
President 
Randolph 


A. 
Hearst 
said 
Monday 
in 
announcing 
the 
grants. 
Lincoln 
Centers 
share 
will 
be 
$200,000 
for 
its 
new 
building, 
which 
will 
house 
young 
artists 
and 
provide 
new 
public 
facilities. 
The 
Metropolitan 
Opera 
Co. 
and 
New 
York 
Philharmonic 
will 
receive 
$400,000 
each. 
The 
Juilliard 
School, 
where 
music 
and 
the 
performing 
arts 
are 
taught, 
will 
get 
$250,000. 
The 
Hearst 
Foundation, 
a 
leading 
supporter 
of 
the 
Lincoln 
Center 
Consolidated 
Corporate 
Fund, 
will 
make 
its 
usual 
annual 
$100,000 
donation, 
too. 
(b)
(a) 
Figure 
20.15: 
(a): 
A 
subset 
of 
the 
latent 
topics 
discovered 
by 
LDA 
and 
the 
high 
probability 
words 
associated 
with 
each 
topic. 
Each 
column 
represents 
a 
topic, 
with 
the 
topic 
name 
such 
as 
`art’ 
assigned 
by 
hand 
after 
viewing 
the 
most 
likely 
words 
corresponding 
to 
the 
topic. 
(b): 
A 
document 
from 
the 
training 
data 
in 
which 
the 
words 
are 
coloured 
according 
to 
the 
most 
likely 
latent 
topic. 
This 
demonstrates 
the 
mixed-membership 
nature 
of 
the 
model, 
assigning 
the 
datapoint 
(document 
in 
this 
case) 
to 
several 
clusters 
(topics). 
Reproduced 
from 
[44]. 


1 
2 


Figure 
20.16: 
(a) 
The 
social 
network 
of 
a 
set 
of 
5 
individuals, 
represented 
as 
an 
undirected 
graph. 
Here 
individual 
3 
belongs 
to 
the 
group 



3 
(1, 
2, 
3) 
and 
also 
(3, 
4, 
5). 
(b) 
By 
contrast, 
in 
graph 
partitioning, 
one 


4512
breaks 
the 
graph 
into 
roughly 
equally 
sized 
disjoint 
partitions 
such 


3 


45
(a) 
(b) 
that 
each 
node 
is 
a 
member 
of 
only 
a 
single 
partition, 
with 
a 
minimal 
number 
of 
edges 
between 
partitions. 


Another 
example 
is 
that 
nodes 
in 
the 
graph 
represent 
products 
and 
a 
link 
between 
nodes 
i 
and 
j 
indicates 
that 
customers 
who 
by 
product 
i 
frequently 
also 
buy 
product 
j. 
The 
aim 
is 
to 
decompose 
the 
graph 
into 
groups, 
each 
corresponding 
to 
products 
that 
are 
commonly 
co-bought 
by 
customers[116]. 
A 
growing 
area 
of 
application 
of 
graph 
based 
representations 
is 
in 
bioinformatics 
in 
which 
nodes 
represent 
genes, 
and 
a 
link 
between 
them 
representing 
that 
the 
two 
genes 
have 
similar 
activity 
proles. 
The 
task 
is 
then 
to 
identify 
groups 
of 
similarly 
behaving 
genes[5]. 


20.6.3 
Dyadic 
data 
Consider 
two 
kinds 
of 
objects, 
for 
example, 
lms 
and 
customers. 
Each 
lm 
is 
indexed 
by 
f 
=1;:::;F 
and 
each 
user 
by 
u 
=1;:::;U. 
The 
interaction 
of 
user 
u 
with 
lm 
f 
can 
be 
described 
by 
the 
element 
of 
a 
matrix 
Muf 
representing 
the 
rating 
a 
user 
gives 
to 
a 
lm. 
A 
dyadic 
dataset 
consists 
of 
such 
a 
matrix 
and 
the 
aim 
is 
to 
decompose 
this 
matrix 
to 
explain 
the 
ratings 
by 
nding 
types 
of 
lms 
and 
types 
of 
user. 


Another 
example 
is 
to 
consider 
a 
collection 
of 
documents, 
summarised 
by 
an 
interaction 
matrix 
in 
which 
Mwd 
is1if 
word 
w 
appears 
in 
document 
d 
and 
zero 
otherwise. 
This 
matrix 
can 
be 
represented 
as 
a 
bipartite 
graph, 
as 
in 
g(20.17a). 
The 
upper 
nodes 
represent 
documents, 
and 
the 
lower 
nodes 
words, 
with 
a 
link 
between 
them 
if 
that 
word 
occurs 
in 
that 
document. 
One 
the 
seeks 
assignments 
of 
documents 
to 
groups 
or 
latent 
`topics’ 
to 
succinctly 
explain 
the 
link 
structure 
of 
the 
bipartite 
graph 
via 
a 
small 
number 
of 
latent 
nodes, 
as 
schematically 
depicted 
in 
g(20.17b). 
One 
may 
view 
this 
as 
a 
form 
of 
matrix 
factorisation[136, 
189] 


X

Mwd 
UwtV 
T 
(20.6.5)

td 
t 


where 
t 
indexes 
the 
topics 
and 
the 
feature 
matrices 
U 
and 
V 
control 
the 
word-to-topic 
mapping 
and 
the 
topic-to-document 
mapping. 
This 
diers 
from 
latent 
Dirichlet 
allocation 
which 
has 
a 
probabilistic 
interpretation 
of 
rst 
generating 
a 
topic 
and 
then 
a 
word, 
conditional 
on 
the 
chosen 
topic. 
Here 
the 
interaction 
between 
document-topic 
matrix 
V 
and 
word-topic 
matrix 
U 
is 
non-probabilistic. 
More 
generally, 
we 
can 


382 
DRAFT 
March 
9, 
2010 



Mixed 
Membership 
Models 



(a) 
Figure 
20.17: 
Graphical 
representation 
of 
dyadic 
data. 
(a): 
There 
are 
6 
documents 
and 
13 
words. 
A 
link 
represents 
that 
a 
particular 
word-
document 
pair 
occurs 
in 
the 
dataset. 
(b): 
A 
latent 
decomposition 
of 
(a) 
using 
3 
`topics'. 
A 
topic 
corresponds 
to 
a 
collection 
of 
words, 
and 
each 
document 
a 
collection 
of 
topics. 
The 
open 
nodes 
indicate 
latent 
variables. 


(b) 
write 
a 
distribution 


p(MjU, 
V) 
(20.6.6) 
where 
U 
and 
V 
are 
feature 
matrices. 
In 
[189], 
real-valued 
data 
is 
modelled 
using 






p(MjU, 
W, 
V)= 
NM 


UWVT;2I(20.6.7) 
where 
U 
and 
V 
are 
assumed 
binary 
and 
the 
real-valued 
W 
is 
a 
topic-interaction 
matrix. 
In 
this 
viewpoint 


learning 
then 
consists 
of 
inferring 
U,W,V, 
given 
the 
dyadic 
observation 
matrix 
M. 
Assuming 
factorised 
priors, 
the 
posterior 
over 
the 
matrices 
is 
p(U, 
W, 
VjM) 
. 
p(MjU, 
W, 
V)p(U)p(W)p(V) 
(20.6.8) 


A 
convenient 
choice 
is 
a 
Gaussian 
prior 
distribution 
for 
W, 
with 
the 
feature 
matrices 
U 
and 
V 
sampled 
from 
Beta-Bernoulli 
priors. 
The 
resulting 
posterior 
distribution 
is 
formally 
computationally 
intractable, 
and 
in 
[189] 
this 
is 
addressed 
using 
a 
sampling 
approximation. 


20.6.4 
Monadic 
data 
In 
monadic 
data 
there 
is 
only 
one 
type 
of 
object 
and 
the 
interaction 
between 
the 
objects 
is 
represented 
by 
a 
square 
interaction 
matrix. 
For 
example 
one 
might 
have 
a 
matrix 
with 
elements 
Aij 
= 
1 
if 
proteins 
i 
and 
j 
can 
bind 
to 
each 
other 
and 
0 
otherwise. 
A 
depiction 
of 
the 
interaction 
matrix 
is 
given 
by 
a 
graph 
in 
which 
an 
edge 
represents 
an 
interaction, 
for 
example 
g(20.18). 
In 
the 
following 
section 
we 
discuss 
a 
particular 
mixed 
membership 
model 
and 
highlight 
potential 
applications. 
The 
method 
is 
based 
on 
clique 
decompositions 
of 
graphs 
and 
as 
such 
we 
require 
a 
short 
digression 
into 
clique-based 
graph 
representations. 


20.6.5 
Cliques 
and 
adjacency 
matrices 
for 
monadic 
binary 
data 
A 
symmetric 
adjacency 
matrix 
has 
elements 
Aij 
2f0, 
1g, 
with 
a 
1 
indicating 
a 
link 
between 
nodes 
i 
and 


j. 
For 
the 
graph 
in 
g(20.18), 
the 
adjacency 
matrix 
is 
.


.


1 
1 
1 
0 
1 
1 
1 
1 
1 
1 
1 
1 
0 
1 
1 
1 


B. 


C.


A 
= 


(20.6.9) 
where 
we 
include 
self 
connections 
on 
the 
diagonal. 
Given 
A, 
our 
aim 
is 
to 
nd 
a 
`simpler’ 
description 
that 
reveals 
the 
underlying 
cluster 
structure, 
such 
as 
(1, 
2, 
3) 
and 
(2, 
3, 
4) 
in 
g(20.18). 
Given 
the 
undirected 
graph 
in 
g(20.18), 
the 
incidence 
matrix 
Finc 
is 
an 
alternative 
description 
of 
the 
adjacency 
structure[81]. 


1 



4 


23
DRAFT 
March 
9, 
2010 


Figure 
20.18: 
The 
minimal 
clique 
cover 
is 
(1, 
2, 
3), 
(2, 
3, 
4). 



Mixed 
Membership 
Models 


Figure 
20.19: 
Bipartite 
representations 
of 
the 
decompositions 
of 
g(20.18). 
Shaded 
nodes 
represent 
observed 
vari


1 
2 
3 
41 
2 
3 
4 


ables, 
and 
open 
nodes 
latent 
variables. 
(a) 
Incidence 
ma


(a) 
(b) 
trix 
representation. 
(b) 
Minimal 
clique 
decomposition. 


Given 
the 
V 
nodes 
in 
the 
graph, 
we 
construct 
Finc 
as 
follows: 
For 
each 
link 
i 
~ 
j 
in 
the 
graph, 
form 
a 
column 
of 
the 
matrix 
Finc 
with 
zero 
entries 
except 
for 
a 
1 
in 
the 
ith 
and 
jth 
row. 
The 
column 
ordering 
is 
arbitrary. 
For 
example, 
for 
the 
graph 
in 
g(20.18) 
an 
incidence 
matrix 
is 


.


.


1 
1 
0 
0 
0 
1 
0 
1 
1 
0 
0 
1 
1 
0 
1 
0 
0 
0 
1 
1 


B. 


C.


Finc 
= 


(20.6.10) 
The 
incidence 
matrix 
has 
the 
property 
that 
the 
adjacency 
structure 
of 
the 
original 
graph 
is 
given 
by 
the 
outer 
product 
of 
the 
incidence 
matrix 
with 
itself. 
The 
diagonal 
entries 
contain 
the 
degree 
(number 
of 
links) 
of 
each 
node. 
For 
our 
example, 
this 
gives 


.


.


2 
1 
1 
0 
1 
3 
1 
1 
1 
1 
3 
1 
0 
1 
1 
2 


B. 


C.


FincFT 


inc 


= 


(20.6.11) 
so 
that 






A 
= 
HFincFT 
(20.6.12)

inc

Here 
H() 
is 
the 
element-wise 
Heaviside 
step 
function, 
[H(M)]ij 
= 
1 
if 
Mij 
> 
0 
and 
is 
0 
otherwise. 
A 
useful 
viewpoint 
of 
the 
incidence 
matrix 
is 
that 
it 
identies 
two-cliques 
in 
the 
graph 
(here 
we 
are 
using 
the 
term 
`clique’ 
in 
the 
non-maximal 
sense). 
There 
are 
ve 
2-cliques 
in 
g(20.18), 
and 
each 
column 
of 
Finc 
species 
which 
elements 
are 
in 
each 
2-clique. 
Graphically 
we 
can 
depict 
this 
incidence 
decomposition 
as 
a 
bipartite 
graph, 
as 
in 
g(20.19a) 
where 
the 
open 
nodes 
represent 
the 
ve 
2-cliques. 
The 
incidence 
matrix 
can 
be 
generalised 
to 
describe 
larger 
cliques. 
Consider 
the 
following 
matrix 
as 
a 
decomposition 
for 
g(20.18), 
and 
its 
outer-product: 


.


0

1

.


10 


1 
1 
1 
0 
1 
2 
2 
1 
1 
2 
2 
1 


11 


11 


B. 


C.


, 


FFT 


= 


B. 


C.


F 
= 


(20.6.13) 
01 
0111 


The 
interpretation 
is 
that 
F 
represents 
a 
decomposition 
into 
two 
3-cliques. 
As 
in 
the 
incidence 
matrix, 
each 
column 
represents 
a 
clique, 
and 
the 
rows 
containing 
a 
`1’ 
express 
which 
elements 
are 
in 
the 
clique 
dened 
by 
that 
column. 
This 
decomposition 
can 
be 
represented 
as 
the 
bipartite 
graph 
of 
g(20.19b). 
For 
the 
graph 
of 
g(20.18), 
both 
Finc 
and 
F 
satisfy 






A 
= 
HFFT= 
HFincFT 
(20.6.14)

inc

One 
can 
view 
equation 
(20.6.14) 
as 
a 
form 
of 
binary 
matrix 
factoristation 
of 
the 
binary 
square 
(symmetric) 
matrix 
A 
into 
non-square 
binary 
matrices. 
For 
our 
clustering 
purposes, 
the 
decomposition 
using 
F 
is 
to 
be 
preferred 
to 
the 
incidence 
decomposition 
since 
F 
decomposes 
the 
graph 
into 
a 
smaller 
number 
of 
larger 
cliques. 
A 
formal 
specication 
of 
the 
problem 
of 
nding 
a 
minimum 
number 
of 
maximal 
fully-connected 


subsets 
is 
the 
computational 
problem 
MIN 
CLIQUE 
COVER[103, 
251]. 
Indeed, 
F 
solves 
MIN 
CLIQUE 
COVER 
for 
g(20.18). 
384 
DRAFT 
March 
9, 
2010 



Mixed 
Membership 
Models 


-2-1.5-1-0.500.511.5200.20.40.60.81x
....1

(0:5..x)

Figure 
20.20: 
The 
function 
(x) 
1+ 
efor 
ß 
=1, 
10, 
100. 
As 
ß 
increases, 
this 
sigmoid 
function 
tends 
to 
a 
step 
function. 


Denition 
106 
(Clique 
matrix). 
Given 
an 
adjacency 
matrix 
[A]ij 
, 
i, 
j 
=1;:::;V 
(Aii 
= 
1), 
a 
clique 
matrix 
F 
has 
elements 
Fic 
2f0, 
1} 
;i 
=1, 
. 
. 
. 
, 
V, 
c 
=1;:::;C 
such 
that 
A 
= 
H(FFT). 
Diagonal 
elements



FFT 
express 
the 
number 
of 
cliques/columns 
that 
node 
i 
occurs 
in. 
O-diagonal 
elementsFFT 


ii 
ij 


contain 
the 
number 
of 
cliques/columns 
that 
nodes 
i 
and 
j 
jointly 
inhabit 
[18]. 


Whilst 
nding 
a 
clique 
decomposition 
F 
is 
easy 
(use 
the 
incidence 
matrix 
for 
example), 
nding 
a 
clique 
decomposition2 
with 
the 
minimal 
number 
of 
columns, 
i.e. 
solving 
MIN 
CLIQUE 
COVER, 
is 
NP-Hard[103, 
10]. 


A 
generative 
model 
of 
adjacency 
matrices 


Solving 
MIN 
CLIQUE 
COVER 
is 
a 
computationally 
hard 
problem 
and 
approximations 
are 
in 
general 
unavoidable. 
Below 
we 
relax 
the 
strict 
clique 
requirement 
and 
assume 
that 
provided 
only 
a 
small 
number 
of 
links 
in 
an 
`almost 
clique’ 
are 
missing, 
this 
may 
be 
considered 
a 
suciently 
well-connected 
group 
of 
nodes 
to 
form 
a 
cluster. 


Given 
an 
adjacency 
matrix 
A 
and 
a 
prior 
on 
clique 
matrices 
F, 
our 
interest 
is 
the 
posterior 


p(FjA) 
. 
p(AjF)p(F) 
(20.6.15) 


We 
rst 
concentrate 
on 
the 
generative 
term 
p(AjF). 
To 
nd 
`well-connected’ 
clusters, 
we 
relax 
the 
constraint 
that 
the 
decomposition 
is 
in 
the 
form 
of 
cliques 
in 
the 
original 
graph 
and 
view 
the 
absence 
of 
links 
as 
statistical 
uctuations 
away 
from 
a 
perfect 
clique. 
Given 
a 
V 
× 
C 
matrix 
F, 
we 
desire 
that 
the 
higher 
the 
overlap 
between 
rows3 
fi 
and 
fj 
is, 
the 
greater 
the 
probability 
of 
a 
link 
between 
i 
and 
j. 
This 
may 
be 
achieved 
using, 
for 
example, 




p(Aij 
=1jF)= 
fifT 
(20.6.16)

j

with 


..1 


(0:5..x)

(x) 
1+ 
e 
(20.6.17) 


where 
ß 
controls 
the 
steepness 
of 
the 
function, 
see 
g(20.20). 
The 
0.5 
shift 
in 
equation 
(20.6.17) 
ensures 
that 
s 
approximates 
the 
step-function 
since 
the 
argument 
of 
s 
is 
an 
integer. 
Under 
equation 
(20.6.16), 
if 
fi 
and 
fj 
have 
at 
least 
one 
`1’ 
in 
the 
same 
position, 
fifT 
- 
0:5 
> 
0 
and 
p(Aij 
=1jF) 
is 
high. 
Absent 
links 


j 


contribute 
p(Aij 
=0jF)=1 
- 
p(Aij 
=1jF). 
The 
parameter 
ß 
controls 
how 
strictly 
(FFT) 
matches 
A; 
for 
large 
, 
very 
little 
exibility 
is 
allowed 
and 
only 
cliques 
will 
be 
identied. 
For 
small 
, 
subsets 
that 
would 
be 
cliques 
if 
it 
were 
not 
for 
a 
small 
number 
of 
missing 
links, 
are 
clustered 
together. 
The 
setting 
of 
ß 
is 
user 
and 
problem 
dependent. 


Assuming 
each 
element 
of 
the 
adjacency 
matrix 
is 
sampled 
independently 
from 
the 
generating 
process, 
the 
joint 
probability 
of 
observing 
A 
is 
(neglecting 
its 
diagonal 
elements), 


YY

p(AjF)=fifT 
1 
- 
fifT 
(20.6.18)

jj
iji6j


The 
ultimate 
quantity 
of 
interest 
is 
the 
posterior 
distribution 
of 
clique 
structure, 
equation 
(20.6.15), 
for 
which 
we 
now 
specify 
a 
prior 
p(F) 
over 
clique 
matrices. 


3We 
use 
lower 
indices 
fi 
to 
denote 
the 
ith 
row 
of 
F. 


DRAFT 
March 
9, 
2010 
385 



Mixed 
Membership 
Models 


2040608010020406080100
2040608010012014020406080100
2040608010020406080100
(a)(b)(c)
Figure 
20.21: 
(a): 
Adjacency 
matrix 
of 
105 
Political 
Books 
(black=1). 
(b): 
Clique 
matrix: 
521 
non-zero 
entries. 
(c): 
Adjacency 
reconstruction 
using 
an 
approximate 
clique 
matrix 
with 
10 
cliques 
– 
see 
also 
g(20.22) 
and 
demoCliqueDecomp.m. 


05101000 Years for Revenge 
Bush vs. the Beltway 
Charlie Wilson’s War 
Losing Bin Laden 
Sleeping With the Devil 
The Man Who Warned America 
Why America Slept 
Ghost Wars 
A National Party No More 
Bush Country 
Dereliction of Duty 
Legacy 
Off with Their Heads 
Persecution 
Rumsfeld’s War 
Breakdown 
Betrayal 
Shut Up and Sing 
Meant To Be 
The Right Man 
Ten Minutes from Normal 
Hillary’s Scheme 
The French Betrayal of America 
Tales from the Left Coast 
Hating America 
The Third Terrorist 
Endgame 
Spin Sisters 
All the Shah’s Men 
Dangerous Dimplomacy 
The Price of Loyalty 
House of Bush, House of Saud 
The Death of Right and Wrong 
Useful Idiots 
The O’Reilly Factor 
Let Freedom Ring 
Those Who Trespass 
Bias 
Slander 
The Savage Nation 
Deliver Us from Evil 
Give Me a Break 
The Enemy Within 
The Real America 
Who’s Looking Out for You? 
The Official Handbook Vast Right Wing Conspiracy 
Power Plays 
Arrogance 
The Perfect Wife 
The Bushes 
Things Worth Fighting For 
Surprise, Security, the American Experience 
Allies 
Why Courage Matters 
Hollywood Interrupted 
Fighting Back 
We Will Prevail 
The Faith of George W Bush 
Rise of the Vulcans 
Downsize This! 
Stupid White Men 
Rush Limbaugh Is a Big Fat Idiot 
The Best Democracy Money Can Buy 
The Culture of Fear 
America Unbound 
The Choice 
The Great Unraveling 
Rogue Nation 
Soft Power 
Colossus 
The Sorrows of Empire 
Against All Enemies 
American Dynasty 
Big Lies 
The Lies of George W. Bush 
Worse Than Watergate 
Plan of Attack 
Bush at War 
The New Pearl Harbor 
Bushwomen 
The Bubble of American Supremacy 
Living History 
The Politics of Truth 
Fanatics and Fools 
Bushwhacked 
Disarming Iraq 
Lies and the Lying Liars Who Tell Them 
MoveOn’s 50 Ways to Love Your Country 
The Buying of the President 2004 
Perfectly Legal 
Hegemony or Survival 
The Exception to the Rulers 
Freethinkers 
Had Enough? 
It’s Still the Economy, Stupid! 
We’re Right They’re Wrong 
What Liberal Media? 
The Clinton Wars 
Weapons of Mass Deception 
Dude, Where’s My Country? 
Thieves in High Places 
Shrub 
Buck Up Suck Up 
The Future of Freedom 
Empire 
Figure 
20.22: 
Political 
Books. 
105 
× 
10 
dimensional 
clique 
matrix 
broken 
into 
3 
groups 
by 
a 
politically 
astute 
reader. 
A 
black 
square 
indicates 
q(fic) 
> 
0:5. 
Liberal 
books 
(red), 
Conservative 
books 
(green), 
Neutral 
books(yellow). 
By 
inspection, 
cliques 
5,6,7,8,9 
largely 
correspond 
to 
`conservative’ 
books. 


Clique 
matrix 
prior 
p(F) 


Since 
we 
are 
interested 
in 
clustering, 
ideally 
we 
want 
to 
place 
as 
many 
nodes 
in 
the 
graph 
as 
possible 
in 
a 
cluster. 
This 
means 
that 
we 
wish 
to 
bias 
the 
contributions 
to 
the 
adjacency 
matrix 
A 
to 
occur 
from 
a 
small 
number 
of 
columns 
of 
F. 
To 
achieve 
this 
we 
rst 
reparameterise 
F 
as 


..

fCmax

F 
=1f1;:::;Cmax 
(20.6.19) 


where 
c 
2f0, 
1} 
play 
the 
role 
of 
indicators 
and 
fc 
is 
column 
c 
of 
F. 
Cmax 
is 
an 
assumed 
maximal 
number 
of 
clusters. 
Ideally, 
we 
would 
like 
to 
nd 
an 
F 
with 
a 
low 
number 
of 
indicators 
1;:::;Cmax 
in 
state 
1. 
To 
achieve 
this 
we 
dene 
a 
prior 
distribution 
on 
the 
binary 
hypercube 
a 
=(1;:::;Cmax 
), 


Y

p(j)=c 
(1 
- 
)1..c 
(20.6.20) 
c 


To 
encourage 
a 
small 
number 
of 
the 
0s 
to 
be 
1, 
we 
use 
a 
Beta 
prior 
p() 
with 
suitable 
parameters 
to 


c

ensure 
that 
. 
is 
less 
than 
0.5. 
This 
gives 
rise 
to 
a 
Beta-Bernoulli 
distribution 


Z

B(a 
+ 
N, 
b 
+ 
Cmax 
- 
N) 


p()=p(j)p() 
= 
(20.6.21)

B(a, 
b)

. 


PCmax

where 
B(a, 
b) 
is 
the 
beta 
function 
and 
N 
=is 
the 
number 
of 
indicators 
in 
state 
1. 
To 
encourage 


c=1 
c 
that 
only 
a 
small 
number 
of 
components 
should 
be 
active, 
we 
set 
a 
=1;b 
= 
3 
(which 
corresponds 
to 
a 
mean 
. 
of 
0.25 
and 
variance 
0.0375. 
The 
distribution 
(20.6.21) 
is 
on 
the 
vertices 
of 
the 
binary 
hypercube 
f0, 
1gCmax 
with 
a 
bias 
towards 
vertices 
close 
to 
the 
origin 
(0;:::, 
0). 
Through 
equation 
(20.6.19), 
the 
prior 
on 
a 
induces 
a 
prior 
on 
F. 
The 
resulting 
distribution 
p(F, 
jA) 
. 
p(Fj)p() 
is 
formally 
intractable 
and 
in 
[18] 
this 
is 
addressed 
using 
a 
variational 
technique. 
See 
cliquedecomp.c 
and 
cliquedecomp.m. 


Clique 
matrices 
also 
play 
a 
natural 
role 
in 
the 
parameterisation 
of 
positive 
denite 
matrices, 
see 
exercise(206)[18]. 


DRAFT 
March 
9, 
2010 



Exercises 


Example 
91 
(Political 
Books 
Clustering). 
The 
data 
consists 
of 
105 
books 
on 
US 
politics 
sold 
by 
the 
online 
bookseller 
Amazon. 
The 
adjacency 
matrix 
with 
element 
Aij 
= 
1 
g(20.21a), 
represents 
frequent 
co-purchasing 
of 
books 
i 
and 
j 
(Valdis 
Krebs, 
www.orgnet.com). 
Additionally, 
books 
are 
labelled 
`liberal', 
`neutral', 
or 
`conservative’ 
according 
to 
the 
judgement 
of 
a 
politically 
astute 
reader 
(www-personal.umich.edu/mejn/netdata/). 
The 
interest 
is 
to 
assign 
books 
to 
clusters, 
using 
A 
alone, 
and 
then 
see 
if 
these 
clusters 
correspond 
in 
some 
way 
to 
the 
ascribed 
political 
leanings 
of 
each 
book. 
Note 
that 
the 
information 
here 
is 
minimal 
– 
all 
that 
is 
known 
to 
the 
clustering 
algorithm 
is 
which 
books 
were 
co-bought 
(matrix 
A); 
no 
other 
information 
on 
the 
content 
or 
title 
of 
the 
books 
are 
exploited 
by 
the 
algorithm. 
With 
an 
initial 
Cmax 
= 
200 
cliques, 
Beta 
parameters 
a 
=1;b 
= 
3 
and 
steepness 
ß 
= 
10, 
the 
most 
probably 
posterior 
marginal 
solution 
contains 
142 
cliques 
g(20.21b), 
giving 
a 
perfect 
reconstruction 
of 
the 
adjacency 
A. 
For 
comparison, 
the 
incidence 
matrix 
has 
441 
2-cliques. 
However, 
this 
clique 
matrix 
is 
too 
large 
to 
provide 
a 
compact 
interpretation 
of 
the 
data 
– 
indeed 
there 
are 
more 
clusters 
than 
books. 
To 
cluster 
the 
data 
more 
aggressively, 
we 
x 
Cmax 
= 
10 
and 
re-run 
the 
algorithm. 
This 
results 
only 
in 
an 
approximate 
clique 
decomposition, 
A 
˜ 
H(FFT), 
as 
plotted 
in 
g(20.21c). 
The 
resulting 
105 
× 
10 
approximate 
clique 
matrix 
is 
plotted 
in 
g(20.22) 
and 
demonstrates 
how 
individual 
books 
are 
present 
in 
more 
than 
one 
cluster. 
Interestingly, 
the 
clusters 
found 
only 
on 
the 
basis 
of 
the 
adjacency 
matrix 
have 
some 
correspondence 
with 
the 
ascribed 
political 
leanings 
of 
each 
book; 
cliques 
5, 
6, 
7, 
8, 
9 
correspond 
to 
largely 
`conservative’ 
books. 
Most 
books 
belong 
to 
more 
than 
a 
single 
clique/cluster, 
suggesting 
that 
they 
are 
not 
single 
topic 
books, 
consistent 
with 
the 
assumption 
of 
a 
mixed 
membership 
model. 


20.7 
Further 
Reading 
The 
literature 
on 
mixture 
modelling 
is 
extensive, 
and 
a 
good 
overview 
and 
entrance 
to 
the 
literature 
is 
contained 
in 
[188]. 


20.8 
Code 
MIXprodBern.m: 
EM 
training 
of 
a 
Mixture 
of 
product 
Bernoulli 
distributions 
demoMixBernoulli.m: 
Demo 
of 
a 
Mixture 
of 
product 
Bernoulli 
distributions 


GMMem.m: 
EM 
training 
of 
a 
mixture 
of 
Gaussians 
GMMloglik.m: 
GMM 
log 
likelihood 
demoGMMem.m: 
Demo 
of 
a 
EM 
for 
mixture 
of 
Gaussians 
demoGMMclass.m: 
Demo 
GMM 
for 
classication 


Kmeans.m: 
K-means 
demoKmeans.m: 
Demo 
of 
K-means 


demoPolya.m: 
Demo 
of 
the 
number 
of 
active 
clusters 
from 
a 
Polya 
distribution 
dirrnd.m: 
Dirichlet 
random 
distribution 
generator 


cliquedecomp.m: 
Clique 
Matrix 
decomposition 
cliquedecomp.c: 
Clique 
Matrix 
decomposition 
(C-code) 
DemoCliqueDecomp.m: 
Demo 
clique 
matrix 
decomposition 


20.9 
Exercises 
Exercise 
200. 
Consider 
a 
mixture 
of 
factorised 
models 


XY

p(v)=p(h)p(vijh) 
(20.9.1) 


hi 


DRAFT 
March 
9, 
2010 



Exercises 


n

For 
assumed 
i.i.d. 
data 
v;n 
=1;:::;N, 
some 
observation 
components 
may 
be 
missing 
so 
that, 
for 
example 


5

the 
third 
component 
of 
the 
fth 
datapoint, 
vis 
unknown. 
Show 
that 
Maximum 
Likelihood 
training 
on 
the 


3 


n

observed 
data 
corresponds 
to 
ignoring 
components 
vthat 
are 
missing. 


i 


Exercise 
201. 
Derive 
the 
optimal 
EM 
update 
for 
tting 
a 
mixture 
of 
Gaussians 
under 
the 
constraint 
that 
the 
covariances 
are 
diagonal. 


Exercise 
202. 
Consider 
a 
mixture 
of 
K 
isotropic 
Gaussians, 
each 
with 
the 
same 
covariance, 
Si 
= 
2I. 
In 
the 
limit 
2 
. 
0 
show 
that 
the 
EM 
algorithm 
tends 
to 
the 
K-means 
clustering 
algorithm. 


Exercise 
203. 
Consider 
the 
term 


N

XP

hlog 
p(h)iold(hjvn) 
(20.9.2)

p
n=1 


We 
wish 
to 
optimise 
the 
above 
with 
respect 
to 
the 
distribution 
p(h). 
This 
can 
be 
achieved 
by 
dening 
the 
Lagrangian 


N

XXP

L 
= 
hlog 
p(h)) 
old(hjvn) 
+ 
. 
1 
- 
p(h) 
(20.9.3)

p
n=1 
h 


P

By 
dierentiating 
the 
Lagrangian 
with 
respect 
to 
p(h) 
and 
using 
the 
normalisation 
constrainth 
p(h)=1, 
show 
that, 
optimally 


X

1 
N

p(h)= 
p 
old(hjv 
n) 
(20.9.4)

N 


n=1 


Exercise 
204. 
We 
showed 
that 
tting 
an 
unconstrained 
mixture 
of 
Gaussians 
using 
Maximum 
Likelihood 
is 
problematic 
since, 
by 
placing 
one 
of 
the 
Gaussians 
over 
a 
datapoints 
and 
letting 
the 
covariance 
determinant 
go 
to 
zero, 
we 
obtain 
an 
innite 
likelihood. 
In 
contrast, 
when 
tting 
a 
single 
Gaussian 
N 
(x 


, 
) 
to 
i.i.d. 
data 
x1 
, 
x2 
;:::, 
xN 
show 
that 
the 
Maximum 
Likelihood 
optimum 
for 
S 
has 
non-zero 
determinant, 
and 
that 
the 
optimal 
likelihood 
remains 
nite. 


Exercise 
205. 
Modify 
GMMem.m 
suitably 
so 
that 
it 
can 
deal 
with 
the 
semi-supervised 
scenario 
in 
which 
the 
mixture 
component 
h 
of 
some 
of 
the 
observations 
v 
is 
known. 


Exercise 
206. 
You 
wish 
to 
parameterise 
covariance 
matrices 
S 
under 
the 
constraint 
that 
specied 
elements 
are 
zero. 
The 
constraints 
are 
specied 
using 
a 
matrix 
A 
with 
elements 
Aij 
=0 
if 
Sij 
=0 
and 
Aij 
=1 
otherwise. 
Consider 
a 
clique 
matrix 
Z, 
for 
which 


A 
= 
H(ZZT) 
(20.9.5) 


and 
matrix 


S* 
= 
ZZT 
(20.9.6)

* 


with 




0 
if 
Zij 
=0 


[Z]ij 
=(20.9.7)

ij 
if 
Zij 
=1 


for 
parameters 
. 
Show 
that 
for 
any 
, 
S* 
is 
positive 
semidenite 
and 
parameterises 
covariance 
matrices 
under 
the 
zero 
constraints 
specied 
by 
A. 


DRAFT 
March 
9, 
2010 



CHAPTER 
21 


Latent 
Linear 
Models 


21.1 
Factor 
Analysis 
In 
chapter(15) 
we 
discussed 
Principal 
Components 
Analysis 
which 
forms 
lower 
dimensional 
representations 
of 
data 
based 
on 
assuming 
that 
the 
data 
lies 
close 
to 
a 
hyperplane. 
Here 
we 
describe 
a 
related 
probabilistic 
model 
for 
which 
extensions 
to 
Bayesian 
methods 
can 
be 
envisaged. 
Any 
probabilistic 
model 
may 
also 
be 
used 
as 
a 
component 
of 
a 
larger 
more 
complex 
model, 
such 
as 
a 
mixture 
model, 
enabling 
natural 
generalisations. 


We 
use 
v 
to 
describe 
a 
real 
data 
vector 
to 
emphasise 
that 
this 
is 
a 
visible 
(observable) 
quantity. 
The 
dataset 
is 
then 
given 
by 
a 
set 
of 
vectors, 


	

1 
N

V 
=v;:::, 
v(21.1.1) 


where 
dim 
(v)= 
D. 
Our 
interest 
is 
to 
nd 
a 
lower 
dimensional 
probabilistic 
description 
of 
this 
data. 
If 
data 
lies 
close 
to 
a 
H-dimensional 
hyperplane 
we 
may 
accurately 
approximate 
each 
datapoint 
by 
a 
low 
H-dimensional 
coordinate 
system. 
In 
general, 
datapoints 
will 
not 
lie 
exactly 
on 
the 
hyperplane 
and 
we 
model 
this 
discrepancy 
with 
Gaussian 
noise. 
Mathematically, 
the 
FA 
model 
generates 
an 
observation 
v 
according 
to 


v 
= 
Fh 
+ 
c 
+ 
E 
(21.1.2) 


where 
the 
noise 
E 
is 
Gaussian 
distributed, 


E 
N 
(E 


0, 
	) 
(21.1.3) 


The 
constant 
bias 
c 
sets 
the 
origin 
of 
the 
coordinate 
system. 
The 
factor 
loading 
matrix 
F 
plays 
a 
similar 
role 
as 
the 
basis 
matrix 
in 
PCA, 
see 
section(15.2). 
Similarly, 
the 
hidden 
coordinates 
h 
plays 
the 
role 
of 
the 
components 
we 
used 
in 
section(15.2). 


The 
dierence 
between 
PCA 
and 
Factor 
Analysis 
is 
in 
the 
choice 
of 
	: 


Probabilistic 
PCA 


. 
= 
2I 
(21.1.4) 


Factor 
Analysis 


. 
= 
diag 
( 1;:::; D) 
(21.1.5) 


389 



h1h2h3
v1v2v3v4v5
h1h2h3
v1v2v3v4v5
Factor 
Analysis 


Figure 
21.1: 
Factor 
Analysis. 
The 
visible 
vector 
variable 
v 
is 
related 
to 
the 
vector 
hidden 
variable 
h 
by 
a 
linear 
mapping, 
with 
independent 
additive 
Gaussian 
noise 
on 
each 
visible 
variable. 
The 
prior 
on 
the 
hidden 
variable 
may 
be 
taken 
to 
be 
an 
isotropic 
Gaussian, 
thus 
being 
independent 
across 
its 
components. 


A 
probabilistic 
description 


From 
equation 
(21.1.2) 
and 
equation 
(21.1.3), 
given 
h, 
the 
data 
is 
Gaussian 
distributed 
with 
mean 
Fh 
+ 
c 
and 
covariance 
. 


- 
1 
(v..Fh..c)T	..1(v..Fh..c)

p 
(vjh)= 
N 
(v 


Fh 
+ 
c, 
	) 
. 
e 
2 
(21.1.6) 


To 
complete 
the 
model, 
we 
need 
to 
specify 
the 
hidden 
distribution 
p(h). 
A 
convenient 
choice 
is 
a 
Gaussian 


..hTh=2 


p 
(h)= 
N 
(h 


0, 
I) 
. 
e 
(21.1.7) 


Under 
this 
prior 
the 
coordinates 
h 
will 
be 
preferentially 
concentrated 
around 
values 
close 
to 
0. 
If 
we 
sample 
a 
h 
from 
p(h) 
and 
then 
draw 
a 
value 
for 
v 
using 
p(vjh), 
the 
sampled 
v 
vectors 
would 
produce 
a 
saucer 
or 
`pancake’ 
of 
points 
in 
the 
v 
space. 
Using 
a 
correlated 
Gaussian 
prior 
p(h)= 
N 
(h 


0, 
H 
) 
has 
no 
eect 
on 
the 
complexity 
of 
the 
model 
since 
H 
can 
be 
absorbed 
into 
F, 
exercise(209). 
Since 
v 
is 
linearly 
related 
to 
h 
through 
equation 
(21.1.2) 
and 
both 
E 
and 
h 
are 
Gaussian, 
then 
v 
is 
Gaussian 
distributed. 
The 
mean 
and 
covariance 
can 
be 
computed 
using 
the 
propagation 
results 
in 
section(8.6.3): 




p 
(v)= 
p 
(vjh) 
p 
(h) 
dh 
= 
Nv 


c, 
FFT 
+ 
	(21.1.8) 


Invariance 
of 
the 
likelihood 
under 
factor 
rotation 


Since 
the 
matrix 
F 
only 
appears 
in 
the 
nal 
model 
p(v) 
through 
FFT 
+ 
	, 
the 
likelihood 
is 
unchanged 
if 
we 
rotate 
F 
using 
FR, 
with 
RRT 
= 
I: 


FR(FR)T 
+ 
. 
= 
FRRTFT 
+ 
. 
= 
FFT 
+ 
. 
(21.1.9) 


The 
solution 
space 
for 
F 
is 
therefore 
not 
unique 
– 
we 
can 
arbitrarily 
rotate 
the 
matrix 
F 
and 
produce 
an 
equally 
likely 
model 
of 
the 
data. 
Some 
care 
is 
therefore 
required 
when 
interpreting 
the 
entries 
of 
F. 
Varimax 
provides 
a 
more 
interpretable 
F 
by 
using 
a 
suitable 
rotation 
matrix 
R. 
The 
aim 
is 
to 
produce 
a 
rotated 
F 
for 
which 
each 
column 
has 
only 
a 
small 
number 
of 
large 
values. 
Finding 
a 
suitable 
rotation 
results 
in 
a 
non-linear 
optimisation 
problem 
and 
needs 
to 
be 
solved 
numerically. 
See 
[185] 
for 
details. 


21.1.1 
Finding 
the 
optimal 
bias 
For 
a 
set 
of 
data 
V 
and 
using 
the 
usual 
i.i.d. 
assumption, 
the 
log 
likelihood 
is 


XX

N1 
N
N

log 
p(VjF, 
	) 
= 
log 
p(vn)= 
- 
(vn 
- 
c)T 
..1 
(vn 
- 
c) 
- 
log 
det 
(2D) 
(21.1.10)

2 
D 
2 


n=1 
n=1 


where 


D 
= 
FFT 
+ 
. 
(21.1.11) 
Dierentiating 
equation 
(21.1.10) 
with 
respect 
to 
c 
and 
equating 
to 
zero, 
we 
arrive 
at 
the 
Maximum 
Likelihood 
optimal 
setting 
that 
the 
bias 
c 
is 
the 
mean 
of 
the 
data, 


c 
= 
1 
N 
NXvn 
= 
v 
(21.1.12) 
n=1 
390 
DRAFT 
March 
9, 
2010 



Factor 
Analysis 
: 
Maximum 
Likelihood 


-4-20246-4-20246-2-101234
-4-20246-4-20246-2-101234
(a) 
(b) 
Figure 
21.2: 
Factor 
Analysis: 
1000 
points 
generated 
from 
the 
model. 
(a): 
1000 
latent 
two-dimensional 
points 
hn 
sampled 
from 
N 
(h 


0, 
I). 
These 
are 
transformed 
to 
a 
point 
on 
the 
three-dimensional 
plane 
by 
xn 
= 
c 
+ 
Fhn 
. 
The 
covariance 
of 
x0 
is 
degenerate, 
with 
covariance 
matrix 
FFT 
. 
(b): 
For 
each 
point 
xn 


00 


on 
the 
plane 
a 
random 
noise 
vector 
is 
drawn 
from 
N 
(E 


0, 
	) 
and 
added 
to 
the 
in-plane 
vector 
to 
form 
a 
sample 
xn, 
plotted 
in 
red. 
The 
distribution 
of 
points 
forms 
a 
`pancake’ 
in 
space. 
Points 
`underneath’ 
the 
plane 
are 
not 
shown. 


We 
will 
use 
this 
setting 
throughout. 
With 
this 
setting 
the 
log 
likelihood 
equation 
(21.1.10) 
can 
be 
written 


N 
....

log 
p(VjF, 
	)= 
- 
trace..1S+ 
log 
det 
(2D)(21.1.13)

2D 


where 
S 
is 
the 
sample 
covariance 
matrix 


N

X

1 
v)T

S 
=(v 
- 
v)(v 
- 
(21.1.14)

N 


n=1 


21.2 
Factor 
Analysis 
: 
Maximum 
Likelihood 
We 
now 
specialise 
to 
the 
assumption 
that 
. 
= 
diag 
( 1;:::; D). 
We 
consider 
two 
methods 
for 
learning 
the 
factor 
loadings 
F: 
a 
`direct’ 
approach1, 
section(21.2.1) 
and 
an 
EM 
approach, 
section(21.2.2). 


21.2.1 
Direct 
likelihood 
optimisation 
Optimal 
F 
for 
xed 
. 


To 
nd 
the 
maximum 
likelihood 
setting 
of 
F 
we 
dierentiate 
the 
log 
likelihood 
equation 
(21.1.13) 
with 
respect 
to 
F 
and 
equate 
to 
zero. 
This 
gives 


....

..1 
..1

0 
= 
trace(@FD)..1S- 
trace@F(21.2.1)

DD 
D 


Using 
@F(FFT)= 
F(@FFT)+(@FF)FT, 
a 
stationary 
point 
is 
given 
when 
..1

F 
= 
..1S..1F 
(21.2.2)

D 
DD 
Since 
D 
is 
invertible, 
the 
optimal 
F 
satises 
F 
= 
S..1F 
(21.2.3)

D 
Using 
the 
denition 
of 
D, 
equation 
(21.1.11), 
one 
can 
rewrite 
..1F 
as 
(see 
exercise(210)) 


D 


..1 
D 
F 
= 
	..1FI 
+ 
FT	..1F..1 
(21.2.4) 
1The 
presentation 
here 
follows 
closely 
that 
of 
[302]. 
DRAFT 
March 
9, 
2010 
391 



Factor 
Analysis 
: 
Maximum 
Likelihood 


Plugging 
this 
into 
the 
zero 
derivative 
condition, 
equation 
(21.2.3) 
becomes 




FI 
+ 
FT	..1F= 
S	..1F 
(21.2.5) 
Using 
the 
reparameterisations 


F~
	..

1 


2

F, 


S~
	..

1 


2

S	..

1

(21.2.6)
2

equation 
(21.2.5) 
can 
be 
written 
in 
the 
`isotropic’ 
form 




F~
F~
TF~
S~
F~
(21.2.7) 


We 
assume 
that 
the 
transformed 
factor 
matrix 
F~
has 
a 
thin 
SVD 
decomposition 


~

F 
= 
UH 
LVT 
(21.2.8) 


where 
dim 
UH 
= 
D 
× 
H, 
dim 
L 
= 
H 
× 
H, 
dim 
V 
= 
H 
× 
H 
and 


UT 
UH 
= 
IH 
, 
VTV 
= 
IH 
(21.2.9)

H 


and 
L 
= 
diag 
(l1;:::;lH 
) 
are 
the 
singular 
values 
of 
F~
. 
Plugging 
this 
assumption 
into 
equation 
(21.2.7) 
we 
obtain 




~

UH 
LVTIH 
+ 
VL2VT= 
SUH 
LVT 
(21.2.10) 


which 
gives 


....

~l2

UHIH 
+ 
L2= 
SUH 
, 
L2 
= 
diag1;:::;l2 
(21.2.11)

H

Equation(21.2.11) 
is 
then 
an 
eigen-equation 
for 
UH 
. 
Intuitively, 
it's 
clear 
that 
we 
need 
to 
nd 
then 
the 
eigendecomposition 
of 
S~
and 
then 
set 
the 
columns 
of 
UH 
to 
those 
eigenvectors 
corresponding 
to 
the 
largest 
eigenvalues. 
This 
is 
derived 
more 
formally 
below. 


Determining 
the 
appropriate 
eigenvalues 


We 
can 
relate 
the 
form 
of 
the 
solution 
to 
the 
eigen-decomposition 
of 
S~

~

S 
= 
UUT 
, 
U 
=[UH 
jUr] 
(21.2.12) 


where 
Ur 
are 
arbitrary 
additional 
columns 
chosen 
to 
complete 
UH 
to 
form 
an 
orthogonal 
U, 
UTU 
= 


v 


UUT 
= 
I. 
Using 
. 
= 
diag 
(1;:::;D), 
equation 
(21.2.11) 
stipulates 
1 
+ 
l2 
= 
i, 
or 
li 
= 
i 
- 
1. 
Given 


i 


the 
solution 
for 
F~
, 
the 
solution 
for 
F 
is 
found 
from 
equation 
(21.2.6). 
To 
determine 
the 
optimal 
i 
we 
write 
the 
log 
likelihood 
in 
terms 
of 
the 
i 
as 
follows. 
Using 
the 
new 
parameterisation, 




1 


2

F~
F~
T 
+ 
I

.


1 


2

(21.2.13)
D 
= 
	

1

~

S	

1

and 
S 
= 
.


, 
we 
have 


2

2



....1 


trace..1S= 
traceF~F~
T 
+ 
IDS~
(21.2.14)

D 


The 
log 
likelihood 
equation 
(21.1.10) 
in 
this 
new 
parameterisation 
is 




2 


- 
log 
p(VjF, 
	) 
= 
traceID 
+ 
F~
F~
T..1 
S~
+ 
log 
detID 
+ 
F~
F~
T+ 
log 
det 
(2	) 
(21.2.15)

N 


Using 
i 
= 
1 
+ 
l2 
i 
, 
and 
equation 
(21.2.8) 
we 
can 
write 
ID 
+ 
~F
~F
T 
= 
ID 
+ 
UH 
L2UT 
H 
= 
Udiag 
(1, 
. 
. 
. 
, 
H 
, 
1, 
. 
. 
. 
, 
1) 
UT 
(21.2.16) 
392 
DRAFT 
March 
9, 
2010 



Factor 
Analysis 
: 
Maximum 
Likelihood 


so 
that 




..1 
X

i 
i 
i 
= 
H 


traceID 
+ 
F~
F~
S~
;0=(21.2.17)

i

0i 
1 
i>H 


i 


Similarly 


H

X

log 
detID 
+ 
F~
F~
log 
i 
(21.2.18) 
i=1 


Using 
this 
we 
can 
write 
the 
log 
likelihood 
as 
a 
function 
of 
the 
eigenvalues 
(for 
xed 
	) 
as 


XX

2 
HD

- 
log 
p(VjF, 
	) 
= 
log 
i 
+ 
H 
+ 
i 
+ 
log 
det 
(2	) 
(21.2.19)

N 


i=1 
i=H+1 


To 
maximise 
the 
likelihood 
we 
need 
to 
minimise 
the 
right 
hand 
side 
of 
the 
above. 
Since 
log 
<. 
we

P

should 
place 
the 
largest 
H 
eigenvalues 
in 
thelog 
i 
term. 
A 
solution 
for 
xed 
. 
is 
therefore

i 


F 
= 
.


1 


2

UH 
(H 
- 
IH 
) 


1 


2

R 


(21.2.20) 
where 
H 
= 
diag 
(1;:::;H 
) 
(21.2.21) 


are 
the 
H 
largest 
eigenvalues 
of 
	..

1 


2

S	..

1 


2

, 
with 
UH 
being 
the 
matrix 
of 
the 
corresponding 
eigenvectors. 


R 
is 
an 
arbitrary 
orthogonal 
matrix. 
SVD 
based 
approach 


rather 
than 
nding 
the 
eigen-decomposition 
of 
	..

1 


2

S	..

1 


2

we 
can 
avoid 
forming 
the 
covariance 
matrix 
by 


considering 
the 
thin 
SVD 
decomposition 
of 


1

~	..

X 
= 
v 


1 


2

X 


(21.2.22)
N 


where 
the 
data 
matrix 
is 




1 
N

X 
x


(21.2.23)
;:::, 
x


Given 
a 
thin 
decomposition 


~~

X 
= 
UH 
VT 
(21.2.24) 


~
2

we 
obtain 
the 
eigenvalues 
i 
= 
For 
>N 
this 
is 
convenient 
since 
the 
computational 
complexity

ii. 
D 
Dusing 
this 
SVD 
method 
is 
Omin 
(H, 
N)3 
. 
When 
the 
matrix 
X 
is 
too 
large 
to 
store 
in 
memory, 
online 
SVD 
methods 
are 
available[49]. 


Finding 
the 
optimal 
. 


The 
zero 
derivative 
of 
the 
log 
likelihood 
occurs 
when 




. 
= 
diagS 
- 
FFT 
(21.2.25) 


where 
F 
is 
given 
by 
equation 
(21.2.20). 
There 
is 
no 
closed 
form 
solution 
to 
equations(21.2.25, 
21.2.20). 
A 
simple 
iterative 
scheme 
is 
to 
rst 
guess 
values 
for 
the 
diagonal 
entries 
of 
. 
and 
then 
nd 
the 
optimal 
F 
using 
equation 
(21.2.20). 
Subsequently 
. 
is 
updated 
using 




	new 
= 
diagS 
- 
FFT 
(21.2.26) 


We 
update 
F 
using 
equation 
(21.2.20) 
and 
. 
using 
equation 
(21.2.26) 
until 
convergence. 


Alternative 
schemes 
for 
updating 
the 
noise 
matrix 
. 
can 
improve 
convergence 
considerably. 
For 
example 
updating 
only 
a 
single 
component 
of 
. 
with 
the 
rest 
xed 
can 
be 
achieved 
using 
a 
closed 
form 
expression[302]. 


DRAFT 
March 
9, 
2010 



Factor 
Analysis 
: 
Maximum 
Likelihood 


21.2.2 
Expectation 
maximisation 
A 
popular 
way 
to 
train 
Factor 
Analysis 
in 
machine 
learning 
is 
to 
use 
EM. 
We 
assume 
that 
the 
bias 
c 
has 
been 
optimally 
set 
to 
the 
data 
mean 
v. 


M-step 


As 
usual, 
we 
need 
to 
consider 
the 
energy 
which, 
neglecting 
constants, 
is 




N

X

E 
(F, 
	)= 
- 
1(dn 
- 
Fh)T 
	..1 
(dn 
- 
Fh)- 
N 
log 
det 
(	) 
(21.2.27)

22 


q(hjvn)

n=1

where 
dn 
= 
vn 
- 
v. 
The 
optimal 
variational 
distribution 
q(hjvn) 
is 
determined 
by 
the 
E-step 
below. 
Maximising 
E 
(F, 
	) 
with 
respect 
to 
F 
gives 


Fnew 


= 
AH..1 
(21.2.28) 


where 


DE

XX

1 
dn 
hhiT 
1

A 
= 
q(hjvn) 
, 
H 
= 
hhT(21.2.29)

NN 
q(hjvn)

nn

Finally 


XDE X!

	new 
= 
1 
diag 
(dn 
- 
Fh)(dn 
- 
Fh)T= 
diag 
1 
dn(dn)T 
- 
2FAT 
+ 
FHFT 


N 
q(hjvn) 
N 


nn 


(21.2.30) 
E-step 





The 
above 
recursions 
depend 
on 
the 
statistics 
hhiq(hjvn) 
andhhT
q(hjvn). 
Using 
the 
EM 
optimal 
choice 
for 
the 
E-step 
we 
have 


q(hjvn) 
. 
p(vnjh)p(h)= 
N 
(h 


mn 
, 
) 
(21.2.31) 


with 


..1 
..1 
mn 
I 
+ 
FT	..1FFT	..1dn 
I 
+ 
FT	..1F


= 
hh) 
q(hjvn) 
= 
, 
S 
= 
(21.2.32) 


Using 
these 
results 
we 
can 
express 
the 
statistics 
in 
equation 
(21.2.29) 
as 


X

1

H 
= 
S 
+ 
mn(mn)T 
(21.2.33)

N 


n 


Equations 
(21.2.28,21.2.30,21.2.32) 
are 
iterated 
till 
convergence. 
As 
for 
any 
EM 
algorithm, 
the 
likelihood 
equation 
(21.1.10) 
(under 
the 
diagonal 
constraint 
on 
	) 
increases 
at 
each 
iteration. 
Convergence 
using 
this 
EM 
technique 
can 
be 
slower 
than 
that 
of 
the 
direct 
eigen-approach 
of 
section(21.2.1) 
and 
commercial 
implementations 
usually 
avoid 
EM 
for 
this 
reason. 
Provided 
however 
that 
a 
reasonable 
initialisation 
is 
used, 
the 
performance 
of 
the 
two 
training 
algorithms 
can 
be 
similar. 
A 
useful 
initialisation 
is 
to 
use 
PCA 
and 
then 
set 
F 
to 
the 
principal 
directions. 


Mixtures 
of 
FA 


An 
advantage 
of 
probabilistic 
models 
is 
that 
they 
may 
be 
used 
as 
components 
in 
more 
complex 
models, 
such 
as 
mixtures 
of 
FA[275]. 
Training 
can 
then 
be 
achieved 
using 
EM 
or 
gradient 
based 
approaches. 
Bayesian 
extensions 
are 
clearly 
of 
interest; 
whilst 
formally 
intractable 
they 
can 
be 
addressed 
using 
approximate 
methods, 
for 
example 
[98, 
178, 
109]. 


DRAFT 
March 
9, 
2010 



Interlude: 
Modelling 
Faces 


G 


F 
Figure 
21.3: 
Latent 
Identity 
Model. 
The 
mean 
µ 
represents 
the 
mean 
of 
the 
faces. 
The 
subspace 
F 
repre


x21

G 


sents 
the 
directions 
of 
variation 
of 
dierent 
faces 
so 
that 
f2
x12 
f1 
= 
µ 
+ 
Fh1 
is 
a 
mean 
face 
for 
individual 
1, 
and 
similarly 




x13 
for 
f2 
= 
+Fh2. 
The 
subspace 
G 
denotes 
the 
directions 
of 
x22 


variability 
for 
any 
individual 
face, 
caused 
by 
pose, 
lighting 


f1 


etc. 
This 
variability 
is 
assumed 
the 
same 
for 
each 
person. 
A 
particular 
mean 
face 
is 
then 
given 
by 
the 
mean 
face 
of 
the 
person 
plus 
pose/illumination 
variation, 
for 
example 


x11 


x12 
= 
f1 
+ 
Gw12. 
A 
sample 
face 
is 
then 
given 
by 
a 
mean 
origin 
face 
xij 
plus 
Gaussian 
noise 
from 
N 
(ij 


0, 
). 


xij
wij
hi

J
I
Figure 
21.4: 
The 
jth 
image 
of 
the 
ith 
person, 
xij, 
is 
modelled 
using 
a 
linear 
latent 
model 
with 
parameters 
. 


21.3 
Interlude: 
Modelling 
Faces 
Factor 
Analysis 
has 
widespread 
application 
in 
statistics 
and 
machine 
learning. 
As 
an 
inventive 
application 
of 
FA, 
highlighting 
the 
probabilistic 
nature 
of 
the 
model, 
we 
describe 
a 
face 
modelling 
technique 
that 
has 
as 
its 
heart 
a 
latent 
linear 
model[228]. 
Consider 
a 
gallery 
of 
face 
images 
X 
= 
fxij;i 
=1;:::;I; 
j 
=1;:::;Jg

so 
that 
the 
vector 
xij 
represents 
the 
jth 
image 
of 
the 
ith 
person. 
As 
a 
latent 
linear 
model 
of 
faces 
we 
consider 
xij 
= 
µ 
+ 
Fhi 
+ 
Gwij 
+ 
ij 
(21.3.1) 


Here 
F 
(dim 
F 
= 
D 
× 
F 
) 
is 
used 
to 
model 
variability 
between 
people, 
and 
G 
(dim 
G 
= 
D 
× 
G) 
models 
variability 
related 
to 
pose, 
illumination 
etc. 
within 
the 
dierent 
images 
of 
each 
person. 
The 
contribution 


fi 
= 
µ 
+ 
Fhi 
(21.3.2) 


accounts 
for 
variability 
between 
dierent 
people, 
being 
constant 
for 
individual 
i. 
For 
xed 
i, 
the 
contribution 


Gwij 
+ 
ij 
(21.3.3) 


accounts 
for 
the 
variability 
over 
the 
images 
of 
person 
i, 
explaining 
why 
two 
images 
of 
the 
same 
person 
do 
not 
look 
identical. 
See 
g(21.3) 
for 
a 
graphical 
representation. 
As 
a 
probabilistic 
linear 
latent 
variable 
model, 
we 
have 
for 
an 
image 
xij: 


p(xijjhi, 
wij;)= 
N 
(xij 


µ 
+ 
Fhi 
+ 
Gwij, 
) 
(21.3.4) 


p(hi)= 
N 
(hi 


0, 
I) 
;p(wij)= 
N 
(wij 


0, 
I) 
(21.3.5) 


The 
parameters 
are 
. 
= 
fF, 
G, 
, 
g. 
For 
the 
collection 
of 
images, 
assuming 
i.i.d. 
data, 


89ZI<ZJ=

YY

p(X 
, 
w, 
hj)= 
p(xijjhi, 
wij;)p(wij) 
p(hi) 
(21.3.6)

. 
;Z

i=1 
j=1 


for 
which 
the 
graphical 
model 
is 
depicted 
in 
g(21.4). 
The 
task 
of 
learning 
is 
then 
to 
maximise 
the 
likelihood 


Z

p(Xj)=p(X 
, 
w, 
hj) 
(21.3.7) 


w;h 


DRAFT 
March 
9, 
2010 



Interlude: 
Modelling 
Faces 


(a) 
Mean 
(b) 
Variance 
(c) 
(d) 
(e) 
(f) 
(g) 
(h) 
Figure 
21.5: 
Latent 
Identity 
Model 
of 
face 
images. 
Each 
image 
is 
represented 
by 
a 
70 
× 
70 
× 
3 
vector 
(the 
3 
comes 
from 
the 
RGB 
colour 
coding). 
There 
are 
I 
= 
195 
individuals 
in 
the 
database 
and 
J 
=4 
images 
per 
person. 
(a): 
Mean 
of 
the 
data. 
(b): 
Per 
pixel 
standard 
deviation 
– 
black 
is 
low, 
white 
is 
high. 
(c,d,e): 
Three 
directions 
from 
the 
between-individual 
subspace 
F. 
(f,g,h): 
Three 
samples 
from 
the 
model 
with 
h 
xed 
and 
drawing 
randomly 
from 
w 
in 
the 
within-individual 
subspace 
G. 
Reproduced 
from 
[228]. 


This 
model 
can 
be 
seem 
as 
a 
constrained 
version 
of 
Factor 
Analysis 
by 
using 
stacked 
vectors 
(here 
for 
only 
a 
single 
individual, 
I 
= 
1) 


.
. 


1

0

101

0

.
.


h1

FG 
0 
::. 
0 


11

x11


µ 


. 


. 


. 


BBBBB
. 


CCCCC
. 


+ 


w11 


w12 


. 


. 


. 


BBB
. 


CCC
. 


= 


BBB
. 


CCC
. 


+ 


BBB
. 


CCC
. 


BBB
.


CCC.


F0G 
::. 
0 


12 


. 


. 


.


x12 


. 


. 


. 


(21.3.8)
... 
.

.

.. 
. 


..


.


.. 
. 


. 


F0 
0 
::. 
G 


1J

x1J

µ


w1J 


The 
generalisation 
to 
multiple 
individuals 
I> 
1 
is 
straightforward. 
The 
model 
can 
be 
trained 
using 
either 
a 
constrained 
form 
of 
the 
direct 
method, 
or 
EM 
as 
described 
in 
[228]. 
Example 
images 
from 
the 
trained 
model 
are 
presented 
in 
g(21.5). 


Recognition 


In 
closed 
set 
face 
recognition 
a 
new 
`probe’ 
face 
x6is 
to 
be 
matched 
to 
a 
person 
n 
in 
the 
gallery 
of 
training 
faces. 
In 
model 
Mn 
the 
nth 
gallery 
face 
is 
forced 
to 
share 
its 
latent 
identity 
variable 
hn 
with 
the 
test 
face, 
indicating 
that 
these 
faces 
belong 
to 
the 
same 
person2 
. 
Assuming 
a 
single 
exemplar 
per 
person 
(J 
= 
1), 


IYZp(x1, 
. 
. 
. 
, 
xI 
, 
xjMn) 
= 
p(xn, 
x) 
i=1;i6=n 
p(xi) 
(21.3.9) 
Bayes’ 
rule 
then 
gives 
the 
posterior 
class 
assignment 
p(Mnjx1, 
. 
. 
. 
, 
xI 
, 
x) 
. 
p(x1, 
. 
. 
. 
, 
xI 
, 
xjMn)p(Mn) 
(21.3.10) 


For 
a 
uniform 
prior, 
the 
term 
p(Mn) 
is 
constant 
and 
can 
be 
neglected. 
The 
quantities 
we 
require 
above 
are 
given 
by 


Z

Z

p(xn) 
=p(xn, 
hn, 
wn), 
p(x) 
=p(x, 
h, 
w) 
(21.3.11) 
hnwn 
hw* 
2

This 
is 
analogous 
to 
Bayesian 
outcome 
analysis 
in 
section(13.5) 
in 
which 
the 
hypotheses 
assume 
that 
either 
the 
errors 
were 
generated 
from 
the 
same 
or 
a 
dierent 
model. 


DRAFT 
March 
9, 
2010 



Probabilistic 
Principal 
Components 
Analysis 


x1x2x
h1h2
w1w2w
x1x2x
h1h2
w1w2w
(a) 
M1 
(b) 
M2 
p(xn, 
x)= 
p(xn, 
x, 
hn, 
wn, 
w) 


hnwn;w* 


Figure 
21.6: 
Face 
recognition 
model 
(depicted 
only 
for 
a 
single 
examplar 
per 
person, 
J 
= 
1). 
(a): 
In 
model 
M1 
the 
test 
image 
(or 
`probe') 
x* 
is 
assumed 
to 
be 
from 
person 
1, 
albeit 
with 
a 
dierent 
pose/illumination. 
(b): 
For 
model 
M2 
the 
test 
image 
is 
assumed 
to 
be 
from 
person 
2. 
One 
calculates 
p(x1, 
x2, 
xjM1) 
and 
p(x1, 
x2, 
xjM2) 
and 
then 
uses 
Bayes’ 
rule 
to 
infer 
which 
person 
the 
test 
image 
x* 
most 
likely 
belongs. 


(21.3.12) 
where 
p(x, 
h, 
w) 
is 
obtained 
from 
equation 
(21.3.6) 
(where 
we 
assume 
the 
parameters 
. 
are 
xed, 
having 
been 
learned 
using 
Maximum 
Likelihood). 
Note 
that 
in 
equation 
(21.3.12) 
we 
do 
not 
introduce 
h* 
since 
both 
xn 
and 
x* 
are 
assumed 
to 
be 
generated 
from 
the 
same 
person 
with 
the 
latent 
identity 
hn. 
These 
marginal 
probabilities 
are 
straightforward 
to 
derive 
since 
they 
are 
marginals 
of 
Gaussians. 


In 
practice, 
the 
best 
results 
are 
obtained 
using 
a 
between-individual 
subspace 
dimension 
F 
and 
within-
individual 
subspace 
dimension 
G 
both 
equal 
to 
128. 
This 
model 
then 
has 
performance 
competitive 
with 
the 
state-of-the-art[228]. 
A 
benet 
of 
the 
probabilistic 
model 
is 
that 
the 
extension 
to 
mixtures 
of 
this 
model 
is 
essentially 
straightforward, 
which 
boosts 
performance 
further. 
Related 
models 
can 
also 
be 
used 
for 
the 
`open 
set’ 
face 
recognition 
problem 
in 
which 
the 
probe 
face 
may 
or 
may 
not 
belong 
to 
one 
of 
the 
individuals 
in 
the 
database[228]. 


21.4 
Probabilistic 
Principal 
Components 
Analysis 
PPCA 
corresponds 
to 
Factor 
Analysis 
under 
the 
restriction 
. 
= 
2ID. 
Plugging 
this 
assumption 
into 
the 
direct 
optimisation 
solution 
equation 
(21.2.20) 
gives 


F 
= 
UH 
(H 
- 
IH 
)2
1 
R 
(21.4.1) 


where 
the 
eigenvalues 
(diagonal 
entries 
of 
H 
) 
and 
corresponding 
eigenvectors 
(columns 
of 
UH 
) 
are 
the 
largest 
eigenvalues 
of 
..2S. 
Since 
the 
eigenvalues 
of 
..2S 
are 
those 
of 
S 
simply 
scaled 
by 
..2 
(and 
the 
eigenvectors 
are 
unchanged), 
we 
can 
equivalently 
write 


..1 


F 
= 
UHH 
- 
2IH
2 
R 
(21.4.2) 


where 
R 
is 
an 
arbitrary 
orthogonal 
matrix 
with 
RTR 
= 
I 
and 
UH 
, 
H 
are 
the 
eigenvectors 
and 
corresponding 
eigenvalues 
of 
the 
sample 
covariance 
S. 
Classical 
PCA, 
section(15.2), 
is 
recovered 
in 
the 
limit 
2 
. 
0. 
Note 
that 
for 
a 
full 
correspondence 
with 
PCA, 
one 
needs 
to 
set 
R 
= 
I, 
which 
points 
F 
along 
the 
principal 
directions. 


Optimal 
2 


A 
particular 
convenience 
of 
PPCA 
is 
that 
the 
optimal 
noise 
2 
can 
be 
found 
immediately. 
We 
order 
the 
eigenvalues 
of 
S 
so 
that 
1 
= 
2, 
::. 
= 
D. 
In 
equation 
(21.2.19) 
an 
expression 
for 
the 
log 
likelihood 
is 
given 
in 
which 
the 
eigenvalues 
are 
those 
..2S. 
On 
replacing 
i 
with 
i=2 
we 
can 
therefore 
write 
an 
explicit 
expression 
for 
the 
log 
likelihood 
in 
terms 
of 
2 
and 
the 
eigenvalues 
of 
A, 


XX

N 
H1 
D

L(2)= 
- 
D 
log(2) 
+ 
log 
i 
+ 
i 
+(D 
- 
H) 
log 
2 
+ 
H 
(21.4.3)

2 
2 


i=1 
i=H+1 


DRAFT 
March 
9, 
2010 



Canonical 
Correlation 
Analysis 
and 
Factor 
Analysis 


Figure 
21.7: 
For 
a 
5 
hidden 
unit 
model, 
here 
are 
plotted 
the 
results 
of 
training 
PPCA 
and 
FA 
on 
100 
examples 
of 
the 
handwritten 
digit 
seven. 
The 
top 
row 
contains 
the 
5 
Factor 
Analysis 
factors 
and 
the 
bottom 
row 
the 
5 
largest 
eigenvectors 
from 
PPCA 
are 
plotted. 


FAFAFAFAFAPCAPCAPCAPCAPCA
By 
dierentiating 
L(2) 
and 
equating 
to 
zero, 
the 
Maximum 
Likelihood 
optimal 
setting 
for 
2 
is 


D

X

2 
=
1 
j 
(21.4.4)

D 
- 
H 


j=H+1 


In 
summary 
PPCA 
is 
obtained 
by 
taking 
the 
principal 
eigenvalues 
and 
corresponding 
eigenvectors 
of 
the 
sample 
covariance 
matrix 
S, 
and 
setting 
the 
variance 
by 
equation 
(21.4.4). 
The 
single-shot 
training 
nature 
of 
PPCA 
makes 
it 
an 
attractive 
algorithm 
and 
also 
gives 
a 
useful 
initialisation 
for 
Factor 
Analysis. 


Example 
92 
(A 
Comparison 
of 
FA 
and 
PPCA). 
We 
trained 
both 
PPCA 
and 
FA 
to 
model 
handwritten 
digits 
of 
the 
number 
7. 
From 
a 
database 
of 
100 
such 
images, 
we 
tted 
both 
PPCA 
and 
FA 
(100 
iterations 
of 
EM 
in 
each 
case 
from 
the 
same 
random 
initialisation) 
using 
5 
hidden 
units. 
The 
learned 
factors 
for 
these 
models 
are 
in 
g(21.7). 
To 
get 
a 
feeling 
for 
how 
well 
each 
of 
these 
models 
the 
data, 
we 
drew 
25 
samples 
from 
each 
model, 
as 
given 
in 
g(21.8a). 
Compared 
with 
PPCA, 
in 
FA 
the 
individual 
noise 
on 
each 
observation 
pixel 
enables 
a 
cleaner 
representation 
of 
the 
regions 
of 
zero 
sample 
variance. 


21.5 
Canonical 
Correlation 
Analysis 
and 
Factor 
Analysis 
We 
outline 
how 
CCA, 
as 
discussed 
in 
section(15.8), 
is 
related 
to 
a 
constrained 
form 
of 
FA. 
As 
a 
brief 
reminder, 
CCA 
considers 
two 
spaces 
X 
and 
Y 
where, 
for 
example, 
X 
might 
represent 
an 
audio 
sequence 
of 
a 
person 
speaking 
and 
Y 
the 
corresponding 
video 
sequence 
of 
the 
face 
of 
the 
person 
speaking. 
The 
two 
streams 
of 
data 
are 
dependent 
since 
we 
would 
expect 
the 
parts 
around 
the 
mouth 
region 
to 
be 
correlated 
with 
the 
speech 
signal. 
The 
aim 
in 
CCA 
is 
to 
nd 
a 
low 
dimensional 
representation 
that 
explains 
the 
correlation 
between 
the 
X 
and 
Y 
spaces. 
A 
model 
that 
achieves 
a 
similar 
eect 
to 
CCA 
is 
to 
use 
a 
latent 
factor 
h 
to 
underlie 
the 
data 
in 
both 
the 
X 
and 
Y 
spaces. 
That 
is 


Z

p(x, 
y)=p(xjh)p(yjh)p(h)dh 
(21.5.1) 


where 


p(xjh)= 
N 
(x 


ha, 
	x) 
;p(yjh)= 
N 
(y 


hb, 
	y) 
;p(h)= 
N 
(h 


0, 
1) 
(21.5.2) 



Figure 
21.8: 
(a): 
25 
samples 
from 
the 
learned 
FA 
model. 
Note 
how 
the 
noise 
variance 
depends 
on 
the 
pixel, 
being 
zero 
for 
pixels 
on 
the 
boundary 
of 
the 
image. 
(b): 
25 
samples 
from 
the 
learned 
PPCA 
model. 


(a) 
Factor 
Analysis 
(b) 
PPCA 
DRAFT 
March 
9, 
2010 



Independent 
Components 
Analysis 


h
xy
Canonical 
Correlation 
Analysis. 
CCA 
corresponds 
to 
the 
latent 
variable 
model 
in 
which 
a 
common 
latent 
variable 
generates 
both 
the 
observed 
x 
and 
y 
variables. 
This 
is 
therefore 
a 
formed 
of 
constrained 
Factor 
Analysis. 


We 
can 
express 
equation 
(21.5.2) 
as 
a 
form 
of 
Factor 
Analysis 
by 
writing


x 
y
=
a 
bh 
+x
y
;x 
~ 
N 
(x 
0, 
	x) 
;y 
~ 
N 
(y 
0, 
	y) 
(21.5.3) 
By 
using 
the 
stacked 
vectors 
z 
=
x 
y, 
f 
=
a 
b, 
(21.5.4) 
and 
integrating 
out 
the 
latent 
variable 
h, 
we 
obtain 
p(z) 
= 
N 
(z 
0, 
) 
, 
S 
= 
T 
+ 
	, 
. 
=
	x 
0 
0 
	y
(21.5.5) 
From 
the 
FA 
results 
equation 
(21.2.5) 
the 
optimal 
f 
is 
given 
by 
f1 
+ 
fT	..1f= 
S	..1f 
. 
f 
. 
S	..1f 
(21.5.6) 
so 
that 
optimally 
f 
is 
given 
by 
the 
principal 
eigenvector 
of 
S	..1 
. 
By 
imposing 
	x 
= 
2 
xI, 
	y 
= 
2 
yI 
the 
above 
equation 
can 
be 
expressed 
as 
the 
coupled 
equations 
a 
. 
1 
2 
x 
Sxxa 
+ 
1 
2 
y 
Sxyb 
(21.5.7) 
b 
. 
1 
2 
x 
Syxa 
+ 
1 
2 
y 
Syyb 
(21.5.8) 
Eliminating 
b 
we 
have, 
for 
an 
arbitrary 
proportionality 
constant 
,
I 
- 
. 
2 
x 
Sxxa 
= 
2 
2 
x2 
y 
SxyI 
- 
. 
2 
y 
Syy..1 
Syxa 
(21.5.9) 


In 
the 
limit 
2;2 
. 
0, 
this 
tends 
to 
the 
zero 
derivative 
condition 
equation 
(15.8.8) 
so 
that 
CCA 
can 
be 


xy 


seen 
as 
in 
fact 
a 
form 
limiting 
of 
FA 
(see 
[12] 
for 
a 
more 
thorough 
correspondence). 
By 
viewing 
CCA 
in 
this 
manner 
extensions 
to 
using 
more 
than 
a 
single 
latent 
dimension 
H 
become 
clear, 
see 
exercise(208), 
in 
addition 
to 
the 
benets 
of 
a 
probabilistic 
interpretation. 


As 
we've 
indicated, 
CCA 
corresponds 
to 
training 
a 
form 
of 
FA 
by 
maximising 
the 
joint 
likelihood 
p(x, 
yjw, 
u). 
Alternatively, 
training 
based 
on 
the 
maximising 
the 
conditional 
p(yjx, 
w, 
u) 
corresponds 
to 
a 
special 
case 
of 
a 
technique 
called 
Partial 
Least 
Squares, 
see 
for 
example 
[78]. 
This 
correspondence 
is 
left 
as 
an 
exercise 
for 
the 
interested 
reader. 


Extending 
FA 
to 
kernel 
variants 
is 
not 
straightforward 
since 
under 
replacing 
x 
with 
a 
non-linear 
mapping 


..((x)..wh)2 


(x), 
normalising 
the 
expression 
eis 
in 
general 
intractable. 


21.6 
Independent 
Components 
Analysis 
Independent 
Components 
Analysis 
(ICA) 
is 
a 
linear 
decomposition 
of 
the 
data 
v 
in 
which 
the 
components 
of 
underlying 
latent 
vector 
variable 
h 
are 
independent[221, 
138]. 
In 
other 
words, 
we 
seek 
a 
linear 
coordinate 
system 
in 
which 
the 
coordinates 
are 
independent. 
Such 
independent 
coordinate 
systems 
arguably 
form 


DRAFT 
March 
9, 
2010 
399 



Independent 
Components 
Analysis 


-2-1.5-1-0.500.511.5-1.5-1-0.500.511.5
Figure 
21.9: 
Latent 
data 
is 
sampled 
from 
the 
prior 
p(xi) 
/

pY

exp(..5 
jxij) 
with 
the 
mixing 
matrix 
A 
shown 
in 
green 
to 
create 
the 
observed 
two 
dimensional 
vectors 
y 
= 
Ax. 
The 
red 
lines 
are 
the 
mixing 
matrix 
estimated 
by 
ica.m 
based 
on 
the 
observations. 
For 
comparison, 
PCA 
produces 
the 
blue 
(dashed) 
components. 
Note 
that 
the 
components 
have 
been 
scaled 
to 
improve 
visualisation. 
As 
expected, 
PCA 
nds 
the 
orthogonal 
directions 
of 
maximal 
variation. 
ICA 
however, 
correctly 
estimates 
the 
directions 
in 
which 
the 
components 
were 
independently 
generated. 
See 
demoICA.m. 


a 
natural 
representation 
of 
the 
data 
and 
can 
give 
rise 
to 
very 
dierent 
representations 
than 
PCA, 
see 
g(21.9). 
From 
a 
probabilistic 
viewpoint, 
the 
model 
is 
described 
by 


Y

p(v, 
hjA)= 
p(vjh, 
A)p(hi) 
(21.6.1) 


i 


In 
ICA 
it 
is 
common 
to 
assume 
that 
the 
observations 
are 
linearly 
related 
to 
the 
latent 
variables 
h. 
For 
technical 
reasons, 
the 
most 
convenient 
practical 
choice 
is 
to 
use3 


v 
= 
Ah 
(21.6.2) 


where 
A 
is 
a 
square 
mixing 
matrix 
so 
that 
the 
likelihood 
of 
an 
observation 
v 
is 


YYY

1 


p(v)= 
p(vjh, 
A)p(hi)dh 
= 
d 
(v 
- 
Ah)p(hi)dh 
= 
p(A..1v) 
(21.6.3)

i

jdet 
(A) 
j

i 
ii 


The 
underlying 
independence 
assumptions 
are 
then 
the 
same 
as 
for 
PPCA 
(in 
the 
limit 
of 
zero 
output 
noise). 
Below, 
however, 
we 
will 
choose 
a 
non-Gaussian 
prior 
p(hi). 


..

For 
a 
given 
set 
of 
data 
V 
=v1 
;:::, 
vNand 
prior 
p(h), 
our 
aim 
is 
to 
nd 
A. 
For 
i.i.d. 
data, 
the 
log 
likelihood 
is 
conveniently 
written 
in 
terms 
of 
B 
= 
A..1 
, 


XX

L(B)= 
N 
log 
det 
(B) 
+log 
p([Bvn]i) 
(21.6.4) 


ni 


Note 
that 
for 
a 
Gaussian 
prior 


p(h) 
. 
e 
..h2 
(21.6.5) 


the 
log 
likelihood 
becomes 


X

L(B)= 
N 
log 
det 
(B) 
..(vn)T 
BTBvn 
+ 
const. 
(21.6.6) 


n 


which 
is 
invariant 
with 
respect 
to 
an 
orthogonal 
rotation 
B 
. 
RB, 
with 
RTR 
= 
I. 
This 
means 
that 
for 
a 
Gaussian 
prior 
p(h), 
we 
cannot 
estimate 
uniquely 
the 
mixing 
matrix. 
To 
break 
this 
rotational 
invariance 
we 
therefore 
need 
to 
use 
a 
non-Gaussian 
prior. 
Assuming 
we 
have 
a 
non-Gaussian 
prior 
p(h), 
taking 
the 
derivative 
w.r.t. 
Bab 
we 
obtain 


X

n

@
L(B)= 
NAba 
+([Bv])v 
(21.6.7)

@Bab 
ab 
n 


where 
(x) 
= 
d 
dx 
log 
p(x) 
= 
1 
p(x) 
d 
dx
p(x) 
3This 
treatment 
follows 
that 
presented 
in 
[183]. 
(21.6.8) 
400 
DRAFT 
March 
9, 
2010 



Exercises 


A 
simple 
gradient 
ascent 
learning 
rule 
for 
B 
is 
then 


 !

X

Bnew 
= 
B 
+ 
B..T 
+
1 
(Bvn)(vn)T(21.6.9)

N

n 


An 
alternative 
`natural 
gradient’ 
algorithm[8, 
183] 
that 
approximates 
a 
Newton 
update 
is 
given 
by 
multiplying 
the 
gradient 
by 
BTB 
on 
the 
right 
to 
give 
the 
update 


 !

X

Bnew 
= 
B 
+ 
I 
+
1 
(Bvn)(Bvn)TB 
(21.6.10)

N

n 


Here 
. 
is 
a 
learning 
rate 
which 
in 
the 
code 
ica.m 
we 
nominally 
set 
to 
0.5. 


A 
natural 
extension 
is 
to 
consider 
noise 
on 
the 
outputs, 
exercise(212), 
for 
which 
an 
EM 
algorithm 
is 
readily 
available. 
However, 
in 
the 
limit 
of 
low 
output 
noise, 
the 
EM 
formally 
fails, 
an 
eect 
which 
is 
related 
to 
the 
general 
discussion 
in 
section(11.4). 


A 
popular 
alternative 
estimation 
method 
is 
FastICA4 
and 
can 
be 
related 
to 
an 
iterative 
Maximum 
Likelihood 
optimisation 
procedure. 
ICA 
can 
also 
be 
motivated 
from 
several 
alternative 
directions, 
including 
information 
theory[29]. 
We 
refer 
the 
reader 
to 
[138] 
for 
an 
in-depth 
discussion 
of 
ICA 
and 
related 
extensions. 


21.7 
Code 
FA.m: 
Factor 
Analysis 
demoFA.m: 
Demo 
of 
Factor 
Analysis 
ica.m: 
Independent 
Components 
Analysis 
demoIca.m: 
Demo 
ICA 


21.8 
Exercises 
Exercise 
207. 
Factor 
Analysis 
and 
scaling. 
Assume 
that 
a 
H-factor 
model 
holds 
for 
x. 
Now 
consider 
the 
the 
transformation 
y 
= 
Cx, 
where 
C 
is 
a 
non-singular 
square 
diagonal 
matrix. 
Show 
that 
Factor 
Analysis 
is 
scale 
invariant, 
i.e. 
that 
the 
H-factor 
model 
also 
holds 
for 
y, 
with 
the 
factor 
loadings 
appropriately 
scaled. 
How 
must 
the 
specic 
factors 
be 
scaled? 


Exercise 
208. 
For 
the 
constrained 
Factor 
Analysis 
model 




A0 


x 
=h 
+;N 
(

0, 
diag 
( 1;:::; n)) 
, 
h 
N 
(h 


0, 
I) 
(21.8.1)

0B

derive 
a 
Maximum 
Likelihood 
EM 
algorithm 
for 
the 
matrices 
A 
and 
B, 
assuming 
the 
datapoints 
x1 
;:::, 
xN 
are 
i.i.d. 


Exercise 
209. 
An 
apparent 
extension 
of 
FA 
analysis 
is 
to 
consider 
a 
correlated 
prior 


p(h)= 
N 
(h 


0, 
H 
) 
(21.8.2) 


Show 
that, 
provided 
no 
constraints 
are 
placed 
on 
the 
factor 
loading 
matrix 
F, 
using 
a 
correlated 
prior 
p(h) 
is 
an 
equivalent 
model 
to 
the 
original 
uncorrelated 
FA 
model. 


Exercise 
210. 
Using 
the 
Woodbury 
identity 
and 
the 
denition 
of 
D 
in 
equation 
(21.2.2), 
show 
that 
one 
can 
rewrite 
D 
..1F 
as 
..1 
D 
..1F 
= 
	..1FI 
+ 
FT	..1F 
(21.8.3) 


4See 
www.cis.hut.fi/projects/ica/fastica/ 


DRAFT 
March 
9, 
2010 



Exercises 


Exercise 
211. 
For 
the 
log 
likelihood 
function 


XX

N 
H1 
D

L(2)= 
- 
D 
log(2) 
+ 
log 
i 
+ 
i 
+(D 
- 
H) 
log 
2 
+ 
H 
(21.8.4)

2 
2 


i=1 
i=H+1 


Show 
L(2) 
is 
maximal 
for 


X

1 
D

2 
= 
j 
(21.8.5)

D 
- 
H 


j=H+1 


Exercise 
212. 
Consider 
an 
ICA 
model 


YY

p(y, 
xjW)=p(yjjx, 
W)p(xi), 
W 
=[w1;:::, 
wJ 
] 
(21.8.6) 


ji 


with 




T 


p(yjjx, 
W)= 
Nyj 


wj 
x;2(21.8.7) 


1. 
For 
the 
above 
model 
derive 
an 
EM 
algorithm 
for 
a 
set 
of 
i.i.d. 
data 
y1 
;:::, 
yN 
and 
show 
that 
the 

Y
required 
statistics 
for 
the 
M-step 
are 
hxip(xjyn 
;W) 
and 
xxT 
p(xjyn 
;W). 


2. 
Show 
that 
for 
a 
non-Gaussian 
prior 
p(xi), 
the 
posterior 
p(xjy, 
W) 
(21.8.8) 
is 
non-factorised, 
non-Gaussian 
and 
generally 
intractable 
(its 
normalisation 
constant 
cannot 
be 
comp
uted 
eciently). 


3. 
Show 
that 
in 
the 
limit 
2 
. 
0, 
the 
EM 
algorithm 
fails. 
DRAFT 
March 
9, 
2010 



CHAPTER 
22 


Latent 
Ability 
Models 


22.1 
The 
Rasch 
Model 
Consider 
an 
exam 
in 
which 
student 
s 
answers 
question 
q 
either 
correctly 
xqs 
= 
1 
or 
incorrectly 
xqs 
= 
0. 
For 
a 
set 
of 
N 
students 
and 
Q 
questions, 
the 
performance 
of 
all 
students 
is 
given 
in 
the 
Q 
× 
N 
binary 
matrix 
X. 
Based 
on 
this 
data 
alone 
we 
wish 
to 
evaluate 
the 
ability 
of 
each 
student. 
One 
approach 
is 
to 
dene 
the 
ability 
as 
as 
the 
fraction 
of 
questions 
student 
s 
answered 
correctly. 
A 
more 
subtle 
analysis 
is 
to 
accept 
that 
some 
questions 
are 
more 
dicult 
than 
others 
so 
that 
a 
student 
who 
answered 
dicult 
questions 
should 
be 
awarded 
more 
highly 
than 
a 
student 
who 
answered 
the 
same 
number 
of 
easy 
questions. 
A 
priori 
we 
do 
not 
know 
which 
are 
the 
dicult 
questions 
and 
this 
needs 
to 
be 
estimated 
based 
on 
X. 
To 
account 
for 
inherent 
dierences 
in 
question 
diculty 
we 
may 
model 
the 
probability 
that 
a 
student 
s 
gets 
a 
question 
q 
correct 
based 
on 
the 
student's 
latent 
ability 
s 
and 
the 
latent 
diculty 
of 
the 
question 
q.A 
simple 
generative 
model 
of 
the 
response 
is 


p(xqs 
=1j, 
)= 
s 
(s 
- 
q) 
(22.1.1) 


where 
s 
(x)=1=(1 
+ 
e..x). 
Under 
this 
model, 
the 
higher 
the 
latent 
ability 
is 
above 
the 
latent 
diculty 
of 
the 
question, 
the 
more 
likely 
it 
is 
that 
the 
student 
will 
answer 
the 
question 
correctly. 


22.1.1 
Maximum 
Likelihood 
training 
Making 
the 
i.i.d. 
assumption, 
the 
likelihood 
of 
the 
data 
X 
under 
this 
model 
is 


SQ

YYX

p(Xj, 
)= 
s 
(s 
- 
q)xqs 
(1 
- 
s 
(s 
- 
q))1..xqs 
(22.1.2) 
s=1 
q=1 


The 
log 
likelihood 
is 


X

L 
= 
log 
p(Xj, 
)=xqs 
log 
s 
(s 
- 
q) 
+ 
(1 
- 
xqs) 
log 
(1 
- 
s 
(s 
- 
q)) 
(22.1.3) 
q;s 


xqss
q
S
Q
Figure 
22.1: 
The 
Rasch 
model 
for 
analysing 
questions. 
Each 
element 
of 
the 
binary 
matrix 
X, 
with 
xqs 
= 
1 
if 
student 
s 
gets 
question 
q 
correct, 
is 
generated 
using 
the 
latent 
ability 
of 
the 
student 
s 
and 
the 
latent 
diculty 
of 
the 
question 
q. 


403 



Competition 
Models 


with 
derivatives 


QS

XX

@L 
@L 


=(xqs 
- 
s 
(s 
- 
q)) 
, 
= 
- 
(xqs 
- 
s 
(s 
- 
q)) 
(22.1.4)

@s 
@q

q=1 
s=1 


A 
simple 
way 
to 
learn 
the 
parameters 
is 
to 
use 
gradient 
ascent, 
see 
demoRasch.m, 
with 
extensions 
to 
Newton 
methods 
being 
straightforward. 


The 
generalisation 
to 
more 
than 
two 
responses 
xqs 
2f1, 
2;:::} 
can 
be 
achieved 
using 
a 
softmax-style 
function. 
More 
generally, 
the 
Rasch 
model 
is 
an 
example 
of 
an 
item 
response 
theory, 
a 
subject 
dealing 
with 
the 
analysis 
of 
questionnaires[94]. 


Missing 
data 


Assuming 
the 
data 
is 
missing 
at 
random, 
missing 
data 
can 
be 
treated 
by 
computing 
the 
likelihood 
of 
only 
the 
observed 
elements 
of 
X. 
In 
rasch.m 
missing 
data 
is 
assumed 
to 
be 
coded 
as 
nan 
so 
that 
the 
likelihood 
and 
gradients 
are 
straightforward 
to 
compute 
based 
on 
summing 
only 
over 
terms 
containing 
non 
nan 
entries. 


Example 
93. 
We 
display 
an 
example 
of 
the 
use 
of 
the 
Rasch 
model 
in 
g(22.2), 
estimating 
the 
latent 
abilities 
of 
20 
students 
based 
on 
a 
set 
of 
50 
questions. 
Based 
on 
using 
the 
number 
of 
questions 
each 
student 
answered 
correctly, 
the 
best 
students 
are 
(ranked 
from 
rst) 
8, 
6, 
1, 
19, 
4, 
17, 
20, 
7, 
15, 
5, 
12, 
16, 
2, 
3, 
18, 
9, 
11, 
14, 
10, 
13. 
Alternatively, 
ranking 
students 
according 
to 
the 
latent 
ability 
gives 
8, 
6, 
19, 
1, 
20, 
4, 
17, 
7, 
15, 
12, 
5, 
16, 
2, 
3, 
18, 
9, 
11, 
14, 
10, 
13. 
This 
diers 
(only 
slightly 
in 
this 
case) 
from 
the 
number-correct 
ranking 
since 
the 
Rasch 
model 
takes 
into 
account 
the 
fact 
that 
some 
students 
answered 
dicult 
questions 
correctly. 
For 
example 
student 
20 
answered 
some 
dicult 
questions 
correctly. 


22.1.2 
Bayesian 
Rasch 
models 
The 
Rasch 
model 
will 
potentially 
overt 
the 
data 
especially 
when 
there 
is 
only 
a 
small 
amount 
of 
data. 
For 
this 
case 
a 
natural 
extension 
is 
to 
use 
a 
Bayesian 
technique, 
placing 
independent 
priors 
on 
the 
ability 
and 
question 
diculty, 
so 
that 
the 
posterior 
ability 
and 
question 
diculty 
is 
given 
by 


p(, 
jX) 
. 
p(Xj, 
)p()p() 
(22.1.5) 


Natural 
priors 
are 


Y..Y..

p()=Ns 


0;2;p()=Nq 


0;t 
2(22.1.6) 


sq 


where 
2 
and 
2 
are 
hyperparameters 
that 
can 
be 
learned 
by 
maximising 
p(Xj2;2). 
Even 
using 
Gaussian 
priors, 
the 
posterior 
distribution 
p(, 
jX) 
is 
not 
of 
a 
standard 
form 
and 
approximations 
are 
required. 
In 
this 
case 
however, 
the 
posterior 
is 
log 
concave 
so 
that 
approximation 
methods 
based 
on 
variational 
or 
Laplace 
techniques 
are 
potentially 
adequate, 
or 
alternatively 
one 
may 
use 
sampling 
approximations. 


22.2 
Competition 
Models 
22.2.1 
Bradly-Terry-Luce 
model 
The 
Bradly-Terry-Luce 
model 
assesses 
the 
ability 
of 
players 
based 
on 
one-on-one 
matches. 
Here 
we 
describe 
games 
in 
which 
only 
win/lose 
outcomes 
arise, 
leaving 
aside 
the 
complicating 
possibility 
of 
draws. 
For 
this 


404 
DRAFT 
March 
9, 
2010 



Competition 
Models 


questionstudent51015202530354045502468101214161820
05101520253035404550-8-6-4-20246questionestimated difficulty
(a) 
(b) 
0246810121416182000.10.20.30.40.50.60.7studentfraction of questions correct
02468101214161820-2.5-2-1.5-1-0.500.511.5studentestimated ability
(c) 
(d) 
Figure 
22.2: 
Rasch 
model. 
(a): 
The 
data 
of 
correct 
answers 
(white) 
and 
incorrect 
answers 
(black). 
(b): 
The 
estimated 
latent 
diculty 
of 
each 
question. 
(c): 
The 
fraction 
of 
questions 
each 
student 
answered 
correctly. 
(d): 
The 
estimated 
latent 
ability. 


win/lose 
scenario, 
the 
BTL 
model 
is 
a 
straightforward 
modication 
of 
the 
Rasch 
model 
so 
that 
for 
latent 
ability 
i 
of 
player 
i 
and 
latent 
ability 
j 
of 
player 
j, 
the 
probability 
that 
i 
beats 
j 
is 
given 
by 


p(iBjj)= 
s 
(i 
- 
j) 
(22.2.1) 


where 
iBj 
stands 
for 
player 
i 
beats 
player 
j. 
Based 
on 
a 
matrix 
of 
games 
data 
X 
with 




1 
if 
iBj 
in 
game 
n

n 


xij 
=(22.2.2)

0 
otherwise 


the 
likelihood 
of 
the 
model 
is 
given 
by 


YYY

n 


p(Xj)=[s 
(i 
- 
j)]xij 
=[s 
(i 
- 
j)]Mij 
(22.2.3) 
nij 
ij 


P

n

where 
Mij 
=n 
xij 
is 
the 
number 
of 
times 
player 
i 
beat 
player 
j. 
Training 
using 
Maximum 
Likelihood 
or 
a 
Bayesian 
technique 
can 
then 
proceed 
as 
for 
the 
Rasch 
model. 


For 
the 
case 
of 
only 
two 
objects 
interacting, 
these 
models 
are 
called 
pairwise 
comparison 
models. 
Thurstone 
in 
the 
1920's 
applied 
such 
models 
to 
a 
wide 
range 
of 
data, 
and 
the 
Bradley-Terry-Luce 
model 
is 
in 
fact 
a 
special 
case 
of 
his 
work[73]. 


Example 
94. 
An 
example 
application 
of 
the 
BTL 
model 
is 
given 
in 
g(22.3) 
in 
which 
a 
matrix 
X 
containing 
the 
number 
of 
times 
that 
competitor 
i 
beat 
competitor 
j 
is 
given. 
The 
matrix 
entries 
X 
were 
drawn 
from 
a 
BTL 
model 
based 
on 
`true 
abilities'. 
Using 
X 
alone 
the 
Maximum 
Likelihood 
estimate 
of 
these 
latent 
abilities 
is 
in 
close 
agreement 
with 
the 
true 
abilities. 


DRAFT 
March 
9, 
2010 



Competition 
Models 


competitorcompetitor 
2040608010010203040506070809010000.511.522.533.544.55
-10-50510-10-50510true abilityestimated ability
(a) 
(b) 
Figure 
22.3: 
BTL 
model. 
(a): 
The 
data 
X 
with 
Xij 
being 
the 
number 
of 
times 
that 
competitor 
i 
beat 
competitor 
j. 
(b): 
The 
true 
versus 
estimated 
ability. 
Even 
though 
the 
data 
is 
quite 
sparse, 
a 
reasonable 
estimate 
of 
the 
latent 
ability 
of 
each 
competitor 
is 
found. 


22.2.2 
Elo 
ranking 
model 
The 
Elo 
system 
[89] 
used 
in 
chess 
ranking 
is 
closely 
related 
to 
the 
BTL 
model 
above, 
though 
there 
is 
the 
added 
complication 
of 
the 
possibility 
of 
draws. 
In 
addition, 
the 
Elo 
system 
takes 
into 
account 
a 
measure 
of 
the 
variability 
in 
performance. 
For 
a 
given 
ability 
i, 
the 
actual 
performance 
i 
of 
player 
i 
in 
a 
game 
is 
given 
by 


i 
= 
i 
+ 
i 
(22.2.4) 


..



where 
i 
Ni 


0;2. 
The 
variance 
2 
is 
xed 
across 
all 
players 
and 
thus 
takes 
into 
account 
intrinsic 
variability 
in 
the 
performance. 
More 
formally 
the 
Elo 
model 
modies 
the 
BTL 
model 
to 
give 


..



p(Xj)= 
p(Xj)p(j);p(j)= 
Np 


;2I(22.2.5) 


p 


where 
p(Xj) 
is 
given 
by 
equation 
(22.2.3) 
on 
replacing 
a 
with 
. 


22.2.3 
Glicko 
and 
TrueSkill 
Glicko[114] 
and 
TrueSkill[130] 
are 
essentially 
Bayesian 
versions 
of 
the 
Elo 
model 
with 
the 
renement 
that 
the 
latent 
ability 
is 
modelled, 
not 
by 
a 
single 
number, 
but 
by 
a 
Gaussian 
distribution 


..



p(iji)= 
Ni 


i;2 
(22.2.6)

i

This 
can 
capture 
the 
fact 
that 
a 
player 
may 
be 
consistently 
reasonable 
(quite 
high 
i 
and 
low 
2) 
or 
an 


i 
erratic 
genius 
(high 
i 
but 
with 
large 
2). 
The 
parameters 
of 
the 
model 
are 
then 


i 


	

. 
=i;i 
2;i 
=1;:::;S(22.2.7) 


for 
a 
set 
of 
S 
players. 
The 
interaction 
model 
p(Xj) 
is 
as 
for 
the 
win/lose 
Elo 
model, 
equation 
(22.2.1). 
The 
likelihood 
for 
the 
model 
given 
the 
parameters 
is 


p(Xj)= 
p(Xj)p(j) 
(22.2.8) 


a 


This 
integral 
is 
formally 
intractable 
and 
numerical 
approximations 
are 
required. 
In 
this 
context 
Expectation 
Propagation 
has 
proven 
to 
be 
a 
useful 
technique[195]. 


The 
TrueSkill 
system 
is 
used 
for 
example 
to 
assess 
the 
abilities 
of 
players 
in 
online 
gaming, 
also 
taking 
into 
account 
the 
abilities 
of 
teams 
of 
individuals 
in 
tournaments. 
A 
temporal 
extension 
has 
recently 
been 
used 
to 
reevaluate 
the 
change 
in 
ability 
of 
chess 
players 
with 
time[72]. 


DRAFT 
March 
9, 
2010 



Exercises 


22.3 
Code 
rasch.m: 
Rasch 
model 
training 
demoRasch.m: 
Demo 
for 
the 
Rasch 
model 


22.4 
Exercises 
Exercise 
213 
(Bucking 
Bronco). 
bronco.mat 
contains 
information 
about 
a 
bucking 
bronco 
competition. 
There 
are 
500 
competitors 
and 
20 
bucking 
broncos. 
A 
competitor 
j 
attempts 
to 
stay 
on 
a 
bucking 
bronco 
i 
for 
a 
minute. 
If 
the 
competitor 
succeeds, 
the 
entry 
Xij 
is 
1, 
otherwise 
0. 
Each 
competitor 
gets 
to 
ride 
three 
bucking 
broncos 
only 
(the 
missing 
data 
is 
coded 
as 
nan). 
Having 
viewed 
all 
the 
500 
amateurs, 
desperate 
Dan 
enters 
the 
competition 
and 
bribes 
the 
organisers 
into 
letting 
him 
avoid 
having 
to 
ride 
the 
dicult 
broncos. 
Based 
on 
using 
a 
Rasch 
model, 
which 
are 
the 
top 
10 
most 
dicult 
broncos, 
in 
order 
of 
the 
most 
dicult 
rst? 


Exercise 
214 
(BTL 
training). 


1. 
Show 
that 
the 
log 
likelihood 
for 
the 
Bradly-Terry-Luce 
model 
is 
given 
by 
H 


L()= 
Xij 
log 
s 
(i 
- 
j) 
(22.4.1) 
ij 


where 
Xij 
is 
the 
number 
of 
times 
that 
player 
i 
beats 
player 
j 
in 
a 
set 
of 
games. 


2. 
Compute 
the 
gradient 
of 
L(). 
3. 
Compute 
the 
Hessian 
of 
the 
BTL 
model 
and 
verify 
that 
it 
is 
negative 
semidenite. 
Exercise 
215 
(La 
Reine). 


1. 
Program 
a 
simple 
gradient 
ascent 
routine 
to 
learn 
the 
latent 
abilities 
of 
competitors 
based 
on 
a 
series 
of 
win/lose 
outcomes. 
2. 
In 
a 
modied 
form 
of 
Swiss 
cow 
`ghting', 
a 
set 
of 
cows 
compete 
by 
pushing 
each 
other 
until 
submiss
ion. 
At 
the 
end 
of 
the 
competition 
one 
cow 
is 
deemed 
to 
be 
`la 
reine'. 
Based 
on 
the 
data 
in 
BTL.mat 
(for 
which 
Xij 
contains 
the 
number 
of 
times 
cow 
i 
beat 
cow 
j), 
t 
a 
BTL 
model 
and 
return 
a 
ranked 
list 
of 
the 
top 
ten 
best 
ghting 
cows, 
`la 
reine’ 
rst. 
Exercise 
216. 
An 
extension 
of 
the 
BTL 
model 
is 
to 
consider 
additional 
`factors’ 
that 
describe 
the 
state 
of 
the 
competitors 
when 
they 
play. 
For 
example, 
we 
have 
a 
set 
of 
S 
football 
teams, 
and 
a 
set 
of 
matrices 
X1 
;:::, 
XN 
, 
with 
Xij
n 
=1 
if 
team 
i 
beat 
team 
j 
in 
match 
n. 
In 
addition 
we 
have 
for 
each 
match 
and 
team 
a 
vector 
of 
binary 
factors 
fn 
2f0, 
1} 
that 
describes 
the 
team. 
For 
example, 
for 
the 
team 
i 
=1 
(Madchester

i 


United), 
the 
factor 
f1;1 
=1 
if 
Bozo 
is 
playing, 
0 
if 
not. 
It 
is 
suggested 
that 
the 
ability 
of 
team 
i 
in 
game 
n 
is 
measured 
by 


H

H 


n 
= 
di 
+ 
wh;ifn 
(22.4.2)

i 
h;i 
h=1 


where 
fn 
=1 
if 
factor 
h 
is 
present 
in 
team 
i 
in 
game 
n. 
di 
is 
a 
default 
latent 
ability 
of 
the 
team 
which 


h;i 
is 
assumed 
constant 
across 
all 
games. 
We 
have 
such 
a 
set 
of 
factors 
for 
each 
match, 
giving 
fn 


h;i. 


1. 
Using 
the 
above 
denition 
of 
the 
latent 
ability 
in 
the 
BTL 
model, 
our 
interest 
is 
to 
nd 
the 
weights 
W 
and 
abilities 
d 
that 
best 
predict 
the 
ability 
of 
the 
team, 
given 
that 
we 
have 
a 
set 
of 
historical 
plays 
(Xn 
, 
Fn);n 
=1;:::;N. 
Write 
down 
the 
likelihood 
for 
the 
BTL 
model 
as 
a 
function 
of 
the 
set 
of 
all 
team 
weights 
W, 
d. 
DRAFT 
March 
9, 
2010 
407 



Exercises 


2. 
Compute 
the 
gradient 
of 
the 
log 
likelihood 
of 
this 
model. 
3. 
Explain 
how 
this 
model 
can 
be 
used 
to 
assess 
the 
importance 
of 
Bozo's 
contribution 
to 
Madchester 
United's 
ability. 
4. 
Given 
learned 
W, 
d 
and 
the 
knowledge 
that 
Madchester 
United 
(team 
1) 
will 
play 
Chelski 
(team 
2) 
tomorrow 
explain 
how, 
given 
the 
list 
of 
factors 
f 
for 
Chelski 
(which 
includes 
issues 
such 
as 
who 
will 
be 
playing 
in 
the 
team), 
one 
can 
select 
the 
best 
Madchester 
United 
team 
to 
maximise 
the 
probability 
of 
winning 
the 
game. 
DRAFT 
March 
9, 
2010 



Part 
IV 
Dynamical 
Models 


409 



CHAPTER 
23 


Discrete-State 
Markov 
Models 


23.1 
Markov 
Models 
Time-series 
are 
datasets 
for 
which 
the 
constituent 
datapoints 
can 
be 
naturally 
ordered. 
This 
order 
often 
corresponds 
to 
an 
underlying 
single 
physical 
dimension, 
typically 
time, 
though 
any 
other 
single 
dimension 
may 
be 
used. 
The 
time-series 
models 
we 
consider 
are 
probability 
models 
over 
a 
collection 
of 
random 
variables 
v1;:::;vT 
with 
individual 
variables 
vt 
indexed 
by 
a 
time 
index 
t. 
These 
indices 
are 
elements 
of 
the 
index 
set 
T 
. 
For 
nonnegative 
indices, 
T 
= 
N+, 
the 
model 
is 
a 
discrete-time 
process. 
Continuous-
time 
processes, 
T 
= 
R, 
are 
natural 
in 
particular 
application 
domains 
yet 
require 
additional 
notation 
and 
concepts. 
We 
therefore 
focus 
exclusively 
on 
discrete-time 
models. 
A 
probabilistic 
time-series 
model 
requires 
a 
specication 
of 
the 
joint 
distribution 
p(v1;:::;vT 
). 
For 
the 
case 
in 
which 
the 
observed 
data 
vt 
is 
discrete, 
the 
joint 
probability 
table 
for 
p(v1;:::;vT 
) 
has 
exponentially 
many 
entries. 
We 
cannot 
expect 
to 
independently 
specify 
all 
the 
exponentially 
many 
entries 
and 
are 
therefore 
forced 
to 
make 
simplied 
models 
under 
which 
these 
entries 
can 
be 
parameterised 
in 
a 
lower 
dimensional 
manner. 
Such 
simplications 
are 
at 
the 
heart 
of 
time-series 
modelling 
and 
we 
will 
discuss 
some 
classical 
models 
in 
the 
following 
sections. 


Denition 
107 
(Time-Series 
Notation). 


xa:b 
= 
xa;xa+1;:::;xb, 
with 
xa:b 
= 
xa 
for 
b 
= 
a 
(23.1.1) 


For 
timeseries 
data 
v1;:::;vT 
, 
we 
need 
a 
model 
p(v1:T 
). 
For 
consistency 
with 
the 
causal 
nature 
of 
time, 
it 
is 
meaningful 
to 
consider 
the 
decomposition 


T

T 


p(v1:T 
)= 
p(vtjv1:t..1) 
(23.1.2) 
t=1 


with 
the 
convention 
p(vtjv1:t..1)= 
p(v1) 
for 
t 
= 
1. 
It 
is 
often 
natural 
to 
assume 
that 
the 
inuence 
of 
the 
immediate 
past 
is 
more 
relevant 
than 
the 
remote 
past 
and 
in 
Markov 
models 
only 
a 
limited 
number 
of 
previous 
observations 
are 
required 
to 
predict 
the 
future. 


Denition 
108 
(Markov 
chain). 
A 
Markov 
chain 
dened 
on 
either 
discrete 
or 
continuous 
variables 
v1:T 
is 
one 
in 
which 
the 
following 
conditional 
independence 
assumption 
holds: 


p(vtjv1;:::;vt..1)= 
p(vtjvt..L;:::;vt..1) 
(23.1.3) 


411 



v1v2v3v4v1v2v3v4v1v2v3v4v1v2v3v4
Markov 
Models 


(a) 
(b) 
Figure 
23.1: 
(a): 
First 
order 
Markov 
chain. 
(b): 
Second 
order 
Markov 
chain. 


where 
L 
= 
1 
is 
the 
order 
of 
the 
Markov 
chain 
and 
vt 
= 
Ø 
for 
t< 
1. 
For 
a 
rst 
order 
Markov 
chain, 
p(v1:T 
)= 
p(v1)p(v2jv1)p(v3jv2) 
:::p(vT 
jvT 
..1) 
(23.1.4) 


0

For 
a 
stationary 
Markov 
chain 
the 
transitions 
p(vt 
= 
s0jvt..1 
= 
s)= 
f(s;s) 
are 
time-independent. 
Other


0

wise 
the 
chain 
is 
non-stationary, 
p(vt 
= 
s0jvt..1 
= 
s)= 
f(s;s;t). 


23.1.1 
Equilibrium 
and 
stationary 
distribution 
of 
a 
Markov 
chain 
The 
stationary 
distribution 
p8 
of 
a 
Markov 
chain 
with 
transition 
matrix 
M 
is 
dened 
by 
the 
condition 


X

p1(i)=p(xt 
= 
ijxt..1 
= 
j) 
p1(j) 
(23.1.5)

|{z}X

j 


Mij 


In 
matrix 
notation 
this 
can 
be 
written 
as 
the 
vector 
equation 


p8 
= 
Mp8 
(23.1.6) 


so 
that 
the 
stationary 
distribution 
is 
proportional 
to 
the 
eigenvector 
with 
unit 
eigenvalue 
of 
the 
transition 
matrix. 
Note 
that 
there 
may 
be 
more 
than 
one 
stationary 
distribution. 
See 
exercise(217) 
and 
[122]. 


Given 
a 
state 
x1, 
we 
can 
iteratively 
draw 
samples 
x2;:::, 
xt 
from 
the 
Markov 
chain 
drawing 
a 
sample 
from 
p(x2jx1 
= 
x1), 
and 
then 
from 
p(x3jx2) 
etc. 
As 
we 
repeatedly 
sample 
a 
new 
state 
from 
the 
chain, 
the 
distribution 
at 
time 
t, 
for 
an 
initial 
distribution 
p1(i)= 
d 
(i, 
x1), 
is 


pt 
= 
Mtp1 
(23.1.7) 


If 
for 
t 
!1, 
p8 
is 
independent 
of 
the 
initial 
distribution 
p1, 
then 
p8 
is 
called 
the 
equilibrium 
distribution 
of 
the 
chain. 
See 
exercise(218) 
for 
an 
example 
of 
a 
Markov 
chain 
which 
does 
not 
have 
an 
equilibrium 
distribution. 


Example 
95 
(PageRank). 
Despite 
their 
apparent 
simplicity, 
Markov 
chains 
have 
been 
put 
to 
interesting 


use 
in 
information 
retrieval 
and 
search-engines. 
Dene 
the 
matrix 
Aij 
=
1 
0 
if 
website 
j 
has 
a 
hyperlink 
to 
website 
i 
otherwise 
(23.1.8) 
From 
this 
we 
can 
dene 
a 
Markov 
transition 
matrix 
with 
elements 
Mij 
= 
AijPi 
Aij 
(23.1.9) 


The 
equilibrium 
distribution 
of 
this 
Markov 
Chain 
has 
the 
interpretation 
: 
If 
follow 
links 
at 
random, 
jumping 
from 
website 
to 
website, 
the 
equilibrium 
distribution 
component 
p1(i) 
is 
the 
relative 
number 
of 
times 
we 
will 
visit 
website 
i. 
This 
has 
a 
natural 
interpretation 
as 
the 
`importance’ 
of 
website 
i; 
if 
a 
website 


DRAFT 
March 
9, 
2010 



Markov 
Models 


23
is 
isolated 
in 
the 
web, 
it 
will 
be 
visited 
infrequently 
by 
random 
hopping; 
if 
a 
website 
is 
linked 
by 
many 
others 
it 
will 
be 
visited 
more 
frequently. 


A 
crude 
search 
engine 
works 
then 
as 
follows. 
For 
each 
website 
i 
a 
list 
of 
words 
associated 
with 
that 
website 
is 
collected. 
After 
doing 
this 
for 
all 
websites, 
one 
can 
make 
an 
`inverse’ 
list 
of 
which 
websites 
contain 
word 


w. 
When 
a 
user 
searches 
for 
word 
w, 
the 
list 
of 
websites 
that 
that 
word 
is 
then 
returned, 
ranked 
according 
to 
the 
importance 
of 
the 
site 
(as 
dened 
by 
the 
equilibrium 
distribution). 
This 
is 
a 
crude 
summary 
as 
how 
early 
search 
engines 
worked, 
infolab.stanford.edu/backrub/google.html. 
1 


Figure 
23.2: 
A 
state 
transition 
diagram 
for 
a 
three 
state 
Markov 
chain. 
Note 
that 
a 
state 
transition 
diagram 
is 
not 
a 
graphical 
model 
– 
it 
simply 
displays 
the 
non-zero 
entries 
of 
the 
transition 
matrix 
p(ijj). 
The 
absence 
of 
link 
from 
j 
to 
i 
indicates 
that 
p(ijj) 
= 
0. 


23.1.2 
Fitting 
Markov 
models 
Given 
a 
sequence 
v1:T 
, 
tting 
a 
stationary 
Markov 
chain 
by 
Maximum 
Likelihood 
corresponds 
to 
setting 
the 
transitions 
by 
counting 
the 
number 
of 
observed 
(rst-order) 
transitions 
in 
the 
sequence: 


T

XY

p(vt 
= 
ijv..1 
= 
j) 
. 
I[vt 
= 
i, 
vt..1 
= 
j] 
(23.1.10) 
t=2 


To 
show 
this, 
for 
convenience 
we 
write 
p(vt 
= 
ijv..1 
= 
j) 
= 
ijj, 
so 
that 
the 
likelihood 
is 
(assuming 
v1 
is 
known): 


TT

YYYY

I[vt 
=i;vt..1=j]

p(v2:T 
j, 
v1)= 
vtjvt..1 
= 
ijj 
(23.1.11) 
t=2ij 
t=2ij 


Taking 
logs 
and 
adding 
the 
Lagrange 
constraint 
for 
the 
normalisation, 


 !

T

XXYXXY

L()= 
I[vt 
= 
i, 
vt..1 
= 
j] 
log 
ijj 
+ 
j1 
- 
ijj(23.1.12) 
t=2 
ij 
ji 


Dierentiating 
with 
respect 
to 
ijj 
and 
equating 
to 
zero, 
we 
immediately 
arrive 
at 
the 
intuitive 
setting, 
n

equation 
(23.1.10). 
For 
a 
set 
of 
timeseries, 
v1:Tn 
;n 
=1;:::;N, 
the 
transition 
is 
given 
by 
counting 
all 
transitions 
across 
time 
and 
datapoints. 
The 
Maximum 
Likelihood 
setting 
for 
the 
initial 
rst 
timestep 


P

n

distribution 
is 
p(v1 
= 
i) 
/I[v= 
i].

n 
1 


Bayesian 
tting 


For 
simplicity, 
we 
assume 
a 
factorised 
prior 
on 
the 
transition 


Y

p()=p(jj) 
(23.1.13) 


j 


A 
convenient 
choice 
for 
each 
conditional 
transition 
is 
a 
Dirichlet 
distribution 
with 
hyperparameters 
uj,

..Y

p(jj) 
= 
Dirichlet 
jjjuj, 
since 
this 
is 
conjugate 
to 
the 
categorical 
transition 
and 


YYY..Y

I[vt 
=i;vt..1=j] 
uij 
..1 


p(jv1:T 
) 
. 
p(v1:T 
j)p() 
/ijj 
ijj 
=Dirichlet 
jjju^j(23.1.14) 
tij 
j 


PT

where 
u^j 
=I[vt..1 
= 
i, 
vt 
= 
j], 
being 
the 
number 
of 
j 
. 
i 
transitions 
in 
the 
dataset. 


t=2 


DRAFT 
March 
9, 
2010 



h
v1v2v3v4
h
v1v2v3v4
Markov 
Models 


Figure 
23.3: 
Mixture 
of 
rst 
order 
Markov 
chains. 
The 
discrete 
hidden 
variable 
dom(h)= 
f1;:::;Hgindexes 
the 
Markov 
chain 
p(vtjvt..1;h). 
Such 
models 
can 
be 
useful 
as 
simple 
sequence 
clustering 
tools. 


(a) 
23.1.3 
Mixture 
of 
Markov 
models 
n

Given 
a 
set 
of 
sequences 
V 
= 
fv1:T 
;n 
=1;:::;Ng, 
how 
might 
we 
cluster 
them? 
To 
keep 
the 
notation 
less 
cluttered, 
we 
assume 
that 
all 
sequences 
are 
of 
the 
same 
length 
T 
with 
the 
extension 
to 
diering 
lengths 
being 
straightforward. 
One 
simple 
approach 
is 
to 
t 
a 
mixture 
of 
Markov 
models. 
Assuming 
the 
data 


Q(

n

is 
i.i.d., 
p(V)= 
p(v), 
we 
dene 
a 
mixture 
model 
for 
a 
single 
datapoint 
v1:T 
. 
Here 
we 
assume 
each 


n 
1:T 


component 
model 
is 
rst 
order 
Markov 


HHT

X(XY(

p(v1:T 
)= 
p(h)p(v1:T 
jh)= 
p(h) 
p(vtjvt..1;h) 
(23.1.15) 
h=1 
h=1 
t=1 


The 
graphical 
model 
is 
depicted 
in 
g(23.3). 
Clustering 
can 
then 
be 
achieved 
by 
nding 
the 
Maximum 


n

Likelihood 
parameters 
p(h), 
p(vtjvt..1;h) 
and 
subsequently 
assigning 
the 
clusters 
according 
to 
p(hjv).

1:T 
Below 
we 
discuss 
the 
application 
of 
the 
EM 
algorithm 
to 
this 
model 
to 
learn 
the 
Maximum 
Likelihood 
parameters. 


EM 
algorithm 


Under 
the 
i.i.d. 
data 
assumption, 
the 
log 
likelihood 
is 


NHT

XX(Y(

nn

log 
p(V) 
= 
log 
p(h) 
p(v 
jvt..1;h) 
(23.1.16)

t 
n=1 
h=1 
t=1 


For 
the 
M-step, 
our 
task 
is 
to 
maximise 
the 
energy 


()

NNT

X(XX(

n

E 
= 
hlog 
p(v1:T 
;h)) 
old(hjvn 
= 
hlog 
p(h)) 
old(hjvn 
+ 
hlog 
p(vtjvt..1;h)) 
old(hjvn

p) 
p) 
p)

1:T 
1:T 
1:T 
n=1 
n=1t=1 


The 
contribution 
to 
the 
energy 
from 
the 
parameter 
p(h) 
is 


N

X(

hlog 
p(h)ipold(hjvn 
) 
(23.1.17)

1:T 
n=1 


By 
dening 


N

X(

p^old(h) 
. 
p 
old(hjv 
n 
) 
(23.1.18)

1:T 
n=1 
one 
can 
view 
maximising 
(23.1.17) 
as 
equivalent 
to 
minimising 




KLp^old(h)jp(h)(23.1.19) 


new

so 
that 
the 
optimal 
choice 
from 
the 
M-step 
is 
to 
set 
p= 
p^old, 
namely 


N

X(

p 
new(h) 
. 
p 
old(hjv 
n 
) 
(23.1.20)

1:T 
n=1 
For 
those 
less 
comfortable 
with 
this 
argument, 
a 
direct 
maximisation 
including 
a 
Lagrange 
term 
to 
ensure 
normalisation 
of 
p(h) 
can 
be 
used 
to 
derive 
the 
same 
result. 


DRAFT 
March 
9, 
2010 



Markov 
Models 


Similarly, 
the 
M-step 
for 
p(vtjvt..1;h) 
is 


NT

XX

p 
new(vt 
= 
ijvt..1 
= 
j, 
h 
= 
k) 
. 
p 
old(h 
= 
kjv 
n 
) 
I[v 
n 
= 
i] 
Iv 
n 
= 
j(23.1.21)

1:Tt 
t..1 
n=1 
t=2 
The 
initial 
term 
p(v1jh) 
is 
updated 
using 


N

X

p 
new(v1 
= 
ijh 
= 
k) 
. 
p 
old(h 
= 
kjv 
n 
)I[v 
n 
= 
i] 
(23.1.22)

1:T 
1 
n=1 
The 
E-step 
sets 


T

Y

old(hjv 
nn 
nn 


p 
) 
. 
p(h)p(v1:T 
jh)= 
p(h) 
p(v 
jvt..1;h) 
(23.1.23)

1:Tt 
t=1 
Given 
an 
initialisation, 
the 
EM 
algorithm 
then 
iterates 
(23.1.20),(23.1.21), 
(23.1.22) 
and 
(23.1.23) 
until 
convergence. 


For 
long 
sequences, 
explicitly 
computing 
the 
product 
of 
many 
terms 
may 
lead 
to 
numerical 
underow 
issues. 
In 
practice 
it 
is 
therefore 
best 
to 
work 
with 
logs, 


T

X

log 
p 
old(hjv 
n 
) 
= 
log 
p(h) 
+ 
log 
p(v 
njvt
n 
..1;h)+ 
const. 
(23.1.24)

1:Tt 
t=1 
In 
this 
way 
any 
large 
constants 
common 
to 
all 
h 
can 
be 
removed 
and 
the 
distribution 
may 
be 
computed 
accurately. 
See 
mixMarkov.m. 


Example 
96 
(Gene 
Clustering). 
Consider 
the 
20 
ctitious 
gene 
sequences 
below 
presented 
in 
an 
arbitrarily 
chosen 
order. 
Each 
sequence 
consists 
of 
20 
symbols 
from 
the 
set 
fA, 
C, 
G, 
T 
g. 
The 
task 
is 
to 
try 
to 
cluster 
these 
sequences 
into 
two 
groups, 
based 
on 
the 
(perhaps 
biologically 
unrealistic) 
assumption 
that 
gene 
sequences 
in 
the 
same 
cluster 
follow 
a 
stationary 
Markov 
chain. 


CATAGGCATTCTATGTGCTG 
CCAGTTACGGACGCCGAAAG 
TGGAACCTTAAAAAAAAAAA 
GTCTCCTGCCCTCTCTGAAC 
GTGCCTGGACCTGAAAAGCC 
CGGCCGCGCCTCCGGGAACG 
AAAGTGCTCTGAAAACTCAC 
ACATGAACTACATAGTATAA 
GTTGGTCAGCACACGGACTG 
CCTCCCCTCCCCTTTCCTGC 
CACTACGGCTACCTGGGCAA 
CGGTCCGTCCGAGGCACTC 
(23.1.25)
TAAGTGTCCTCTGCTCCTAA 
CACCATCACCCTTGCTAAGG 
AAAGAACTCCCCTCCCTGCC 
CAAATGCCTCACGCGTCTCA 
GCCAAGCAGGGTCTCAACTT 
CATGGACTGCTCCACAAAGG 
AAAAAAACGAAAAACCTAAG 
GCGTAAAAAAAGTCCTGGGT 


A 
simple 
approach 
is 
to 
assume 
that 
the 
sequences 
are 
generated 
from 
a 
two-component 
H 
= 
2 
mixture 
of 
Markov 
models 
and 
train 
the 
model 
using 
Maximum 
Likelihood. 
The 
likelihood 
has 
local 
optima 
so 
that 
the 
procedure 
needs 
to 
be 
run 
several 
times 
and 
the 
solution 
with 
the 
highest 
likelihood 
chosen. 
One 
can 


n

then 
assign 
each 
of 
the 
sequences 
by 
examining 
p(h 
=1jv). 
If 
this 
posterior 
probability 
is 
greater 
than 


1:T 
0.5, 
we 
assign 
it 
to 
class 
1, 
otherwise 
to 
class 
2. 
Using 
this 
procedure, 
we 
nd 
the 
following 
clusters: 


CATAGGCATTCTATGTGCTG 
TGGAACCTTAAAAAAAAAAA 


CCAGTTACGGACGCCGAAAG 
GTCTCCTGCCCTCTCTGAAC 


CGGCCGCGCCTCCGGGAACG 
GTGCCTGGACCTGAAAAGCC 


ACATGAACTACATAGTATAA 
AAAGTGCTCTGAAAACTCAC 


GTTGGTCAGCACACGGACTG 
CCTCCCCTCCCCTTTCCTGC 


CACTACGGCTACCTGGGCAA 
TAAGTGTCCTCTGCTCCTAA 
(23.1.26)

CGGTCCGTCCGAGGCACTCG 
AAAGAACTCCCCTCCCTGCC 


CACCATCACCCTTGCTAAGG 
AAAAAAACGAAAAACCTAAG 


CAAATGCCTCACGCGTCTCA 
GCGTAAAAAAAGTCCTGGGT 


GCCAAGCAGGGTCTCAACTT 


CATGGACTGCTCCACAAAGG 


where 
sequences 
in 
the 
rst 
column 
are 
assigned 
to 
cluster 
1, 
and 
sequences 
in 
the 
second 
column 
to 
cluster 


2. 
In 
this 
case 
the 
data 
in 
(23.1.25) 
was 
in 
fact 
generated 
by 
a 
two-component 
Markov 
mixture 
and 
the 
posterior 
assignment 
(23.1.26) 
is 
in 
agreement 
with 
the 
known 
clusters. 
See 
demoMixMarkov.m 
DRAFT 
March 
9, 
2010 



Hidden 
Markov 
Models 


v1v2v3v4
h1h2h3h4
Figure 
23.4: 
A 
rst 
order 
hidden 
Markov 
model 
with 
`hidden’ 
variables 
dom(ht)= 
f1;:::;Hg, 
t 
=1: 
T 
. 
The 
`visible’ 
variables 
vt 
can 
be 
either 
discrete 
or 
continuous. 


23.2 
Hidden 
Markov 
Models 
The 
Hidden 
Markov 
Model 
(HMM) 
denes 
a 
Markov 
chain 
on 
hidden 
(or 
`latent') 
variables 
h1:T 
. 
The 
observed 
(or 
`visible') 
variables 
are 
dependent 
on 
the 
hidden 
variables 
through 
an 
emission 
p(vtjht). 
This 
denes 
a 
joint 
distribution 


T

T 


p(h1:T 
;v1:T 
)= 
p(v1jh1)p(h1) 
p(vtjht)p(htjht..1) 
(23.2.1) 
t=2 


for 
which 
the 
graphical 
model 
is 
depicted 
in 
g(23.4). 
For 
a 
stationary 
HMM 
the 
transition 
p(htjht..1) 
and 
emission 
p(vtjht) 
distributions 
are 
constant 
through 
time. 
The 
use 
of 
the 
HMM 
is 
widespread 
and 
a 
subset 
of 
the 
many 
applications 
of 
HMMs 
is 
given 
in 
section(23.5). 


Denition 
109 
(Transition 
Distribution). 
For 
a 
stationary 
HMM 
the 
transition 
distribution 
p(ht+1jht) 
is 
dened 
by 
the 
H 
× 
H 
transition 
matrix 


Ai0;i 
= 
p(ht+1 
= 
i0jht 
= 
i) 
(23.2.2) 


and 
an 
initial 
distribution 


ai 
= 
p(h1 
= 
i). 
(23.2.3) 


Denition 
110 
(Emission 
Distribution). 
For 
a 
stationary 
HMM 
and 
emission 
distribution 
p(vtjht) 
with 
discrete 
states 
vt 
2f1;:::;V 
g, 
we 
dene 
a 
V 
× 
H 
emission 
matrix 


Bi;j 
= 
p(vt 
= 
ijht 
= 
j) 
(23.2.4) 


For 
continuous 
outputs, 
ht 
selects 
one 
of 
H 
possible 
output 
distributions 
p(vtjht), 
ht 
2f1;:::;Hg. 


In 
the 
engineering 
and 
machine 
learning 
communities, 
the 
term 
HMM 
typically 
refers 
to 
the 
case 
of 
discrete 
variables 
ht, 
a 
convention 
that 
we 
adopt 
here. 
In 
statistics 
the 
term 
HMM 
often 
refers 
to 
any 
model 
with 
the 
independence 
structure 
in 
equation 
(23.2.1), 
regardless 
of 
the 
form 
of 
the 
variables 
ht 
(see 
for 
example 
[54]). 


23.2.1 
The 
classical 
inference 
problems 
Filtering 
(Inferring 
the 
present) 
p(htjv1:t) 


Prediction 
(Inferring 
the 
future) 
p(htjv1:s) 
t>s 


Smoothing 
(Inferring 
the 
past) 
p(htjv1:u) 
t<u 


Likelihood 
p(v1:T 
) 


Most 
likely 
Hidden 
path 
(Viterbi 
alignment) 
argmax 
p(h1:T 
jv1:T 
) 


h1:T 


The 
most 
likely 
hidden 
path 
problem 
is 
termed 
Viterbi 
alignment 
in 
the 
engineering 
literature. 
All 
these 
classical 
inference 
problems 
are 
straightforward 
since 
the 
distribution 
is 
singly-connected, 
so 
that 
any 
standard 
inference 
method 
can 
be 
adopted 
for 
these 
problems. 
The 
factor 
graph 
and 
junction 
trees 
for 


416 
DRAFT 
March 
9, 
2010 



Hidden 
Markov 
Models 


h1
h2
h3
h4v1;h1
h1 


h1;h2
h2 


h2;h3
h3 
h2 
h3 
h4 


h3;h4
v1v2v3v4v2;h2v3;h3v4;h4
(a) 
(b) 
Figure 
23.5: 
(a): 
Factor 
graph 
for 
the 
rst 
order 
HMM 
of 
g(23.4). 
(b): 
Junction 
tree 
for 
g(23.4). 


the 
rst 
order 
HMM 
are 
given 
in 
g(23.5). 
In 
both 
cases, 
after 
suitable 
setting 
of 
the 
factors 
and 
clique 
potentials, 
ltering 
corresponds 
to 
passing 
messages 
from 
left 
to 
right 
and 
upwards; 
smoothing 
corresponds 
to 
a 
valid 
schedule 
of 
message 
passing/absorption 
both 
forwards 
and 
backwards 
along 
all 
edges. 
It 
is 
also 
straightforward 
to 
derive 
appropriate 
recursions 
directly. 
This 
is 
instructive 
and 
also 
useful 
in 
constructing 
compact 
and 
numerically 
stable 
algorithms. 


23.2.2 
Filtering 
p(htjv1:t) 
We 
rst 
compute 
the 
joint 
marginal 
p(ht;v1:t) 
from 
which 
the 
conditional 
marginal 
p(htjv1:t) 
can 
subsequently 
be 
obtained 
by 
normalisation. 
A 
recursion 
for 
p(ht;v1:t) 
is 
obtained 
by 
considering: 


X

p(ht;v1:t)=p(ht;ht..1;v1:t..1;vt) 
(23.2.5) 


ht..1 


X

=p(vtjv1:t..1;ht..1)p(v1:t..1;ht..1)

v1:t..1;ht;ht..
4
1)p(htj(23.2.6) 


ht..1 


X

=p(vtjht)p(htjht..1)p(ht..1;v1:t..1) 
(23.2.7) 


ht..1 


The 
cancellations 
follow 
from 
the 
conditional 
independence 
assumptions 
of 
the 
model. 
Hence 
if 
we 
dene 


(ht)= 
p(ht;v1:t) 
(23.2.8) 


equation 
(23.2.7) 
above 
gives 
the 
-recursion 


X

(ht) 
= 
(vtjht)p
p|
{z}X
p(htjht..1)(ht..1), 
t 
> 
1 
(23.2.9) 
correctorht..1|{z}Xpredictor 
with 
(h1) 
= 
p(h1, 
v1) 
= 
p(v1jh1)p(h1) 
(23.2.10) 


This 
recursion 
has 
the 
interpretation 
that 
the 
ltered 
distribution 
(ht..1) 
is 
propagated 
forwards 
by 
the 
dynamics 
for 
one 
timestep 
to 
reveal 
a 
new 
`prior’ 
distribution 
at 
time 
t. 
This 
distribution 
is 
then 
modulated 
by 
the 
observation 
vt, 
incorporating 
the 
new 
evidence 
into 
the 
ltered 
distribution 
(this 
is 
also 
referred 
to 
as 
a 
predictor-corrector 
method). 
Since 
each 
a 
is 
smaller 
than 
1, 
and 
the 
recursion 
involves 
multiplication 
by 
terms 
less 
than 
1, 
the 
's 
can 
become 
very 
small. 
To 
avoid 
numerical 
problems 
it 
is 
therefore 
advisable 
to 
work 
with 
log 
(ht), 
see 
HMMforward.m. 


Normalisation 
gives 
the 
ltered 
posterior 


p(htjv1:t) 
. 
(ht) 
(23.2.11) 


If 
we 
only 
require 
the 
ltered 
posterior 
we 
are 
free 
to 
rescale 
the 
's 
as 
we 
wish. 
In 
this 
case 
an 
alternative 
to 
working 
with 
log 
a 
messages 
is 
to 
work 
with 
normalised 
a 
messages 
so 
that 
the 
sum 
of 
the 
components 
is 
always 
1. 


DRAFT 
March 
9, 
2010 



Hidden 
Markov 
Models 


We 
can 
write 
equation 
(23.2.7) 
above 
directly 
as 
a 
recursion 
for 
the 
ltered 
distribution 


X

p(htjv1:t) 
/p(vtjht)p(htjht..1)p(ht..1jv1:t..1) 
t> 
1 
(23.2.12) 


ht..1 


Intuitively, 
the 
term 
p(ht..1jv1:t..1) 
has 
the 
eect 
of 
removing 
all 
nodes 
in 
the 
graph 
before 
time 
t 
- 
1 
and 
replacing 
their 
inuence 
by 
a 
modied 
`prior’ 
distribution 
on 
ht. 
One 
may 
interpret 
p(vtjht)p(htjht..1) 
as 
a 
likelihood, 
giving 
rise 
to 
the 
joint 
posterior 
p(ht;ht..1jv1:t) 
under 
Bayesian 
updating. 
At 
the 
next 
timestep 
the 
previous 
posterior 
becomes 
the 
new 
prior. 


23.2.3 
Parallel 
smoothing 
p(htjv1:T 
) 
There 
are 
two 
main 
approaches 
to 
computing 
p(htjv1:T 
). 
Perhaps 
the 
most 
common 
in 
the 
HMM 
literature 
is 
the 
parallel 
method 
which 
is 
equivalent 
to 
message 
passing 
on 
factor 
graphs. 
In 
this 
one 
separates 
the 
smoothed 
posterior 
into 
contributions 
from 
the 
past 
and 
future: 


p(ht;v1:T 
)= 
p(ht;v1:t;vt+1:T 
)= 
(ht;v1:t)(vt+1:T 
jht;v1:t)= 
(ht)(ht) 
(23.2.13)

p
p|
{z}p
p|
{z}X

past 
future 


The 
term 
(ht) 
is 
obtained 
from 
the 
`forward’ 
a 
recursion, 
(23.2.9). 
The 
term 
(ht) 
may 
be 
obtained 
using 
a 
`backward’ 
ß 
recursion 
as 
we 
show 
below. 
The 
forward 
and 
backward 
recursions 
are 
independent 
and 
may 
therefore 
be 
run 
in 
parallel, 
with 
their 
results 
combined 
to 
obtain 
the 
smoothed 
posterior. 
This 
approach 
is 
also 
sometimes 
termed 
the 
two-lter 
smoother. 


The 
ß 
recursion 


X

p(vt:T 
jht..1)=p(vt;vt+1:T 
;htjht..1) 
(23.2.14) 


ht

X



=p(vtjht..
1)p(vt+1:T 
;htjht..1)

vt+1:T 
;ht;(23.2.15) 


ht

X

=p(vtjht)p(vt+1:T 
jht;(23.2.16)

ht..
1)p(htjht..1) 


ht 


Dening 


(ht) 
= 
p(vt+1:T 
jht) 
(23.2.17) 


equation 
(23.2.16) 
above 
gives 
the 
-recursion 


X

(ht..1)=p(vtjht)p(htjht..1)(ht), 
2 
= 
t 
= 
T 
(23.2.18) 


ht 


with 
(hT 
) 
= 
1. 
As 
for 
the 
forward 
pass, 
working 
in 
log 
space 
is 
recommended 
to 
avoid 
numerical 
diculties. 
If 
one 
only 
desires 
posterior 
distributions, 
one 
can 
also 
perform 
local 
normalisation 
at 
each 
stage 
since 
only 
the 
relative 
magnitude 
of 
the 
components 
of 
ß 
are 
of 
importance. 
The 
smoothed 
posterior 
is 
then 
given 
by 


(ht)(ht)

p(htjv1:T 
) 
= 
(ht)= 
P(23.2.19) 
ht 
(ht)(ht) 


Together 
the 
a 
- 
ß 
recursions 
are 
called 
the 
Forward-Backward 
algorithm. 


23.2.4 
Correction 
smoothing 
An 
alternative 
to 
the 
parallel 
method 
is 
to 
form 
a 
recursion 
directly 
for 
the 
smoothed 
posterior. 
This 
can 
be 
achieved 
by 
recognising 
that 
conditioning 
on 
the 
present 
makes 
the 
future 
redundant: 


XX



p(htjv1:T 
)=p(ht;ht+1jv1:T 
)=p(htjht+1;v1:t;(23.2.20)

vt+1:T 
)p(ht+1jv1:T 
) 


ht+1 
ht+1 


DRAFT 
March 
9, 
2010 



Hidden 
Markov 
Models 


This 
gives 
a 
recursion 
for 
(ht) 
= 
p(htjv1:T 
): 


X

(ht)=p(htjht+1;v1:t)(ht+1) 
(23.2.21) 


ht+1 


with 
(hT 
) 
. 
(hT 
). 
The 
term 
p(htjht+1;v1:t) 
may 
be 
computed 
using 
the 
ltered 
results 
p(htjv1:t): 


p(htjht+1;v1:t) 
. 
p(ht+1;htjv1:t) 
. 
p(ht+1jht)p(htjv1:t) 
(23.2.22) 


where 
the 
proportionality 
constant 
is 
found 
by 
normalisation. 
This 
is 
a 
form 
of 
dynamics 
reversal, 
as 
if 
we 
are 
reversing 
the 
direction 
of 
the 
hidden 
to 
hidden 
arrow 
in 
the 
HMM. 
This 
procedure, 
also 
termed 
the 
Rauch-Tung-Striebel 
smoother1, 
is 
sequential 
since 
we 
need 
to 
rst 
complete 
the 
a 
recursions, 
after 
which 
the 
. 
recursion 
may 
begin. 
This 
is 
a 
so-called 
correction 
smoother 
since 
it 
`corrects’ 
the 
ltered 
result. 
Interestingly, 
once 
ltering 
has 
been 
carried 
out, 
the 
evidential 
states 
v1:T 
are 
not 
needed 
during 
the 
subsequent 
. 
recursion. 


The 
a 
- 
ß 
and 
a 
- 
. 
recursions 
are 
related 
through 


(ht) 
. 
(ht)(ht) 
(23.2.23) 


Computing 
the 
pairwise 
marginal 
p(ht;ht..1jv1:T 
) 


To 
implement 
the 
EM 
algorithm 
for 
learning, 
section(23.3.1), 
we 
require 
terms 
such 
as 
p(ht;ht..1jv1:T 
). 
These 
can 
be 
obtained 
by 
message 
passing 
on 
either 
a 
factor 
graph 
or 
junction 
tree 
(for 
which 
the 
pairwise 
marginals 
are 
contained 
in 
the 
cliques, 
see 
g(23.4b)). 
Alternatively, 
an 
explicit 
recursion 
is 
as 
follows: 


p(ht;ht+1jv1:T 
) 
. 
p(v1:t;vt+1;vt+2:T 
;ht+1;ht) 


(

(

v1:t;vt+1;ht;ht+1)p(v1:t;vt+1;ht+1;ht)

= 
p(vt+2:T 
j((((



v1:t;ht;ht+1)p(v1:t;ht+1;ht)

= 
p(vt+2:T 
jht+1)p(vt+1j

= 
p(vt+2:T 
jht+1)p(vt+1jht+1)p(ht+1jv1:t
;ht)p(v1:t;ht) 
(23.2.24) 


Rearranging, 
we 
therefore 
have 


p(ht;ht+1jv1:T 
) 
. 
(ht)p(vt+1jht+1)p(ht+1jht)(ht+1) 
(23.2.25) 


See 
HMMsmooth.m. 


The 
likelihood 
p(v1:T 
) 


The 
likelihood 
of 
a 
sequence 
of 
observations 
can 
be 
computed 
from 


XX

p(v1:T 
)=p(hT 
;v1:T 
)=(hT 
) 
(23.2.26) 


hT 
hT 


An 
alternative 
computation 
can 
be 
found 
by 
making 
use 
of 
the 
decomposition 


T

YX

p(v1:T 
)= 
p(vtjv1:t..1) 
(23.2.27) 
t=1 


Each 
factor 
can 
be 
computed 
using 


X

p(vtjv1:t..1)=p(vt;htjv1:t..1) 
(23.2.28) 


ht

X

=p(vtjht;(23.2.29)

v1:t..1)p(htjv1:t..1) 


ht

XX



=p(vtjht)

p(htjht..1;(23.2.30)

v1:t..1)p(ht..1jv1:t..1) 


ht 
ht..1 


1It 
is 
most 
common 
to 
use 
this 
terminology 
for 
the 
continuous 
variable 
case, 
though 
we 
adopt 
it 
here 
also 
for 
the 
discrete 
variable 
case. 


DRAFT 
March 
9, 
2010 
419 



Hidden 
Markov 
Models 



Figure 
23.6: 
Localising 
the 
burglar. 
The 
latent 
variable 
ht 
. 
f1;:::, 
25} 
denotes 
the 
positions, 
dened 
over 
the 
5 
× 
5 
grid 
of 
the 
ground 
oor 
of 
the 
house. 
(a): 
A 
representation 
of 
the 
probability 
that 
the 
`oor 
will 
creak’ 
at 
each 
of 
the 
25 
positions, 
p(vcreakjh). 
Light 
squares 
represent 
probability 
0.9 
and 
dark 
square 
0.1. 
(b): 
A 
representation 
of 
the 
probability 
p(vbumpjh) 


(a) 
`Creaks’ 
(b) 
`Bumps’ 
that 
the 
burglar 
will 
bump 
into 
something 
in 
each 
of 
the 
25 
positions. 
where 
the 
nal 
term 
p(ht..1jv1:t..1) 
is 
the 
ltered 
result. 


In 
both 
approaches 
the 
likelihood 
of 
an 
output 
sequence 
requires 
only 
a 
forward 
computation 
(ltering). 
If 
required, 
one 
can 
also 
compute 
the 
likelihood 
using, 
(23.2.13), 


X

p(v1:T 
)=(ht)(ht) 
(23.2.31) 


ht 


which 
is 
valid 
for 
any 
1 
= 
t 
= 
T 
. 


23.2.5 
Most 
likely 
joint 
state 
The 
most 
likely 
path 
h1:T 
of 
p(h1:T 
jv1:T 
) 
is 
the 
same 
as 
the 
most 
likely 
state 
(for 
xed 
v1:T 
) 
of 


Y

p(h1:T 
;v1:T 
)=p(vtjht)p(htjht..1) 
(23.2.32) 
t 


The 
most 
likely 
path 
can 
be 
found 
using 
the 
max-product 
version 
of 
the 
factor 
graph 
or 
max-absorption 
on 
the 
junction 
tree. 
Alternatively, 
an 
explicit 
derivation 
can 
be 
obtained 
by 
considering: 


T 
T 
..1

 
Y

max 
p(vtjht)p(htjht..1)= 
p(vtjht)p(htjht..1) 
max 
p(vT 
jhT 
)p(hT 
jhT 
..1) 
(23.2.33) 
hT 
hT 


t=1 
t=1 
|X{zX}X

(hT 
..1) 


The 
message 
(hT 
..1) 
conveys 
information 
from 
the 
end 
of 
the 
chain 
to 
the 
penultimate 
timestep. 
We 
can 
continue 
in 
this 
manner, 
dening 
the 
recursion 


(ht..1) 
= 
max 
p(vtjht)p(htjht..1)(ht), 
2 
= 
t 
= 
T 
(23.2.34) 
ht 


with 
(hT 
) 
= 
1. 
This 
means 
that 
the 
eect 
of 
maximising 
over 
h2;:::;hT 
is 
compressed 
into 
a 
message 
(h1) 
so 
that 
the 
most 
likely 
state 
h* 
is 
given 
by 


1 


h 
* 
1 
= 
argmax 
p(v1jh1)p(h1)(h1) 
(23.2.35) 


h1 


Once 
computed, 
backtracking 
gives 


h 
* 
= 
argmax 
p(vtjht)p(htjh 
* 
)(ht) 
(23.2.36) 


tt..1
ht 


This 
special 
case 
of 
the 
max-product 
algorithm 
is 
called 
the 
Viterbi 
algorithm. 
Similarly, 
one 
may 
use 
the 
N-max-product 
algorithm, 
section(5.2.1), 
to 
obtain 
the 
N-most 
likely 
hidden 
paths. 


Example 
97 
(A 
localisation 
example). 
You're 
asleep 
upstairs 
in 
your 
house 
and 
awoken 
by 
noises 
from 
downstairs. 
You 
realise 
that 
a 
burglar 
is 
on 
the 
ground 
oor 
and 
attempt 
to 
understand 
where 
he 
his 
from 
listening 
to 
his 
movements. 
You 
mentally 
partition 
the 
ground 
oor 
into 
a 
5 
× 
5 
grid. 
For 
each 
grid 
position 
you 
know 
the 
probability 
that 
if 
someone 
is 
in 
that 
position 
the 
oorboard 
will 
creak, 
g(23.6a). 


DRAFT 
March 
9, 
2010 



Hidden 
Markov 
Models 


(a)CreaksandBumps
(b)Filtering
(c)Smoothing
(d)Viterbi
(e)TrueBurglarposition
Figure 
23.7: 
Localising 
the 
burglar 
through 
time 
for 
10 
time 
steps. 
(a): 
Each 
panel 
represents 
the 


h 


creak 
bump 
creak 


visible 
information 
vt 
= 
v;v, 
where 
v= 
1 
means 
that 
there 
was 
a 
`creak 
in 
the 
oorboard’ 


tt 
t 


creak 
bump

(v= 
2 
otherwise) 
and 
v= 
1 
meaning 
`bumped 
into 
something’ 
(and 
is 
in 
state 
2 
otherwise). 


tt 


1

There 
are 
10 
panels, 
one 
for 
each 
time 
t 
=1;:::, 
10. 
The 
left 
half 
of 
the 
panel 
represents 
vand 
the 
right 


t 


2

half 
vt 
. 
The 
lighter 
colour 
represents 
the 
occurrence 
of 
a 
creak 
or 
bump, 
the 
darker 
colour 
the 
absence. 
(b): 
The 
ltered 
distribution 
p(htjv1:t) 
representing 
where 
we 
think 
the 
burglar 
is. 
(c): 
The 
smoothed 
distribution 
p(htjv1:10) 
so 
that 
we 
can 
gure 
out 
where 
we 
think 
the 
burglar 
went. 
(d): 
The 
most 
likely 
(Viterbi) 
burglar 
path 
arg 
maxh1:10 
p(h1:10jv1:10). 
(e): 
The 
actual 
path 
of 
the 
burglar. 


Similarly 
you 
know 
for 
each 
position 
the 
probability 
that 
someone 
will 
bump 
into 
something 
in 
the 
dark, 
g(23.6b). 
The 
oorboard 
creaking 
and 
bumping 
into 
objects 
can 
occur 
independently. 
In 
addition 
you 
assume 
that 
the 
burglar 
will 
move 
only 
one 
grid 
square 
– 
forwards, 
backwards, 
left 
or 
right 
in 
a 
single 
timestep. 
Based 
on 
a 
series 
of 
bump/no 
bump 
and 
creak/no 
creak 
information, 
g(23.7a), 
you 
try 
to 
gure 
out 
based 
on 
your 
knowledge 
of 
the 
ground 
oor, 
where 
the 
burglar 
might 
be. 


We 
can 
represent 
the 
scenario 
using 
a 
HMM 
where 
h 
2f1;:::, 
25} 
denotes 
the 
grid 
square. 
The 
visible 
creak 
. 
v

variable 
has 
a 
factorised 
form 
v 
= 
vbump 
and, 
to 
use 
our 
standard 
code, 
we 
form 
a 
new 
visible 
variable 
with 
4 
states 
using 


p(vjh)= 
p(v 
creakjh)p(vbumpjh) 
(23.2.37) 


Based 
on 
the 
past 
information, 
our 
belief 
as 
to 
where 
the 
burglar 
might 
be 
is 
represented 
by 
the 
ltered 
distribution 
p(htjv1:t), 
g(23.7). 
After 
the 
burglar 
has 
left 
at 
T 
= 
10, 
the 
police 
arrive 
and 
try 
to 
piece 
together 
where 
the 
burglar 
went, 
based 
on 
the 
sequence 
of 
creaks 
and 
bumps 
you 
provide. 
At 
any 
time 
t, 
the 
information 
as 
to 
where 
the 
burglar 
could 
have 
been 
is 
represented 
by 
the 
smoothed 
distribution 
p(htjv1:10). 
The 
police's 
single 
best-guess 
for 
the 
sequence 
of 
burglar 
positions 
is 
provided 
by 
the 
most 
likely 
joint 
hidden 
state 
arg 
maxh1:10 
p(h1:10jv1:10). 


23.2.6 
Self 
localisation 
and 
kidnapped 
robots 
A 
robot 
has 
an 
internal 
grid-based 
map 
of 
its 
environment 
and 
for 
each 
location 
h 
2f1;:::;Hg, 
knows 
the 
likely 
sensor 
readings 
he 
would 
expect 
in 
that 
location. 
The 
robot 
is 
`kidnapped’ 
and 
placed 
somewhere 
in 
the 
environment. 
The 
robot 
then 
starts 
to 
move, 
gathering 
sensor 
information. 
Based 
on 
these 
readings 
v1:t 
and 
intended 
movements 
m1:t, 
the 
robot 
attempts 
to 
gure 
out 
his 
location. 
Due 
to 
wheel 
slippage 
on 
the 
oor 
an 
intended 
action 
by 
the 
robot, 
such 
as 
`move 
forwards', 
might 
not 
be 
successful. 
Given 
all 
the 
information 
the 
robot 
has, 
he 
would 
like 
to 
infer 
p(htjv1:t;m1:t). 
This 
problem 
diers 
from 
the 
burglar 
scenario 
in 
that 
the 
robot 
now 
has 
knowledge 
of 
the 
intended 
movements 
he 
makes. 
This 
should 
give 
more 


DRAFT 
March 
9, 
2010 
421 



Learning 
HMMs 


v1v2v3v4
h1h2h3h4
m1m2m3
Figure 
23.8: 
A 
model 
for 
robot 
self-localisation. 
At 
each 
time 
the 
robot 
makes 
an 
intended 
movement, 
mt. 
As 
a 
generative 
model, 
knowing 
the 
intended 
movement 
mt 
and 
the 
current 
grid 
position 
ht, 
the 
robot 
has 
an 
idea 
of 
where 
he 
should 
be 
at 
the 
next 
time-step 
and 
what 
sensor 
reading 
vt+1 
he 
would 
expect 
there. 
Based 
on 
only 
the 
sensor 
information 
v1:T 
and 
the 
intended 
movements 
m1:T 
, 
the 
task 
is 
to 
infer 
a 
distribution 
over 
robot 
locations 
p(h1:T 
jm1:T 
;v1:T 
). 


information 
as 
to 
where 
he 
could 
be. 
One 
can 
view 
this 
as 
extra 
`visible’ 
information, 
though 
it 
is 
more 
natural 
to 
think 
of 
this 
as 
additional 
input 
information. 
A 
model 
of 
this 
scenario 
is, 
see 
g(23.8), 


T

Y

p(v1:T 
;m1:T 
;h1:T 
)= 
p(vtjht)p(htjht..1;mt..1)p(mt) 
(23.2.38) 
t=1 


The 
visible 
variables 
v1:T 
are 
known, 
as 
are 
the 
intended 
movements 
m1:T 
. 
The 
model 
expresses 
that 
the 
movements 
selected 
by 
the 
robot 
are 
random 
(hence 
no 
decision 
making 
in 
terms 
of 
where 
to 
go 
next). 
We 
assume 
that 
the 
robot 
has 
full 
knowledge 
of 
the 
conditional 
distributions 
dening 
the 
model 
(he 
knows 
the 
`map’ 
of 
his 
environment 
and 
all 
state 
transition 
and 
emission 
probabilities). 
If 
our 
interest 
is 
only 
in 
localising 
the 
robot, 
since 
the 
inputs 
m 
are 
known, 
this 
model 
is 
in 
fact 
a 
form 
of 
time-dependent 
HMM: 


T

Y

p(v1:T 
;h1:T 
)= 
p(vtjht)p(htjht..1;t) 
(23.2.39) 


t=1 
for 
a 
time-dependent 
transition 
p(htjht..1;t) 
dened 
by 
the 
intended 
movement 
mt..1. 
Any 
inference 
task 
required 
then 
follows 
the 
standard 
stationary 
HMM 
algorithms, 
albeit 
on 
replacing 
the 
time-independent 
transitions 
p(htjht..1) 
with 
the 
known 
time-dependent 
transitions. 


In 
self 
localisation 
and 
mapping 
(SLAM) 
the 
robot 
does 
not 
know 
the 
map 
of 
his 
environment. 
This 
corresponds 
to 
having 
to 
learn 
the 
transition 
and 
emission 
distributions 
on-the-y 
as 
he 
explores 
the 
environment. 


23.2.7 
Natural 
language 
models 
A 
simple 
generative 
model 
of 
language 
can 
be 
obtained 
from 
the 
letter-to-letter 
transitions 
(a 
so-called 
bigram). 
In 
the 
example 
below, 
we 
use 
this 
in 
a 
HMM 
to 
clean 
up 
the 
mis-typings. 


Example 
98 
(Stubby 
ngers). 
A 
`stubby 
ngers’ 
typist 
has 
the 
tendency 
to 
hit 
either 
the 
correct 
key 
or 
a 
neighbouring 
key. 
For 
simplicity 
we 
assume 
that 
there 
are 
27 
keys: 
lower 
case 
a 
to 
lower 
case 
z 
and 
the 
space 
bar. 
To 
model 
this 
we 
use 
an 
emission 
distribution 
Bij 
= 
p(v 
= 
ijh 
= 
j) 
where 
i 
=1;:::, 
27, 
j 
=1;:::, 
27, 
as 
depicted 
in 
g(23.9). 
A 
database 
of 
letter-to-next-letter 
frequencies 
(www.data-compression.com/english.shtml), 
yields 
the 
transition 
matrix 
Aij 
= 
p(h' 
= 
ijh 
= 
j) 
in 
English. 
For 
simplicity 
we 
assume 
that 
p(h1) 
is 
uniform. 
Also 
we 
assume 
that 
each 
intended 
key 
press 
results 
in 
a 
single 
press. 
Given 
a 
typed 
sequence 
kezrninh 
what 
is 
the 
most 
likely 
word 
that 
this 
corresponds 
to? 
By 
listing 
the 
200 
most 
likely 
hidden 
sequences 
(using 
the 
N-max-product 
algorithm) 
and 
discarding 
those 
that 
are 
not 
in 
a 
standard 
English 
dictionary 
(www.curlewcommunications.co.uk/wordlist.html), 
the 
most 
likely 
word 
that 
was 
intended 
is 
learning. 
See 
demoHMMbigram.m. 


23.3 
Learning 
HMMs 
	

n

Given 
a 
set 
of 
data 
V 
=v1 
;:::, 
vNof 
N 
sequences, 
where 
sequence 
vn 
= 
v1:Tn 
is 
of 
length 
Tn, 
we 
seek 
the 
HMM 
transition 
matrix 
A, 
emission 
matrix 
B, 
and 
initial 
vector 
a 
most 
likely 
to 
have 
have 
generated 


422 
DRAFT 
March 
9, 
2010 



Learning 
HMMs 


 
abcdefghijklmnopqrstuvwxyz 
abcdefghijklmnopqrstuvwxyz 
00.10.20.30.40.50.60.70.80.9 
abcdefghijklmnopqrstuvwxyz 
abcdefghijklmnopqrstuvwxyz 
0.050.10.150.20.250.30.350.40.450.50.55
(a) 
(b) 
Figure 
23.9: 
(a): 
The 
letter-to-letter 
transition 
matrix 
for 
English 
p(h/ 
= 
ijh 
= 
j). 
(b): 
The 
letter 
emission 
matrix 
for 
a 
typist 
with 
`stubby 
ngers’ 
in 
which 
the 
key 
or 
its 
neighbours 
on 
the 
keyboard 
are 
likely 
to 
be 
hit. 


V. 
We 
make 
the 
i.i.d. 
assumption 
so 
that 
each 
sequence 
is 
independently 
generated 
and 
assume 
that 
we 
know 
the 
number 
of 
hidden 
states 
H. 
For 
simplicity 
we 
concentrate 
here 
on 
the 
case 
of 
discrete 
visible 
variables, 
assuming 
also 
we 
know 
the 
number 
of 
states 
V 
. 
23.3.1 
EM 
algorithm 
The 
application 
of 
EM 
to 
the 
HMM 
model 
is 
called 
the 
Baum-Welch 
algorithm 
and 
follows 
the 
general 
strategy 
outlined 
in 
section(11.2). 


M-step 


Assuming 
i.i.d. 
data, 
the 
M-step 
is 
given 
by 
maximising 
the 
`energy': 


N

XP

nn 
n

hlog 
p(v1 
;v 
2 
:::;v 
T 
n 
;hn 
1 
;h2 
n;:::;hT
n 
n 
)iold(hnjvn) 
(23.3.1)

p
n=1 


with 
respect 
to 
the 
parameters 
A, 
B, 
a; 
hn 
denotes 
h1:Tn 
. 
Using 
the 
form 
of 
the 
HMM, 
we 
obtain 


()

NTXn..1 
Tn

XXP

n

hlog 
p(h1)) 
pold(h1jvn) 
+ 
hlog 
p(ht+1jht)) 
pold(ht;ht+1jvn) 
+ 
hlog 
p(v 
jht)) 
pold(htjvn)(23.3.2)

t 
n=1t=1 
t=1 


where 
for 
compactness 
we 
drop 
the 
sequence 
index 
from 
the 
h 
variables. 
To 
avoid 
potential 
confusion, 
we 
new(h1

write 
p= 
i) 
to 
denote 
the 
(new) 
table 
entry 
for 
the 
probability 
that 
the 
initial 
hidden 
variable 
is 
in 
state 
i. 
Optimising 
equation 
(23.3.2) 
with 
respect 
to 
p(h1), 
(and 
enforcing 
p(h1) 
to 
be 
a 
distribution) 
we 
obtain 


XP

new 
new(h1 
1 
N
old(h1

a 
= 
p 
= 
i)= 
p 
= 
ijvn) 
(23.3.3)

i 


N 


n=1 


which 
is 
the 
average 
number 
of 
times 
that 
the 
rst 
hidden 
variable 
is 
in 
state 
i. 
Similarly, 


NTXn..1

XP

Anew 
new(ht+1 
old(ht

0;i 
= 
p 
= 
i0jht 
= 
i) 
. 
p 
= 
i, 
ht+1 
= 
i0jvn) 
(23.3.4)

i
n=1 
t=1 


which 
is 
the 
number 
of 
times 
that 
a 
transition 
from 
hidden 
state 
i 
to 
hidden 
state 
i/ 
occurs, 
averaged 
over 
all 
times 
(since 
we 
assumed 
stationarity) 
and 
training 
sequences. 
Normalising, 
we 
obtain 


PN 
PTn..1 


pold(ht 
= 
i, 
ht+1 
= 
i0jvn)

Anew 
n=1t=1 


i0;i 
=PPN 
PTn..1 
old(ht 
(23.3.5) 
p= 
i, 
ht+1 
= 
i0jvn)

i0n=1t=1 


DRAFT 
March 
9, 
2010 



Learning 
HMMs 


Finally, 


NTn

XT 


Bnew 
new(vt 
n 
old(ht

hp 
= 
jjht 
= 
i) 
/hI[v 
= 
j] 
p 
= 
ijvn) 
(23.3.6)

j;i 
t 
n=1 
t=1 


which 
is 
the 
expected 
number 
of 
times 
that, 
for 
the 
observation 
being 
in 
state 
j, 
the 
hidden 
state 
is 
i. 
The 
proportionality 
constant 
is 
determined 
by 
the 
normalisation 
requirement. 


E-step 


In 
computing 
the 
M-step 
above 
the 
quantities 
pold(h1 
= 
ijvn), 
pold(ht 
= 
i, 
ht+1 
= 
i0jvn) 
and 
pold(ht 
= 
ijvn) 
are 
obtained 
by 
inference 
using 
the 
techniques 
described 
in 
section(23.2.1). 


Equations 
(23.3.3,23.3.5,23.3.6) 
are 
repeated 
until 
convergence. 
See 
HMMem.m 
and 
demoHMMlearn.m. 


Parameter 
initialisation 


The 
EM 
algorithm 
converges 
to 
a 
local 
maxima 
of 
the 
likelihood 
and, 
in 
general, 
there 
is 
no 
guarantee 
that 
the 
algorithm 
will 
nd 
the 
global 
maximum. 
How 
best 
to 
initialise 
the 
parameters 
is 
a 
thorny 
issue, 
with 
a 
suitable 
initialisation 
of 
the 
emission 
distribution 
often 
being 
critical 
for 
success[229]. 
A 
practical 
strategy 
is 
to 
initialise 
the 
emission 
p(vjh) 
based 
on 
rst 
tting 
a 
simpler 
non-temporal 
mixture 
model 


s 


h 
p(vjh)p(h) 
to 
the 
data. 


Continuous 
observations 


For 
a 
continuous 
vector 
observation 
vt, 
with 
dim 
vt 
= 
D, 
we 
require 
a 
model 
p(vtjht) 
mapping 
the 
discrete 
state 
ht 
to 
a 
distribution 
over 
outputs. 
Using 
a 
continuous 
output 
does 
not 
change 
any 
of 
the 
standard 
inference 
message 
passing 
equations 
so 
that 
inference 
can 
be 
carried 
out 
for 
essentially 
arbitrarily 
complex 
emission 
distributions. 
Indeed, 
ltering, 
smoothing 
and 
Viterbi 
inference, 
the 
normalisation 
Z 
of 
the 
emission 
p(vjh)= 
(v, 
h)=Z 
is 
not 
required. 
For 
learning, 
however, 
the 
emission 
normalisation 
constant 
is 
required 
since 
this 
is 
a 
dependent 
on 
the 
parameters 
of 
the 
model. 


23.3.2 
Mixture 
emission 
To 
make 
a 
richer 
emission 
model 
(particularly 
for 
continuous 
observations), 
one 
approach 
is 
use 
a 
mixture 


T 


p(vtjht)= 
p(vtjkt;ht)p(ktjht) 
(23.3.7) 


kt 


where 
kt 
is 
a 
discrete 
summation 
variable. 
For 
learning, 
it 
is 
useful 
to 
consider 
the 
kt 
as 
additional 
latent 
variables 
so 
that 
updates 
for 
each 
component 
of 
the 
emission 
model 
can 
be 
derived. 
To 
achieve 
this, 
consider 
the 
contribution 
to 
the 
energy 
from 
the 
emission 
(assuming 
equal 
length 
sequences): 


T

XT 


n

Ev 
hhlog 
p(vt 
jht)iq(htjvn 
) 
(23.3.8)

1:T 
nt=1

As 
it 
stands, 
the 
parameters 
of 
each 
component 
p(vtjkt;ht) 
are 
coupled 
in 
the 
above 
expression. 
One 
approach 
is 
to 
consider 


KL(q(ktjht)jp(ktjht;vt)) 
h0 
(23.3.9) 


from 
which 
we 
immediately 
obtain 
the 
bound 


log 
p(vt;ht) 
..hlog 
q(ktjht)iq(ktjht) 
+hlog 
p(vt;kt;ht)iq(ktjht) 
(23.3.10) 


and 


nn

log 
p(vt 
jht
n) 
..hlog 
q(ktjht
n)iq(ktjhn) 
+hlog 
p(vt 
jkt;ht
n)iq(ktjhn) 
+hlog 
p(ktjht
n)iq(ktjhn) 
(23.3.11) 


t 
tt 


DRAFT 
March 
9, 
2010 



Learning 
HMMs 


Using 
this 
in 
the 
energy 
contribution 
(23.3.8) 
we 
have 
the 
bound 
on 
the 
energy 
contribution 


TDE

XXP

n

Ev 
= 
..hlog 
q(ktjht
n)iq(ktjhn) 
+ 
hlog 
p(vt 
jkt;ht
n)iq(ktjhn) 
+ 
hlog 
p(ktjht
n)iq(ktjhn) 
(23.3.12) 


t 
ttn

q(hnjv)

t 
1:T

nt=1 


We 
may 
now 
maximise 
this 
lower 
bound 
on 
the 
energy 
(instead 
of 
the 
energy 
itself). 
The 
contribution 
from 
each 
emission 
component 
p(v 
= 
vjh 
= 
h;k 
= 
k) 
is 


T

XXP

nn 


q(kt 
= 
kjhn 
= 
h)q(hn 
= 
hjv 
) 
log 
p(v 
jh 
= 
h;k 
= 
k) 
(23.3.13)

tt 
1:Tt 
nt=1 


The 
above 
can 
then 
be 
optimised 
(M-step) 
for 
xed 
q(kt 
= 
kjhn 
= 
h), 
with 
these 
distributions 
updated 


t 


using 


q 
new(ktjhn) 
. 
p(v 
njhn;kt)p(ktjht) 
(23.3.14)

tt 


The 
contribution 
to 
the 
energy 
bound 
from 
the 
mixture 
weights 
is 
given 
by 


T

XXP

n

log 
p(k 
= 
kjh 
= 
h) 
q(kt 
= 
kjhn 
= 
h)q(hn 
= 
hjv 
) 
(23.3.15)

tt 
1:T 
nt=1 


so 
that 
the 
M-step 
update 
for 
the 
mixture 
weights 
is, 


T

XXP

n 


p(k 
= 
kjh 
= 
h) 
. 
q(kt 
= 
kjhn 
= 
h)q(hn 
= 
hjv 
) 
(23.3.16)

tt 
1:T 
nt=1 


In 
this 
case 
the 
EM 
algorithm 
is 
composed 
of 
an 
`emission’ 
EM 
loop 
in 
which 
the 
transitions 
and 
q(hn 
= 


t 


n

hjv) 
is 
xed, 
during 
which 
the 
emissions 
p(vjh, 
k) 
are 
learned, 
along 
with 
updating 
q(kt 
= 
kjhn 
= 
h).

1:Tt 
The 
`transition’ 
EM 
loop 
xes 
the 
emission 
distribution 
p(vjh) 
and 
learns 
the 
best 
transition 
p(htjht..1). 


An 
alternative 
to 
the 
above 
derivation 
is 
to 
consider 
the 
k 
as 
hidden 
variables, 
and 
then 
use 
standard 
EM 
algorithm 
on 
the 
joint 
latent 
variables 
(ht;kt). 
The 
reader 
may 
show 
that 
the 
two 
approaches 
are 
equivalent. 


23.3.3 
The 
HMM-GMM 
A 
common 
continuous 
observation 
mixture 
emission 
model 
component 
is 
a 
Gaussian 


..

p(vtjkt;ht)= 
Nvt 


kt;ht 
, 
kt;ht(23.3.17) 


so 
that 
kt;ht 
indexes 
the 
K 
× 
H 
mean 
vectors 
and 
covariance 
matrices. 
EM 
updates 
for 
these 
means 
and 
covariances 
are 
straightforward 
to 
derive 
from 
equation 
(23.3.12), 
see 
exercise(232). 
These 
models 
are 
common 
in 
tracking 
applications, 
in 
particular 
in 
speech 
recognition 
(usually 
under 
the 
constraint 
that 
the 
covariances 
are 
diagonal). 


23.3.4 
Discriminative 
training 
n

HMMs 
can 
be 
used 
for 
supervised 
learning 
of 
sequences. 
That 
is, 
for 
each 
sequence 
v, 
we 
have 
a 


1:T 
corresponding 
class 
label 
cn. 
For 
example, 
we 
might 
associated 
a 
particular 
composer 
c 
with 
a 
sequence 
v1:T 
and 
wish 
to 
make 
a 
model 
that 
will 
predict 
the 
composer 
for 
a 
novel 
music 
sequence. 
A 
generative 
approach 
to 
using 
HMMs 
for 
classication 
is 
to 
train 
a 
separate 
HMM 
for 
each 
class, 
p(v1:T 
jc) 
and 
subsequently 
use 




Bayes’ 
rule 
to 
form 
the 
classication 
for 
a 
novel 
sequence 
v 
using

1:T 
* 


p(v 
jc 
)p(c 
)

* 
1:T 
p(c 
* 
jv 
) 
= 
(23.3.18)

1:T 
PC 
* 


c0=1 
p(v1:T 
jc0)p(c0) 


If 
the 
data 
is 
noisy 
and 
dicult 
to 
model, 
however, 
this 
generative 
approach 
may 
not 
work 
well 
since 
much 
of 
the 
expressive 
power 
of 
each 
model 
is 
used 
to 
model 
the 
complex 
data, 
rather 
than 
focussing 
on 


DRAFT 
March 
9, 
2010 



Related 
Models 


v1v2v3v4
h1h2h3h4
c1c2c3c4
Figure 
23.10: 
An 
explicit 
duration 
HMM. 
The 
counter 
variables 
ct 
deterministically 
count 
down 
to 
zero. 
When 
they 
reach 
one, 
a 
h 
transition 
is 
allowed, 
and 
the 
new 
value 
for 
ct 
is 
sampled. 


the 
decision 
boundary. 
In 
applications 
such 
as 
speech 
recognition, 
improvements 
in 
performance 
are 
often 
reported 
when 
the 
models 
are 
trained 
in 
a 
discriminative 
way. 
In 
discriminative 
training, 
see 
for 
example 
[150], 
one 
denes 
a 
new 
single 
discriminative 
model, 
formed 
from 
the 
C 
HMMs 
using 


p(v1:T 
jc)p(c)

p(cjv1:T 
)= 
PC 
(23.3.19) 
c0=1 
p(v1:T 
jc0)p(c0) 


and 
then 
maximises 
the 
likelihood 
of 
a 
set 
of 
observed 
classes 
and 
corresponding 
observations 
v1:T 
. 
For 
a 
nn

single 
data 
pair, 
(c;v), 
the 
log 
likelihood 
is 


1:T 
C

XY

nn 
n

log 
p(c 
njv1:T 
) 
= 
log 
p(v1:T 
jc) 
+ 
log 
p(c) 
- 
log 
p(v1:T 
jc0)p(c0) 
(23.3.20)

|{z}Y

c0=1

generative 
likelihood 


The 
rst 
term 
above 
represents 
the 
generative 
likelihood 
term, 
with 
the 
last 
term 
accounting 
for 
the 
discrimination. 
Whilst 
deriving 
EM 
style 
updates 
is 
hampered 
by 
the 
discriminative 
terms, 
computing 
the 
gradient 
is 
straightforward 
using 
the 
technique 
described 
in 
section(11.7). 


In 
some 
applications, 
a 
class 
label 
ct 
is 
available 
at 
each 
timestep, 
together 
with 
an 
observation 
vt. 
Given 
a 
training 
sequence 
v1:T 
;c1:T 
(or 
more 
generally 
a 
set 
of 
sequences) 
the 
aim 
is 
to 
nd 
the 
optimal 
class 


* 


sequence 
c 
for 
a 
novel 
observation 
sequence 
v 
One 
approach 
is 
to 
train 
a 
generative 
model 


1:T 
1:T 
. 
Y

p(v1:T 
;c1:T 
)=p(vtjct)p(ctjct..1) 
(23.3.21) 
t 




and 
subsequently 
use 
Viterbi 
to 
form 
the 
class 
c 
= 
arg 
maxc1:T 
p(c1:T 
jv1:T 
). 
However, 
this 
approach 
may 


1:T 
not 
be 
optimal 
in 
terms 
of 
class 
discrimination. 
A 
cheap 
surrogate 
is 
to 
train 
a 
discriminative 
classication 
model 
~p(ctjvt) 
separately. 
With 
this 
one 
can 
form 
the 
emission 
(here 
written 
for 
continuous 
vt) 


p~(ctjvt)~p(vt)

p(vtjct)= 
R(23.3.22) 


p~(ctjvt)~p(vt)

vt 


R

where 
~p(vt) 
is 
user 
dened. 
Whilst 
computing 
the 
local 
normalisationp~(ctjvt)~p(vt) 
may 
be 
problematic, 


vt 




if 
the 
only 
use 
of 
p(vtjct) 
is 
to 
nd 
the 
optimal 
class 
sequence 
for 
a 
novel 
observation 
sequence 
v

1:T 
, 
* 


c1:T 
= 
argmax 
p(c1:T 
jv1:T 
) 
(23.3.23) 


c1:T 


then 
the 
local 
normalisations 
play 
no 
role 
since 
they 
are 
independent 
of 
c. 
Hence, 
during 
Viterbi 
decoding 
we 
may 
replace 
the 
term 
p(vtjht) 
with 
~p(ctjvt) 
without 
aecting 
the 
optimal 
sequence. 
Using 
a 
model 
in 
this 
way 
is 
a 
special 
case 
of 
the 
general 
hybrid 
procedure 
described 
in 
section(13.2.4). 
The 
approach 
is 
suboptimal 
since 
learning 
the 
classier 
is 
divorced 
from 
learning 
the 
transition 
model. 
Nevertheless, 
this 
heuristic 
historically 
has 
some 
support 
in 
the 
speech 
recognition 
community. 


23.4 
Related 
Models 
23.4.1 
Explicit 
duration 
model 
For 
a 
HMM 
with 
self-transition 
p(ht 
= 
ijht..1 
= 
i) 
= 
i, 
the 
probability 
that 
the 
latent 
dynamics 
stays 
in 
state 
i 
for 
t 
timesteps 
is 
t 
, 
which 
decays 
exponentially 
with 
time. 
In 
practice, 
however, 
we 
would 


i 


426 
DRAFT 
March 
9, 
2010 



Related 
Models 


v1v2v3v4
h1h2h3h4
x1x2x3x4
Figure 
23.11: 
A 
rst 
order 
input-output 
hidden 
Markov 
model. 
The 
input 
x 
and 
output 
v 
nodes 
are 
shaded 
to 
emphasise 
that 
their 
states 
are 
known 
during 
training. 
During 
testing, 
the 
inputs 
are 
known 
and 
the 
outputs 
are 
predicted. 


often 
like 
to 
constrain 
the 
dynamics 
to 
remain 
in 
the 
same 
state 
for 
a 
minimum 
number 
of 
timesteps, 
or 
to 
have 
a 
specied 
duration 
distribution. 
A 
way 
to 
enforce 
this 
is 
to 
use 
a 
latent 
counter 
variable 
ct 
which 
at 
the 
beginning 
is 
initialised 
to 
a 
duration 
sampled 
from 
the 
duration 
distribution 
pdur(ct) 
with 
maximal 
duration 
Dmax. 
Then 
at 
each 
timestep 
the 
counter 
decrements 
by 
1, 
until 
it 
reaches 
1, 
after 
which 
a 
new 
duration 
is 
sampled: 




d 
(ct;ct..1 
- 
1) 
ct..1 
> 
1 


p(ctjct..1) 
=(23.4.1)

pdur(ct) 
ct..1 
=1 


The 
state 
ht 
can 
transition 
only 
when 
ct 
= 
1: 




d 
(ht;ht..1) 
ct 
> 
1 


p(htjht..1;ct) 
=(23.4.2)

ptran(htjht..1) 
ct 
=1 


Including 
the 
counter 
variable 
c 
denes 
a 
joint 
latent 
variable 
(c1:T 
;h1:T 
) 
distribution 
that 
ensures 
h 
remains 
in 
a 
desired 
minimal 
number 
of 
timesteps, 
see 
g(23.10). 
Since 
dim 
..Xct 
. 
ht 
= 
DmaxH, 
naively 
the 




computational 
complexity 
of 
inference 
in 
this 
model 
scales 
as 
O 
TH2D2 
. 
However, 
when 
one 
runs 


max

the 
forward 
and 
backward 
recursions, 
the 
deterministic 
nature 
of 
the 
transitions 
means 
that 
this 
can 
be 


..X

reduced 
to 
O 
TH2Dmax[199] 
– 
see 
also 
exercise(233). 


The 
hidden 
semi-Markov 
model 
generalises 
the 
explicit 
duration 
model 
in 
that 
once 
a 
new 
duration 
ct 
is 
sampled, 
the 
model 
emits 
a 
distribution 
p(vt:t+ct..1jht) 
dened 
on 
a 
segment 
of 
the 
next 
ct 
observations[216]. 


23.4.2 
Input-Output 
HMM 
The 
IOHMM[31] 
is 
a 
HMM 
with 
additional 
input 
variables 
x1:T 
, 
see 
g(23.11). 
Each 
input 
can 
be 
continuous 
or 
discrete 
and 
modulates 
the 
transitions 


Y

p(v1:T 
;h1:T 
jx1:T 
)=p(vtjht;xt)p(htjht..1;xt) 
(23.4.3) 
t 


The 
IOHMM 
may 
be 
used 
as 
a 
conditional 
predictor, 
where 
the 
outputs 
vt 
represent 
the 
prediction 
at 
time 
t. 
In 
the 
case 
of 
continuous 
inputs 
and 
discrete 
outputs, 
the 
tables 
p(vtjht;xt) 
and 
p(htjht..1;xt) 
are 
usually 
parameterised 
using 
a 
non-linear 
function, 
for 
example 


p(vt 
T

wx

= 
yjht 
= 
h, 
xt 
= 
x, 
w) 
. 
e 
h;y(23.4.4) 


Inference 
then 
follows 
in 
a 
similar 
manner 
as 
for 
the 
standard 
HMM. 
Dening 


(ht) 
= 
p(ht;v1:tjx1:t) 
(23.4.5) 


the 
forward 
pass 
is 
given 
by 


X

(ht)=p(ht;ht..1;v1:t..1;vtjx1:t) 
(23.4.6) 


ht..1 


X

=p(vtjv1:t..1;x1:t;ht;ht..1)p(htjv1:t..1;x1:t;ht..1)p(v1:t..1;ht..1jx1:t) 
(23.4.7) 


ht..1 


X

= 
p(vtjxt;ht)p(htjht..1;xt)(ht..1) 
(23.4.8) 


ht..1 


DRAFT 
March 
9, 
2010 



x
y1y2y3
x
y1y2y3
Related 
Models 


Figure 
23.12: 
Linear 
chain 
CRF. 
Since 
the 
input 
x 
is 
observed, 
the 
distribution 
is 
just 
a 
linear 
chain 
factor 
graph. 
The 
inference 
of 
pairwise 
marginals 
p(yt;yt..1jx) 
is 
therefore 
straightforward 
using 
message 
passing. 


The 
. 
backward 
pass 
is 


XX

p(htjx1:T 
;v1:T 
)=p(ht;ht+1jx1:t+1;xt+2:T 
;v1:T 
)=p(htjht+1;x1:t+1;v1:t)p(ht+1jx1:T 
;v1:T 
) 


ht+1 
ht+1 
(23.4.9) 
for 
which 
we 
need 


p(ht+1;htjx1:t+1;v1:t) 
p(ht+1jht;xt+1)p(htjx1:t;v1:t)

p(htjht+1;x1:t+1;v1:t)= 
= 
P(23.4.10) 


p(ht+1jx1:t+1;v1:t) 
p(ht+1jht;xt+1)p(htjx1:t;v1:t)

ht 


P

The 
likelihood 
can 
be 
found 
fromhT 
(hT 
). 


Direction 
bias 


Consider 
predicting 
the 
output 
distribution 
p(vtjx1:T 
) 
given 
both 
past 
and 
future 
input 
information 
x1:T 
. 
Because 
the 
hidden 
states 
are 
unobserved 
we 
have 
p(vtjx1:T 
)= 
p(vtjx1:t). 
Thus 
the 
prediction 
uses 
only 
past 
information 
and 
discards 
any 
future 
contextual 
information. 
This 
`direction 
bias’ 
is 
sometimes 
considered 
problematic 
(particularly 
in 
natural 
language 
modelling) 
and 
motivates 
the 
use 
of 
undirected 
models, 
such 
as 
conditional 
random 
elds. 


23.4.3 
Linear 
chain 
CRFs 
Linear 
chain 
Conditional 
Random 
Fields 
(CRFs) 
are 
an 
extension 
of 
the 
unstructured 
CRFs 
we 
briey 
discussed 
in 
section(9.4.6) 
and 
have 
application 
to 
modelling 
the 
distribution 
of 
a 
set 
of 
outputs 
y1:T 
given 
an 
input 
vector 
x. 
For 
example, 
x 
might 
represent 
a 
sentence 
in 
English, 
and 
y1:T 
should 
represent 
the 
translation 
into 
French. 
Note 
that 
the 
vector 
x 
does 
not 
have 
to 
have 
dimension 
T 
. 
A 
rst 
order 
linear 
chain 
CRF 
has 
the 
form 


Y

1 
T

p(y1:T 
jx, 
)= 
t(yt;yt..1, 
x, 
) 
(23.4.11)

Z(x, 
) 


t=2 


where 
. 
are 
the 
free 
parameters 
of 
the 
potentials. 
In 
practice 
it 
is 
common 
to 
use 
potentials 
of 
the 
form 


K

X

exp 
kfk;t(yt;yt..1, 
x) 
(23.4.12) 
k=1 


where 
fk;t(yt;yt..1;x) 
are 
`features', 
see 
also 
section(9.4.6). 
Given 
a 
set 
of 
input-output 
sequence 
pairs, 
n

xn;y1:T 
;n 
=1;:::;N 
(assuming 
all 
sequenced 
have 
equal 
length 
T 
for 
simplicity), 
we 
can 
learn 
the 
parameters 
. 
by 
Maximum 
Likelihood. 
Under 
the 
standard 
i.i.d. 
data 
assumption, 
the 
log 
likelihood 
is 


XXX

nn

L()=kfk(yt 
;y 
t..1, 
xn) 
..log 
Z(xn 
, 
) 
(23.4.13) 
t;nk 
n 


The 
reader 
may 
readily 
check 
that 
the 
log 
likelihood 
is 
concave 
so 
that 
the 
objective 
function 
has 
no 
local 
optima. 
The 
gradient 
is 
given 
by 




X

. 


nn

L()=fi(yt 
;y 
t..1, 
xn) 
..hfi(yt;yt..1, 
xn)ip(yt;yt..1jxn;)(23.4.14)

@i 


n;t

Learning 
therefore 
requires 
inference 
of 
the 
marginal 
terms 
p(yt;yt..1jx, 
). 
Since 
equation 
(23.4.11) 
corresponds 
to 
a 
linear 
chain 
factor 
graph, 
see 
g(23.12), 
inference 
of 
pairwise 
marginals 
is 
straightforward 


DRAFT 
March 
9, 
2010 



Related 
Models 


05101520253035404550-14-12-10-8-6-4-20
0510152025303540-20-15-10-50510152025
(a) 
(b) 
Figure 
23.13: 
Using 
a 
linear 
chain 
CRF 
to 
learn 
the 
sequences 
in 
table(23.1). 
(a): 
The 
evolution 
of 
the 
log 
likelihood 
under 
gradient 
ascent. 
(b): 
The 
learned 
parameter 
vector 
. 
at 
the 
end 
of 
training. 


using 
message 
passing. 
This 
can 
be 
achieved 
using 
either 
the 
standard 
factor 
graph 
message 
passing 
or 
by 
deriving 
an 
explicit 
algorithm, 
see 
exercise(227). 


Finding 
the 
most 
likely 
output 
sequence 
for 
a 
novel 
input 
x 
* 
is 
straightforward 
since 


Y

* 


y 
= 
argmax 
t(yt;yt..1, 
x 
, 
) 
(23.4.15)

1:T 
y1:T

t 


corresponds 
again 
to 
a 
simple 
linear 
chain, 
for 
which 
max-product 
inference 
yields 
the 
required 
result, 
see 
also 
exercise(226). 


In 
some 
applications, 
particularly 
in 
natural 
language 
processing, 
the 
dimension 
K 
of 
the 
vector 
of 
features 
f1;:::;fK 
may 
be 
many 
hundreds 
of 
thousands. 
This 
means 
that 
the 
storage 
of 
the 
Hessian 
is 
not 
feasible 
for 
Newton 
based 
training 
and 
either 
limited 
memory 
methods 
or 
conjugate 
gradient 
techniques 
are 
typically 
preferred[288]. 


7 
4 
7 
2 
3 
4 
5 
7 
3 
5 
3 
1 
3 
1 
2 
3 
3 
1 
2 
1 
10 
3 
2 
1 
1 
1 
9 
8 
3 
9 
2 
3 
3 
3 
7 
8 
8 
4 
6 
10 
2 
7 
7 
6 
6 
10 
2 
3 
3 
1 
3 
1 
1 
3 
2 
2 
3 
1 
7 
9 
3 
3 
4 
8 
8 
3 
1 
2 
3 
3 
3 
3 


Table 
23.1: 
A 
subset 
of 
the 
10 
training 
input-output 
sequences. 
Each 
row 
contains 
an 
input 
xt 
(upper 
entry) 
and 
output 
yt 
(lower 
entry). 
There 
are 
10 
input 
states 
and 
3 
output 
states. 


Example 
99 
(Linear 
Chain 
CRF). 
As 
a 
model 
for 
the 
data 
in 
table(23.1), 
a 
linear 
CRF 
model 
has 


P

potentials 
(yt;yt..1;xt) 
= 
exp(ifi(yt;yt..1;xt)) 
where 
we 
set 
the 
binary 
feature 
functions 
by 
rst 


i 


mapping 
each 
of 
the 
dim 
(x) 
× 
dim 
(y)2 
states 
to 
a 
unique 
integer 
i(a, 
b, 
c) 
from 
1 
to 
dim 
(x) 
× 
dim 
(y)2 


fi(a;b;c)(yt;yt..1;xt)= 
I[yt 
= 
a] 
I[yt..1 
= 
b] 
I[xt 
= 
c] 
(23.4.16) 


That 
is, 
each 
joint 
conguration 
of 
yt;yt..1;xt 
is 
mapped 
to 
an 
index, 
and 
in 
this 
case 
the 
feature 
vector 
f 
will 
trivially 
have 
only 
a 
single 
non-zero 
entry. 
The 
evolution 
of 
the 
gradient 
ascent 
training 
algorithm 
is 
plotted 
in 
g(23.13). 
In 
practice 
one 
would 
use 
richer 
feature 
functions 
dened 
to 
seek 
features 
of 
the 
input 
sequence 
x 
and 
also 
to 
produce 
a 
feature 
vector 
with 
more 
than 
one 
non-zero 
entry. 
See 
demoLinearCRF.m. 


DRAFT 
March 
9, 
2010 



Applications 


x1(t)
x2(t)
x3(t)
x1(t+1)
x2(t+1)
x3(t+1)
Figure 
23.14: 
A 
Dynamic 
Bayesian 
Network. 
Possible 
transitions 
between 
variables 
at 
the 
same 
time-
slice 
have 
not 
been 
shown. 


v1(t)
h1(t)
h2(t)
v2(t)
v1(t+1)
h1(t+1)
h2(t+1)
v2(t+1)
Figure 
23.15: 
A 
Coupled 
HMM. 
For 
example 
the 
upper 
HMM 
might 
model 
speech, 
and 
the 
lower 
the 
corresponding 
video 
sequence. 
The 
upper 
hidden 
units 
then 
correspond 
to 
phonemes, 
and 
the 
lower 
to 
mouth 
positions; 
this 
model 
therefore 
captures 
the 
expected 
coupling 
between 
mouth 
positions 
and 
phonemes. 


23.4.4 
Dynamic 
Bayesian 
networks 
A 
DBN 
is 
dened 
as 
a 
belief 
networkreplicated 
through 
time. 
For 
a 
multivariate 
xt, 
with 
dim 
xt 
= 
D, 
the 
DBN 
denes 
a 
joint 
model 


TD

YD 


p(x1;:::, 
xT 
)= 
p(xi(t)jxni(t), 
x(t 
- 
1)) 
(23.4.17) 


t=1 
i=1 


where 
xni(t) 
denotes 
the 
set 
of 
variables 
at 
time 
t, 
except 
for 
xi(t). 
The 
form 
of 
each 
p(xi(t)jxni(t), 
x(t..1)) 
is 
chosen 
such 
that 
the 
overall 
distribution 
remains 
acyclic. 
At 
each 
time-step 
t 
there 
is 
a 
set 
of 
variables 
xi(t);i 
=1;:::;X, 
some 
of 
which 
may 
be 
observed. 
In 
a 
rst 
order 
DBN, 
each 
variable 
xi(t) 
has 
parental 
variables 
taken 
from 
the 
set 
of 
variables 
in 
the 
previous 
time-slice, 
xt..1, 
or 
from 
the 
present 
time-slice. 
In 
most 
applications, 
the 
model 
is 
temporally 
homogeneous 
so 
that 
one 
may 
fully 
describe 
the 
distribution 
in 
terms 
of 
a 
two-time-slice 
model, 
g(23.14). 
The 
generalisation 
to 
higher-order 
models 
is 
straightforward. 
A 
coupled 
HMM 
is 
a 
special 
DBN 
that 
may 
be 
used 
to 
model 
coupled 
`streams’ 
of 
information, 
for 
example 
video 
and 
audio, 
see 
g(23.15)[209]. 


23.5 
Applications 
23.5.1 
Object 
tracking 
HMMs 
are 
used 
to 
track 
moving 
objects, 
based 
on 
an 
understanding 
of 
the 
dynamics 
of 
the 
object 
(encoded 
in 
the 
transition 
distribution) 
and 
an 
understanding 
of 
how 
an 
object 
with 
a 
known 
position 
would 
be 
observed 
(encoded 
in 
the 
emission 
distribution). 
Given 
an 
observed 
sequence, 
the 
hidden 
position 
can 
then 
be 
inferred. 
The 
burglar, 
example(10) 
is 
a 
case 
in 
point. 
HMMs 
have 
been 
applied 
in 
a 
many 
tracking 
contexts, 
including 
tracking 
people 
in 
videos, 
musical 
pitch, 
and 
many 
more[54, 
229, 
51]. 


23.5.2 
Automatic 
speech 
recognition 
Many 
speech 
recognition 
systems 
make 
use 
of 
HMMs[300]. 
Roughly 
speaking, 
a 
continuous 
output 
vector 
vt 
at 
time 
t, 
represents 
which 
frequencies 
are 
present 
in 
the 
speech 
signal 
in 
a 
small 
window 
around 
time 
t. 
These 
acoustic 
vectors 
are 
typically 
formed 
from 
taking 
a 
discrete 
Fourier 
transform 
of 
the 
speech 
signal 
over 
a 
small 
window 
around 
time 
t, 
with 
additional 
transformations 
to 
mimic 
human 
auditory 
processing. 
Alternatively, 
related 
forms 
of 
linear 
coding 
of 
the 
observed 
acoustic 
waveform 
may 
be 
used[131]. 


The 
corresponding 
discrete 
latent 
state 
ht 
represents 
a 
phoneme 
– 
a 
basic 
unit 
of 
human 
speech 
(for 
which 
there 
are 
44 
in 
standard 
English). 
Training 
data 
is 
painstakingly 
constructed 
by 
a 
human 
linguist 
who 


DRAFT 
March 
9, 
2010 



Applications 


determines 
the 
phoneme 
ht 
for 
each 
time 
t 
and 
many 
dierent 
observed 
sequences 
vt. 
Given 
then 
each 
acoustic 
vector 
vt 
and 
an 
associated 
phoneme 
ht, 
one 
may 
use 
maximum 
likelihood 
to 
t 
a 
mixture 
of 
(usually 
isotropic) 
Gaussians 
p(vtjht) 
to 
vt. 
This 
forms 
the 
emission 
distribution 
for 
a 
HMM. 


Using 
the 
database 
of 
labelled 
phonemes, 
the 
phoneme 
transition 
p(htjht..1) 
can 
be 
learned 
(by 
simple 
counting) 
and 
forms 
the 
transition 
distribution 
for 
a 
HMM. 
Note 
that 
in 
this 
case, 
since 
the 
`hidden’ 
variable 
h 
and 
observation 
v 
are 
known 
during 
training, 
training 
the 
HMM 
is 
straightforward 
and 
boils 
down 
to 
training 
the 
emission 
and 
transition 
distributions 
independently. 


For 
a 
new 
sequence 
of 
`acoustic’ 
vectors 
v1:T 
we 
can 
then 
use 
the 
HMM 
to 
infer 
the 
most 
likely 
phoneme 
sequence 
through 
time, 
arg 
maxh1:T 
p(h1:T 
jv1:T 
), 
which 
takes 
into 
account 
both 
the 
way 
that 
phonemes 
appear 
as 
acoustic 
vectors, 
and 
also 
the 
prior 
language 
constraints 
of 
likely 
phoneme 
to 
phoneme 
transitions. 
The 
fact 
that 
people 
speak 
at 
dierent 
speeds 
can 
be 
addressed 
using 
time-warping 
in 
which 
the 
latent 
phoneme 
remains 
in 
the 
same 
state 
for 
a 
number 
of 
timesteps. 


HMM 
models 
are 
typically 
trained 
on 
the 
assumption 
of 
`clean’ 
underlying 
speech. 
In 
practice 
noise 
corrupts 
the 
speech 
signal 
in 
a 
complex 
way, 
so 
that 
the 
resulting 
model 
is 
inappropriate, 
and 
performance 
degrades 
signicantly. 
To 
account 
for 
this, 
it 
is 
traditional 
to 
attempt 
to 
denoise 
the 
signal 
before 
sending 
this 
to 
a 
standard 
HMM 
recogniser. 


If 
the 
HMM 
is 
used 
to 
model 
a 
single 
word, 
it 
is 
natural 
to 
constrain 
the 
hidden 
state 
sequence 
to 
go 
`forwards’ 
through 
time, 
visiting 
a 
set 
of 
states 
in 
sequence 
(since 
the 
phoneme 
order 
for 
the 
word 
is 
known). 
In 
this 
case 
the 
structure 
of 
the 
transition 
matrices 
is 
upper 
triangular 
(or 
lower, 
depending 
on 
your 
denition), 
or 
even 
a 
banded 
triangular 
matrix. 
Such 
forward 
constraints 
describe 
a 
so-called 
left-to-right 
transition 
matrix. 


23.5.3 
Bioinformatics 
In 
the 
eld 
of 
Bioinformatics 
HMMs 
have 
been 
widely 
applied 
to 
modelling 
genetic 
sequences. 
Multiple 
sequence 
alignment 
using 
forms 
of 
constrained 
HMMs 
have 
been 
particularly 
successful. 
Other 
applications 
involve 
gene 
nding 
and 
protein 
family 
modelling[163, 
84]. 


23.5.4 
Part-of-speech 
tagging 
Consider 
the 
sentence 
below 
in 
which 
each 
word 
has 
been 
linguistically 
tagged 


hospitality_NN 
is_BEZ 
an_AT 
excellent_JJ 
virtue_NN 
,_, 
but_CC 
not_XNOT 
when_WRB 
the_ATI 
guests_NNS 
have_HV 
to_TO 
sleep_VB 
in_IN 
rows_NNS 
in_IN 
the_ATI 
cellar_NN 
!_! 


The 
subscripts 
denote 
a 
linguistic 
tag, 
for 
example 
NN 
is 
the 
singular 
common 
noun 
tag, 
ATI 
is 
the 
article 
tag 
etc. 
Given 
a 
training 
set 
of 
such 
tagged 
sequences, 
the 
task 
is 
to 
tag 
a 
novel 
word 
sequence. 
One 
approach 
is 
to 
use 
ht 
to 
be 
a 
tag, 
and 
vt 
to 
be 
a 
word 
and 
t 
a 
HMM 
to 
this 
data. 
For 
the 
training 
data, 
both 
the 
tags 
and 
words 
are 
observed 
so 
that 
Maximum 
Likelihood 
training 
of 
the 
transition 
and 
emission 
distribution 
can 
be 
achieved 
by 
simple 
counting. 
Given 
a 
new 
sequence 
of 
words, 
the 
most 
likely 
tag 
sequence 
can 
be 
inferred 
using 
the 
Viterbi 
algorithm. 


More 
recent 
part-of-speech 
taggers 
tend 
to 
use 
conditional 
random 
elds 
in 
which 
the 
input 
sequence 
x1:T 
is 
the 
sentence 
and 
the 
output 
sequence 
y1:T 
is 
the 
tag 
sequence. 
One 
possible 
parameterisation 
of 
for 
a 
linear 
chain 
CRF 
is 
to 
use 
a 
potential 
of 
the 
form 
(yt..1;yt)(yt, 
x) 
in 
which 
the 
rst 
factor 
encodes 
the 
grammatical 
structure 
of 
the 
language 
and 
the 
second 
the 
a 
priori 
likely 
tag 
yt[166]. 


DRAFT 
March 
9, 
2010 



Exercises 


23.6 
Code 
demoMixMarkov.m: 
Demo 
for 
Mixture 
of 
Markov 
models 
mixMarkov.m: 
Mixture 
of 
Markov 
models 
demoHMMinference.m: 
Demo 
of 
HMM 
Inference 
HMMforward.m: 
Forward 
a 
recursion 
HMMbackward.m: 
Forward 
ß 
recursion 
HMMgamma.m: 
RTS 
. 
`correction’ 
recursion 
HMMsmooth.m: 
Single 
and 
Pairwise 
a 
- 
ß 
smoothing 
HMMviterbi.m: 
Most 
Likely 
State 
(Viterbi) 
algorithm 
demoHMMburglar.m: 
Demo 
of 
Burglar 
Localisation 
demoHMMbigram.m: 
demo 
of 
stubby 
ngers 
typing 
HMMem.m: 
EM 
algorithm 
for 
HMM 
(Baum-Welch) 
demoHMMlearn.m: 
demo 
of 
EM 
algorithm 
for 
HMM 
(Baum-Welch) 
demoLinearCRF.m: 
demo 
of 
learning 
a 
linear 
chain 
CRF 


The 
following 
linear 
chain 
CRF 
potential 
is 
particularly 
simple 
and 
in 
practice 
one 
would 
use 
a 
more 
complex 
one. 
linearCRFpotential.m: 
Linear 
CRF 
potential 
The 
following 
likelihood 
and 
gradient 
routines 
are 
valid 
for 
any 
linear 
CRF 
potential 
(yt..1;yt, 
x). 
linearCRFgrad.m: 
Linear 
CRF 
gradient 
linearCRFloglik.m: 
Linear 
CRF 
log 
likelihood 


23.7 
Exercises 
P

Exercise 
217. 
A 
stochastic 
matrix 
Mij 
as 
non-negative 
entries 
withMij 
=1. 
Consider 
an 
eigenvalue 


i

PP

. 
and 
eigenvector 
e 
suchj 
Mijej 
= 
ei. 
By 
summing 
over 
i 
show 
that, 
providedi 
ei 
> 
0, 
then 
. 
must 
be 
equal 
to 
1. 


Exercise 
218. 
Consider 
the 
Markov 
chain 
with 
transition 
matrix 




01

M 
=(23.7.1)

10

Show 
that 
this 
Markov 
chain 
does 
not 
have 
an 
equilibrium 
distribution 
and 
state 
a 
stationary 
distribution 
for 
this 
chain. 


Exercise 
219. 
Consider 
a 
HMM 
with 
3 
states 
(M 
=3) 
and 
2 
output 
symbols, 
with 
a 
left-to-right 
state 
transition 
matrix 


01P

0:50:00:0 
A 
= 
@P0:30:60:0 
AP(23.7.2) 
0:20:41:0 
where 
Aij 
= 
p(h(t 
+1) 
= 
ijh(t)= 
j), 
emission 
matrix 
Bij 
= 
p(v(t)= 
ijh(t)= 
j) 




0:70:40:8
B 
=(23.7.3)

0:30:60:2
and 
initial 
state 
probability 
vector 
a 
= 
(0:90:10:0)T. 
Given 
the 
observed 
symbol 
sequence 
is 
v1:3 
= 
(0, 
1, 
1): 


1. 
Compute 
p(v1:3). 
2. 
Compute 
p(h1jv1:3). 
3. 
Find 
the 
most 
probable 
hidden 
state 
sequence 
arg 
maxh1:3 
p(h1:3jv1:3). 
432 
DRAFT 
March 
9, 
2010 



Exercises 


Exercise 
220. 
This 
exercise 
follows 
from 
example(98). 
Given 
the 
27 
long 
character 
string 
rgenmonleunosbpnntje 
vrancg 
typed 
with 
`stubby 
ngers', 
what 
is 
the 
most 
likely 
correct 
English 
sentence 
intended? 
In 
the 
list 
of 
decoded 
sequences, 
what 
value 
is 
log 
p(h1:27jv1:27) 
for 
this 
sequence? 
You 
will 
need 
to 
modify 
demoHMMbigram.m 
suitably. 


Exercise 
221. 
Show 
that 
if 
a 
transition 
probability 
Aij 
= 
p(ht 
= 
ijht..1 
= 
j) 
in 
a 
HMM 
is 
initialised 
to 
zero 
for 
EM 
training, 
then 
it 
will 
remain 
at 
zero 
throughout 
training. 


Exercise 
222. 
Consider 
the 
problem 
: 
Find 
the 
most 
likely 
joint 
output 
sequence 
v1:T 
for 
a 
HMM. 
That 
is, 


v 
* 
1:T 
= 
argmax 
p(v1:T 
) 
(23.7.4) 
v1:T 
where 


T

T 


p(h1:T 
;v1:T 
)= 
p(vtjht)p(htjht..1) 
(23.7.5) 
t=1 


1. 
Explain 
why 
a 
local 
message 
passing 
algorithm 
cannot, 
in 
general, 
be 
found 
for 
this 
problem 
and 
discuss 
the 
computational 
complexity 
of 
nding 
an 
exact 
solution. 
2. 
Explain 
how 
to 
adapt 
the 
Expectation-Maximisation 
algorithm 
to 
form 
a 
recursive 
algorithm, 
for 


nding 
an 
approximate 
v 
Explain 
which 
it 
guarantees 
an 
improved 
solution 
at 
each 
iteration. 


1:T 
. 
Additionally, 
explain 
how 
the 
algorithm 
can 
be 
implemented 
using 
local 
message 
passing. 
Exercise 
223. 
Explain 
how 
to 
train 
a 
HMM 
using 
EM, 
but 
with 
a 
constrained 
transition 
matrix. 
In 
particular, 
explain 
how 
to 
learn 
a 
transition 
matrix 
with 
an 
upper 
triangular 
structure. 
Exercise 
224. 
Write 
a 
program 
to 
t 
a 
mixture 
of 
Lth 
order 
Markov 
models. 
Exercise 
225. 


1. 
Using 
the 
correspondence 
A 
=1;C 
=2;G 
=3;T 
=4 
dene 
a 
4 
× 
4 
transition 
matrix 
p 
that 
produces 
sequences 
of 
the 
form 
A, 
C, 
G, 
T, 
A, 
C, 
G, 
T, 
A, 
C, 
G, 
T, 
A, 
C, 
G, 
T, 
. 
. 
. 
(23.7.6) 


Now 
dene 
a 
new 
transition 
matrix 


pnew 
= 
0.9*p 
+ 
0.1*ones(4)/4 
(23.7.7) 


Dene 
a 
4 
× 
4 
transition 
matrix 
q 
that 
produces 
sequences 
of 
the 
form 


T, 
G, 
C, 
A, 
T, 
G, 
C, 
A, 
T, 
G, 
C, 
A, 
T, 
G, 
C, 
A, 
. 
. 
. 
(23.7.8) 
Now 
dene 
a 
new 
transition 
matrix 


qnew 
= 
0.9*q 
+ 
0.1*ones(4)/4 
(23.7.9) 


Assume 
that 
the 
probability 
of 
being 
in 
the 
initial 
state 
of 
the 
Markov 
chain 
p(h1) 
is 
constant 
for 
all 
four 
states 
A, 
C, 
G, 
T 
. 
What 
is 
the 
probability 
that 
the 
Markov 
chain 
pnew 
generated 
the 
sequence 
S 
given 
by 


S 
= 
A, 
A, 
G, 
T, 
A, 
C, 
T, 
T, 
A, 
C, 
C, 
T, 
A, 
C, 
G, 
C 
(23.7.10) 


2. 
Similarly 
what 
is 
the 
probability 
that 
S 
was 
generated 
by 
qnew? 
Does 
it 
make 
sense 
that 
S 
has 
a 
higher 
likelihood 
under 
pnew 
compared 
with 
qnew? 
DRAFT 
March 
9, 
2010 



Exercises 


3. 
Using 
the 
function 
randgen.m, 
generate 
100 
sequences 
of 
length 
16 
from 
the 
Markov 
chain 
dened 
by 
pnew. 
Similarly, 
generate 
100 
sequences 
each 
of 
length 
16 
from 
the 
Markov 
chain 
dened 
by 
qnew. 
Concatenate 
all 
these 
sequences 
into 
a 
cell 
array 
v 
so 
that 
vf1g6contains 
the 
rst 
sequence 
and 
vf200g6the 
last 
sequence. 
Use 
MixMarkov.m 
to 
learn 
the 
optimum 
Maximum 
Likelihood 
parameters 
that 
generated 
these 
sequences. 
Assume 
that 
there 
are 
H 
=2 
kinds 
of 
Markov 
chain. 
The 
result 
returned 
in 
phgvn 
indicate 
the 
posterior 
probability 
of 
sequence 
assignment. 
Do 
you 
agree 
with 
the 
solution 
found? 
4. 
Take 
the 
sequence 
S 
as 
dened 
in 
equation 
(23.7.10). 
Dene 
an 
emission 
distribution 
that 
has 
4 
output 
states 
such 
that 


0:7 
i 
= 
j
p(v 
= 
ijh 
= 
j) 
=(23.7.11)

0:1 
i=6
j 
Using 
this 
emission 
distribution 
and 
the 
transition 
given 
by 
pnew 
dened 
in 
equation 
(23.7.7), 
adapt 
demoHMMinferenceSimple.m 
suitably 
to 
nd 
the 
most 
likely 
hidden 
sequence 
hp 
that 
generated 
the 


1:16 
observed 
sequence 
S. 
Repeat 
this 
computation 
but 
for 
the 
transition 
qnew 
to 
give 
hq 
. 
Which 
hidden 


1:16
sequence 
– 
hp 
is 
to 
be 
preferred? 
Justify 
your 
answer. 


1:16 
or 
hq 
1:16 
Exercise 
226. 
Derive 
an 
algorithm 
that 
will 
nd 
the 
most 
likely 
joint 
state 


T

Y

argmax 
t(ht..1;ht) 
(23.7.12) 
h1:T 


t=2 


for 
arbitrarily 
dened 
potentials 
t(ht..1;ht). 


1. 
First 
consider 
T

Y

max 
t(ht..1;ht) 
(23.7.13) 
h1:T 


t=2 


Show 
that 
how 
the 
maximisation 
over 
hT 
may 
be 
pushed 
inside 
the 
product 
and 
that 
the 
result 
of 
the 
maximisation 
can 
be 
interpreted 
as 
a 
message 


T 
..1 T 
(hT 
..1) 
(23.7.14) 


2. 
Derive 
the 
recursion 
t..1 t(ht..1) 
= 
max 
t(ht;ht..1)t t+1(ht) 
(23.7.15) 
ht 


3. 
Explain 
how 
the 
above 
recursion 
enables 
the 
computation 
of 
T

Y

argmax 
t(ht;ht..1) 
(23.7.16) 
h1 


t=2 


4. 
Explain 
how 
once 
the 
most 
likely 
state 
for 
h1 
is 
computed, 
one 
may 
eciently 
compute 
the 
remaining 
optimal 
states 
h2;:::;hT 
. 
Exercise 
227. 
Derive 
an 
algorithm 
that 
will 
compute 
pairwise 
marginals 


p(ht;ht..1) 
(23.7.17) 


from 
the 
joint 
distribution 


T

Y

p(h1:T 
) 
/6t(ht..1;ht) 
(23.7.18) 
t=2 


for 
arbitrarily 
dened 
potentials 
t(ht..1;ht). 


DRAFT 
March 
9, 
2010 



Exercises 


1. 
First 
consider
T

XYX

t(ht;ht..1) 
(23.7.19) 
h1;:::;hT 
t=2 


Show 
that 
how 
the 
summation 
over 
h1 
may 
be 
pushed 
inside 
the 
product 
and 
that 
the 
result 
of 
the 
maximisation 
can 
be 
interpreted 
as 
a 
message 


X

1!2(h2)=2(h1;h2) 
(23.7.20) 


h1 


2. 
Derive 
the 
recursion 
X

t..1!t(ht)=t(ht..1;ht)t..2!t..1(ht..1) 
(23.7.21) 


ht..1 


3. 
Similarly, 
show 
that 
one 
can 
push 
the 
summation 
of 
hT 
inside 
the 
product 
to 
dene 
X

T 
..1 T 
(hT 
..1)=T 
(hT 
..1;hT 
) 
(23.7.22) 


hT 


and 
that 
by 
pushing 
in 
hT 
..1 
etc. 
one 
can 
dene 
messages 


X

t t+1(ht)=t+1(ht;ht+1)t+1 t+2(ht+1) 
(23.7.23) 


ht+1 


4. 
Show 
that 
X

p(ht;ht..1) 
/t..2!t..1(ht..1)(ht..1;ht)t t+1(ht) 
(23.7.24) 


ht+1 


Exercise 
228. 
A 
second 
order 
HMM 
is 
dened 
as 


T

YX

p 
HMM2(h1:T 
;v1:T 
)= 
p(h1)p(v1jh1)p(h2jh1)p(v2jh2) 
p(htjht..1;ht..2)p(vtjht) 
(23.7.25) 
t=3 


Following 
a 
similar 
approach 
to 
the 
rst 
order 
HMM, 
derive 
explicitly 
a 
message 
passing 
algorithm 
to 
compute 
the 
most 
likely 
joint 
state 


argmax 
p 
HMM2(h1:T 
jv1:T 
) 
(23.7.26) 
h1:T 


Exercise 
229. 
Since 
the 
likelihood 
of 
the 
HMM 
can 
be 
computed 
using 
ltering 
only, 
in 
principle 
we 
do 
not 
need 
smoothing 
to 
maximise 
the 
likelihood 
(contrary 
to 
the 
EM 
approach). 
Explain 
how 
to 
compute 
the 
likelihood 
gradient 
by 
the 
use 
of 
ltered 
information 
alone 
(i.e. 
using 
only 
a 
forward 
pass). 


Exercise 
230. 
Derive 
the 
EM 
updates 
for 
tting 
a 
HMM 
with 
an 
emission 
distribution 
given 
by 
a 
mixture 
of 
multi-variate 
Gaussians. 


Exercise 
231. 
Consider 
the 
HMM 
dened 
on 
hidden 
variables 
H 
= 
fh1;:::;hT 
} 
and 
observations 
V 
= 
fv1;:::;vT 
} 


T

YX

p(V, 
H)= 
p(h1)p(v1jh1) 
p(htjht..1)p(vtjht) 
(23.7.27) 


t=2 


Show 
that 
the 
posterior 
p(HjV) 
is 
a 
Markov 
chain 


T

YX

p(HjV)= 
p~(h1) 
p~(htjht..1) 
(23.7.28) 


t=2 


where 
p~(htjht..1) 
and 
p~(h1) 
are 
suitably 
dened 
distributions. 


DRAFT 
March 
9, 
2010 



Exercises 


Exercise 
232. 
For 
training 
a 
HMM 
with 
a 
Gaussian 
mixture 
emission 
(the 
HMM-GMM 
model) 
in 
section(23.3.3), 
derive 
the 
following 
EM 
update 
formulae 
for 
the 
means 
and 
covariances: 


NT

XX

new 


k;h 
= 
k;h(t, 
n)vt
n 
(23.7.29) 
n=1 
t=1 


and 


NT

XX....T

new 


k;h 
= 
k;h(t, 
n)vt
n 
..6k;hvt
n 
..6k;h(23.7.30) 
n=1 
t=1 


where 


n

q(kt 
= 
kjhn 
= 
h)q(hn 
= 
hjv)

tt 
1:T

k;h(t, 
n)= 
PP(23.7.31)

n

q(kt 
= 
kjhn 
= 
h)q(hn 
= 
hjv)

nt 
tt 
1:T 


Exercise 
233. 
Consider 
the 
HMM 
duration 
model 
dened 
by 
equation 
(23.4.2) 
and 
equation 
(23.4.1) 
with 
emission 
distribution 
p(vtjht). 
Our 
interest 
is 
to 
derive 
a 
recursion 
for 
the 
ltered 
distribution 


t(ht;ct) 
6p(ht;ct;v1:t) 
(23.7.32) 


1. 
Show 
that 
: 
X

t(ht;ct)= 
p(vtjht)p(htjht..1;ct)p(ctjct..1)t..1(ht..1;ct..1) 
(23.7.33) 


ht..1;ct..1 


2. 
Using 
this 
derive 
X

t(ht;ct) 


=p(htjht..1;c)p(ctjct..1 
= 
1)t..1(ht..1;ct..1 
= 
1) 


p(vtjht) 


ht..1 


Dmax

XX

+p(htjht..1;c) 
p(cjct..1)t..1(ht..1;ct..1) 
(23.7.34) 


ht..1 
ct..1=2 


3. 
Show 
that 
the 
right 
hand 
side 
of 
the 
above 
can 
be 
written 
as
X

p(htjht..1;ct 
= 
c)p(ct 
= 
cjct..1 
= 
1)t..1(ht..1, 
1) 


ht..1 


X

+ 
I[c6]p(htjht..1;c)t..1(ht..1;c 
+ 
1) 
(23.7.35)
= 
Dmax

ht..1 


4. 
Show 
that 
the 
recursion 
for 
a 
is 
then 
given 
by 
X

t(h, 
1) 
= 
p(vtjht 
= 
h)pdur(1)ptran(hjht..1)t..1(ht..1, 
1) 


ht..1 


X

+ 
I[Dmax6
= 
1] 
p(vtjht 
= 
h)ptran(htjht..1)t..1(ht..1, 
2) 
(23.7.36) 
ht..1 


and 
for 
c> 
1 


t(h, 
c)= 
p(vtjht 
= 
h) 
fpdur(c)t..1(h, 
1) 
+ 
I[c6
] 
t..1(h, 
c 
+ 
1)g6(23.7.37)

= 
Dmax

..

5. 
Explain 
why 
the 
computational 
complexity 
of 
ltered 
inference 
in 
the 
duration 
model 
is 
OTH2Dmax. 
6. 
Derive 
an 
ecient 
smoothing 
algorithm 
for 
this 
duration 
model. 
DRAFT 
March 
9, 
2010 



CHAPTER 
24 


Continuous-state 
Markov 
Models 


24.1 
Observed 
Linear 
Dynamical 
Systems 
In 
many 
practical 
timeseries 
applications 
the 
data 
is 
naturally 
continuous, 
particularly 
for 
models 
of 
the 
physical 
environment. 
In 
contrast 
to 
discrete-state 
Markov 
models, 
chapter(23), 
continuous 
state 
distributions 
are 
not 
automatically 
closed 
under 
operations 
such 
as 
products 
and 
marginalisation. 
To 
make 
practical 
algorithms 
for 
which 
inference 
and 
learning 
can 
be 
carried 
eciently, 
we 
therefore 
are 
heavily 
restricted 
in 
the 
form 
of 
the 
continuous 
transition 
p(vtjvt..1). 
A 
simple 
yet 
powerful 
class 
of 
such 
transitions 
are 
the 
linear 
dynamical 
systems. 
A 
deterministic 
observed 
linear 
dynamical 
system1 
(OLDS) 
denes 
the 
temporal 
evolution 
of 
a 
vector 
vt 
according 
to 
the 
discrete-time 
update 
equation 


vt 
= 
Atvt..1 
(24.1.1) 


where 
At 
is 
the 
transition 
matrix 
at 
time 
t. 
For 
the 
case 
that 
At 
is 
invariant 
with 
t, 
the 
process 
is 
called 
time-invariant, 
which 
we 
assume 
throughout 
unless 
explicitly 
stated 
otherwise. 


A 
motivation 
for 
studying 
OLDSs 
is 
that 
many 
equations 
that 
describe 
the 
physical 
world 
can 
be 
written 
as 
an 
OLDS. 
OLDSs 
are 
interesting 
since 
they 
may 
be 
used 
as 
simple 
prediction 
models: 
if 
vt 
describes 
the 
state 
of 
the 
environment 
at 
time 
t, 
then 
Avt 
predicts 
the 
environment 
at 
time 
t+1. 
As 
such, 
these 
models, 
have 
widespread 
application 
in 
many 
branches 
of 
science, 
from 
engineering 
and 
physics 
to 
economics. 


The 
OLDS 
equation 
(24.1.1) 
is 
deterministic 
so 
that 
if 
we 
specify 
v1, 
all 
future 
values 
v2, 
v3;:::, 
are 
dened. 
For 
a 
dim 
v 
= 
V 
dimensional 
vector, 
its 
evolution 
is 
described 
by 


= 
At..1

vt 
v1 
= 
Pt..1P..1v1 
(24.1.2) 


where 
. 
= 
diag 
(1;:::;V 
), 
is 
the 
diagonal 
eigenvalue 
matrix, 
and 
P 
is 
the 
corresponding 
eigenvector 
matrix 
of 
A. 
If 
i 
> 
1 
then 
for 
large 
t, 
vt 
will 
explode. 
On 
the 
other 
hand, 
if 
i 
< 
1, 
then 
t..1 
will 
tend 


i 


to 
zero. 
For 
stable 
systems 
we 
require 
therefore 
no 
eigenvalues 
of 
magnitude 
greater 
than 
1 
and 
only 
unit 
eigenvalues 
will 
contribute 
in 
long 
term. 
Note 
that 
the 
eigenvalues 
may 
be 
complex 
which 
corresponds 
to 
rotational 
behaviour, 
see 
exercise(234). 
More 
generally, 
we 
may 
consider 
additive 
noise 
on 
v 
and 
dene 
a 
stochastic 
OLDS. 


Denition 
111 
(Observed 
Linear 
Dynamical 
System). 


vt 
= 
Atvt..1 
+ 
t 
(24.1.3) 


1We 
use 
the 
terminology 
`observed’ 
LDS 
to 
dierentiate 
from 
the 
more 
general 
LDS 
state-space 
model. 
In 
some 
texts, 
however, 
the 
term 
LDS 
is 
applied 
to 
the 
models 
under 
discussion 
in 
this 
chapter. 


437 



Auto-Regressive 
Models 


where 
t 
is 
a 
noise 
vector 
sampled 
from 
a 
Gaussian 
distribution, 
N 
(t 


t, 
t) 
(24.1.4) 
This 
is 
equivalent 
to 
a 
rst 
order 
Markov 
model 
p(vtjvt..1)= 
N 
(vt 


Atvt..1 
+ 
t, 
t) 
(24.1.5) 
At 
t 
= 
1 
we 
have 
an 
initial 
distribution 
p(v1)= 
N 
(v1 


1, 
1). 
For 
t> 
1 
if 
the 
parameters 
are 
time-
independent, 
t 
= 
, 
At 
= 
A, 
t 
= 
, 
the 
process 
is 
called 
time-invariant. 


24.1.1 
Stationary 
distribution 
with 
noise 
Consider 
the 
one-dimensional 
linear 
system 


..



vt 
= 
avt..1 
+ 
t;t 
Nt 


0;2 
(24.1.6)

v

If 
we 
start 
at 
some 
state 
v1, 
and 
then 
for 
t> 
1 
recursively 
sample 
according 
to 
vt 
= 
avt..1 
+ 
t, 
does 
the 
distribution 
of 
the 
vt;t 
» 
1 
tend 
to 
a 
steady, 
xed 
distribution? 
Assuming 
that 
we 
can 
represent 
the 


..



distribution 
of 
vt..1 
as 
a 
Gaussian 
with 
mean 
t..1 
and 
variance 
t
2 
..1, 
vt..1 
Nvt..1 


t..1;2 
, 
then 


t..1

using 
ht) 
= 
0 
we 
have 


hvt) 
= 
a 
hvt..1) 
+ 
hti. 
t 
= 
at..1 
(24.1.7)






2 
22 


v 
= 
havt..1 
+ 
ti2 
= 
av 
+2 
hvt..1iht) 
+2 
(24.1.8)

tt..1t

. 
2 
= 
a 
2t
2 
..1 
+ 
2 
(24.1.9)
tv 


so 
that 


..



22 


vt 
Nvt 


at..1;a 
t..1 
+ 
2 
(24.1.10)

v

Assuming 
there 
is 
a 
xed 
variance 
2 


8 
for 
the 
innite 
time 
case, 
the 
stationary 
distribution 
satises 


2 


v

222 
. 
2 


8 
= 
a 
8 
+ 
v 
2 
(24.1.11)

= 


8 
1 
- 
a2 


1

Similarly, 
the 
mean 
is 
given 
by 
8 
= 
a1. 
If 
a 
= 
1 
the 
variance 
(and 
mean) 
increases 
indenitely 
with 


t. 
For 
a< 
1, 
the 
mean 
tends 
to 
zero 
yet 
the 
variance 
remains 
nite. 
Even 
though 
the 
magnitude 
of 
vt..1 
is 
decreased 
by 
a 
factor 
of 
a 
at 
each 
iteration, 
the 
additive 
noise 
on 
average 
boosts 
the 
magnitude 
so 
that 
it 
remains 
steady 
in 
the 
long 
run. 
More 
generally 
for 
a 
system 
updating 
a 
vector 
vt 
according 
to 
vt 
= 
Avt..1 
+ 
t 
(24.1.12) 


for 
the 
existence 
of 
a 
steady 
state 
we 
require 
that 
all 
eigenvalues 
of 
A 
must 
be 
= 
1. 


24.2 
Auto-Regressive 
Models 
A 
scalar 
time-invariant 
auto-regressive 
model 
is 
dened 
by 


L

L 
..



vt 
= 
alvt..l 
+ 
t;t 
Nt 


, 
2(24.2.1) 
l=1 


where 
a 
=(a1;:::;aL)T 
are 
called 
the 
AR 
coecients 
and 
2 
is 
called 
the 
innovation 
noise. 
The 
model 
predicts 
the 
future 
based 
on 
a 
linear 
combination 
of 
the 
previous 
L 
observations. 
As 
a 
belief 
network, 
the 
AR 
model 
can 
be 
written 
as 
an 
Lth 
order 
Markov 
model: 


T

T 


p(v1:T 
)= 
p(vtjvt..1;:::;vt..L), 
with 
vi 
= 
Ø 
for 
i 
= 
0 
(24.2.2) 
t=1 


438 
DRAFT 
March 
9, 
2010 



Auto-Regressive 
Models 


020406080100120140-50050100150200
Figure 
24.1: 
Fitting 
an 
order 
3 
AR 
model 
to 
the 
training 
points. 
The 
x 
axis 
represents 
time, 
and 
the 
y 
axis 
the 
value 
of 
the 
timeseries. 
The 
solid 
line 
is 
the 
mean 
prediction 
and 
the 
dashed 
lines 
± 
one 
standard 
deviation. 
See 
demoARtrain.m 


with 


 

!

L

X

p(vtjvt..1;:::;vt..L)= 
Nvt 


alvt..l;2(24.2.3) 
l=1 


T 


Introducing 
the 
vector 
of 
the 
L 
previous 
observations 


v^t..1 
= 
[vt..1;vt..2;:::;vt..L]T 
(24.2.4) 


we 
can 
write 
more 
compactly 




p(vtjvt..1;:::;vt..L)= 
Nvt 


aTv^t..1;2 
(24.2.5) 


AR 
models 
are 
heavily 
used 
in 
nancial 
time-series 
prediction 
(see 
for 
example 
[272]), 
being 
able 
to 
capture 
simple 
trends 
in 
the 
data. 
Another 
common 
application 
area 
is 
in 
speech 
processing 
whereby 
for 
a 
one-
dimensional 
speech 
signal 
partitioned 
into 
windows 
of 
length 
T 
, 
the 
AR 
coecients 
best 
able 
to 
describe 
the 
signal 
in 
each 
window 
are 
found[215]. 
These 
AR 
coecients 
then 
form 
a 
compressed 
representation 
of 
the 
signal 
and 
subsequently 
transmitted 
for 
each 
window, 
rather 
than 
the 
original 
signal 
itself. 
The 
signal 
can 
then 
be 
approximately 
reconstructed 
based 
on 
the 
AR 
coecients. 
Such 
a 
representation 
is 
used 
for 
example 
in 
telephones 
and 
known 
as 
a 
linear 
predictive 
vocoder[257]. 


24.2.1 
Training 
an 
AR 
model 
Maximum 
Likelihood 
training 
of 
the 
AR 
coecients 
is 
straightforward 
based 
on 


XX2

T1 
T
T 


vt..1)= 
- 
t..1a 
log(22)

log 
p(v1:T 
)= 


log 
p(vtj^

vt 
- 
v^

(24.2.6)
-


22 


2 


t=1 


t=1

Dierentiating 
w.r.t. 
a 
and 
equating 
to 
zero 
we 
arrive 
at 




X

T 


t..1a 
t

so 
that 
optimally 


 !..1

XX

vt 
- 
v^

v^t..1 
= 
0 
(24.2.7) 


a 
=


v^t..1v^

T 


vt

t..1

v^t..1 
(24.2.8) 


T 


tt 


These 
equations 
can 
be 
solved 
by 
Gaussian 
elimination. 
Similarly, 
optimally, 


X

1 
T
2 
t..1a


r


2 


vt 
- 
v^

(24.2.9)
= 


T 


t=1

Above 
we 
assume 
that 
`negative’ 
timepoints 
are 
available 
in 
order 
to 
keep 
the 
notation 
simple. 
If 
times 
before 
the 
window 
over 
which 
we 
learn 
the 
coecients 
are 
not 
available, 
a 
minor 
adjustment 
is 
required 
to 
start 
the 
summations 
from 
t 
= 
L 
+ 
1. 


Given 
a 
trained 
a, 
future 
predictions 
can 
be 
made 
using 
vt+1 
= 
v^

T 


t 


a. 
As 
we 
see, 
the 
model 
is 
capable 
of 


capturing 
the 
trend 
in 
the 
data. 


DRAFT 
March 
9, 
2010 



Auto-Regressive 
Models 


v1v2v3v4
a1a2a3a4
Figure 
24.2: 
A 
time-varying 
AR 
model 
as 
a 
latent 
LDS. 
Since 
the 
observations 
are 
known, 
this 
model 
is 
a 
time-varying 
latent 
LDS, 
for 
which 
smoothed 
inference 
determines 
the 
time-varying 
AR 
coecients. 


24.2.2 
AR 
model 
as 
an 
OLDS 
We 
can 
write 
equation 
(24.2.1) 
as 
an 
OLDS 
using 


101

01

01

0

vt 
a1 
a2 
::. 
aL 
vt..1 
t 


BBB
. 


vt..1 


. 


. 


. 


CCC
. 


= 


BBB
. 


BBB
. 
CCC
. 


vt..2 


. 


. 


. 


CCC
. 


+ 


BBB
. 


0 


. 


. 


. 


CCC
.


10 
::. 
0 
(24.2.10)
. 


. 


. 


1 
::. 
0 
vt..L+1 
0 
::. 
10 
vt..L 
0 
We 
can 
write 
equation 
(24.2.1) 
as 
the 
OLDS 
v^t 
= 
Av^t..1 
+ 
t, 
t 
N 
(t 


0, 
) 
(24.2.11) 


where 
we 
dene 
the 
block 
matrices 








 


2 


01;1:L..1 


01:L..1;1 


01:L..1;1:L..1

aL

a1:L..1 


I 


0

, 
S 
=


A 
=


(24.2.12) 
In 
this 
representation, 
the 
rst 
component 
of 
the 
vector 
is 
updated 
according 
to 
the 
standard 
AR 
model, 
with 
the 
remaining 
components 
being 
copies 
of 
the 
previous 
values. 


24.2.3 
Time-varying 
AR 
model 
An 
alternative 
to 
Maximum 
Likelihood 
is 
to 
view 
learning 
the 
AR 
coecients 
as 
a 
problem 
in 
inference 
in 
a 
latent 
LDS, 
a 
model 
which 
is 
discussed 
in 
detail 
in 
section(24.3). 
If 
at 
are 
the 
latent 
AR 
coecients, 
the 
term 


 


 


vt 


= 


vˆ


T 


t..1at 
+ 
t, 


0;2

(24.2.13)
t 
Nt 


can 
be 
viewed 
as 
the 
emission 
distribution 
of 
a 
latent 
LDS 
in 
which 
the 
hidden 
variable 
is 
at 
and 
the 
time-dependent 
emission 
matrix 
is 
given 
by 
v^t
T 
..1. 
By 
placing 
a 
simple 
latent 
transition 


 


 


0;2 


a

at 
= 
at..1 
+ 
t
a 
, 
a 
Na 


tt 


I(24.2.14) 


we 
encourage 
the 
AR 
coecients 
to 
change 
slowly 
with 
time. 
This 
denes 
a 
model 


 


X

p(v1:T 
, 
a1:T 
)=p(vtjat, 
v^t..1)p(atjat..1) 
(24.2.15) 
t 


Our 
interest 
is 
then 
in 
the 
conditional 
p(a1:T 
jv1:T 
) 
from 
which 
we 
can 
compute 
the 
a-posteriori 
most 
likely 
sequence 
of 
AR 
coecients. 
Standard 
smoothing 
algorithms 
can 
then 
be 
applied 
to 
yield 
the 
time-varying 
AR 
coecients, 
see 
demoARlds.m. 


Denition 
112 
(Discrete 
Fourier 
Transform). 
For 
a 
sequence 
x0:N..1 
the 
DFT 
f0:N..1 
is 
dened 
as 


N..1

- 
2i 


kn 


N 
;k 
=0;:::;N 
- 
1 
(24.2.16)

fk 
= 


xne 


n=0 


fk 
is 
a 
(complex) 
representation 
as 
to 
how 
much 
frequency 
k 
is 
present 
in 
the 
sequence 
x0:N..1. 
The 
power 
of 
component 
k 
is 
dened 
as 
the 
absolute 
length 
of 
the 
complex 
fk. 


DRAFT 
March 
9, 
2010 



Auto-Regressive 
Models 



(a) 
(b) 
(c) 
(d) 
(e) 
Figure 
24.3: 
(a): 
The 
raw 
recording 
of 
5 
seconds 
of 
a 
nightingale 
song 
(with 
additional 
background 
birdsong). 
(b): 
Spectrogram 
of 
(a) 
up 
to 
20,000 
Hz. 
(c): 
Clustering 
of 
the 
results 
in 
panel 
(b) 
using 
an 
8 
component 
Gaussian 
mixture 
model. 
The 
index 
(from 
1 
to 
8) 
of 
the 
component 
most 
probably 
responsible 
for 
the 
observation 
is 
indicated 
vertically 
in 
black. 
(d): 
The 
20 
AR 
coecients 
learned 
using 
2 
=0:001, 
2 
=0:001, 
see 
ARlds.m. 
(e): 
Clustering 
the 
results 
in 
panel 
(d) 
using 
a 
Gaussian 
mixture 


vh 


model 
with 
8 
components. 
The 
AR 
components 
group 
roughly 
according 
to 
the 
dierent 
song 
regimes. 


Denition 
113 
(Spectrogram). 
Given 
a 
timeseries 
x1:T 
the 
spectrogram 
at 
time 
t 
is 
a 
representation 
of 
the 
frequencies 
present 
in 
a 
window 
localised 
around 
t. 
For 
each 
window 
one 
computes 
the 
Discrete 
Fourier 
Transform, 
from 
which 
we 
obtain 
a 
vector 
of 
log 
power 
in 
each 
frequency. 
The 
window 
is 
then 
moved 
(usually) 
one 
step 
forward 
and 
the 
DFT 
recomputed. 
Note 
that 
by 
taking 
the 
logarithm, 
small 
values 
in 
the 
original 
signal 
can 
translate 
to 
visibly 
appreciable 
values 
in 
the 
spectrogram. 


Example 
100 
(Nightingale). 
In 
g(24.3a) 
we 
plot 
the 
raw 
acoustic 
recording 
for 
a 
5 
second 
fragment 
of 
a 
nightingale 
song 
freesound.org/samplesViewSingle.php?id=17185. 
The 
spectrogram 
is 
also 
plotted 
and 
gives 
an 
indication 
of 
which 
frequencies 
are 
present 
in 
the 
signal 
as 
a 
function 
of 
time. 
The 
nightingale 
song 
is 
very 
complicated 
but 
at 
least 
locally 
can 
be 
very 
repetitive. 
A 
crude 
way 
to 
nd 
which 
segments 
repeat 
is 
to 
form 
a 
cluster 
analysis 
of 
the 
spectrogram. 
In 
g(24.3c) 
we 
show 
the 
results 
of 
tting 
a 
Gaussian 
mixture 
model, 
section(20.3), 
with 
8 
components, 
from 
which 
we 
see 
there 
is 
some 
repetition 
of 
components 
locally 
in 
time. 
An 
alternative 
representation 
of 
the 
signal 
is 
given 
by 
the 
time-varying 
AR 
coecients, 
section(24.2.3), 
as 
plotted 
in 
g(24.3d). 
A 
GMM 
clustering 
with 
8 
components 
g(24.3e) 


DRAFT 
March 
9, 
2010 



Latent 
Linear 
Dynamical 
Systems 


v1v2v3v4
h1h2h3h4
Figure 
24.4: 
A 
(latent) 
LDS. 
Both 
hidden 
and 
visible 
variables 
are 
Gaussian 
distributed. 


in 
this 
case 
produces 
a 
somewhat 
clearer 
depiction 
of 
the 
dierent 
phases 
of 
the 
nightingale 
singing 
than 
that 
aorded 
by 
the 
spectrogram. 


24.3 
Latent 
Linear 
Dynamical 
Systems 
The 
Latent 
LDS 
denes 
a 
stochastic 
linear 
dynamical 
system 
in 
a 
latent 
(or 
`hidden') 
space 
on 
a 
sequence 
of 
vectors 
h1:T 
. 
Each 
observation 
vt 
is 
as 
linear 
function 
of 
the 
latent 
vector 
ht. 
This 
model 
is 
also 
called 
a 
linear 
Gaussian 
state 
space 
model2. 
The 
model 
can 
also 
be 
considered 
a 
form 
of 
LDS 
on 
the 
joint 
variables 
xt 
=(vt;ht), 
with 
parts 
of 
the 
vector 
xt 
missing. 
For 
this 
reason 
we 
will 
also 
refer 
to 
this 
model 
as 
a 
Linear 
Dynamical 
System 
(without 
the 
`latent’ 
prex). 


Denition 
114 
(Latent 
linear 
dynamical 
system). 


..p





ht 
= 
Atht..1 
+ 
t
h 
t
h 
N 
t
h 


ht, 
H 
transition 
model 


t

..p

p(24.3.1)

= 
Btht 
+ 
v 
v 
N 
v 


vt, 
V 
emission 
model 


vt 
ttt 


t 


where 
h 
and 
v 
are 
noise 
vectors. 
At 
is 
called 
the 
transition 
matrix 
and 
Bt 
the 
emission 
matrix. 
The

tt 
terms 
h
t 
and 
vt 
are 
the 
hidden 
and 
output 
bias 
respectively. 
The 
transition 
and 
emission 
models 
dene 
a 
rst 
order 
Markov 
model 


T

Yp

p(h1:T 
, 
v1:T 
)= 
p(h1)p(v1jh1) 
p(htjht..1)p(vtjht) 
(24.3.2) 
t=2 


with 
the 
transitions 
and 
emissions 
given 
by 
Gaussian 
distributions 


..p

p

p(htjht..1)= 
N 
ht 


Atht..1 
+ 
h
t, 
H 
;p(h1)= 
N 
(h1 


, 
) 
(24.3.3)

t 


..p

p

p(vtjht)= 
N 
vt 


Btht 
+ 
vt, 
V 
(24.3.4)

t 


This 
(latent) 
LDS 
can 
be 
represented 
as 
a 
belief 
networkin 
g(24.4) 
with 
the 
extension 
to 
higher 
orders 
being 
intuitive. 
One 
may 
also 
include 
an 
external 
input 
ot 
at 
each 
time, 
which 
will 
add 
Cot 
to 
the 
mean 
of 
the 
hidden 
variable 
and 
Dot 
to 
the 
mean 
of 
the 
observation. 


Explicit 
expressions 
for 
the 
transition 
and 
emission 
distributions 
are 
given 
below 
for 
the 
time-invariant 


case 
with 
vt 
= 
0, 
ht 
= 
0. 
Each 
hidden 
variable 
is 
a 
multidimensional 
Gaussian 
distributed 
vector 
ht, 
with 




11 


p(htjht..1)= 
pexp- 
(ht 
- 
Aht..1)T 
..1 
(ht 
- 
Aht..1)(24.3.5)

j2H 
| 
2 
H 


which 
states 
that 
ht+1 
has 
a 
mean 
equal 
to 
Aht 
with 
Gaussian 
uctuations 
described 
by 
the 
covariance 
matrix 
H 
. 
Similarly, 




11 


p(vtjht)= 
pexp- 
(vt 
- 
Bht)T 
..1 
(vt 
- 
Bht)(24.3.6)

j2V 
| 
2 
V 


describes 
an 
output 
vt 
with 
mean 
Bht 
and 
covariance 
V 
. 


2These 
models 
are 
also 
often 
called 
Kalman 
Filters. 
We 
avoid 
this 
terminology 
here 
since 
the 
word 
`lter’ 
refers 
to 
a 
specic 
kind 
of 
inference 
and 
runs 
the 
risk 
of 
confusing 
a 
ltering 
algorithm 
with 
the 
model 
itself. 


442 
DRAFT 
March 
9, 
2010 



Inference 



Figure 
24.5: 
A 
single 
phasor 
plotted 
as 
a 
damped 
two 
dimensional 
rotation 
ht+1 
= 
Rht 
with 
a 
damping 
factor 
0 
<< 
1. 
By 
taking 
a 
projection 
onto 
the 
y 
axis, 
the 
phasor 
generates 
a 
damped 
sinusoid. 


Example 
101. 
Consider 
a 
dynamical 
system 
dened 
on 
two 
dimensional 
vectors 
ht: 




cos 
. 
- 
sin 


ht+1 
= 
Rht, 
with 
R. 
=(24.3.7)

sin 
. 
cos 


R. 
rotates 
the 
vector 
ht 
through 
angle 
. 
in 
one 
timestep. 
Under 
this 
LDS 
h 
will 
trace 
out 
points 
on 
a 
circle 
through 
time. 
By 
taking 
a 
scalar 
projection 
of 
ht, 
for 
example, 


vt 
=[ht]1 
= 
[1 
0]Tht, 
(24.3.8) 


the 
elements 
vt, 
t 
=1;:::;T 
describe 
a 
sinusoid 
through 
time, 
see 
g(24.5). 
By 
using 
a 
block 
diagonal 
R 
= 
blkdiag 
(R1 
;:::, 
Rm 
) 
and 
taking 
a 
scalar 
projection 
of 
the 
extended 
m 
× 
2 
dimensional 
h 
vector, 
one 
can 
construct 
a 
representation 
of 
a 
signal 
in 
terms 
of 
m 
sinusoidal 
components. 


24.4 
Inference 
Given 
an 
observation 
sequence 
v1:T 
we 
wish 
to 
consider 
ltering 
and 
smoothing, 
as 
we 
did 
for 
the 
HMM, 
section(23.2.1). 
For 
the 
HMM, 
in 
deriving 
the 
various 
message 
passing 
recursions, 
we 
used 
only 
the 
independence 
structure 
encoded 
by 
the 
belief 
network. 
Since 
the 
LDS 
has 
the 
same 
independence 
structure 
as 
the 
HMM, 
we 
can 
use 
the 
same 
independence 
assumptions 
in 
deriving 
the 
updates 
for 
the 
LDS. 
However, 
in 
implementing 
them 
we 
need 
to 
deal 
with 
the 
issue 
that 
we 
now 
have 
continuous 
hidden 
variables, 
rather 
than 
discrete 
states. 
The 
fact 
that 
the 
distributions 
are 
Gaussian 
means 
that 
we 
can 
deal 
with 
continuous 
messages 
exactly. 
In 
translating 
the 
HMM 
message 
passing 
equations, 
we 
rst 
replace 
summation 
with 
integration. 
For 
example, 
the 
ltering 
recursion 
(23.2.7) 
becomes 


p(htjv1:t) 
. 
p(vtjht)p(htjht..1)p(ht..1jv1:t..1), 
t> 
1 
(24.4.1) 


ht..1 


Since 
the 
product 
of 
two 
Gaussians 
is 
another 
Gaussian, 
and 
the 
integral 
of 
a 
Gaussian 
is 
another 
Gaussian, 
the 
resulting 
p(htjv1:t) 
is 
also 
Gaussian. 
This 
closure 
property 
of 
Gaussians 
means 
that 
we 
may 
represent 
p(ht..1jv1:t..1)= 
N 
(ht..1 


ft..1, 
Ft..1) 
with 
mean 
ft..1 
and 
covariance 
Ft..1. 
The 
eect 
of 
equation 
(24.4.1) 
is 
equivalent 
to 
updating 
the 
mean 
ft..1 
and 
covariance 
Ft..1 
into 
a 
mean 
ft 
and 
covariance 
Ft 
for 
p(htjv1:t). 
Our 
task 
below 
is 
to 
nd 
explicit 
algebraic 
formulae 
for 
these 
updates. 


Numerical 
stability 


Translating 
the 
message 
passing 
inference 
techniques 
we 
developed 
for 
the 
HMM 
into 
the 
LDS 
is 
largely 
straightforward. 
Indeed, 
one 
could 
simply 
run 
a 
standard 
sum-product 
algorithm 
(albeit 
for 
continuous 
variables), 
see 
demoSumprodGaussCanonLDS.m. 
In 
long 
timeseries 
numerical 
instabilities 
can 
build 
up 
and 
may 
result 
in 
grossly 
inaccurate 
results, 
depending 
on 
the 
transition 
and 
emission 
distribution 
parameters 
and 
the 
method 
of 
implementing 
the 
message 
updates. 
For 
this 
reason 
specialised 
routines 
have 
been 
developed 
that 
are 
reasonably 
numerically 
stable 
under 
certain 
parameter 
regimes[283]. 
For 
the 
HMM 
in 
section(23.2.1), 
we 
discussed 
two 
alternative 
methods 
for 
smoothing, 
the 
parallel 
ß 
approach, 
and 
the 
sequential 
. 
approach. 
The 
ß 
recursion 
is 
suitable 
when 
the 
emission 
and 
transition 
covariance 
entries 
are 
small, 
and 
the 
. 
recursion 
usually 
preferable 
in 
the 
more 
standard 
case 
of 
small 
covariance 
values. 


DRAFT 
March 
9, 
2010 
443 



Inference 


Analytical 
shortcuts 


In 
deriving 
the 
inference 
recursions 
we 
need 
to 
frequently 
multiply 
and 
integrate 
Gaussians. 
Whilst 
in 
principle 
straightforward, 
this 
can 
be 
algebraically 
tedious 
and, 
wherever 
possible, 
it 
is 
useful 
to 
appeal 
to 
known 
shortcuts. 
For 
example, 
one 
can 
exploit 
the 
general 
result 
that 
the 
linear 
transform 
of 
a 
Gaussian 
random 
variable 
is 
another 
Gaussian 
random 
variable. 
Similarly 
it 
is 
convenient 
to 
make 
use 
of 
the 
conditioning 
formulae, 
as 
well 
as 
the 
dynamics 
reversal 
intuition. 
These 
results 
are 
stated 
in 
section(8.6), 
and 
below 
we 
derive 
the 
most 
useful 
for 
our 
purposes 
here. 


Consider 
a 
linear 
transformation 
of 
a 
Gaussian 
random 
variable: 


y 
= 
Mx 
+ 
, 
. 
N 
(. 


, 
) 
, 
x 
N 
(x 


, 
x) 
(24.4.2)

x

where 
x 
and 
. 
are 
assumed 
to 
be 
generated 
from 
independent 
processes. 
To 
nd 
the 
distribution 
p(y), 
one 
approach 
would 
be 
to 
write 
this 
formally 
as 


p(y)= 
N 
(y 


Mx 
+ 
, 
) 
N 
(x 


, 
x) 
dx 
(24.4.3)

x

and 
carry 
out 
the 
integral 
(by 
completing 
the 
square). 
However, 
since 
a 
Gaussian 
variable 
under 
linear 
transformation 
is 
another 
Gaussian, 
we 
can 
take 
a 
shortcut 
and 
just 
nd 
the 
mean 
and 
covariance 
of 
the 
transformed 
variable. 
Its 
mean 
is 
given 
by 


hy) 
= 
M 
hx) 
+ 
h) 
= 
M+ 
µ 
(24.4.4)
x 


To 
nd 
the 
covariance, 
consider 
the 
displacement 
of 
a 
variable 
h 
from 
its 
mean, 
which 
we 
write 
as 


h 
= 
h 
..hh) 
(24.4.5) 





The 
covariance 
is, 
by 
denition,hhT. 
For 
y, 
the 
displacement 
is 


y 
= 
Mx 
+, 
(24.4.6) 


So 
that 
the 
covariance 
is

DEDE

T

yy=(Mx 
+)(Mx 
+)T

DEDEDEDE

= 
MxxTMT 
+ 
MxT+xTMT 
+T




Since 
the 
noises 
. 
and 
x 
are 
assumed 
independent,xT= 
0 
we 
have 
y 
= 
MxMT 
+ 
S 
(24.4.7) 


24.4.1 
Filtering 
We 
represent 
the 
ltered 
distribution 
as 
a 
Gaussian 
with 
mean 
ft 
and 
covariance 
Ft, 
p(htjv1:t) 
N 
(ht 


ft, 
Ft) 
(24.4.8) 


This 
is 
called 
the 
moment 
representation. 
Our 
task 
is 
then 
to 
nd 
a 
recursion 
for 
ft, 
Ft 
in 
terms 
of 
ft..1, 
Ft..1. 
A 
convenient 
approach 
is 
to 
rst 
nd 
the 
joint 
distribution 
p(ht, 
vtjv1:t..1) 
and 
then 
condition 
on 
vt 
to 
nd 
the 
distribution 
p(htjv1:t). 
The 
term 
p(ht, 
vtjv1:t..1) 
is 
a 
Gaussian 
whose 
statistics 
can 
be 
found 
from 
the 
relations 


vt 
= 
Bht 
+ 
t
v 
, 
ht 
= 
Aht..1 
+ 
h 
(24.4.9)

t 


Using 
the 
above, 
and 
assuming 
time-invariance 
and 
zero 
biases, 
we 
readily 
nd

DEDE

hthT 
t 
jv1:t..1= 
Aht..1hT 
t..1jv1:t..1AT 
+ 
H 
= 
AFt..1AT 
+ 
H 
(24.4.10) 


DRAFT 
March 
9, 
2010 



Inference 


Algorithm 
20 
LDS 
Forward 
Pass. 
Compute 
the 
ltered 
posteriors 
p(htjv1:t) 
N 
(ft, 
Ft) 
for 
a 
LDS 
with

	



parameters 
t 
=A, 
B, 
h 
, 
v 
, 
h, 
v. 
The 
log-likelihood 
L 
= 
log 
p(v1:T 
) 
is 
also 
returned. 


t 


ff1, 
F1;p1} 
= 
LDSFORWARD(0, 
0, 
v1; 
t) 


F0 
. 
0, 
f0 
. 
0, 
L 
. 
log 
p1 
for 
t 
. 
2;T 
do 


fft, 
Ft;pt} 
= 
LDSFORWARD(ft..1, 
Ft..1, 
vt; 
) 
L 
. 
L 
+ 
log 
pt 


end 
for 
function 
ldsforward(f, 
F, 
v; 
) 
h 
. 
Af 
+ 
h
. 
Bh 
+ 
vI 
Mean 
of 
p(ht, 
vtjv1:t..1)

v 
hh 
. 
AFAT 
+ 
h 
, 
vv 
. 
BhhBT 
+ 
v 
, 
vh 
. 
Bhh 
I 
Covariance 
of 
p(ht, 
vtjv1:t..1) 
f' 
. 
h 
T 
..1 
(v 
- 
), 
vh..1vh 
I 
Find 
p(htjv1:t) 
by 
conditioning:

+ 
+
Dvhvv 
vF
' 
. 
hh 
- 
T 
vv
p

..1

pi 
. 
exp 
(v 
- 
)T 
..1 
(v 
- 
)=det 
(2vv) 
I 
Compute 
p(vtjv1:t..1)

2 
vvv 
v

0

return 
f0, 
F0;p
end 
function

DEDED

vthTjv1:t..1= 
BhthTjv1:t..1= 
B 
AFt..1AT 
+ 
H(24.4.11)

tt 


DEDED

T

vtvjv1:t..1= 
BhthTjv1:t..1BT 
+ 
V 
= 
B 
AFt..1AT 
+ 
HBT 
+ 
V 
(24.4.12)

tt 


hvtjv1:t..1) 
= 
BA 
hht..1jv1:t..1) 
, 
hhtjv1:t..1) 
= 
A 
hht..1jv1:t..1) 
(24.4.13) 
In 
the 
above, 
using 
our 
moment 
representation 
of 
the 
forward 
messages 


DE

hht..1jv1:t..1i= 
ft..1;ht..1ht
T 
..1jv1:t..1= 
Ft..1 
(24.4.14) 


Then, 
using 
conditioning3 
p(htjvt, 
v1:t..1) 
will 
have 
mean 
DEDE..1 


TT

ft 
hhtjv1:t..1) 
+htvt 
jv1:t..1vtvjv1:t..1(vt 
..hvtjv1:t..1i) 
(24.4.15)

t 


and 
covariance 


DEDEDE..1DE

TT

hthT 
..vthT 
(24.4.16)

Ft 
t 
jv1:t..1htvt 
jv1:t..1vtvt 
jv1:t..1t 
jv1:t..1

Writing 
out 
the 
above 
explicitly 
we 
have 
for 
the 
mean: 


D..1 
ft 
= 
Aft..1 
+ 
PBT 
BPBT 
+ 
V(vt 
- 
BAft..1) 
(24.4.17) 


and 
covariance 


D..1 
Ft 
= 
P 
+ 
H 
- 
PBT 
BPBT 
+ 
VBP 
(24.4.18) 
where 


P 
= 
AFt..1AT 
+ 
H 
(24.4.19) 
The 
ltering 
procedure 
is 
presented 
in 
algorithm(20) 
with 
a 
single 
update 
in 
LDSforwardUpdate.m. 


One 
can 
write 
the 
covariance 
update 
as 


Ft 
=(I 
- 
KB) 
P 
(24.4.20) 
where 
we 
dene 
the 
Kalman 
gain 
matrix 


D..1 
K 
= 
PBT 
V 
+ 
BPBT(24.4.21) 


We 
present 
in 
algorithm(??) 
the 
recursion 
in 
standard 
engineering 
notation. 
See 
also 
LDSsmooth.m. 
The 
iteration 
is 
expected 
to 
be 
numerically 
stable 
when 
the 
noise 
covariances 
are 
small. 


..

3 
..1 
..1 


p(xjy) 
is 
a 
Gaussian 
with 
mean 
+ 
xyy 
- 
yand 
covariance 
xx 
- 
xyyy 
yx.

x 
yy

DRAFT 
March 
9, 
2010 
445 



Inference 


Symmetrising 
the 
updates 


A 
potential 
numerical 
issue 
with 
the 
covariance 
update 
(24.4.20) 
is 
that 
it 
is 
the 
dierence 
of 
two 
positive 
denite 
matrices. 
If 
there 
are 
numerical 
errors, 
the 
Ft 
may 
not 
be 
positive 
denite, 
nor 
symmetric. 
Using 
the 
Woodbury 
identity, 
denition(132), 
equation 
(24.4.18) 
can 
be 
written 
more 
compactly 
as 


..1 
Ft 
=P..1 
+ 
BT..1B(24.4.22)

V 


Whilst 
this 
is 
positive 
semidenite, 
this 
is 
numerically 
expensive 
since 
it 
involves 
two 
matrix 
inversions. 
An 
alternative 
is 
to 
use 
the 
denition 
of 
K, 
from 
which 
we 
can 
write 


KV 
KT 
=(I 
..hKB) 
PBTKT 
(24.4.23) 


Hence 
we 
arrive 
at 
Joseph's 
symmetrized 
update[104] 




(I 
..hKB) 
P 
(I 
..hKB)T 
+ 
KV 
KT 
h(I 
..hKB)P 
(I 
..hKB)T 
+ 
PBTKTh(I 
..hKB) 
P 
(24.4.24) 


The 
left 
hand 
side 
is 
the 
addition 
of 
two 
positive 
denite 
matrices 
so 
that 
the 
resulting 
update 
for 
the 
covariance 
is 
more 
numerically 
stable. 
A 
similar 
method 
can 
be 
used 
in 
the 
backward 
pass 
below. 
An 
alternative 
is 
to 
avoid 
using 
covariance 
matrices 
directly 
and 
use 
their 
square 
root 
as 
the 
parameter, 
deriving 
updates 
for 
these 
instead[226, 
38]. 


24.4.2 
Smoothing 
: 
Rauch-Tung-Striebel 
correction 
method 
The 
smoothed 
posterior 
p(htjv1:T 
) 
is 
necessarily 
Gaussian 
since 
it 
is 
the 
conditional 
marginal 
of 
a 
larger 
Gaussian. 
By 
representing 
the 
posterior 
as 
a 
Gaussian 
with 
mean 
gt 
and 
covariance 
Gt, 


p(htjv1:T 
) 
Nh(ht 


gt, 
Gt) 
(24.4.25) 


we 
can 
form 
a 
recursion 
for 
gt 
and 
Gt 
as 
follows: 


Z

p(htjv1:T 
)=p(ht, 
ht+1jv1:T 
) 
(24.4.26) 


ht+1

ZZ

=p(htjv1:T 
, 
ht+1)p(ht+1jv1:T 
)=p(htjv1:t, 
ht+1)p(ht+1jv1:T 
) 
(24.4.27) 


ht+1 
ht+1 


The 
term 
p(htjv1:t, 
ht+1) 
can 
be 
found 
by 
conditioning 
the 
joint 
distribution 


p(ht, 
ht+1jv1:t)= 
p(ht+1jht;v1:t
)p(htjv1:t) 
(24.4.28) 


which 
is 
obtained 
in 
the 
usual 
manner 
by 
nding 
its 
mean 
and 
covariance: 
The 
term 
p(htjv1:t) 
is 
a 
known 
Gaussian 
from 
ltering 
with 
mean 
ft 
and 
covariance 
Ft. 
Hence 
the 
joint 
distribution 
p(ht, 
ht+1jv1:t) 
has 
means

hhtjv1:ti= 
ft;hht+1jv1:ti= 
Aft 
(24.4.29) 


and 
covariance 
elements

DEDEDE

hthT 
t 
jv1:t= 
Ft;hthT 
t+1jv1:t= 
FtAT 
;ht+1ht
T 
+1jv1:t= 
AFtAT 
+ 
H 


(24.4.30) 
To 
nd 
p(htjv1:t, 
ht+1) 
we 
may 
use 
the 
conditioned 
Gaussian 
results, 
denition(78). 
It 
is 
useful 
to 
use 
the 
system 
reversal 
result, 
section(8.6.1), 
which 
interprets 
p(htjv1:t, 
ht+1) 
as 
an 
equivalent 
linear 
system 
going 
backwards 
in 
time: 


 

..

....

= 
mt 
+ 
 (24.4.31)

ht 
Atht+1 
+ 
 . 
t 


where 


DEDE..1

 

..

At 
hthT 
t+1jv1:tht+1hT 
t+1jv1:t(24.4.32) 


DRAFT 
March 
9, 
2010 



Inference 


Algorithm 
21 
LDS 
Backward 
Pass. 
Compute 
the 
smoothed 
posteriors 
p(htjv1:T 
). 
This 
requires 
the 
ltered 
results 
from 
algorithm(20). 


GT 
. 
FT 
, 
gT 
. 
fT 
for 
t 
. 
T 
- 
1, 
1 
do 


fgt, 
Gt} 
= 
LDSBACKWARD(gt+1, 
Gt+1, 
ft, 
Ft; 
) 
end 
for 
function 
ldsbackward(g, 
G, 
f, 
F; 
) 


h 
. 
Af 
+ 
h
h0hl 
. 
AFAT 
+ 
h 
, 
h0h 
. 
AF 
I 
Statistics 
of 
p(ht, 
ht+1jv1:t)

 ....1 
 ..1 
  
....

..

S 
. 
F 
- 
T 
h0hh0hl 
h0h, 
A 
. 
T 
h0hh0hl 
, 
m 
. 
f 
- 
Ah 
I 
Dynamics 
Reversal 
p(htjht+1, 
v1:t)

 ..   
......

..

g' 
. 
Ag 
+ 
 G' 
. 
AGAT 
+ 
I 
Backward 
propagation 


m, 
S 
return 
g0, 
G' 
end 
function 


DEDE..1

 

..

mt 
hhtjv1:ti..htht
T 
+1jv1:tht+1ht
T 
+1jv1:thht+1jv1:t) 
(24.4.33) 




 

..

.. 

..

and 
 . 
t

. 
t 
N

0, 
t, 
with 


DEDEDE..1DE

 

..

t 
hthT 
t 
..t+1jv1:tt+1jv1:tjv1:t

jv1:ththT 
ht+1hT 
ht+1hT 
t 
(24.4.34) 


Using 
dynamics 
reversal, 
equation 
(24.4.31) 
and 
assuming 
that 
ht+1 
is 
Gaussian 
distributed, 
it 
is 
then 
straightforward 
to 
work 
out 
the 
statistics 
of 
p(htjv1:T 
). 
The 
mean 
is 
given 
by 


 .. 

..

....

gt 
hhtjv1:T 
) 
= 
At 
hht+1jv1:T 
) 
+ 
 Atgt+1 
+ 
 (24.4.35)

mt 
= 
mt 


and 
covariance 


DED

 ..E     
..........

AT

Gt 
htht 
Tjv1:T= 
Atht+1ht
T 
+1jv1:Tt 
+ 
t 
= 
AtGt+1 
At 
T 
+ 
t 
(24.4.36) 


This 
procedure 
is 
the 
Rauch-Tung-Striebel 
Kalman 
smoother[231]. 
This 
is 
called 
a 
`correction’ 
method 
since 
it 
takes 
the 
ltered 
estimate 
p(htjv1:t) 
and 
`corrects’ 
it 
to 
form 
a 
smoothed 
estimate 
p(htjv1:T 
). 
The 
procedure 
is 
outlined 
in 
algorithm(21) 
and 
is 
detailed 
in 
LDSbackwardUpdate.m. 
See 
also 
LDSsmooth.m. 


The 
cross 
moment 


An 
advantage 
of 
the 
dynamics 
reversal 
interpretation 
given 
above 
is 
that 
the 
cross 
moment 
(which 
is 
required 
for 
learning) 
is 
immediately 
obtained 
from 


DE

 .. 

..

T

hhtht+1jv1:T 
) 
= 
AtGt+1 
)hthT 
t+1jv1:T= 
AtGt+1 
+ 
gtg(24.4.37)

t+1 


24.4.3 
The 
likelihood 
We 
can 
compute 
the 
likelihood 
using 
the 
decomposition 


T

YD

p(v1:T 
)= 
p(vtjv1:t..1) 
(24.4.38) 
t=1 


in 
which 
each 
conditional 
p(vtjv1:t..1) 
is 
a 
Gaussian 
in 
vt. 
It 
is 
straightforward 
to 
show 
that 
the 
term 
p(vtjv1:t..1) 
has 
mean 
and 
covariance 


1 
= 
Bµ 
1 
= 
BBT 
+ 
V 
t 
=1 


..D(24.4.39)
t 
= 
BAft..1 
t 
= 
B 
AFt..1AT 
+ 
HBT 
+ 
V 
t> 
1 


The 
log 
likelihood 
is 
then 
given 
by 


XDhi

1 
T

log 
p(v1:T 
)= 
- 
(vt 
- 
t)T 
..1 
(vt 
- 
t) 
+ 
log 
det 
(2t)(24.4.40)

t

2 


t=1

DRAFT 
March 
9, 
2010 



Inference 


24.4.4 
Most 
likely 
state 
Since 
the 
mode 
of 
a 
Gaussian 
is 
equal 
to 
its 
mean, 
there 
is 
no 
dierence 
between 
the 
most 
probable 
joint 


posterior 
state 
argmax 
h1:T 
p(h1:T 
jv1:T 
) 
(24.4.41) 
and 
the 
set 
of 
most 
probable 
marginal 
states 
ht 
= 
argmax 
htT 
p(htjv1:T 
), 
t 
= 
1, 
. 
. 
. 
, 
T 
(24.4.42) 
Hence 
the 
most 
likely 
hidden 
state 
sequence 
is 
equivalent 
to 
the 
smoothed 
mean 
sequence. 


24.4.5 
Time 
independence 
and 
Riccati 
equations 
Both 
the 
ltered 
Ft 
and 
smoothed 
Gt 
covariance 
recursions 
are 
independent 
of 
the 
observations 
v1:T 
, 
depending 
only 
on 
the 
parameters 
of 
the 
model. 
This 
is 
a 
general 
characteristic 
of 
linear 
Gaussian 
systems. 
Typically 
the 
covariance 
recursions 
converge 
quickly 
to 
values 
that 
are 
reasonably 
constant 
throughout 
the 
dynamics, 
with 
only 
appreciable 
dierences 
at 
the 
boundaries 
t 
= 
1 
and 
t 
= 
T 
. 
In 
practice 
one 
often 
drops 
the 
time-dependence 
of 
the 
covariances 
and 
approximates 
them 
with 
a 
single 
time-independent 
covariance. 
This 
approximation 
dramatically 
reduces 
storage 
requirements. 
The 
converged 
ltered 
F 
satises 
the 
recursion 


..1 


F 
= 
AFAT 
+ 
H 
..AFAT 
+ 
HBTBAFAT 
+ 
HBT 
+ 
VBAFAT 
+ 
H(24.4.43) 


which 
can 
be 
related 
to 
a 
form 
of 
algebraic 
Riccati 
equation. 
A 
technique 
to 
solve 
these 
equations 
is 
to 
being 
with 
setting 
the 
covariance 
to 
. 
With 
this, 
a 
new 
F 
is 
found 
using 
the 
right 
hand 
side 
of 
(24.4.43), 
and 
subsequently 
recursively 
updated. 
Alternatively, 
using 
the 
Woodbury 
identity, 
the 
converged 
covariance 
satises 


..1

..1 


F 
=AFAT 
+ 
H+ 
BT..1B(24.4.44)

V 


although 
this 
form 
is 
less 
numerically 
convenient 
in 
forming 
an 
iterative 
solver 
for 
F 
since 
it 
requires 
two 
matrix 
inversions. 


Example 
102 
(Newtonian 
Trajectory 
Analysis). 
A 
toy 
rocket 
with 
unknown 
mass 
and 
initial 
velocity 
is 
launched 
in 
the 
air. 
In 
addition, 
the 
constant 
accelerations 
from 
the 
rocket's 
propulsion 
system 
are 
unknown. 
It 
is 
known 
is 
that 
Newton's 
laws 
apply 
and 
an 
instrument 
can 
measure 
the 
vertical 
height 
and 
horizontal 
distance 
of 
the 
rocket 
at 
each 
time 
x(t);y(t) 
from 
the 
origin. 
Based 
on 
noisy 
measurements 
of 
x(t) 
and 
y(t), 
our 
task 
is 
to 
infer 
the 
position 
of 
the 
rocket 
at 
each 
time. 


Although 
this 
is 
perhaps 
most 
appropriately 
considered 
from 
the 
using 
continuous 
time 
dynamics, 
we 
will 
translate 
this 
into 
a 
discrete 
time 
approximation. 
Newton's 
law 
states 
that 


d2 
fx(t) 
d2 
fy(t) 


x 
= 
;y 
= 
(24.4.45)

dt2 
m 
dt2 
m 


where 
m 
is 
the 
mass 
of 
the 
object 
and 
fx(t);fy(t) 
are 
the 
horizontal 
and 
vertical 
forces 
respectively. 
Hence 
As 
they 
stand, 
these 
equations 
are 
not 
in 
a 
form 
directly 
usable 
in 
the 
LDS 
framework. 
A 
naive 
approach 
is 
to 
reparameterise 
time 
to 
use 
the 
variable 
t~such 
that 
t 
= 
t~, 
where 
t~is 
integer 
and 
. 
is 
a 
unit 
of 
time. 
The 
dynamics 
is 
then 


x((t~+ 
1)) 
= 
x(t~) 
+ 
x0(t~) 
(24.4.46) 


y((t~+ 
1)) 
= 
y(t~) 
+ 
y0(t~) 
(24.4.47) 


DRAFT 
March 
9, 
2010 



Learning 
Linear 
Dynamical 
Systems 


-1000100200300400500600700800-150-100-50050100150200250300xy
Figure 
24.6: 
Estimate 
of 
the 
trajectory 
of 
a 
Newtonian 
ballistic 
object 
based 
on 
noisy 
observations 
(small 
circles). 
All 
time 
labels 
are 
known 
but 
omitted 
in 
the 
plot. 
The 
`x’ 
points 
are 
the 
true 
positions 
of 
the 
object, 
and 
the 
crosses 
`+’ 
are 
the 
estimated 
smoothed 
mean 
positions 
hxt;ytjv1:T 
) 
of 
the 
object 
plotted 
every 
several 
time 
steps. 
See 


demoLDStracking.m 


0(t) 
= 
dy

where 
y. 
We 
can 
write 
an 
update 
equation 
for 
the 
x/ 
and 
y/ 
as

dt 


x0((t~+ 
1)) 
= 
x0(t~) 
+ 
fx=m, 
y0((t~+ 
1)) 
= 
y0(t~) 
+ 
fy=m 
(24.4.48) 


These 
are 
discrete 
time 
dierence 
equations 
indexed 
by 
t~. 
The 
instrument 
which 
measures 
x(t) 
and 
y(t) 
is 
not 
completely 
accurate. 
For 
simplicity, 
we 
relabel 
ax(t)= 
fx(t)=m(t), 
ay(t)= 
fy(t)=m(t) 
– 
these 
accelerations 
will 
be 
assumed 
to 
be 
roughly 
constant, 
but 
unknown 
: 


ax((t~+ 
1)) 
= 
ax(t~) 
+ 
x;ay((t~+ 
1)) 
= 
ay(t~) 
+ 
y, 
(24.4.49) 


where 
x 
and 
y 
are 
small 
noise 
terms. 
The 
initial 
distributions 
for 
the 
accelerations 
are 
assumed 
vague, 
using 
a 
zero 
mean 
Gaussian 
with 
large 
variance. 


We 
describe 
the 
above 
model 
by 
considering 
x(t), 
x0(t), 
y(t), 
y0(t), 
ax(t), 
ay(t) 
as 
hidden 
variables, 
giving 
rise 
to 
a 
H 
= 
6 
dimensional 
LDS 
with 
transition 
and 
emission 
matrices 
as 
below: 


.


.


1 
0 
0 
0 
. 
0 
. 
1 
0 
0 
0 
0 
0 
0 
1 
0 
0 
. 
0 
0 
. 
1 
0 
0 
0 
0 
0 
0 
1 
0 
0 
0 
0 
0 
0 
1 


BBBBBB
. 


CCCCCC
. 






010000 


(24.4.50)
A 
= 


, 
B 
=


000100


We 
place 
a 
large 
variance 
on 
their 
initial 
values, 
and 
attempt 
to 
infer 
the 
unknown 
trajectory. 
A 
demonstration 
is 
given 
in 
g(24.6). 
Despite 
the 
signicant 
observation 
noise, 
the 
object 
trajectory 
can 
be 
accurately 
inferred. 


24.5 
Learning 
Linear 
Dynamical 
Systems 
Whilst 
in 
many 
applications, 
particularly 
of 
underlying 
known 
physical 
processes, 
the 
paremeters 
of 
the 
LDS 
are 
known, 
in 
many 
machine 
learning 
tasks 
we 
need 
to 
learn 
the 
parameters 
of 
the 
LDS 
based 
on 
v1:T 
. 
For 
simplicity 
we 
assume 
that 
we 
know 
the 
dimensionality 
H 
of 
the 
LDS. 


24.5.1 
Identiability 
issues 
An 
interesting 
question 
is 
whether 
we 
can 
uniquely 
identify 
(learn) 
the 
parameters 
of 
an 
LDS. 
There 
are 
always 
trivial 
redundancies 
in 
the 
solution 
obtained 
by 
permuting 
the 
hidden 
variables 
arbitrarily 
and 
ipping 
their 
signs. 
To 
show 
that 
there 
are 
potentially 
many 
more 
equivalent 
solutions, 
consider 
the 
following 
LDS 


vt 
= 
Bht 
+ 
v
t 
, 
ht 
= 
Aht..1 
+ 
h 
(24.5.1)

t 


DRAFT 
March 
9, 
2010 
449 



Learning 
Linear 
Dynamical 
Systems 


We 
now 
attempt 
to 
transform 
this 
original 
system 
to 
a 
new 
form 
which 
will 
produce 
exactly 
the 
same 
outputs 
v1:T 
. 
For 
an 
invertible 
matrix 
R 
we 
consider 


Rht 
= 
RAR..1Rht..1 
+ 
Rh 
(24.5.2)

t 


which 
is 
representable 
as 
a 
new 
latent 
dynamics 


^A^
^h

ht 
= 
ht..1 
+ 
^(24.5.3)

t 


where 
A^
RAR..1 
, 
h^
Rht, 
^
Rt
h 
. 
In 
addition, 
we 
can 
reexpress 
the 
outputs 
to 
be 
a 
function 
of 


t 


the 
transformed 
h: 


vt 
= 
BR..1Rht 
+ 
t
v 
= 
B^
h^
t
v 
(24.5.4) 


Hence, 
provided 
we 
place 
no 
constraints 
on 
A, 
B 
and 
H 
there 
exists 
an 
innite 
space 
of 
equivalent 
solutions, 
A^
RA 
R..1 
, 
B^
BR..1 
, 
^
RH 
RT, 
all 
with 
the 
same 
likelihood 
value. 
This 
means 
that 
directly 
interpreting 
the 
learned 
parameters 
needs 
to 
be 
done 
with 
some 
care. 
This 
redundancy 
can 
be 
mitigated 
by 
imposing 
constraints 
on 
the 
parameters. 


24.5.2 
EM 
algorithm 
For 
simplicity, 
we 
assume 
we 
have 
a 
single 
sequence 
v1:T 
, 
to 
which 
we 
wish 
to 
t 
a 
LDS 
using 
Maximum 
Likelihood. 
Since 
the 
LDS 
contains 
latent 
variables 
one 
approach 
is 
to 
use 
the 
EM 
algorithm. 
As 
usual, 
the 
M-step 
of 
the 
EM 
algorithm 
requires 
us 
to 
maximise 
the 
energy 


hlog 
p(v1:T 
, 
h1:T 
)) 
old(h1:T 
jv1:T 
) 
(24.5.5)

p

with 
respect 
to 
the 
parameters 
A, 
B, 
a, 
, 
V 
, 
H 
. 
Thanks 
to 
the 
form 
of 
the 
LDS 
the 
energy 
decomposes 
as 


TT

XX

hlog 
p(h1)iold(h1jv1:T 
) 
+ 
hlog 
p(htjht..1)iold(ht;ht..1jv1:T 
) 
+ 
hlog 
p(vtjht)iold(htjv1:T 
) 
(24.5.6)

ppp
t=2 
t=1 


It 
is 
straightforward 
to 
derive 
that 
the 
M-step 
for 
the 
parameters 
is 
given 
by 
(angled 
brackets 
h) 
denote 
expectation 
with 
respect 
to 
the 
smoothed 
posterior 
p(h1:T 
jv1:T 
)): 


DE

X

new 
TT

=
1 
vtv- 
vthhtiT 
BT 
- 
B 
hht) 
v+ 
BhthT 
BT(24.5.7)

Vt 
tt

T

t

DEDEDEDE

X

1

new 


= 
- 
AhthT 
..AT 
+ 
AhthT 
AT(24.5.8)

H 
ht+1ht
T 
+1t+1ht+1hT 
tt

T 
- 
1

t
new 


µ 
= 
hh1) 
(24.5.9)

DE

new 
T

=h1hT 
- 
µ 
(24.5.10)

1
 !..1


T 
..1DET 
..1DE

XX

Anew 


= 
ht+1hT 
ththT 
t(24.5.11) 


t=1t=1
T XT!..1


DE

XX

Bnew 
= 
vt 
hhtiT 
hthT 
(24.5.12)

t
t=1 
t=1


If 
B 
is 
updated 
according 
to 
the 
above, 
the 
rst 
equation 
can 
be 
simplied 
to 




X

new 
T

=
1 
vtv- 
vt 
hhtiT 
BT(24.5.13)

Vt

T

t

Similarly, 
if 
A 
is 
updated 
according 
to 
EM 
algorithm, 
then 
the 
second 
equation 
can 
be 
simplied 
to 


DEDE

X

1

new 


= 
ht+1hT 
- 
AhthT 
(24.5.14)

Ht+1t+1

T 
- 
1

t

DRAFT 
March 
9, 
2010 



Learning 
Linear 
Dynamical 
Systems 


The 
statistics 
required 
therefore 
include 
smoothed 
means, 
covariances, 
and 
cross 
moments. 
The 
extension 
to 
learning 
multiple 
timeseries 
is 
straightforward 
since 
the 
energy 
is 
simply 
summed 
over 
the 
individual 
sequences. 


The 
performance 
of 
the 
EM 
algorithm 
for 
the 
LDS 
often 
depends 
heavily 
on 
a 
the 
initialisation. 
If 
we 
remove 
the 
hidden 
to 
hidden 
links, 
the 
model 
is 
closely 
related 
to 
Factor 
Analysis 
(the 
LDS 
can 
be 
considered 
a 
temporal 
extension 
of 
Factor 
Analysis). 
One 
initialisation 
technique 
is 
therefore 
to 
learn 
the 
B 
matrix 
using 
Factor 
Analysis 
by 
treating 
the 
observations 
as 
temporally 
independent. 


24.5.3 
Subspace 
Methods 
An 
alternative 
to 
EM 
and 
Maximum 
Likelihood 
training 
of 
an 
LDS 
is 
to 
use 
a 
subspace 
method[281, 
249]. 
The 
chief 
benet 
of 
these 
techniques 
is 
that 
they 
avoid 
the 
convergence 
diculties 
of 
EM. 
To 
motivate 
subspace 
techniques, 
consider 
a 
deterministic 
LDS 


ht 
= 
Aht..1, 
vt 
= 
Bht 
(24.5.15) 


Under 
this 
assumption, 
vt 
= 
Bht 
= 
BAht..1 
and, 
more 
generally, 
vt 
= 
BAth1. 
This 
means 
that 
a 
low 
dimensional 
system 
underlies 
all 
visible 
information 
since 
all 
points 
Ath1 
lie 
in 
a 
H-dimensional 
subspace, 
which 
is 
then 
projected 
to 
form 
the 
observation. 
This 
suggests 
that 
some 
form 
of 
subspace 
identication 
technique 
will 
enable 
us 
to 
learn 
A 
and 
B. 


Given 
a 
set 
of 
observation 
vectors 
v1;:::, 
vt, 
consider 
the 
block 
Hankel 
matrix 
formed 
from 
stacking 
the 
vectors. 
For 
an 
order 
L 
matrix, 
this 
is 
a 
VL 
× 
T 
- 
L 
+ 
1 
matrix. 
For 
example, 
for 
T 
= 
6 
and 
L 
= 
3, 
this 
is 


0. 


v1 
v2 
v3 
v4 
M 
= 
. 
v2 
v3 
v4 
v5 
. 
(24.5.16) 


v3 
v4 
v5 
v6 


If 
the 
v 
are 
generated 
from 
a 
(noise 
free) 
LDS, 
we 
can 
write 


M 
= 
. 
. 
Bh1 
BAh1 
BA2h1 
Bh2 
BAh2 
BA2h2 
Bh3 
BAh3 
BA2h3 
Bh4 
BAh4 
BA2h4 
. 
. 
= 
. 
. 
B 
BA 
BA2 
. 
. 
(h1 
h2 
h3 
h4) 
(24.5.17) 
We 
now 
nd 
the 
SVD 
of 
M, 
M 
= 
^U 
ˆ 
^V
TS 
S|
{v 
T 
(24.5.18) 


W 


where 
W 
is 
termed 
the 
extended 
observability 
matrix. 
The 
matrix 
S^
will 
contain 
the 
singular 
values 
up 
to 
the 
dimension 
of 
the 
hidden 
variables 
H, 
with 
the 
remaining 
singular 
values 
0. 
From 
equation 
(24.5.17), 
this 
means 
that 
the 
emission 
matrix 
B 
is 
contained 
in 
U^
1:V;1:H 
. 
The 
estimated 
hidden 
variables 
are 
then 
contained 
in 
the 
submatrix 
W1:H;1:T 
..L+1, 


(h1 
h2 
h3 
h4)= 
W1:H;1:T 
..L+1 
(24.5.19) 


Based 
on 
the 
relation 
ht 
= 
Aht..1 
one 
can 
then 
nd 
the 
best 
least 
squares 
estimate 
for 
A 
by 
minimising 
T

T 


(ht 
- 
Aht..1)2 
(24.5.20) 
t=2 


for 
which 
the 
optimal 
solution 
is 


A 
=(h2 
h3 
::. 
ht)(h1 
h2 
::. 
hT 
..1)† 
(24.5.21) 


where 
† 
denotes 
the 
pseudo 
inverse, 
see 
LDSsubspace.m. 
Estimates 
for 
the 
covariance 
matrices 
can 
also 
be 
obtained 
from 
the 
residual 
errors 
in 
tting 
the 
block 
Hankel 
matrix 
(V 
) 
and 
extended 
observability 
matrix 
(H 
). 
Whilst 
this 
derivation 
formally 
holds 
only 
for 
the 
noise 
free 
case 
one 
can 
nevertheless 
apply 
this 
in 
the 
case 
of 
non-zero 
noise 
and 
hope 
to 
gain 
an 
estimate 
for 
A 
and 
B 
that 
is 
correct 
in 
the 
mean. 
In 
addition 
to 
forming 
a 
solution 
in 
its 
own 
right, 
the 
subspace 
method 
forms 
a 
potentially 
useful 
way 
to 
initialise 
the 
EM 
algorithm. 


DRAFT 
March 
9, 
2010 



Switching 
Auto-Regressive 
Models 


v1v2v3v4
s1s2s3s4
Figure 
24.7: 
A 
rst 
order 
switching 
AR 
model. 
In 
terms 
of 
inference, 
conditioned 
on 
v1:T 
, 
this 
is 
a 
HMM. 


24.5.4 
Structured 
LDSs 
Many 
physical 
equations 
are 
local 
both 
in 
time 
and 
space. 
For 
example 
in 
weather 
models 
the 
atmosphere 
is 
partitioned 
into 
cells 
hi(t) 
each 
containing 
the 
pressure 
at 
that 
location. 
The 
equations 
describing 
how 
the 
pressure 
updates 
only 
depend 
on 
the 
pressure 
at 
the 
current 
cell 
and 
small 
number 
of 
neighbouring 
cells 
at 
the 
previous 
time 
t 
- 
1. 
If 
we 
use 
a 
linear 
model 
and 
measure 
some 
aspects 
of 
the 
cells 
at 
each 
time, 
then 
the 
weather 
is 
describable 
by 
a 
LDS 
with 
a 
highly 
structured 
sparse 
transition 
matrix 
A. 
In 
practice, 
the 
weather 
models 
are 
non-linear 
but 
local 
linear 
approximations 
are 
often 
employed[254]. 
A 
similar 
situation 
arises 
in 
brain 
imaging 
in 
which 
voxels 
(local 
cubes 
of 
activity) 
depend 
only 
on 
their 
neighbours 
from 
the 
previous 
timestep[101]. 


Another 
application 
of 
structured 
LDSs 
is 
in 
temporal 
independent 
component 
analysis. 
This 
is 
dened 
as 
the 
discovery 
of 
a 
set 
of 
independent 
latent 
dynamical 
processes, 
from 
which 
the 
data 
is 
a 
projected 
observation. 
If 
each 
independent 
dynamical 
process 
can 
itself 
be 
described 
by 
a 
LDS, 
this 
gives 
rise 
to 
a 
structured 
LDS 
with 
a 
block 
diagonal 
transition 
matrix 
A. 
Such 
models 
can 
be 
used 
to 
extract 
independent 
components 
under 
prior 
knowledge 
of 
the 
likely 
underlying 
frequencies 
in 
each 
of 
the 
temporal 
compoments[59]. 
See 
also 
exercise(199). 


24.5.5 
Bayesian 
LDSs 
The 
extension 
to 
placing 
priors 
on 
the 
transition 
and 
emission 
parameters 
of 
the 
LDS 
leads 
in 
general 
to 
computational 
diculties 
in 
computing 
the 
likelihood. 
For 
example, 
for 
a 
prior 
on 
A, 
the 
likelihood 
is 


R

p(v1:T 
)=p(v1:T 
jA)p(A) 
which 
is 
dicult 
to 
evaluate 
since 
the 
dependence 
of 
the 
likelihood 
on 
the 


A 


matrix 
A 
is 
a 
complicated 
function. 
Approximate 
treatments 
of 
this 
case 
are 
beyond 
the 
scope 
of 
this 
book, 
although 
we 
briey 
note 
that 
sampling 
methods[54, 
98] 
are 
popular 
in 
this 
context, 
in 
addition 
to 
deterministic 
variational 
approximations[27, 
23, 
59]. 


24.6 
Switching 
Auto-Regressive 
Models 
For 
a 
time-series 
of 
scalar 
values 
v1:T 
an 
Lth 
order 
switching 
AR 
model 
can 
be 
written 
as 


..



T 


vt 
= 
v^t..1a(st)+ 
t;t 
Nt 


0;2(st)(24.6.1) 


	

where 
we 
now 
have 
a 
set 
of 
AR 
coecients 
. 
=a(s);2(s);s 
2f1;:::;Sg. 
The 
discrete 
switch 
variables 


QY

themselves 
have 
a 
Markov 
transition 
p(s1:T 
)= 
p(stjst..1) 
so 
that 
the 
full 
model 
is 


t 


Y

p(v1:T 
;s1:T 
j)=p(vtjvt..1;:::;vt..L;st, 
j)p(stjst..1) 
(24.6.2) 
t 


24.6.1 
Inference 
Given 
an 
observed 
sequence 
v1:T 
and 
parameters 
. 
inference 
is 
straightforward 
since 
this 
is 
a 
form 
of 
HMM. 
To 
make 
this 
more 
apparent 
we 
may 
write 


Y

p(v1:T 
;s1:T 
)=p^(vtjst)p(stjst..1) 
(24.6.3) 
t 


where 


T 


p^(vtjst) 
= 
p(vtjvt..1;:::;vt..L;st)= 
N 
vt 


v^t..1a(st);2(st) 
(24.6.4) 


DRAFT 
March 
9, 
2010 



Switching 
Auto-Regressive 
Models 


Figure 
24.8: 
Learning 
a 
Switching 
AR 


050100150200250300350400-100-50050100sample switches050100150200250300350400-100-50050100learned switches
model. 
The 
upper 
plot 
shows 
the 
training 
data. 
The 
colour 
indicates 
which 
of 
the 
two 
AR 
models 
is 
active 
at 
that 
time. 
Whilst 
this 
information 
is 
plotted 
here, 
this 
is 
assumed 
unknown 
to 
the 
learning 
algorithm, 
as 
are 
the 
coecients 
a(s). 
We 
assume 
that 
the 
order 
L 
= 
2 
and 
number 
of 
switches 
S 
= 
2 
however 
is 
known. 
In 
the 
bottom 
plot 
we 
show 
the 
time 
series 
again 
after 
training 
in 
which 
we 
colour 
the 
points 
according 
to 
the 
most 
likely 


smoothed 
AR 
model 
at 
each 
timestep. 
See 
demoSARlearn.m. 


Note 
that 
the 
emission 
distribution 
^p(vtjst) 
is 
time-dependent. 
The 
ltering 
recursion 
is 
then 


X

(st)=p^(vtjst)p(stjst..1)(st..1) 
(24.6.5) 


st..1 


Smoothing 
can 
be 
achieved 
using 
the 
standard 
recursions, 
modied 
to 
use 
the 
time-dependent 
emissions, 
see 
demoSARinference.m. 


T 


24.6.2 
Maximum 
Likelihood 
Learning 
using 
EM 
To 
t 
the 
set 
of 
AR 
coecients 
and 
innovation 
variances, 
a(s);2(s);s 
=1;:::;S, 
using 
Maximum 
Likelihood 
training 
for 
a 
set 
of 
data 
v1:T 
, 
we 
may 
make 
use 
of 
the 
EM 
algorithm. 


M-step 


Up 
to 
negligible 
constants, 
the 
energy 
is 
given 
by 


XX

E 
=hlog 
p(vtjv^t..1, 
a(st))iold(stjv1:T 
) 
+hlog 
p(stjst..1)iold(st;st..1) 
(24.6.6)

pp
tt 


which 
we 
need 
to 
maximise 
with 
respect 
to 
the 
parameters 
. 
Using 
the 
denition 
of 
the 
emission 
and 
isolating 
the 
dependency 
on 
a, 
we 
have 




X2

1 


t..1a(st)+ 
log 
2(st)

..2E 
=


vt 
- 
v^

+ 
const. 
(24.6.7)
2(st)

old(stjv1:T 
)

p

t

T 


On 
dierentiating 
with 
respect 
to 
a(s) 
and 
equating 
to 
zero, 
the 
optimal 
a(s) 
satises 
the 
linear 
equation

"#

v^t..1v^

t..1 


XX

vtv^t..1

old(st 


old(st 


= 
sjv1:T 
)

= 
sjv1:T 
) 


a(s) 
(24.6.8)


=


p 


p


2(s) 


2(s)

t 


t 


T 


which 
may 
be 
solved 
using 
Gaussian 
elimination. 
Similarly 
one 
may 
show 
that 
updates 
that 
maximise 
the 
energy 
with 
respect 
to 
2 
are 


h

X2 
t..1a(st)

f


1
old(s0

2(s)= 


old(st 


= 
sjv1:T 
)vt 
- 
v^

(24.6.9) 


p


= 
sjv1:T 
)

t' 
p

t 


t 


The 
update 
for 
p(stjst..1) 
follows 
the 
standard 
EM 
for 
HMM 
rule, 
equation 
(23.3.5), 
see 
SARlearn.m. 
Here 
we 
don't 
include 
an 
update 
for 
the 
prior 
p(s1) 
since 
there 
is 
insucient 
information 
at 
the 
start 
of 
the 
sequence 
and 
assume 
p(s1) 
is 
at. 
With 
high 
frequency 
data 
it 
is 
unlikely 
that 
a 
change 
in 
the 
switch 
variable 
is 
reasonable 
at 
each 
time 
t. 
A 
simple 
constraint 
to 
account 
for 
this 
is 
to 
use 
a 
modied 
transition 




p(stjst..1) 
mod 
(t, 
Tskip)=0 


p^(stjst..1) 
=(24.6.10)

d 
(st 
- 
st..1) 
otherwise 


DRAFT 
March 
9, 
2010 



Code 


~v1~v2~v3~v4
v1v2v3v4
s1s2s3s4
1234567891012345678910
Figure 
24.9: 
(a): 
A 
latent 
switching 
(second 
order) 
AR 
model. 
Here 
the 
st 
indicates 
which 
of 
a 
set 
of 
10 
available 
AR 
models 
is 
active 
at 
time 
t. 
The 
square 
nodes 
emphasise 
that 
these 
are 
discrete 
variables. 
The 
`clean’ 
AR 
signal 
vt, 
which 
is 
not 
observed, 
is 
corrupted 
by 
additive 
noise 
to 
form 
the 
noisy 
observations 
v~t. 
In 
terms 
of 
inference, 
conditioned 
on 
~v1:T 
, 
this 
can 
be 
expressed 
as 
a 
Switching 
LDS, 
chapter(25). 
(b): 
Signal 
reconstruction 
using 
the 
latent 
switching 
AR 
model 
in 
(a). 
Top: 
noisy 
signal 
~v1:T 
; 
bottom: 
reconstructed 
clean 
signal 
v1:T 
. 
The 
dashed 
lines 
and 
the 
numbers 
show 
the 
most-likely 
state 
segmentation 
arg 
maxs1:T 
p(s1:T 
jv~1:T 
). 


E-step 


The 
M-step 
requires 
the 
smoothed 
statistics 
pold(st 
= 
sjv1:T 
) 
and 
pold(st 
= 
s;st..1 
= 
s0jv1:T 
) 
which 
can 
be 
obtained 
from 
HMM 
inference. 


Example 
103 
(Learning 
a 
switching 
AR 
model). 
In 
g(24.8) 
the 
training 
data 
is 
generated 
by 
an 
Switching 
AR 
model 
so 
that 
we 
know 
the 
ground 
truth 
as 
to 
which 
model 
generated 
which 
parts 
of 
the 
data. 
Based 
on 
the 
training 
data 
(assuming 
the 
labels 
st 
are 
unknown), 
a 
Switching 
AR 
model 
is 
tted 
using 
EM. 
In 
this 
case 
the 
problem 
is 
straightforward 
so 
that 
a 
good 
estimate 
is 
obtained 
of 
both 
the 
sets 
of 
AR 
parameters 
and 
which 
switches 
were 
used 
at 
which 
time. 


Example 
104 
(Modelling 
parts 
of 
speech). 
In 
g(24.9) 
a 
segment 
of 
a 
speech 
signal 
is 
shown 
described 
by 
a 
Switching 
AR 
model. 
Each 
of 
the 
10 
available 
AR 
models 
is 
responsible 
for 
modelling 
the 
dynamics 
of 
a 
basic 
subunit 
of 
speech[90][192]. 
The 
model 
was 
trained 
on 
many 
example 
sequences 
using 
S 
= 
10 
states 
with 
a 
left-to-right 
transition 
matrix. 
The 
interest 
is 
to 
determine 
when 
each 
subunit 
is 
most 
likely 
to 
be 
active. 
This 
corresponds 
to 
the 
computation 
of 
the 
most-likely 
switch 
path 
s1:T 
given 
the 
observed 
signal 
p(s1:T 
jv~1:T 
). 


24.7 
Code 
In 
the 
Linear 
Dynamical 
System 
code 
below 
only 
the 
simplest 
form 
of 
the 
recursions 
is 
given. 
No 
attempt 
has 
been 
made 
to 
ensure 
numerical 
stability. 
LDSforwardUpdate.m: 
LDS 
forward 
LDSbackwardUpdate.m: 
LDS 
backward 
LDSsmooth.m: 
Linear 
Dynamical 
System 
: 
ltering 
and 
smoothing 
LDSforward.m: 
Alternative 
LDS 
forward 
algorithm 
(see 
SLDS 
chapter) 
LDSbackward.m: 
Alternative 
LDS 
backward 
algorithm 
(see 
SLDS 
chapter) 
demoSumprodGaussCanonLDS.m: 
Sum-product 
algorithm 
for 
smoothed 
inference 


demoLDStracking.m: 
Demo 
of 
tracking 
in 
a 
Newtonian 
system 


454 
DRAFT 
March 
9, 
2010 



Exercises 


LDSsubspace.m: 
Subspace 
Learning 
(Hankel 
matrix 
method) 
demoLDSsubspace.m: 
Demo 
of 
Subspace 
Learning 
method 


24.7.1 
Autoregressive 
models 
Note 
that 
in 
the 
code 
the 
autoregressive 
vector 
a 
has 
as 
its 
last 
entry 
the 
rst 
AR 
coecient 
(i.e. 
in 
reverse 
order 
to 
that 
presented 
in 
the 
text). 
ARtrain.m: 
Learn 
AR 
coecients 
(Gaussian 
Elimination) 
demoARtrain.m: 
Demo 
of 
tting 
an 
AR 
model 
to 
data 
ARlds.m: 
Learn 
AR 
coecients 
using 
a 
LDS 
demoARlds.m: 
Demo 
of 
learning 
AR 
coecients 
using 
an 
LDS 
demoSARinference.m: 
Demo 
for 
inference 
in 
a 
Switching 
Autoregressive 
Model 


In 
SARlearn.m 
a 
slight 
fudge 
is 
used 
since 
we 
do 
not 
deal 
fully 
with 
the 
case 
at 
the 
start 
where 
there 
is 
insucient 
information 
to 
dene 
the 
AR 
model. 
For 
long 
timeseries 
this 
will 
have 
a 
negligible 
eect, 
although 
it 
might 
lead 
to 
small 
decreases 
in 
the 
log 
likelihood 
under 
the 
EM 
algorithm. 
SARlearn.m: 
Learning 
of 
a 
SAR 
using 
EM 
demoSARlearn.m: 
Demo 
of 
SAR 
learning 
HMMforwardSAR.m: 
Switching 
Autoregressive 
HMM 
forward 
pass 
HMMbackwardSAR.m: 
Switching 
Autoregressive 
HMM 
backward 
pass 


24.8 
Exercises 
Exercise 
234. 
Consider 
the 
two-dimension 
linear 
model 


ht 
= 
Rht..1 
(24.8.1) 
where 
R. 
=
cos 
. 
sin 
. 
- 
sin 
. 
cos 

(24.8.2) 


R. 
is 
rotation 
matrix 
which 
rotates 
the 
vector 
ht 
through 
angle 
. 
in 
one 
timestep. 


1. 
By 
writing


xt 
R11 
R12 
xt..1

=(24.8.3)

ytR21 
R22yt..1

eliminate 
yt 
to 
write 
an 
equation 
for 
xt+1 
in 
terms 
of 
xt 
and 
xt..1. 


2. 
Explain 
why 
the 
eigenvalues 
of 
a 
rotation 
matrix 
are 
(in 
general) 
imaginary. 
3. 
Explain 
how 
to 
model 
a 
sinusoid, 
rotating 
with 
angular 
velocity 
. 
using 
a 
two-dimensional 
LDS. 
4. 
Explain 
how 
to 
model 
a 
sinusoid 
using 
an 
AR 
model. 
5. 
Explain 
the 
relationship 
between 
the 
second 
order 
dierential 
equation 
x= 
..x, 
which 
describes 
a 
Harmonic 
Oscillator, 
and 
the 
second 
order 
dierence 
equation 
which 
approximates 
this 
dierent
ial 
equation. 
Is 
it 
possible 
to 
nd 
a 
dierence 
equation 
which 
exactly 
matches 
the 
solution 
of 
the 
dierential 
equation 
at 
chosen 
points? 
Exercise 
235. 
Show 
that 
for 
any 
anti-symmetric 
matrix 
M, 
M 
= 
..MT 
(24.8.4) 
the 
matrix 
exponential 
(in 
MATLAB 
this 
is 
expm) 
A 
= 
e 
M 
(24.8.5) 


DRAFT 
March 
9, 
2010 



Exercises 


is 
orthogonal, 
namely 


ATA 
= 
I 
(24.8.6) 


Explain 
how 
one 
may 
then 
construct 
random 
orthogonal 
matrices 
with 
some 
control 
over 
the 
angles 
of 
the 
complex 
eigenvalues. 
Discuss 
how 
this 
relates 
to 
the 
frequencies 
encountered 
in 
a 
LDS 
where 
A 
is 
the 
transition 
matrix. 


Exercise 
236. 
Run 
the 
demo 
demoLDStracking.m 
which 
tracks 
a 
ballistic 
object 
using 
a 
Linear 
Dynamical 
system, 
see 
example(102). 
Modify 
demoLDStracking.m 
so 
that 
in 
addition 
to 
the 
x 
and 
y 
positions, 
the 
x 
speed 
is 
also 
observed. 
Compare 
and 
contrast 
the 
accuracy 
of 
the 
tracking 
with 
and 
without 
this 
extra 
information. 


Exercise 
237. 
nightsong.mat 
contains 
a 
small 
stereo 
segment 
nightingale 
song 
sampled 
at 
44100 
Hertz. 


1. 
Plot 
the 
original 
waveform 
using 
plot(x(:,1)) 
2. 
Download 
the 
program 
myspecgram.m 
from 
labrosa.ee.columbia.edu/matlab/sgram/myspecgram.m 
and 
plot 
the 
spectrogram 
y=myspecgram(x,1024,44100); 
imagesc(log(abs(y))) 


3. 
The 
routine 
demoGMMem.m 
demonstrates 
tting 
a 
mixture 
of 
Gaussians 
to 
data. 
The 
mixture 
assignm
ent 
probabilities 
are 
contained 
in 
phgn. 
Write 
a 
routine 
to 
cluster 
the 
data 
v=log(abs(y)) 
using 
8 
Gaussian 
components, 
and 
explain 
how 
one 
might 
segment 
the 
series 
x 
into 
dierent 
regions. 
4. 
Examine 
the 
routine 
demoARlds.m 
which 
ts 
autoregressive 
coecients 
using 
an 
interpretation 
as 
a 
Linear 
Dynamical 
System. 
Adapt 
the 
routine 
demoARlds.m 
to 
learn 
the 
AR 
coecients 
of 
the 
data 
x. 
You 
will 
almost 
certainly 
need 
to 
subsample 
the 
data 
x 
– 
for 
example 
by 
taking 
every 
4th 
datapoint. 
With 
the 
learned 
AR 
coecients 
(use 
the 
smoothed 
results) 
t 
a 
Gaussian 
mixture 
with 
8 
components. 
Compare 
and 
contrast 
your 
results 
with 
those 
obtained 
from 
the 
Gaussian 
mixture 
model 
t 
to 
the 
spectrogram. 
Exercise 
238. 
Consider 
a 
supervised 
learning 
problem 
in 
which 
we 
make 
a 
linear 
model 
of 
the 
scaler 
output 
yt 
based 
on 
vector 
input 
xt: 


T 


yt 
= 
wxt 
+ 
y 
(24.8.7)

tt 


where 
y 
is 
zero 
mean 
Gaussian 
noise. 
Training 
data 
D 
= 
f(xt;yt);t 
=1;:::;T 
} 
is 
available. 


t 


1. 
For 
a 
time-invariant 
weight 
vector 
wt 
= 
w, 
explain 
how 
to 
nd 
the 
single 
weight 
vector 
w 
and 
the 
noise 
variance 
2 
by 
Maximum 
Likelihood. 
2. 
Extend 
the 
above 
model 
to 
include 
a 
transition 
wt 
= 
wt..1 
+ 
w 
(24.8.8)

t 


where 
w
t 
is 
zero 
mean 
Gaussian 
noise 
with 
a 
given 
covariance 
; 
w1 
has 
zero 
mean. 
Explain 
how 
to 
cast 
nding 
hwtjD) 
as 
smoothing 
in 
a 
related 
Linear 
Dynamical 
System. 
Write 
a 
routine 
W 
= 
LinPredAR(X,Y,SigmaW,SigmaY) 
that 
takes 
an 
input 
data 
matrix 
X 
=[x1;:::, 
xT 
] 
where 
each 
column 
contains 
an 
input, 
and 
vector 
Y 
=[y1;:::;yT 
]T; 
SigmaW 
is 
the 
additive 
weight 
noise 
and 
SigmaY 
is 
an 
assumed 
known 
time-invariant 
output 
noise. 
The 
returned 
W 
contains 
the 
smoothed 
mean 
weights. 


DRAFT 
March 
9, 
2010 



CHAPTER 
25 


Switching 
Linear 
Dynamical 
Systems 


25.1 
Introduction 
Complex 
timeseries 
which 
are 
not 
well 
described 
globally 
by 
a 
single 
Linear 
Dynamical 
System 
may 
be 
divided 
into 
segments, 
each 
modelled 
by 
a 
potentially 
dierent 
LDS. 
Such 
models 
can 
handle 
situations 
in 
which 
the 
underlying 
model 
`jumps’ 
from 
one 
parameter 
setting 
to 
another. 
For 
example 
a 
single 
LDS 
might 
well 
represent 
the 
normal 
ows 
in 
a 
chemical 
plant. 
When 
a 
break 
in 
a 
pipeline 
occurs, 
the 
dynamics 
of 
the 
system 
changes 
from 
one 
set 
of 
linear 
ow 
equations 
to 
another. 
This 
scenario 
can 
be 
modelled 
suing 
a 
sets 
of 
two 
linear 
systems, 
each 
with 
dierent 
parameters. 
The 
discrete 
latent 
variable 
at 
each 
time 
st 
2fnormal, 
pipe 
broken} 
indicates 
which 
of 
the 
LDSs 
is 
most 
appropriate 
at 
the 
current 
time. 
This 
is 
called 
a 
Switching 
LDS 
and 
used 
in 
many 
disciplines, 
from 
econometrics 
to 
machine 
learning 
[13, 
110, 
174, 
161, 
160, 
60, 
57, 
218, 
303, 
175]. 


25.2 
The 
Switching 
LDS 
At 
each 
time 
t, 
a 
switch 
variable 
st 
. 
1;:::;S 
describes 
which 
of 
a 
set 
of 
LDSs 
is 
to 
be 
used. 
The 
observation 
(or 
`visible') 
variable 
vt 
2RV 
is 
linearly 
related 
to 
the 
hidden 
state 
ht 
2RH 
by 


vt 
= 
B(st)ht 
+ 
v(st), 
v(st) 
N 
(v(st) 


v(st), 
v(st)) 
(25.2.1) 


Here 
st 
describes 
which 
of 
the 
set 
of 
emission 
matrices 
B(1);:::, 
B(S) 
is 
active 
at 
time 
t. 
The 
observation 
noise 
v(st) 
is 
drawn 
from 
a 
Gaussian 
with 
mean 
v(st) 
and 
covariance 
v(st). 
The 
transition 
dynamics 
of 
the 
continuous 
hidden 
state 
ht 
is 
linear, 






ht 
= 
A(st)ht..1 
+ 
h(st), 
h(st) 
Nh(st) 


h(st), 
h(st)(25.2.2) 


and 
the 
switch 
variable 
st 
selects 
a 
single 
transition 
matrix 
from 
the 
available 
set 
A(1);:::, 
A(S). 
The 
Gaussian 
transition 
noise 
h(st) 
also 
depends 
on 
the 
switch 
variable. 
The 
dynamics 
of 
st 
itself 
is 
Markovian, 
with 
transition 
p(stjst..1). 
For 
the 
more 
general 
`augmented’ 
aSLDS 
model 
the 
switch 
st 
is 
dependent 
on 
both 
the 
previous 
st..1 
and 
ht..1. 
The 
model 
denes 
a 
joint 
distribution 
(see 
g(25.1)) 


T

Y

p(v1:T 
, 
h1:T 
;s1:T 
)= 
p(vtjht;st)p(htjht..1;st)p(stjht..1;st..1) 
t=1 


with 






p(vtjht;st)= 
N 
(vt 


v(st)+ 
B(st)ht, 
v(st)) 
;p(htjht..1;st)= 
Nht 


h(st)+ 
A(st)ht, 
h(st)

(25.2.3) 
457 



Gaussian 
Sum 
Filtering 


s1
h1
v1
s2
h2
v2
s3
h3
v3
s4
h4
v4
Figure 
25.1: 
The 
independence 
structure 
of 
the 
aSLDS. 
Square 
nodes 
st 
denote 
discrete 
switch 
variables; 
ht 
are 
continuous 
latent/hidden 
variables, 
and 
vt 
continuous 
observed/
visible 
variables. 
The 
discrete 
state 
st 
determines 
which 
Linear 
Dynamical 
system 
from 
a 
nite 
set 
of 
Linear 
Dynamical 
systems 
is 
operational 
at 
time 
t. 
In 
the 
SLDS 
links 
from 
h 
to 
s 
are 
not 
normally 
considered. 


At 
time 
t 
= 
1, 
p(s1jh0;s0) 
denotes 
the 
prior 
p(s1), 
and 
p(h1jh0;s1) 
denotes 
p(h1js1). 


The 
SLDS 
can 
be 
thought 
of 
as 
a 
marriage 
between 
a 
hidden 
Markov 
model 
and 
a 
Linear 
Dynamical 
System. 
The 
SLDS 
is 
also 
called 
a 
Jump 
Markov 
model/process, 
switching 
Kalman 
Filter, 
Switching 
Linear 
Gaussian 
State 
Space 
model, 
Conditional 
Linear 
Gaussian 
Model. 


25.2.1 
Exact 
inference 
is 
computationally 
intractable 
Both 
exact 
ltered 
and 
smoothed 
inference 
in 
the 
SLDS 
is 
intractable, 
scaling 
exponentially 
with 
time. 
As 
an 
informal 
explanation, 
consider 
ltered 
posterior 
inference, 
for 
which, 
by 
analogy 
with 
equation 
(23.2.9), 
the 
forward 
pass 
is 


Z

X

p(st+1, 
ht+1jv1:t+1)=p(st+1, 
ht+1jst, 
ht, 
vt+1)p(st, 
htjv1:t) 
(25.2.4) 


ht

st

At 
timestep 
1, 
p(s1, 
h1jv1)= 
p(h1js1, 
v1)p(s1jv1) 
is 
an 
indexed 
set 
of 
Gaussians. 
At 
timestep 
2, 
due 
to 
the 
summation 
over 
the 
states 
s1, 
p(s2, 
h2jv1:2) 
will 
be 
an 
indexed 
set 
of 
S 
Gaussians; 
similarly 
at 
timestep 
3, 
it 
will 
be 
S2 
and, 
in 
general, 
gives 
rise 
to 
St..1 
Gaussians 
at 
time 
t. 
Even 
for 
small 
t, 
the 
number 
of 
components 
required 
to 
exactly 
represent 
the 
ltered 
distribution 
is 
therefore 
computationally 
intractable. 
Analogously, 
smoothing 
is 
also 
intractable. 
The 
origin 
of 
the 
intractability 
of 
the 
SLDS 
diers 
from 
`structural 
intractability’ 
that 
we've 
previously 
encountered. 
In 
the 
SLDS, 
in 
terms 
of 
the 
cluster 
variables 
x1:T 
, 
with 
xt 
= 
(st, 
ht) 
and 
visible 
variables 
v1:T 
, 
the 
graph 
of 
the 
distribution 
is 
singly-connected. 
From 
a 
purely 
graph 
theoretic 
viewpoint, 
one 
would 
therefore 
envisage 
little 
diculty 
in 
carrying 
out 
inference. 
Indeed, 
as 
we 
saw 
above, 
the 
derivation 
of 
the 
ltering 
algorithm 
is 
straightforward 
since 
the 
graph 
is 
singly-connected. 
However, 
the 
numerical 
implementation 
of 
the 
algorithm 
is 
intractable 
since 
the 
description 
of 
the 
messages 
requires 
an 
exponentially 
increasing 
number 
of 
terms. 


In 
order 
to 
deal 
with 
this 
intractability, 
several 
approximation 
schemes 
have 
been 
introduced 
[98, 
110, 
174, 
161, 
160]. 
Here 
we 
focus 
on 
techniques 
which 
approximate 
the 
switch 
conditional 
posteriors 
using 
a 
limited 
mixture 
of 
Gaussians. 
Since 
the 
exact 
posterior 
distributions 
are 
mixtures 
of 
Gaussians, 
albeit 
with 
an 
exponentially 
large 
number 
of 
components, 
the 
aim 
is 
to 
drop 
low 
weight 
components 
such 
that 
the 
resulting 
approximation 
accurately 
represents 
the 
posterior. 


25.3 
Gaussian 
Sum 
Filtering 
Equation(25.2.4) 
describes 
the 
exact 
ltering 
recursion 
with 
an 
exponentially 
increasing 
number 
of 
components 
with 
time. 
In 
general, 
the 
inuence 
of 
ancient 
observations 
will 
be 
much 
less 
relevant 
than 
that 
of 
recent 
observations. 
This 
suggests 
that 
the 
`eective 
time’ 
is 
limited 
and 
that 
therefore 
a 
corresponding 
limited 
number 
of 
components 
in 
the 
Gaussian 
mixture 
should 
suce 
to 
accurately 
represent 
the 
ltered 
posterior[6]. 


Our 
aim 
is 
to 
form 
a 
recursion 
for 
p(st, 
htjv1:t) 
based 
on 
a 
Gaussian 
mixture 
approximation 
of 
p(htjst, 
v1:t). 
Given 
an 
approximation 
of 
the 
ltered 
distribution 
p(st, 
htjv1:t) 
˜ 
q(st, 
htjv1:t), 
the 
exact 
recursion 
equation 
(25.2.4) 
is 
approximated 
by 


Z

X

q(st+1, 
ht+1jv1:t+1)=p(st+1, 
ht+1jst, 
ht, 
vt+1)q(st, 
htjv1:t) 
(25.3.1) 


ht

st

DRAFT 
March 
9, 
2010 



Gaussian 
Sum 
Filtering 


This 
approximation 
to 
the 
ltered 
posterior 
at 
the 
next 
timestep 
will 
contain 
S 
times 
more 
components 
than 
in 
the 
previous 
timestep 
and, 
to 
prevent 
an 
exponential 
explosion 
in 
mixture 
components, 
we 
will 
need 
to 
subsequently 
collapse 
the 
mixture 
q(st+1, 
ht+1jv1:t+1) 
in 
a 
suitable 
way. 
We 
will 
deal 
with 
this 
issue 
once 
q(st+1, 
ht+1jv1:t+1) 
has 
been 
computed. 


To 
derive 
the 
updates 
it 
is 
useful 
to 
break 
the 
new 
ltered 
approximation 
from 
equation 
(25.2.4) 
into 
continuous 
and 
discrete 
parts: 


q(ht;stjv1:t)= 
q(htjst, 
v1:t)q(stjv1:t) 
(25.3.2) 


and 
derive 
separate 
ltered 
update 
formulae, 
as 
described 
below. 


25.3.1 
Continuous 
ltering 
..

St..1

The 
exact 
representation 
of 
p(htjst, 
v1:t) 
is 
a 
mixture 
with 
Ocomponents. 
To 
retain 
computational 
feasibility 
we 
therefore 
approximate 
this 
with 
a 
limited 
I-component 
mixture 


I

X

q(htjst, 
v1:t)= 
q(htjit;st, 
v1:t)q(itjst, 
v1:t) 
(25.3.3) 
it=1 


where 
q(htjit;st, 
v1:t) 
is 
a 
Gaussian 
parameterised 
with 
mean 
f(it;st) 
and 
covariance 
F(it;st). 
Strictly 
speaking, 
we 
should 
use 
the 
notation 
ft(it;st) 
since, 
for 
each 
time 
t, 
we 
have 
a 
set 
of 
means 
indexed 
by 
it;st, 
although 
we 
drop 
these 
dependencies 
in 
the 
notation 
used 
here. 


An 
important 
remark 
is 
that 
many 
techniques 
approximate 
p(htjst, 
v1:t) 
using 
a 
single 
Gaussian. 
Nat


PX

urally, 
this 
gives 
rise 
to 
a 
mixture 
of 
Gaussians 
for 
p(htjv1:t)= 
p(htjst, 
v1:t)p(stjv1:t). 
However, 
in 


st 
making 
a 
single 
Gaussian 
approximation 
to 
p(htjst, 
v1:t) 
the 
representation 
of 
the 
posterior 
may 
be 
poor. 


Our 
aim 
here 
is 
to 
maintain 
an 
accurate 
approximation 
to 
p(htjst, 
v1:t) 
by 
using 
a 
mixture 
of 
Gaussians. 


To 
nd 
a 
recursion 
for 
the 
approximating 
distribution 
we 
rst 
assume 
that 
we 
know 
the 
ltered 
approximation 
q(ht;stjv1:t) 
and 
then 
propagate 
this 
forwards 
using 
the 
exact 
dynamics. 
To 
do 
so 
consider 
rst 
the 
relation 


X

q(ht+1jst+1, 
v1:t+1)=q(ht+1;st;itjst+1, 
v1:t+1) 


st;it 


X

=q(ht+1jst;it;st+1, 
v1:t+1)q(st;itjst+1, 
v1:t+1) 
(25.3.4) 


st;it 


Wherever 
possible 
we 
now 
substitute 
the 
exact 
dynamics 
and 
evaluate 
each 
of 
the 
two 
factors 
above. 
The 
usefulness 
of 
decomposing 
the 
update 
in 
this 
way 
is 
that 
the 
new 
ltered 
approximation 
is 
of 
the 
form 
of 
a 
Gaussian 
mixture, 
where 
q(ht+1jst;it;st+1, 
v1:t+1) 
is 
Gaussian 
and 
q(st;itjst+1, 
v1:t+1) 
are 
the 
weights 
or 
mixing 
proportions 
of 
the 
components. 
We 
describe 
below 
how 
to 
compute 
these 
terms 
explicitly. 
Equation(25.3.4) 
produces 
a 
new 
Gaussian 
mixture 
with 
I 
× 
S 
components 
which 
we 
will 
collapse 
back 
to 
I 
components 
at 
the 
end 
of 
the 
computation. 


Evaluating 
q(ht+1jst;it;st+1, 
v1:t+1) 


We 
aim 
to 
nd 
a 
ltering 
recursion 
for 
q(ht+1jst;it;st+1, 
v1:t+1). 
Since 
this 
is 
conditional 
on 
switch 
states 
and 
components, 
this 
corresponds 
to 
a 
single 
LDS 
forward 
step 
which 
can 
be 
evaluated 
by 
considering 
rst 
the 
joint 
distribution 


q(ht+1, 
vt+1jst;it;st+1, 
v1:t)= 
p(ht+1, 
vt+1jht;st;it;(25.3.5)

st+1, 
v1:t)q(htjst;it;st+1, 
v1:t) 


ht 


and 
subsequently 
conditioning 
on 
vt+1. 
In 
the 
above 
we 
used 
the 
exact 
dynamics 
where 
possible. 
Equation(25.3.5) 
states 
that 
we 
know 
the 
ltered 
information 
up 
to 
time 
t, 
in 
addition 
to 
knowing 
the 
switch 
states 
st;st+1 


DRAFT 
March 
9, 
2010 



Gaussian 
Sum 
Filtering 


and 
the 
mixture 
component 
index 
it. 
To 
ease 
the 
burden 
on 
notation 
we 
derive 
this 
for 
h
t, 
vt 
= 
0 
for 
all 


t. 
The 
exact 
forward 
dynamics 
is 
then 
given 
by 
ht+1 
= 
A(st+1)ht 
+ 
h(st+1), 
vt+1 
= 
B(st+1)ht+1 
+ 
v(st+1), 
(25.3.6) 


Given 
the 
mixture 
component 
index 
it, 


q(htjv1:t;it;st)= 
N 
(ht 


f(it;st), 
F(it;st)) 
(25.3.7) 


we 
propagate 
this 
Gaussian 
with 
the 
exact 
dynamics 
equation 
(25.3.6). 
Then 
q(ht+1, 
vt+1jst;it;st+1, 
v1:t) 
is 
a 
Gaussian 
with 
covariance 
and 
mean 
elements 


hh 
= 
A(st+1)F(it;st)AT(st+1)+ 
h(st+1), 
vv 
= 
B(st+1)hhBT(st+1)+ 
v(st+1) 


vh 
= 
B(st+1)hh 
= 
T 
= 
B(st+1)A(st+1)f(it;st), 
h 
= 
A(st+1)f(it;st) 
(25.3.8)

hv, 
v 


These 
results 
are 
obtained 
from 
integrating 
the 
forward 
dynamics, 
equations 
(25.2.1,25.2.2) 
over 
ht, 
using 
the 
results 
in 
section(8.6.3). 


To 
nd 
q(ht+1jst;it;st+1, 
v1:t+1) 
we 
condition 
q(ht+1, 
vt+1jst;it;st+1, 
v1:t) 
on 
vt+1 
using 
the 
standard 
Gaussian 
conditioning 
formulae, 
denition(78), 
to 
obtain 




q(ht+1jst;it;st+1, 
v1:t+1)= 
Nht+1 


hjv, 
hjv(25.3.9) 


with 


= 
h 
+ 
hv..1 
(vt+1 
- 
) 
, 
hjv 
= 
hh 
- 
hv..1vh 
(25.3.10)

hjv 
vv 
vvv 


where 
the 
quantities 
required 
are 
dened 
in 
equation 
(25.3.8). 


Evaluating 
the 
mixture 
weights 
q(st;itjst+1, 
v1:t+1) 


Up 
to 
a 
normalisation 
constant 
the 
mixture 
weight 
in 
equation 
(25.3.4) 
can 
be 
found 
from 


q(st;itjst+1, 
v1:t+1) 
. 
q(vt+1jit;st;st+1, 
v1:t)q(st+1jit;st, 
v1:t)q(itjst, 
v1:t)q(stjv1:t) 
(25.3.11) 


The 
rst 
factor 
in 
equation 
(25.3.11), 
q(vt+1jit;st;st+1, 
v1:t) 
is 
Gaussian 
with 
mean 
and 
covariance 


v 
vv, 
as 
given 
in 
equation 
(25.3.8). 
The 
last 
two 
factors 
q(itjst, 
v1:t) 
and 
q(stjv1:t) 
are 
given 
from 
the 
previous 
ltered 
iteration. 
Finally, 
q(st+1jit;st, 
v1:t) 
is 
found 
from 




hp(st+1jht;st)iq(htjit;st;v1:t) 
augmented 
SLDS 


q(st+1jit;st, 
v1:t) 
=(25.3.12) 


p(st+1jst) 
standard 
SLDS 


where 
the 
result 
above 
for 
the 
standard 
SLDS 
follows 
from 
the 
independence 
assumptions 
present 
in 
the 
standard 
SLDS. 
In 
the 
aSLDS, 
the 
term 
in 
equation 
(25.3.12) 
will 
generally 
need 
to 
be 
computed 
numerically. 
A 
simple 
approximation 
is 
to 
evaluate 
equation 
(25.3.12) 
at 
the 
mean 
value 
of 
the 
distribution 
q(htjit;st, 
v1:t). 
To 
take 
covariance 
information 
into 
account 
an 
alternative 
would 
be 
to 
draw 
samples 
from 
the 
Gaussian 
q(htjit;st, 
v1:t) 
and 
thus 
approximate 
the 
average 
of 
p(st+1jht;st) 
by 
sampling. 
Note 
that 
this 
does 
not 
equate 
Gaussian 
Sum 
ltering 
with 
a 
sequential 
sampling 
procedure, 
such 
as 
Particle 
Filtering, 
section(27.6.2). 
The 
sampling 
here 
is 
exact, 
for 
which 
no 
convergence 
issues 
arise. 


Closing 
the 
recursion 


We 
are 
now 
in 
a 
position 
to 
calculate 
equation 
(25.3.4). 
For 
each 
setting 
of 
the 
variable 
st+1, 
we 
have 
a 
mixture 
of 
I 
× 
S 
Gaussians. 
To 
prevent 
the 
number 
of 
components 
increasing 
exponentially 
with 
time, 
we 
numerically 
collapse 
q(ht+1jst+1, 
v1:t+1) 
back 
to 
I 
Gaussians 
to 
form 


I

X

q(ht+1jst+1, 
v1:t+1) 
. 
q(ht+1jit+1;st+1, 
v1:t+1)q(it+1jst+1, 
v1:t+1) 
(25.3.13) 


it+1=1 


Any 
method 
of 
choice 
may 
be 
supplied 
to 
collapse 
a 
mixture 
to 
a 
smaller 
mixture. 
A 
straightforward 
approach 
is 
to 
repeatedly 
merge 
low-weight 
components, 
as 
explained 
in 
section(25.3.4). 
In 
this 
way 
the 
new 
mixture 
coecients 
q(it+1jst+1, 
v1:t+1), 
it+1 
. 
1;:::;I 
are 
dened. 
This 
completes 
the 
description 
of 
how 
to 
form 
a 
recursion 
for 
the 
continuous 
ltered 
posterior 
approximation 
q(ht+1jst+1, 
v1:t+1) 
in 
equation 
(25.3.2). 


DRAFT 
March 
9, 
2010 



Gaussian 
Sum 
Filtering 


t+1t
Figure 
25.2: 
Gaussian 
Sum 
Filtering. 
The 
leftmost 
column 
depicts 
the 
previous 
Gaussian 
mixture 
approximation 
q(ht;itjv1:t) 
for 
two 
states 
S 
=2 
(red 
and 
blue) 
and 
three 
mixture 
components 
I 
= 
3, 
with 
the 
mixture 
weight 
represented 
by 
the 
area 
of 
each 
oval. 
There 
are 
S 
= 
2 
dierent 
linear 
systems 
which 
take 
each 
of 
the 
components 
of 
the 
mixture 
into 
a 
new 
ltered 
state, 
the 
colour 
of 
the 
arrow 
indicating 
which 
dynamic 
system 
is 
used. 
After 
one 
time-step 
each 
mixture 
component 
branches 
into 
a 
further 
S 
components 
so 
that 
the 
joint 
approximation 
q(ht+1;st+1jv1:t+1) 
contains 
S2I 
components 
(middle 
column). 
To 
keep 
the 
representation 
computationally 
tractable 
the 
mixture 
of 
Gaussians 
for 
each 
state 
st+1 
is 
collapsed 
back 
to 
I 
components. 
This 
means 
that 
each 
coloured 
state 
needs 
to 
be 
approximated 
by 
a 
smaller 
I 
component 
mixture 
of 
Gaussians. 
There 
are 
many 
ways 
to 
achieve 
this. 
A 
naive 
but 
computationally 
ecient 
approach 
is 
to 
simply 
ignore 
the 
lowest 
weight 
components, 
as 
depicted 
on 
the 
right 
column, 
see 
mix2mix.m. 


25.3.2 
Discrete 
ltering 
A 
recursion 
for 
the 
switch 
variable 
distribution 
in 
equation 
(25.3.2) 
is 


X

q(st+1jv1:t+1) 
/q(st+1;it;st, 
vt+1, 
v1:t) 
(25.3.14) 


it;st 


The 
r.h.s. 
of 
the 
above 
equation 
is 
proportional 
to

X

q(vt+1jst+1;it;st, 
v1:t)q(st+1jit;st, 
v1:t)q(itjst, 
v1:t)q(stjv1:t) 
(25.3.15) 


st;it 


for 
which 
all 
terms 
have 
been 
computed 
during 
the 
recursion 
for 
q(ht+1jst+1, 
v1:t+1). 
We 
now 
have 
all 
the 
quantities 
required 
to 
compute 
the 
Gaussian 
Sum 
approximation 
of 
the 
ltering 
forward 
pass. 
A 
schematic 
representation 
of 
Gaussian 
Sum 
ltering 
is 
given 
in 
g(25.2) 
and 
the 
pseudo 
code 
is 
presented 
in 
algorithm(22). 
See 
also 
SLDSforward.m. 


25.3.3 
The 
likelihood 
p(v1:T 
) 
The 
likelihood 
p(v1:T 
) 
may 
be 
found 
from 


T 
..1

YX

p(v1:T 
)= 
p(vt+1jv1:t) 
(25.3.16) 
t=0 


where 


X

p(vt+1jv1:t) 
q(vt+1jit;st;st+1, 
v1:t)q(st+1jit;st, 
v1:t)q(itjst, 
v1:t)q(stjv1:t) 


it;st;st+1 


In 
the 
above 
expression, 
all 
terms 
have 
been 
computed 
in 
forming 
the 
recursion 
for 
the 
ltered 
posterior 


q(ht+1;st+1jv1:t+1). 


25.3.4 
Collapsing 
Gaussians 
Given 
a 
mixture 
of 
N 
Gaussians 


N

X

p(x)= 
piN 
(x 


i, 
i) 
(25.3.17) 
i=1 


we 
wish 
to 
collapse 
this 
to 
a 
smaller 
K<N 
mixture 
of 
Gaussians. 
We 
describe 
a 
simple 
method 
which 
has 
the 
advantage 
of 
computational 
eciency, 
but 
the 
disadvantage 
that 
no 
spatial 
information 
about 


DRAFT 
March 
9, 
2010 



Gaussian 
Sum 
Smoothing 


Algorithm 
22 
aSLDS 
Forward 
Pass. 
Approximate 
the 
ltered 
posterior 
p(stjv1:t) 
= 
t, 
p(htjst, 
v1:t) 


PX

it 
wt(it;st)N 
(ht 


ft(it;st), 
Ft(it;st)). 
Also 
return 
the 
approximate 
log-likelihood 
L 
= 
log 
p(v1:T 
). 
It 
are 
the 
number 
of 
components 
in 
each 
Gaussian 
mixture 
approximation. 
We 
require 
I1 
=1;I2 
= 
S, 
It 
= 


S 
× 
It..1. 
(s)= 
A(s), 
B(s), 
h(s), 
v(s), 
h(s), 
v(s). 


for 
s1 
. 
1 
to 
S 
do 
ff1(1;s1), 
F1(1;s1);p^} 
= 
LDSFORWARD(0, 
0, 
v1; 
(s1)) 
1 
. 
p(s1)^p 


end 
for 


for 
t 
. 
2 
to 
T 
do 
for 
st 
. 
1 
to 
S 
do 


for 
i 
. 
1 
to 
It..1, 
and 
s 
. 
1 
to 
S 
do 
fxjy(i, 
s), 
xjy(i, 
s);p^} 
= 
LDSFORWARD(ft..1(i, 
s), 
Ft..1(i, 
s), 
vt; 
(st)) 
p 
(stji, 
s) 
hp(stjht..1;st..1 
= 
s)ip(ht..1jit..1=i;st..1=s;v1:t..1) 
p0(st, 
i, 
s) 
. 
wt..1(i, 
s)p 
(stji, 
s)t..1(s)^p 


end 
for 


Collapse 
the 
It..1 
× 
S 
mixture 
of 
Gaussians 
dened 
by 
xjy,xjy, 
and 
weights 
p(i, 
sjst) 
. 
p0(st, 
i, 
s) 
to 
a 
Gaussian 
with 
It 
components, 
p(htjst, 
v1:t) 


PIt 


p(itjst, 
v1:t)p(htjst;it, 
v1:t). 
This 
denes 
the 
new 
means 
ft(it;st), 
covariances 


it=1 


Ft(it;st) 
and 
mixture 
weights 
wt(it;st) 
= 
p(itjst, 
v1:t).

PX

Compute 
t(st) 
. 
0(st, 
i, 
s)

i;s 
p

end 
for 


normalise 
t 


PX

L 
. 
L 
+ 
log 
0(st, 
i, 
s)

st;i;s 
p

end 
for 


the 
mixture 
is 
used[277]. 
First 
we 
describe 
how 
to 
collapse 
a 
mixture 
to 
a 
single 
Gaussian. 
This 
can 
be 
achieved 
by 
nding 
the 
mean 
and 
covariance 
of 
the 
mixture 
distribution 
(25.3.17). 
These 
are 




XX

TT 


µ 
=pii, 
S 
=pii 
+ 
iµ 
- 
µ 
(25.3.18)

i
ii 


To 
collapse 
a 
mixture 
then 
to 
a 
K-component 
mixture 
we 
may 
rst 
retain 
the 
K 
- 
1 
Gaussians 
with 
the 
largest 
mixture 
weights. 
The 
remaining 
N 
- 
K 
+ 
1 
Gaussians 
are 
simply 
merged 
to 
a 
single 
Gaussian 
using 
the 
above 
method. 
Alternative 
heuristics 
such 
as 
recursively 
merging 
the 
two 
Gaussians 
with 
the 
lowest 
mixture 
weights 
are 
also 
reasonable. 


More 
sophisticated 
methods 
which 
retain 
some 
spatial 
information 
would 
clearly 
be 
potentially 
useful. 
The 
method 
presented 
in 
[174] 
is 
a 
suitable 
approach 
which 
considers 
removing 
Gaussians 
which 
are 
spatially 
similar 
(and 
not 
just 
low-weight 
components), 
thereby 
retaining 
a 
sense 
of 
diversity 
over 
the 
possible 
solutions. 
In 
applications 
with 
many 
thousands 
of 
timesteps, 
speed 
can 
be 
a 
factor 
in 
determining 
which 
method 
of 
collapsing 
Gaussians 
is 
to 
be 
preferred. 


25.3.5 
Relation 
to 
other 
methods 
Gaussian 
Sum 
Filtering 
can 
be 
considered 
a 
form 
of 
`analytical 
particle 
ltering', 
section(27.6.2), 
in 
which 
instead 
of 
point 
distributions 
(delta 
functions) 
being 
propagated, 
Gaussians 
are 
propagated. 
The 
collapse 
operation 
to 
a 
smaller 
number 
of 
Gaussians 
is 
analogous 
to 
resampling 
in 
Particle 
Filtering. 
Since 
a 
Gaussian 
is 
more 
expressive 
than 
a 
delta-function, 
the 
Gaussian 
Sum 
lter 
is 
generally 
an 
improved 
approximation 
technique 
over 
using 
point 
particles. 
See 
[17] 
for 
a 
numerical 
comparison. 


25.4 
Gaussian 
Sum 
Smoothing 
Approximating 
the 
smoothed 
posterior 
p(ht;stjv1:T 
) 
is 
more 
involved 
than 
ltering 
and 
requires 
additional 
approximations. 
For 
this 
reason 
smoothing 
is 
more 
prone 
to 
failure 
since 
there 
are 
more 
assumptions 
that 
need 
to 
be 
satised 
for 
the 
approximations 
to 
hold. 
The 
route 
we 
take 
here 
is 
to 
assume 
that 
a 
Gaussian 


462 
DRAFT 
March 
9, 
2010 



Gaussian 
Sum 
Smoothing 


Sum 
ltered 
approximation 
has 
been 
carried 
out, 
and 
then 
approximate 
the 
. 
backward 
pass, 
analogous 
to 
that 
of 
section(23.2.4). 
By 
analogy 
with 
the 
RTS 
smoothing 
recursion 
equation 
(23.2.20), 
the 
exact 
backward 
pass 
for 
the 
SLDS 
reads 


Z

X

p(ht;st| 
v1:T 
)=p(ht;st| 
ht+1;st+1, 
v1:t)p(ht+1;st+1| 
v1:T 
) 
(25.4.1) 


ht+1

st+1

where 
p(ht+1;st+1| 
v1:T 
)= 
p(st+1| 
v1:T 
)p(ht+1| 
st+1, 
v1:T 
) 
is 
composed 
of 
the 
discrete 
and 
continuous 
components 
of 
the 
smoothed 
posterior 
at 
the 
next 
time 
step. 
The 
recursion 
runs 
backwards 
in 
time, 
beginning 
with 
the 
initialisation 
p(hT 
;sT 
| 
v1:T 
) 
set 
by 
the 
ltered 
result 
(at 
time 
t 
= 
T 
, 
the 
ltered 
and 
smoothed 
posteriors 
coincide). 
Apart 
from 
the 
fact 
that 
the 
number 
of 
mixture 
components 
will 
increase 
at 
each 
step, 
computing 
the 
integral 
over 
ht+1 
in 
equation 
(25.4.1) 
is 
problematic 
since 
the 
conditional 
distribution 
term 
is 
non-Gaussian 
in 
ht+1. 
For 
this 
reason 
it 
is 
more 
useful 
derive 
an 
approximate 
recursion 
by 
beginning 
with 
the 
exact 
relation 


X

p(st, 
ht| 
v1:T 
)=p(st+1| 
v1:T 
)p(ht| 
st;st+1, 
v1:T 
)p(st| 
st+1, 
v1:T 
) 
(25.4.2) 


st+1 


which 
can 
be 
expressed 
more 
directly 
in 
terms 
of 
the 
SLDS 
dynamics 
as 


X

p(st, 
ht| 
v1:T 
)=v1:T 
) 
( 
p(ht| 
ht+1;st;st+1, 
v1:t;+ 


p(st+1| 
vt+1:T 
)) 
p(ht+1jst;st+1;v1:T 
) 


st+1 


( 
p(st| 
ht+1;st+1, 
v1:T 
)) 
p(ht+1jst+1;v1:T 
) 
(25.4.3) 


In 
forming 
the 
recursion 
we 
assume 
access 
to 
the 
distribution 
p(st+1, 
ht+1| 
v1:T 
) 
from 
the 
future 
timestep. 
However, 
we 
also 
require 
the 
distribution 
p(ht+1| 
st;st+1, 
v1:T 
) 
which 
is 
not 
directly 
known 
and 
needs 
to 
be 
inferred, 
in 
itself 
a 
computationally 
challenging 
task. 
In 
the 
Expectation 
Correction 
(EC) 
approach 
[17] 
one 
assumes 
the 
approximation 
(see 
g(25.3)) 


p(ht+1| 
st;st+1, 
v1:T 
) 
˜ 
p(ht+1| 
st+1, 
v1:T 
) 
(25.4.4) 


resulting 
in 
an 
approximate 
recursion 
for 
the 
smoothed 
posterior, 


X

p(st, 
ht| 
v1:T 
) 
p(st+1| 
v1:T 
) 
( 
p(ht| 
ht+1;st;st+1, 
v1:t)i( 
p(st| 
ht+1;st+1, 
v1:T 
)) 
(25.4.5)

ht+1 
ht+1 
st+1 


where 
h) 
represents 
averaging 
with 
respect 
to 
the 
distribution 
p(ht+1| 
st+1, 
v1:T 
). 
In 
carrying 
out 
the

ht+1 


approximate 
recursion, 
(25.4.5) 
we 
will 
end 
up 
with 
a 
mixture 
of 
Gaussians 
that 
grows 
at 
each 
timestep. 
To 
avoid 
the 
exponential 
explosion 
problem, 
we 
use 
a 
nite 
mixture 
approximation, 
q(ht+1;st+1| 
v1:T 
): 


p(ht+1;st+1| 
v1:T 
) 
˜ 
q(ht+1;st+1| 
v1:T 
)= 
q(ht+1| 
st+1, 
v1:T 
)q(st+1| 
v1:T 
) 
(25.4.6) 


and 
plug 
this 
into 
the 
approximate 
recursion 
above. 
From 
equation 
(25.4.5) 
a 
recursion 
for 
the 
approximation 
is 
given 
by 


X

q(ht;st| 
v1:T 
)=q(st+1| 
v1:T 
) 
( 
q(ht| 
ht+1;st;st+1, 
v1:t)) 
q(ht+1jst+1;v1:T 
) 
( 
q(st| 
ht+1;st+1, 
v1:t)) 
q(ht+1jst+1;v1:T 
) 


st+1 
|{z}|{z}Xq(htjst;st+1;v1:T 
) 
q(stjst+1;v1:T 
) 


(25.4.7) 
As 
for 
ltering, 
wherever 
possible, 
we 
replace 
approximate 
terms 
by 
their 
exact 
counterparts 
and 
parameterise 
the 
posterior 
using 


q(ht+1;st+1| 
v1:T 
)= 
q(ht+1| 
st+1, 
v1:T 
)q(st+1| 
v1:T 
) 
(25.4.8) 


To 
reduce 
the 
notational 
burden 
here 
we 
outline 
the 
method 
only 
for 
the 
case 
of 
using 
a 
single 
component 
approximation 
in 
both 
the 
forward 
and 
backward 
passes. 
The 
extension 
to 
using 
a 
mixture 
to 
approximate 
each 
p(ht+1| 
st+1, 
v1:T 
) 
is 
conceptually 
straightforward 
and 
deferred 
to 
section(25.4.4). 
In 
the 
single 
Gaussian 
case 
we 
assume 
we 
have 
a 
Gaussian 
approximation 
available 
for 


q(ht+1| 
st+1, 
v1:T 
)= 
N 
(ht+1 


g(st+1, 
G(st+1)) 
(25.4.9) 


DRAFT 
March 
9, 
2010 



Gaussian 
Sum 
Smoothing 


Figure 
25.3: 
The 
EC 
backpass 
approximates 


st..1
ht..1
vt..1
st
ht
vt
st+1st+2
ht+1
vt+1
ht+2
vt+2
p(ht+1jst+1;st, 
v1:T 
) 
by 
p(ht+1jst+1, 
v1:T 
). 
The 
motivation 
for 
this 
is 
that 
st 
inuences 
ht+1 
only 
indirectly 
through 
ht. 
However, 
ht 
will 
most 
likely 
be 
heavily 
inuenced 
by 
v1:t, 
so 
that 
not 
knowing 
the 
state 
of 
st 
is 
likely 
to 
be 
of 
secondary 
importance. 
The 
green 
shaded 
node 
is 
the 
variable 
we 
wish 
to 
nd 
the 
posterior 
for. 
The 
values 
of 
the 
blue 
shaded 
nodes 
are 
known, 
and 
the 
red 
shaded 
node 
indicates 
a 
known 


variable 
which 
is 
assumed 
unknown 
in 
forming 
the 
approximation. 


25.4.1 
Continuous 
smoothing 
For 
given 
st;st+1, 
an 
RTS 
style 
recursion 
for 
the 
smoothed 
continuous 
is 
obtained 
from 
equation 
(25.4.7), 
giving 


Z

q(htjst;st+1, 
v1:T 
)=p(htjht+1;st;st+1, 
v1:t)q(ht+1jst+1, 
v1:T 
) 
(25.4.10) 


ht+1 


To 
compute 
equation 
(25.4.10) 
we 
then 
perform 
a 
single 
update 
of 
the 
LDS 
backward 
recursion, 
section(24.4.2). 


25.4.2 
Discrete 
smoothing 
The 
second 
average 
in 
equation 
(25.4.7) 
corresponds 
to 
a 
recursion 
for 
the 
discrete 
variable 
and 
is 
given 
by 


hq(stjht+1;st+1, 
v1:t)) 
q(ht+1jst+1;v1:T 
) 
= 
q(stjst+1, 
v1:T 
). 
(25.4.11) 


The 
average 
of 
q(stjht+1;st+1, 
v1:t) 
with 
respect 
to 
q(ht+1jst+1, 
v1:T 
) 
cannot 
be 
achieved 
in 
closed 
form. 
A 
simple 
approach 
is 
to 
approximate 
the 
average 
by 
evaluation 
at 
the 
mean1 




hq(stjht+1;st+1v1:t)i˜ 
q(stjht+1;st+1, 
v1:t)(25.4.12)

q(ht+1jst+1;v1:T 
) 
ht+1=hht+1jst+1;v1:T 
) 


where 
hht+1jst+1, 
v1:T 
) 
is 
the 
mean 
of 
ht+1 
with 
respect 
to 
q(ht+1jst+1, 
v1:T 
). 


Replacing 
ht+1 
by 
its 
mean 
gives 
the 
approximation 


- 
1 
T

z(st;st+1)..1(st;st+1jv1:t)zt+1(st;st+1)

t+1

1 
e 
2 


hq(stjht+1;st+1, 
v1:t)iq(ht+1jst+1;v1:T 
) 
˜ 
pq(stjst+1, 
v1:t) 
(25.4.13) 


Z 
det 
((st;st+1jv1:t)) 


where 


zt+1(st;st+1) 
hht+1jst+1, 
v1:T 
i..hht+1jst;st+1, 
v1:t) 
(25.4.14) 


and 
Z 
ensures 
normalisation 
over 
st. 
(st;st+1jv1:t) 
is 
the 
ltered 
covariance 
of 
ht+1 
given 
st;st+1 
and 
the 
observations 
v1:t, 
which 
may 
be 
taken 
from 
hh 
in 
equation 
(25.3.8). 
Approximations 
which 
take 
covariance 
information 
into 
account 
can 
also 
be 
considered, 
although 
the 
above 
simple 
(and 
fast) 
method 
may 
suce 
in 
practice 
[17, 
192]. 


25.4.3 
Collapsing 
the 
mixture 
From 
section(25.4.1) 
and 
section(25.4.2) 
we 
now 
have 
all 
the 
terms 
in 
equation 
(25.4.8) 
to 
compute 
the 
approximation 
to 
equation 
(25.4.7). 
Due 
to 
the 
summatino 
over 
st+1 
in 
equation 
(25.4.7), 
the 
number 
of 
mixture 
components 
is 
multiplied 
by 
S 
at 
each 
iteration. 
To 
prevent 
an 
exponential 
explosion 
of 
components, 
the 
mixture 
equation 
(25.4.7) 
is 
then 
collapsed 
to 
a 
single 
Gaussian 


q(ht;stjv1:T 
) 
. 
q(htjst, 
v1:T 
)q(stjv1:T 
)) 
(25.4.15) 


The 
collapse 
to 
a 
mixture 
is 
discussed 
in 
section(25.4.4). 


1In 
general 
this 
approximation 
has 
the 
form 
hf(x)i˜ 
f(hxi). 


DRAFT 
March 
9, 
2010 



Gaussian 
Sum 
Smoothing 


Algorithm 
23 
aSLDS: 
EC 
Backward 
Pass. 
Approximates 
p(stjv1:T 
) 
and 
p(htjst, 
v1:T 
) 


PJt 


jt=1 
ut(jt;st)N 
(gt(jt;st), 
Gt(jt;st)) 
using 
a 
mixture 
of 
Gaussians. 
JT 
= 
IT 
;Jt 
= 
S 
× 
It 
× 
Jt+1. 
This 
routine 
needs 
the 
results 
from 
algorithm(22). 


GT 
. 
FT 
, 
gT 
. 
fT 
, 
uT 
. 
wT 
for 
t 
. 
T 
- 
1 
to 
1 
do 


for 
s 
. 
1 
to 
S, 
s' 
. 
1 
to 
S, 
i 
. 
1 
to 
It, 
j' 
. 
1 
to 
Jt+1 
do 


(, 
)(i, 
s, 
j0;s0)= 
LDSBACKWARD(gt+1(j0;s0), 
Gt+1(j0;s0), 
ft(i, 
s), 
Ft(i, 
s);(s0)) 


p(it;stjjt+1;st+1;v1:T 
)= 
hp(st 
= 
s, 
it 
= 
ijht+1;st+1 
= 
s0;jt+1 
= 
j0, 
v1:t)ip(ht+1jst+1=s0;jt+1=j0;v1:T 
) 


p(i, 
s, 
j0;s0jv1:T 
) 
. 
p(st+1 
= 
s0jv1:T 
)ut+1(j0;s0)p(it;stjjt+1;st+1;v1:T 
) 


end 
for 


for 
st 
. 
1 
to 
S 
do 
Collapse 
the 
mixture 
dened 
by 
weights 
p(it 
= 
i, 
st+1 
= 
s0;jt+1 
= 
j0jst, 
v1T 
) 
. 
p(i, 
st;j0;s0jv1:T 
), 
means 
(it;st;jt+1;st+1) 
and 
covariances 
(it;st;jt+1;st+1) 
to 
a 
mixt
ure 
with 
Jt 
components. 
This 
denes 
the 
new 
means 
gt(jt;st), 
covariances 
Gt(jt;st) 
and 
mixture 
weights 
ut(jt;st).

PX

p(stjv1:T 
) 
. 
l 
p(it;st;j0;s0jv1:T 
)

it;j0;s

end 
for 
end 
for 


25.4.4 
Using 
mixtures 
in 
smoothing 
The 
extension 
to 
the 
mixture 
case 
is 
straightforward 
based 
on 
the 
representation 


J

X

p(htjst, 
v1:T 
) 
˜ 
q(jtjst, 
v1:T 
)q(htjjt, 
v1:T 
). 
(25.4.16) 
jt=1 


Analogously 
to 
the 
case 
with 
a 
single 
component, 


X

q(ht;stjv1:T 
)=p(st+1jv1:T 
)p(jt+1jst+1, 
v1:T 
)q(htjjt+1;st+1;it;st, 
v1:T 
) 
it;jt+1;st+1 
hq(it;stjht+1;jt+1;st+1, 
v1:t)) 
q(ht+1jjt+1;st+1;v1:T 
) 
(25.4.17) 


The 
average 
in 
the 
last 
line 
of 
the 
above 
equation 
can 
be 
tackled 
using 
the 
same 
techniques 
as 
outlined 
in 
the 
single 
Gaussian 
case. 
To 
approximate 
q(htjjt+1;st+1;it;st, 
v1:T 
) 
we 
consider 
this 
as 
the 
marginal 
of 
the 
joint 
distribution 


q(ht, 
ht+1jit;st;jt+1;st+1, 
v1:T 
)= 
q(htjht+1;it;st;jt+1;st+1, 
v1:t)q(ht+1jit;st;jt+1;st+1, 
v1:T 
) 
(25.4.18) 
As 
in 
the 
case 
of 
a 
single 
mixture, 
the 
problematic 
term 
is 
q(ht+1jit;st;jt+1;st+1, 
v1:T 
). 
Analogously 
to 
equation 
(25.4.4), 
we 
make 
the 
assumption 
q(ht+1jit;st;jt+1;st+1, 
v1:T 
) 
˜ 
q(ht+1jjt+1;st+1, 
v1:T 
) 
(25.4.19) 
meaning 
that 
information 
about 
the 
current 
switch 
state 
st;it 
is 
ignored. 
We 
can 
then 
form 


X

p(htjst, 
v1:T 
)=p(it;jt+1;st+1jst, 
v1:T 
)p(htjit;st;jt+1;st+1, 
v1:T 
) 
(25.4.20) 
it;jt+1;st+1 


This 
mixture 
can 
then 
be 
collapsed 
to 
a 
smaller 
mixture 
using 
any 
method 
of 
choice, 
to 
give 


X

p(htjst, 
v1:T 
) 
q(jtjst, 
v1:T 
)q(htjjt, 
v1:T 
) 
(25.4.21) 
jt 


The 
resulting 
procedure 
is 
sketched 
in 
algorithm(23), 
including 
using 
mixtures 
in 
both 
the 
forward 
and 
backward 
passes. 


DRAFT 
March 
9, 
2010 



Gaussian 
Sum 
Smoothing 


25.4.5 
Relation 
to 
other 
methods 
A 
classical 
smoothing 
approximation 
for 
the 
SLDS 
is 
generalised 
pseudo 
Bayes 
(GPB) 
[13, 
160, 
159]. 
In 
GPB 
one 
starts 
from 
the 
exact 
recursion 


X

X

p(stjv1:T 
)=p(st;st+1jv1:T 
)=p(stjst+1, 
v1:T 
)p(st+1jv1:T 
) 
(25.4.22) 


st+1 
st+1 


The 
quantity 
p(stjst+1, 
v1:T 
) 
is 
dicult 
to 
obtain 
and 
GPB 
makes 
the 
approximation 


p(stjst+1, 
v1:T 
) 
˜ 
p(stjst+1, 
v1:t) 
(25.4.23) 
Plugging 
this 
into 
equation 
(25.4.22) 
we 
have 


 


p(stjv1:T 
) 
p(stjst+1, 
v1:t)p(st+1jv1:T 
) 
(25.4.24) 


st+1 


 


p(st+1jst)p(stjv1:t)

p(st+1jst)p(stjv1:t)

st

st+1 


P

p(st+1jv1:T 
) 
(25.4.25)

=


The 
recursion 
is 
initialised 
with 
the 
approximate 
ltered 
p(sT 
jv1:T 
). 
Computing 
the 
smoothed 
recursion 
for 
the 
switch 
states 
in 
GPB 
is 
then 
equivalent 
to 
running 
the 
RTS 
backward 
pass 
on 
a 
hidden 
Markov 
model, 
independently 
of 
the 
backward 
recursion 
for 
the 
continuous 
variables. 
The 
only 
information 
the 
GPB 
method 
uses 
to 
form 
the 
smoothed 
distribution 
p(stjv1:T 
) 
from 
the 
ltered 
distribution 
p(stjv1:t) 
is 
the 
Markov 
switch 
transition 
p(st+1jst). 
This 
approximation 
drops 
information 
from 
the 
future 
since 
information 
passed 
via 
the 
continuous 
variables 
is 
not 
taken 
into 
account. 
In 
contrast 
to 
GPB, 
the 
EC 
Gaussian 
smoothing 
technique 
preserves 
future 
information 
passing 
through 
the 
continuous 
variables. 
As 
for 
EC, 
GPB 
forms 
an 
approximation 
for 
p(htjst, 
v1:T 
) 
by 
using 
the 
recursion 
(25.4.8) 
where 
q(stjst+1, 
v1:T 
) 
is 
given 
by 
q(stjst+1, 
v1:t). 
In 
SLDSbackward.m 
one 
may 
choose 
to 
use 
either 
EC 
or 
GBP. 


Example 
105 
(Trac 
Flow). 
A 
illustration 
of 
modelling 
and 
inference 
with 
a 
SLDS 
is 
to 
consider 
a 
simple 
network 
of 
trac 
ow, 
g(25.4). 
Here 
there 
are 
4 
junctions 
a, 
b, 
c, 
d 
and 
trac 
ows 
along 
the 
roads 
in 
the 
direction 
indicated. 
Trac 
ows 
into 
the 
junction 
at 
a 
and 
then 
goes 
via 
dierent 
routes 
to 
d. 
Flow 
out 
of 
a 
junction 
must 
match 
the 
ow 
in 
to 
a 
junction 
(up 
to 
noise). 
There 
are 
trac 
light 
switches 
at 
junctions 
a 
and 
b 
which, 
depending 
on 
their 
state, 
route 
trac 
dierently 
along 
the 
roads. 


Using 
f 
to 
denote 
the 
clean 
(noise 
free) 
ow, 
we 
model 
the 
ows 
using 
the 
switching 
linear 
system: 


8X. 
>>.
>>>>. 
9X. 
>>.
>>>>. 


= 


a

(t) 
a!d(t) 


a!b(t) 


a

(t 
- 
1) 


a(t 
- 
1) 
(0:75 
× 
I[sa(t) 
= 
1]+1 
× 
I[sa(t) 
= 
2]+0 
× 
I[sa(t) 
= 
3]) 


a(t 
- 
1) 
(0:25 
× 
I[sa(t) 
= 
1]+0 
× 
I[sa(t) 
= 
2]+1 
× 
I[sa(t) 
= 
3]) 


(25.4.26)
b!d(t) 


b!c

(t) 
c!d(t) 


a!b(t 
- 
1) 
(0:5 
× 
I[sb(t) 
= 
1]+0 
× 
I[sb(t) 
= 
2])

>.
>>>>>. 
>.
>>>>>. 


a!b(t 
- 
1) 
(0:5 
× 
I[sb(t) 
= 
1]+1 
× 
I[sb(t) 
= 
2]) 


b!c(t 
- 
1) 


By 
identifying 
the 
ows 
at 
time 
t 
with 
a 
6 
dimensional 
vector 
hidden 
variable 
ht, 
we 
can 
write 
the 
above 
ow 
equations 
as 


ht 
= 
A(s)ht..1 
+ 
h 
(25.4.27)

t 


 


for 
a 
set 
of 
suitably 
dened 
matrices 
A(s) 
indexed 
by 
the 
switch 
variable 
s 
= 
sasb, 
which 
takes 
3 
× 
2 
= 
6 
states. 
We 
additionally 
include 
noise 
terms 
to 
model 
cars 
parking 
or 
de-parking 
during 
a 
single 
timestep. 
The 
covariance 
h 
is 
diagonal 
with 
a 
larger 
variance 
at 
the 
inow 
point 
a 
to 
model 
that 
the 
total 
volume 
of 
trac 
entering 
the 
system 
can 
vary. 


Noisy 
measurements 
of 
the 
ow 
into 
the 
network 
are 
taken 
at 
a 


v1;t 
= 
a(t)+ 
1 
v(t) 
(25.4.28) 


DRAFT 
March 
9, 
2010 



Gaussian 
Sum 
Smoothing 


ab
cd
Figure 
25.4: 
A 
representation 
of 
the 
trac 
ow 
between 
junctions 
at 
a,b,c,d, 
with 
trac 
lights 
at 
a 
and 
b. 
If 
sa 
=1 
a 
. 
d 
and 
a 
. 
b 
carry 
0.75 
and 
0.25 
of 
the 
ow 
out 
of 
a 
respectively. 
If 
sa 
= 
2 
all 
the 
ow 
from 
a 
goes 
through 
a 
. 
d; 
for 
sa 
= 
3, 
all 
the 
ow 
goes 
through 
a 
. 
b. 
For 
sb 
= 
1 
the 
ow 
out 
of 
b 
is 
split 
equally 
between 
b 
. 
d 
and 
b 
. 
c. 
For 
sb 
= 
2 
all 
ow 
out 
of 
b 
goes 
along 
b 
. 
c. 


0204060801000204002040608010002040
Figure 
25.5: 
Time 
evolution 
of 
the 
trac 
ow 
measured 
at 
two 
points 
in 
the 
network. 
Sensors 
measure 
the 
total 
ow 
into 
the 
network 
a(t) 
and 
the 
total 
ow 
out 
of 
the 
network, 
d(t)= 
a!d(t)+ 
b!d(t)+ 
c!d(t). 
The 
total 
inow 
at 
a 
undergoes 
a 
random 
walk. 
Note 
that 
the 
ow 
measured 
at 
d 
can 
momentarily 
drop 
to 
zero 
if 
all 
trac 
is 
routed 
through 
a 
. 
b 
. 
c 
in 
two 
consecutive 
time 
steps. 


along 
with 
a 
noisy 
measurement 
of 
the 
total 
ow 
out 
of 
the 
system 
at 
d, 


v2;t 
= 
d(t)= 
a!d(t)+ 
b!d(t)+ 
c!d(t)+ 
v(t) 
(25.4.29)

2 


The 
observation 
model 
can 
be 
represented 
by 
vt 
= 
Bht 
+ 
v 
using 
a 
constant 
2 
× 
6 
projection 


t 


matrix 
B. 
The 
switch 
variables 
follow 
a 
simple 
Markov 
transition 
p(stjst..1) 
which 
biases 
the 
switches 
to 
remain 
in 
the 
same 
state 
in 
preference 
to 
jumping 
to 
another 
state. 
See 
demoSLDStraffic.m 
for 
details. 


Given 
the 
above 
system 
and 
a 
prior 
which 
initialises 
all 
ow 
at 
a, 
we 
draw 
samples 
from 
the 
model 
using 
forward 
(ancestral) 
sampling 
which 
form 
the 
observations 
v1:100, 
g(25.5). 
Using 
only 
the 
observations 
and 
the 
known 
model 
structure 
we 
then 
attempt 
to 
infer 
the 
latent 
switch 
variables 
and 
trac 
ows 
using 
Gaussian 
Sum 
ltering 
and 
smoothing 
(EC 
method) 
with 
2 
mixture 
components 
per 
switch 
state, 
g(25.6). 


We 
note 
that 
a 
naive 
HMM 
approximation 
based 
on 
discretising 
each 
continuous 
ow 
into 
20 
bins 
would 
contain 
2 
× 
3 
× 
206 
or 
384 
million 
states. 
Even 
for 
modest 
size 
problems, 
a 
naive 
approximation 
based 
on 
discretisation 
is 
therefore 
impractical. 


Example 
106 
(Following 
the 
price 
trend). 
The 
following 
is 
a 
simple 
model 
of 
the 
price 
trend 
of 
a 
stock, 
which 
assumes 
that 
the 
price 
tends 
to 
continue 
going 
up 
(or 
down) 
for 
a 
while 
before 
it 
reverses 
direction: 


h1(t)= 
h1(t 
- 
1) 
+ 
h2(t 
- 
1) 
+ 
h(st) 
(25.4.30)

1 


h2(t)= 
I[s(t) 
= 
1] 
h2(t 
- 
1) 
+ 
h(st) 
(25.4.31)

2 


v(t)= 
h1(t)+ 
v(st) 
(25.4.32) 


here 
h1 
represents 
the 
price 
and 
h2 
the 
direction. 
There 
is 
only 
a 
single 
observation 
variable 
at 
each 
time, 
which 
is 
the 
price 
plus 
a 
small 
amount 
of 
noise. 
There 
are 
two 
switch 
states, 
dom(st)= 
f1, 
2g. 
When 
st 
= 
1, 
the 
model 
functions 
normally, 
with 
the 
direction 
being 
equal 
to 
the 
previous 
direction 
plus 
a 
small 
amount 
of 
noise 
h(st 
= 
1). 
When 
st 
= 
2 
however, 
the 
direction 
is 
sampled 
from 
a 
Gaussian 
with 
a 
large 


2 


variance. 
The 
transition 
p(stjst..1) 
is 
set 
so 
that 
normal 
dynamics 
is 
more 
likely, 
and 
when 
st 
=2 
it 
is 
likely 
to 
go 
back 
to 
normal 
dynamics 
the 
next 
timestep. 
Full 
details 
are 
in 
SLDSpricemodel.mat. 
In 
g(25.7) 
we 
plot 
some 
samples 
from 
the 
model 
and 
also 
smoothed 
inference 
of 
the 
switch 
distribution, 
showing 
how 
we 
can 
a 
posteriori 
infer 
the 
likely 
changes 
in 
the 
stock 
price 
direction. 
See 
also 
exercise(239). 


DRAFT 
March 
9, 
2010 



Reset 
Models 


020406080100010200204060801000102002040608010001020020406080100010200204060801000102002040608010001020
20406080100123
2040608010012
(a)
020406080100010200204060801000102002040608010001020020406080100010200204060801000102002040608010001020
20406080100123
2040608010012
(b)
020406080100010200204060801000102002040608010001020020406080100010200204060801000102002040608010001020
20406080100123
2040608010012
(c)
Figure 
25.6: 
Given 
the 
observations 
from 
g(25.5) 
we 
infer 
the 
ows 
and 
switch 
states 
of 
all 
the 
latent 
variables. 
(a): 
The 
correct 
latent 
ows 
through 
time 
along 
with 
the 
switch 
variable 
state 
used 
to 
generate 
the 
data. 
The 
colours 
corresponds 
to 
the 
ows 
at 
the 
corresponding 
coloured 
edges/nodes 
in 
g(25.4). 
(b): 
Filtered 
ows 
based 
on 
a 
I 
= 
2 
Gaussian 
Sum 
forward 
pass 
approximation. 
Plotted 
are 
the 
6 
components 
of 
the 
vector 
hhtjv1:t) 
with 
the 
posterior 
distribution 
of 
the 
sa 
and 
sb 
trac 
light 
states 


ab

p(sjv1:t),p(sjv1:t) 
plotted 
below. 
(c): 
Smoothed 
ows 
hhtjv1:T 
) 
and 
corresponding 
smoothed 
switch 


tt 


states 
p(stjv1:T 
) 
using 
a 
Gaussian 
Sum 
smoothing 
approximation 
(EC 
with 
J 
= 
1). 


020406080100120140160180200-40-30-20-1001002040608010012014016018020000.20.40.60.81
Figure 
25.7: 
The 
top 
panel 
is 
a 
time 
series 
of 
`prices'. 
The 
prices 
tend 
to 
keep 
going 
up 
or 
down 
with 
infrequent 
changes 
in 
the 
direction. 
Based 
on 
tting 
a 
simple 
SLDS 
model 
to 
capture 
this 
kind 
of 
behaviour, 
the 
probability 
of 
a 
signicant 
change 
in 
the 
price 
direction 
is 
given 
in 
the 
panel 
below 
based 
on 
the 
smoothed 
distribution 
p(st 
=2jv1:T 
). 


25.5 
Reset 
Models 
Reset 
models 
are 
special 
switching 
models 
in 
which 
the 
switch 
state 
isolates 
the 
present 
from 
the 
past, 
resetting 
the 
position 
of 
the 
latent 
dynamics 
(these 
are 
also 
known 
as 
changepoint 
models). 
Whilst 
these 
models 
are 
rather 
general, 
it 
can 
be 
helpful 
to 
consider 
a 
specic 
model, 
and 
here 
we 
consider 
the 
SLDS 
changepoint 
model 
with 
two 
states. 
We 
use 
the 
state 
st 
= 
0 
to 
denote 
that 
the 
LDS 
continues 
with 
the 
standard 
dynamics. 
With 
st 
= 
1, 
however, 
the 
continuous 
dynamics 
is 
reset 
to 
a 
prior: 




p0(htjht..1) 
st 
=0 


p(htjht..1;st) 
=(25.5.1)

p1(ht) 
st 
=1 


where 


..

..



0 


1 


p 
0(htjht..1)= 
Nht 


Aht..1 
+ 
µ 
, 
0;p 
1(ht)= 
Nht 


µ 
, 
1(25.5.2) 


similarly 
we 
write 




p0(vtjht) 
st 
=0 


p(vtjht;st) 
=(25.5.3)

p1(vtjht) 
st 
=1 


For 
simplicity 
we 
assume 
the 
switch 
dynamics 
are 
rst 
order 
Markov 
with 
transition 
p(stjst..1). 
Under 
this 
model 
the 
dynamics 
follows 
a 
standard 
LDS, 
but 
when 
st 
= 
1, 
ht 
is 
reset 
to 
a 
value 
drawn 
from 


468 
DRAFT 
March 
9, 
2010 



Reset 
Models 


h2
v2
h3
v3
h4
v4
the 
state 
dynamics. 
The 
ht 
are 
continuous 
variables, 
and 
vt 
continuous 
observations. 
If 
the 
dynamics 
resets, 
the 
dependence 
of 
the 
continuous 
ht 
on 
the 
past 
is 
cut. 


h1 


c1c2c3c4
Figure 
25.8: 
The 
independence 
structure 
of 
a 
reset 
model. 
Square 
nodes 
ct 
denote 
the 
binary 
reset 
variables 
and 
st 


v1
a 
Gaussian 
distribution, 
independent 
of 
the 
past. 
Such 
models 
might 
be 
of 
interest 
in 
prediction 
where 
the 
time-series 
is 
following 
a 
trend 
but 
suddenly 
changes 
and 
the 
past 
is 
forgotten. 
Whilst 
this 
ma 
not

y
y
..X

seem 
like 
a 
big 
change 
to 
the 
model, 
this 
model 
is 
computationally 
more 
tractable, 
scaling 
with
XOT 
2 
,

..X

compared 
to 
OT 
2T 
in 
the 
general 
two-state 
SLDS. 
To 
see 
this, 
consider 
the 
ltering 
recursion 


Z

X

(ht;st) 
/p(vtjht;st)p(htjht..1;st)p(stjst..1)(ht..1;st..1) 
(25.5.4) 


ht..1

st..1 


We 
now 
consider 
the 
two 
cases 


Z

X

(ht;st 
= 
0) 
/p 
0(vtjht)p 
0(htjht..1)p(st 
=0jst..1)(ht..1;st..1) 
(25.5.5) 


ht..1

st..1 


Z

X

(ht;st 
= 
1) 
. 
p 
1(vtjht)p 
1(ht)p(st 
=1jst..1)(ht..1;st..1) 


ht..1

st..1 


X

. 
p 
1(vtjht)p 
1(ht)p(st 
=1jst..1)(st..1) 
(25.5.6) 


st..1 


Equation(25.5.6) 
shows 
that 
p(ht;st 
=1jv1:t) 
is 
not 
a 
mixture 
model 
in 
ht, 
but 
contains 
only 
a 
single 
component 
proportional 
to 
p1(vtjht)p1(ht). 
If 
we 
use 
this 
information 
in 
equation 
(25.5.5) 
we 
have 


Z

(ht;st 
= 
0) 
/p 
0(vtjht)p 
0(htjht..1)p(st 
=0jst..1 
= 
0)(ht..1;st..1 
= 
0) 


ht..1 


Z

+p 
0(vtjht)p 
0(htjht..1)p(st 
=0jst..1 
= 
1)(ht..1;st..1 
= 
1) 
(25.5.7) 


ht..1 


Assuming 
(ht..1;st..1 
= 
0) 
is 
a 
mixture 
distribution 
with 
K 
components, 
then 
(ht;st 
= 
0) 
will 
be 
a 
mixture 
with 
K 
+ 
1 
components. 
In 
general, 
therefore, 
(ht;st 
= 
0) 
will 
contain 
T 
components 
and 
(ht;st 
= 
1) 
a 
single 
component. 
As 
opposed 
to 
the 
full 
SLDS 
case, 
the 
number 
of 
components 
therefore 
grows 
only 
linearly 
with 
time, 
as 
opposed 
to 
exponentially. 
This 
means 
that 
the 
computational 
eort 
to

..X

perform 
exact 
ltering 
scales 
as 
OT 
2 
. 


Run-length 
formalism 


One 
may 
also 
describe 
reset 
models 
using 
a 
`run-length’ 
formalism 
using 
at 
each 
time 
t 
a 
latent 
variable 
rt 
which 
describes 
the 
length 
of 
the 
current 
segment[3]. 
If 
there 
is 
a 
change, 
the 
run-length 
variable 
is 
reset 
to 
zero, 
otherwise 
it 
is 
increased 
by 
1: 




Pcp 
rt 
=0 


p(rtjrt..1) 
=(25.5.8)

1 
- 
Pcp 
rt 
= 
rt..1 
+1 


where 
Pcp 
is 
the 
probability 
of 
a 
reset 
(or 
`changepoint'). 
The 
joint 
distribution 
is 
given 
by 


Y

p(v1:T 
;r1:T 
)=p(rtjrt..1)p(vtjv1:t..1;rt);p(vtjv1:t..1;rt)= 
p(vtjvt..rt:t..1) 
(25.5.9) 
t 


DRAFT 
March 
9, 
2010 



Reset 
Models 


with 
the 
understanding 
that 
if 
rt 
= 
0 
then 
p(vtjvt..rt:t..1)= 
p(vt). 
The 
graphical 
model 
of 
this 
distribution 
is 
awkward 
to 
draw 
since 
the 
number 
of 
links 
depends 
on 
the 
run-length 
rt. 
Predictions 
can 
be 
made 
using 


X

p(vt+1jv1:t)=p(vt+1jvt..rt:t)p(rtjv1:t) 
(25.5.10) 


rt 


where 
the 
ltered 
`run-length’ 
p(rtjv1:t) 
is 
given 
by 
the 
forward 
recursion: 


XX

p(rt;v1:t)=p(rt;rt..1;v1:t..1;vt)=p(rt;vtjrt..1;v1:t..1)p(rt..1;v1:t..1) 


rt..1 
rt..1 


X

=rt..
1
;v1:t..1)p(rtjrt..1;

v1:t..1)p(rt..1;v1:t..1)

p(vtjrt;

rt..1 


X

=p(rtjrt..1)p(vtjvt..rt:t..1)p(rt..1;v1:t..1) 


rt..1 


..

which 
shows 
that 
ltered 
inference 
scales 
with 
OT 
2. 


25.5.1 
A 
Poisson 
reset 
model 
The 
changepoint 
structure 
is 
not 
limited 
to 
conditionally 
Gaussian 
cases 
only. 
To 
illustrate 
this, 
we 
consider 
the 
following 
model2: 
At 
each 
time 
t, 
we 
observe 
a 
count 
yt 
which 
we 
assume 
is 
Poisson 
distributed 
with 
an 
unknown 
positive 
intensity 
h. 
The 
intensity 
is 
constant, 
but 
at 
certain 
unknown 
times 
t, 
it 
jumps 
to 
a 
new 
value. 
The 
indicator 
variable 
ct 
denotes 
whether 
time 
t 
is 
such 
a 
changepoint 
or 
not. 
Mathematically, 
the 
model 
is: 


p(h0)= 
G(h0; 
a0;b0) 
(25.5.11) 


p(ct)= 
BE(ct; 
) 
(25.5.12) 


p(htjht..1;ct)= 
I[ct 
= 
0] 
(ht;ht..1)+ 
I[ct 
= 
1] 
G(ht; 
, 
b) 
(25.5.13) 


p(vtjht)= 
PO(vt; 
ht) 
(25.5.14) 


The 
symbols 
G, 
BE 
and 
PO 
denote 
the 
Gamma, 
Bernoulli 
and 
the 
Poisson 
distributions 
respectively: 


G(h; 
a, 
b) 
= 
exp 
((a 
- 
1) 
log 
h 
- 
bh 
- 
log 
..(a)+ 
a 
log 
b) 
(25.5.15) 


BE(c; 
) 
= 
exp(c 
log 
p 
+ 
(1 
- 
c) 
log(1 
- 
)) 
(25.5.16) 


PO(v; 
h) 
= 
exp(v 
log 
h 
- 
h 
- 
log 
..(v 
+ 
1)) 
(25.5.17) 


Given 
observed 
counts 
v1:T 
, 
the 
task 
is 
to 
nd 
the 
posterior 
probability 
of 
a 
change 
and 
the 
associated 
intensity 
levels 
for 
each 
region 
between 
two 
consecutive 
changepoints. 
Plugging 
the 
above 
denitions 
in 
the 
generic 
updates 
equation 
(25.5.5) 
and 
equation 
(25.5.6), 
we 
see 
that 
(ht;ct 
= 
0) 
is 
a 
Gamma 
potential, 
and 
that 
(gt;ct 
= 
1) 
is 
a 
mixture 
of 
Gamma 
potentials, 
where 
a 
Gamma 
potential 
is 
dened 
as 


(h)= 
e 
lG(h; 
a, 
b) 
(25.5.18) 


via 
the 
triple 
(a, 
b, 
l). 
For 
the 
corrector 
update 
step 
we 
need 
to 
calculate 
the 
product 
of 
a 
Poisson 
term 
with 
the 
observation 
model 
p(vtjht)= 
PO(vt; 
ht). 
A 
useful 
property 
of 
the 
Poisson 
distribution 
is 
that, 
given 
the 
observation, 
the 
latent 
variable 
is 
Gamma 
distributed: 


PO(v; 
h)= 
v 
log 
h 
- 
h 
- 
log 
..(v 
+ 
1) 
(25.5.19) 


=(v 
+1 
- 
1) 
log 
h 
- 
h 
- 
log 
..(v 
+ 
1) 
(25.5.20) 


= 
G(h; 
v 
+1, 
1) 
(25.5.21) 


Hence, 
the 
update 
equation 
requires 
multiplication 
of 
two 
Gamma 
potentials. 
A 
nice 
property 
of 
the 
Gamma 
density 
is 
that 
the 
product 
of 
two 
Gamma 
densities 
is 
also 
a 
Gamma 
potential: 


(a1;b1;l1) 
× 
(a2;b2;l2)=(a1 
+ 
a2 
- 
1;b1 
+ 
b2;l1 
+ 
l2 
+ 
g(a1;b1;a2;b2)) 
(25.5.22) 


2This 
example 
is 
due 
to 
Taylan 
Cemgil. 


DRAFT 
March 
9, 
2010 



Reset 
Models 


where 
..(a1 
+ 
a2 
..61) 


g(a1;b1;a2;b2) 
6log 
+ 
log(b1 
+ 
b2)+ 
a1 
log(b1=(b1 
+ 
b2)) 
+ 
a2 
log(b2=(b1 
+ 
b2)) 
(25.5.23) 


..(a1)..(a2) 
The 
a 
recursions 
for 
this 
reset 
model 
are 
therefore 
closed 
in 
the 
space 
of 
a 
mixture 
of 
Gamma 
potentials, 
with 
an 
additional 
Gamma 
potential 
in 
the 
mixture 
at 
each 
timestep. 
A 
similar 
approach 
can 
be 
used 
to 
form 
the 
smoothing 
recursions. 


Example 
107 
(Coal 
mining 
disasters). 
We 
illustrate 
the 
algorithm 
on 
the 
coal 
mining 
disaster 
dataset 
[144]. 
The 
data 
set 
consists 
of 
the 
number 
of 
deadly 
coal-mining 
disasters 
in 
England 
per 
year 
over 
a 
time 
span 
of 
112 
years 
from 
1851 
to 
1962. 
It 
is 
widely 
agreed 
in 
the 
statistical 
literature 
that 
a 
change 
in 
the 
intensity 
(the 
expected 
value 
of 
the 
number 
of 
disasters) 
occurs 
around 
the 
year 
1890, 
after 
new 
health 
and 
safety 
regulations 
were 
introduced. 
In 
g(25.9) 
we 
show 
the 
marginals 
p(htjy1:T 
) 
along 
with 
the 
ltering 
density. 
Note 
that 
we 
are 
not 
constraining 
the 
number 
of 
changepoints 
and 
in 
principle 
allow 
any 
number. 
The 
smoothed 
density 
suggests 
a 
sharp 
decrease 
around 
t 
= 
1890. 


186018701880189019001910192019301940195019600246# of accidentsfiltered intensity18601870188018901900191019201930194019501960246smoothed intensityyear18601870188018901900191019201930194019501960246
Figure 
25.9: 
Estimation 
of 
change 
points. 
(Top) 
coal 
mining 
disaster 
dataset. 
(Middle) 
Filtered 
estimate 
of 
the 
marginal 
intensity 
p(htjv1:t) 
and 
(Bottom) 
smoothed 
estimate 
p(htjv1:T 
). 
We 
evaluate 
these 
mixture 
of 
Gamma 
distributions 
on 
a 
xed 
grid 
of 
h 
and 
show 
the 
density 
as 
a 
function 
of 
t. 
Here, 
darker 
color 
means 
higher 
probability. 


25.5.2 
HMM-reset 
The 
reset 
model 
dened 
by 
equations 
(25.5.1,25.5.3) 
above 
is 
useful 
in 
many 
applications, 
but 
is 
limited 
since 
only 
a 
single 
dynamical 
model 
is 
considered. 
An 
important 
extension 
is 
to 
consider 
a 
set 
of 
available 
dynamical 
models, 
indexed 
by 
st 
2f1;:::;Sg, 
with 
a 
reset 
that 
cuts 
dependency 
of 
the 
continuous 
variable 
on 
the 
past[93, 
57]: 




p0(htjht..1;st) 
ct 
=0 


p(htjht..1;st;ct) 
=(25.5.24)

p1(htjst) 
ct 
=1 


The 
states 
st 
follow 
a 
Markovian 
dynamics 
p(stjst..1;ct..1), 
see 
g(25.10). 
A 
reset 
occurs 
if 
the 
state 
st 
changes, 
otherwise, 
no 
reset 
occurs: 


p(ct 
=1jst;st..1)= 
I[st6(25.5.25)

= 
st..1] 


..

The 
computational 
complexity 
of 
ltering 
for 
this 
model 
is 
OS2T 
2which 
can 
be 
understood 
by 
analogy 
with 
the 
reset 
a 
recursions, 
equations(25.5.5,25.5.6) 
on 
replacing 
ht 
by 
(ht;st). 
To 
see 
this 
we 
consider 
the 
ltering 
recursion 
for 
the 
two 
cases 


Z

X

(ht;st;ct 
= 
0) 
=p 
0(vtjht;st)p 
0(htjht..1;st)p(stjst..1;ct..1)p(ct 
=0jst;st..1)(ht..1;ct..1) 


ht..1

st..1;ct..1 


(25.5.26) 
Z

X

(ht;st;ct 
= 
1) 
=p 
1(vtjht;st)p 
1(htjst)p(stjst..1;ct)p(ct 
=1jst;st..1)(ht..1;st..1;ct..1) 


ht..1

st..1;ct..1 


X

= 
p 
1(vtjht;st)p 
1(htjst)p(ct 
=1jst;st..1)p(stjst..1;ct..1)(st..1;ct..1) 


st..1;ct..1 


(25.5.27) 
DRAFT 
March 
9, 
2010 



Exercises 


c1c2c3c4
s1
h1
v1
s2
h2
v2
s3
h3
v3
s4
h4
v4
Figure 
25.10: 
The 
independence 
structure 
of 
a 
HMM-reset 
model. 
Square 
nodes 
ct 
denote 
discrete 
switch 
variables; 
ht 
are 
continuous 
latent 
variables, 
and 
vt 
continuous 
observations. 
The 
discrete 
state 
ct 
determines 
which 
Linear 
Dynamical 
system 
from 
a 
nite 
set 
of 
Linear 
Dynamical 
systems 
is 
operational 
at 
time 
t. 


From 
equation 
(25.5.27) 
we 
see 
that 
(ht;st;ct 
= 
1) 
contains 
only 
a 
single 
component 
proportional 
to 
p1(vtjht;st)p1(htjst). 
This 
is 
therefore 
exactly 
analogous 
to 
the 
standard 
reset 
model, 
except 
that 
we 
need 
now 
to 
index 
a 
set 
of 
messages 
with 
st, 
therefore 
each 
message 
taking 
O 
(S) 
steps 
to 
compute. 
The 


..P

computational 
eort 
to 
perform 
exact 
ltering 
scales 
as 
OS2T 
2 
. 


25.6 
Code 
SLDSforward.m: 
SLDS 
forward 
SLDSbackward.m: 
SLDS 
backward 
(Expectation 
Correction) 
mix2mix.m: 
Collapse 
a 
mixture 
of 
Gaussians 
to 
a 
smaller 
mixture 
of 
Gaussians 
SLDSmargGauss.m: 
Marginalise 
an 
SLDS 
Gaussian 
mixture 
logeps.m: 
Logarithm 
with 
oset 
to 
deal 
with 
log(0) 
demoSLDStraffic.m: 
Demo 
of 
Trac 
Flow 
using 
a 
switching 
Linear 
Dynamical 
System 


25.7 
Exercises 
Exercise 
239. 
Consider 
the 
setup 
described 
in 
example(106), 
for 
which 
the 
full 
SLDS 
model 
is 
given 
in 
SLDSpricemodel.m, 
following 
the 
notation 
used 
in 
demoSLDStraffic.m. 
Given 
the 
data 
in 
the 
vect
or 
v 
your 
task 
is 
to 
t 
a 
prediction 
model 
to 
the 
data. 
To 
do 
so, 
approximate 
the 
ltered 
distribution 
p(h(t);s(t)jv1:t) 
using 
a 
mixture 
of 
I 
=2 
components. 
The 
prediction 
of 
the 
price 
at 
the 
next 
day 
is 
then 


vpred(t 
+1) 
= 
hh1(t)+ 
h2(t)i(25.7.1)

p(h(t)jv1:t) 


P

where 
p(h(t)jv1:t)=p(h(t);stjv1:t). 


st 


1. 
Compute 
the 
mean 
prediction 
error 
mean_abs_pred_error=mean(abs(vpred(2:end)-v(2:end))) 


2. 
Compute 
the 
mean 
naive 
prediction 
error 
mean_abs_pred_error_naive=mean(abs(v(1:end-1)-v(2:end))) 


which 
corresponds 
to 
saying 
that 
tomorrow's 
price 
will 
be 
the 
same 
as 
today's. 


You 
might 
nd 
SLDSmargGauss.m 
of 
interest. 


Exercise 
240. 
The 
data 
in 
g(25.11) 
are 
observed 
prices 
from 
an 
intermittent 
mean-reverting 
process, 
contained 
in 
meanrev.mat. 
There 
are 
two 
states 
S 
=2. 
There 
is 
a 
true 
(latent) 
price 
pt 
and 
an 
observed 
price 
vt 
(which 
is 
plotted). 
When 
s 
=1, 
the 
true 
underlying 
price 
reverts 
back 
towards 
the 
mean 
m 
= 
10 
with 
rate 
r 
=0:9. 
Otherwise 
the 
true 
price 
follows 
a 
random 
walk: 




r(pt..1 
- 
m)+ 
m 
+ 
p 
st 
=1 


t

pt 
=(25.7.2)

pt..1 
+ 
p 
st 
=2 


t 


DRAFT 
March 
9, 
2010 



Exercises 


0501001502002503003504009.51010.511
Figure 
25.11: 
Data 
from 
an 
intermittent 
mean-reverting 
process. 
See 
exercise(240). 


where 




N 
(p 


0, 
0:0001) 
st 
=1 


pt 


(25.7.3)
t 
N 
(p 


0, 
0:01) 
st 
=2 


t 


The 
observed 
price 
vt 
is 
related 
to 
the 
unknown 
price 
pt 
by 


vt 
N 
(vt 


pt, 
0:001) 
(25.7.4) 


It 
is 
known 
that 
95% 
of 
the 
time 
st+1 
is 
in 
the 
same 
state 
as 
at 
time 
t 
and 
that 
at 
time 
t 
=1 
either 
state 
of 
s 
is 
equally 
likely. 
Also 
at 
t 
=1, 
p1 
N 
(p1 


m, 
0:1). 
Based 
on 
this 
information, 
and 
using 
Gaussian 
Sum 
ltering 
with 
I 
=2 
components 
(use 
SLDSforward.m), 
what 
is 
the 
probability 
at 
time 
t 
= 
280 
that 
the 
dynamics 
is 
following 
a 
random 
walk, 
p(s280 
=2jv1:280)? 
Repeat 
this 
computation 
for 
smoothing 
p(s280 
=2jv1:400) 
based 
on 
using 
Expectation 
Correction 
with 
I 
= 
J 
=2 
components. 


DRAFT 
March 
9, 
2010 



Exercises 


DRAFT 
March 
9, 
2010 



CHAPTER 
26 


Distributed 
Computation 


26.1 
Introduction 
How 
natural 
organisms 
process 
information 
is 
a 
fascinating 
subject 
and 
one 
of 
the 
grand 
challenges 
of 
science. 
Whilst 
this 
subject 
is 
still 
in 
its 
early 
stages, 
loosely 
speaking, 
there 
are 
some 
generic 
properties 
that 
most 
such 
systems 
are 
believed 
to 
possess: 
patterns 
are 
stored 
in 
a 
set 
of 
neurons; 
recall 
of 
patterns 
is 
robust 
to 
noise; 
transmission 
between 
neurons 
is 
of 
a 
binary 
nature 
and 
is 
stochastic; 
information 
processing 
is 
distributed 
and 
highly 
modular. 
In 
this 
chapter 
we 
discuss 
some 
of 
the 
classical 
toy 
models 
that 
have 
been 
developed 
as 
a 
test 
bed 
for 
analysing 
such 
properties[62, 
76, 
65, 
132]. 
In 
particular 
we 
discuss 
some 
classical 
models 
from 
a 
probabilistic 
viewpoint. 


26.2 
Stochastic 
Hopeld 
Networks 
Hopeld 
networks 
are 
models 
of 
biological 
memory 
in 
which 
a 
pattern 
is 
represented 
by 
the 
activity 
of 
a 
set 
of 
V 
interconnected 
neurons. 
The 
term 
`network’ 
here 
refers 
to 
the 
set 
of 
neurons, 
see 
g(26.1), 
and 
not 
the 
Belief 
Network 
representation 
of 
distribution 
of 
neural 
states 
unrolled 
through 
time, 
g(26.2). 
At 
time 
t 
neuron 
i 
res 
vi(t) 
= 
+1 
or 
is 
quiescent 
vi(t)= 
..1 
(not 
ring) 
depending 
on 
the 
states 
of 
the 
neurons 
at 
the 
preceding 
time 
t 
- 
1. 
Explicitly, 
neuron 
i 
res 
depending 
on 
the 
potential 


V

v 


ai(t) 
= 
i 
+ 
wijvj(t) 
(26.2.1) 
j=1 


where 
wij 
characterizes 
the 
ecacy 
with 
which 
neuron 
j 
transmits 
a 
binary 
signal 
to 
neuron 
i. 
The 
threshold 
i 
relates 
to 
the 
neuron's 
predisposition 
to 
ring. 
Writing 
the 
state 
of 
the 
network 
at 
time 
t 
as 
v(t) 
= 
(v1(t);:::;vV 
(t)), 
the 
probability 
that 
neuron 
i 
res 
at 
time 
t 
+ 
1 
is 
modelled 
as 


p(vi(t 
+1) 
= 
1jv(t)) 
= 
ß 
(ai(t)) 
(26.2.2) 


where 
(x)=1=(1+e..x) 
and 
ß 
controls 
the 
level 
of 
stochastic 
behaviour 
of 
the 
neuron. 
The 
probability 
of 
being 
in 
the 
quiescent 
state 
is 
given 
by 
normalization 


p(vi(t 
+1) 
= 
..1jv(t)) 
= 
1 
- 
p(vi(t 
+1) 
= 
1jv(t)) 
= 
1 
- 
ß 
(ai(t)) 
(26.2.3) 


These 
two 
rules 
can 
be 
compactly 
written 
as 


p(vi(t 
+ 
1)jv(t)) 
= 
ß 
(vi(t 
+ 
1)ai(t)) 
(26.2.4) 


which 
follows 
directly 
from 
1 
- 
(x)= 
(..x). 


475 



Learning 
Sequences 


v3
v1


v4 


Figure 
26.1: 
A 
depiction 
of 
a 
Hopeld 
network 
(for 
5 
neurons). 
The 
connectivity 
of 
the 
neurons 
is 
described 
by 
a 
weight 
matrix 
with 
elements 
wij. 
The 
graph 
represents 
a 
snapshot 
of 
the 
state 
v2 
of 
all 
neurons 
at 
time 
t 
which 
simultaneously 
update 
as 
function 


of 
the 
network 
at 
the 
previous 
time 
t 
- 
1. 


v4 


In 
the 
limit 
ß 
!1, 
the 
neuron 
updates 
deterministically 


vi(t 
+1) 
= 
sgn(ai(t)) 
(26.2.5) 


In 
a 
synchronous 
Hopeld 
network, 
all 
neurons 
update 
independently 
and 
simultaneously, 
so 
that 
we 
can 
represent 
the 
temporal 
evolution 
of 
the 
neurons 
as 
a 
dynamic 
Bayes 
network, 
g(26.2) 


V

YX

p(v(t 
+ 
1)jv(t)) 
= 
p(vi(t 
+ 
1)jv(t)). 
(26.2.6) 


i=1 


Given 
this 
toy 
description 
of 
how 
neurons 
update, 
how 
can 
we 
use 
the 
network 
to 
do 
interesting 
things, 
for 
example 
to 
store 
a 
set 
of 
patterns 
and 
recall 
them 
under 
some 
cue. 
The 
patterns 
will 
be 
stored 
in 
the 
weights 
and 
in 
the 
following 
section 
we 
address 
how 
to 
learn 
suitable 
parameters 
wij 
and 
i 
to 
learn 
temporal 
sequences 
based 
on 
a 
simple 
local 
learning 
rule. 


26.3 
Learning 
Sequences 
26.3.1 
A 
single 
sequence 
Given 
a 
sequence 
of 
network 
states, 
V 
= 
fv(1);:::, 
v(T 
)g, 
we 
would 
like 
the 
network 
to 
`store’ 
this 
sequence 
such 
that 
it 
can 
be 
recalled 
under 
some 
cue. 
That 
is, 
if 
the 
network 
is 
initialized 
in 
the 
correct 
starting 
state 
of 
the 
training 
sequence 
v(t 
= 
1), 
the 
remainder 
of 
the 
training 
sequence 
for 
t> 
1 
should 
be 
reproduced 
under 
the 
deterministic 
dynamics 
equation 
(26.2.5), 
without 
error. 


Two 
classical 
approaches 
to 
learning 
a 
temporal 
sequence 
are 
the 
Hebb1 
and 
Pseudo 
Inverse 
rules[132]. 
In 
both 
the 
standard 
Hebb 
and 
PI 
cases, 
the 
thresholds 
i 
are 
usually 
set 
to 
zero. 


Standard 
Hebb 
rule 


T 
..1

wij 
= 
1 
V 
Xvi(t 
+ 
1)vj(t) 
(26.3.1) 
t=1 


1Donald 
Hebb, 
a 
neurobiologist 
actually 
stated[127] 


Let 
us 
assume 
that 
the 
persistence 
or 
repetition 
of 
a 
reverberatory 
activity 
(or 
`trace') 
tends 
to 
induce 
lasting 
cellular 
changes 
that 
add 
to 
its 
stability. 
. 
. 
When 
an 
axon 
of 
cell 
A 
is 
near 
enough 
to 
excite 
a 
cell 
B 
and 
repeatedly 
or 
persistently 
takes 
part 
in 
ring 
it, 
some 
growth 
process 
or 
metabolic 
change 
takes 
place 
in 
one 
or 
both 
cells 
such 
that 
A's 
eciency, 
as 
one 
of 
the 
cells 
ring 
B, 
is 
increased. 


This 
statement 
is 
sometimes 
interpreted 
to 
mean 
that 
weights 
are 
exclusively 
of 
the 
correlation 
form 
equation 
(26.3.1)(see 
[270] 
for 
a 
discussion). 
This 
can 
severely 
limit 
the 
performance 
and 
introduce 
adverse 
storage 
artifacts 
including 
local 
minima[132]. 


DRAFT 
March 
9, 
2010 



Learning 
Sequences 


v1(t)
v2(t)
v3(t)
v4(t)
v1(t+1)
v2(t+1)
v3(t+1)
v4(t+1)
Figure 
26.2: 
A 
Dynamic 
Bayesian 
Network 
representation 
of 
a 
Hopeld 
Network. 
The 
network 
operates 
by 
simultaneously 
generating 
a 
new 
set 
of 
neuron 
states 
according 
to 
equation 
(26.2.6). 
Equation(26.2.6) 
denes 
a 
Markov 
transition 
matrix, 
modelling 
the 
transition 
probability 
v(t) 
!6v(t+1) 
and 
furthermore 
imposes 
the 
constraint 
that 
the 
neurons 
are 
conditionally 
independent 
given 
the 
previous 
state 
of 
the 
network. 


The 
Hebb 
rule 
can 
be 
motivated 
mathematically 
by 
considering

1 
T 
..1

XXX

wijvj(t)= 
vi(t 
+ 
1)vj(t 
)vj(t) 
(26.3.2)

V 


j=1 
j 


T 
..1

XXX

1 
2 
1 


= 
vi(t 
+ 
1)vj 
(t)+ 
vi(t 
+ 
1)vj()vj(t) 
(26.3.3)

VV 


j6j

=t 
1 
T 
..1

XX

= 
vi(t 
+ 
1) 
+ 
vi(t 
+ 
1)vj()vj(t) 
(26.3.4)

V 


6j

=t 


If 
the 
patterns 
are 
uncorrelated 
then 
the 
`interference’ 
term 


T 
..1

XX

O 
6vi(t 
+ 
1)vj(t 
)vj(t)=V 
(26.3.5) 
=1 
j 


will 
be 
relatively 
small. 
To 
see 
this, 
we 
rst 
note 
that 
for 
randomly 
drawn 
patterns, 
the 
mean 
of 
O 
is 
zero, 
since 
6

= 
t 
and 
the 
patterns 
are 
randomly 
1. 
The 
variance 
is 
therefore 
given 
by

T 
..1


1 
XX



2= 
V 
2 
vi(t 
+ 
1)vi(t 
06+ 
1)vj(t 
)vj(t)vk(0)vk(t)(26.3.6) 
;06j;k

=t

For 
j6

= 
k, 
all 
the 
terms 
are 
independent 
and 
contribute 
zero 
on 
average. 
Therefore

T 
..1


1 
XX


2


2= 
V 
2 
vi(t 
+ 
1)vi(06+ 
1)vj(t 
)vj(0)v 
(t)(26.3.7)

j 
;0=1j


When 
6

= 
t 
06all 
the 
terms 
are 
independent 
zero 
mean 
and 
contribute 
zero. 
Hence


1 
XX
T 
..61

2 
22


2= 
V 
2v 
(t 
+ 
1)v 
(t 
)v 
(t)= 
(26.3.8)

i 
jj 


V 


6j

=t

Provided 
that 
the 
number 
of 
neurons 
V 
is 
signicantly 
larger 
than 
the 
length 
of 
the 
sequence, 
T 
, 
then 
the 
average 
size 
of 
the 
interference 
will 
be 
small. 
In 
this 
case 
the 
term 
vi(t+1) 
in 
equation 
(26.3.4) 
dominates, 


PX

meaning 
that 
the 
sign 
of 
j 
wijvj(t) 
will 
be 
that 
of 
vi(t 
+ 
1), 
and 
the 
correct 
pattern 
sequence 
recalled. 
The 
Hebb 
rule 
is 
capable 
of 
storing 
a 
random 
(uncorrelated) 
temporal 
sequence 
of 
length 
0:269V 
time 
steps[85]. 
However, 
the 
Hebb 
rule 
performs 
poorly 
for 
the 
case 
of 
correlated 
patterns 
since 
interference 
from 
the 
other 
patterns 
becomes 
signicant[132, 
65]. 


Pseudo 
inverse 
rule 


The 
PI 
rule 
nds 
a 
matrix 
[W]ij 
= 
wij 
that 
solves 
the 
linear 
equations

X

wijvj(t)= 
vi(t 
+ 
1);t 
=1;:::;T 
..61 
(26.3.9) 
j 


DRAFT 
March 
9, 
2010 



Learning 
Sequences 


P

Under 
this 
condition 
sgnj 
wijvj(t)= 
sgn(vi(t 
+ 
1)) 
= 
vi(t 
+ 
1) 
so 
that 
patterns 
will 
be 
correctly 
recalled. 
In 
matrix 
notation 
we 
require 


^

WV 
= 
V 
(26.3.10) 


where 


hi

^

= 
vi(t);t 
=1;:::;T 
- 
1;V= 
vi(t 
+ 
1);t 
=2;:::;T 
(26.3.11)

[V]it 


it 


For 
T 
<V 
the 
problem 
is 
under-determined. 
One 
solution 
is 
given 
by 
the 
pseudo 
inverse: 


..1

^VTVVT

W 
= 
V(26.3.12) 


The 
Pseudo 
Inverse 
(PI) 
rule 
can 
store 
any 
sequence 
of 
V 
linearly 
independent 
patterns. 
Whilst 
attractive 
compared 
to 
the 
standard 
Hebb 
in 
terms 
of 
its 
ability 
to 
store 
longer 
correlated 
sequences, 
this 
rule 
suers 
from 
very 
small 
basins 
of 
attraction 
for 
temporally 
correlated 
patterns, 
see 
g(26.3). 


The 
maximum 
likelihood 
Hebb 
rule 


An 
alternative 
to 
the 
above 
classical 
algorithms 
is 
to 
view 
this 
as 
a 
problem 
of 
pattern 
storage 
in 
the 
DBN, 
equation 
(26.2.6) 
[19]. 
First, 
we 
need 
to 
clarify 
what 
we 
mean 
by 
`store'. 
Given 
that 
we 
initialize 
the 
network 
in 
a 
state 
v(t 
= 
1), 
we 
wish 
that 
the 
remaining 
sequence 
will 
be 
generated 
with 
high 
probability. 
That 
is, 
we 
wish 
to 
adjust 
the 
network 
parameters 
such 
that 
the 
probability 


p(v(T 
), 
v(T 
- 
1);:::, 
v(2)jv(1)) 
(26.3.13) 


is 
maximal2 
. 
Furthermore, 
we 
might 
hope 
that 
the 
sequence 
will 
be 
recalled 
with 
high 
probability 
not 
just 
when 
initialized 
in 
the 
correct 
state 
but 
also 
for 
states 
close 
(in 
Hamming 
distance) 
to 
the 
correct 
initial 
state 
v(1). 


Due 
to 
the 
Markov 
nature 
of 
the 
dynamics, 
the 
conditional 
likelihood 
is 


T 
..1

YP

p(v(T 
), 
v(T 
- 
1);:::, 
v(2)jv(1)) 
= 
p(v(t 
+ 
1)jv(t)) 
(26.3.14) 


t=1 


This 
is 
a 
product 
of 
transitions 
from 
given 
states 
to 
given 
states. 
Since 
these 
transition 
probabilities 
are 
known 
(26.2.6,26.2.2), 
the 
conditional 
likelihood 
can 
be 
easily 
evaluated. 
The 
sequence 
log 
(conditional) 
likelihood 
is 


T 
..1T 
..1V

YXPXXP

L(w;) 
= 
log 
p(v(t 
+ 
1)jv(t))= 
log 
p(v(t 
+ 
1)jv(t))= 
log 
ß 
(vi(t 
+ 
1)ai(t)) 
(26.3.15) 


t=1 
tt=1 
i=1 


Our 
task 
is 
then 
to 
nd 
weights 
w 
and 
thresholds 
. 
that 
maximisise 
L(w;). 
There 
is 
no 
closed 
form 
solution 
and 
the 
weights 
therefore 
need 
to 
be 
determined 
numerically. 
This 
corresponds 
to 
a 
straightforward 
computational 
problem 
since 
the 
log 
likelihood 
is 
a 
convex 
function. 
To 
show 
this, 
we 
compute 
the 
Hessian 
(neglecting 
. 
for 
expositional 
clarity 
– 
this 
does 
not 
aect 
the 
conclusions): 


T 
..1

d2L 
XP= 
..2 
(vi(t 
+ 
1)vj(t)) 
i(t)(1 
- 
i(t))vk(t 
+ 
1)vl(t)ik 
(26.3.16)
dwijdwkl 


t=1 


where 
we 
dened 
i(t) 
= 
1 
- 
ß 
(vi(t 
+ 
1)ai(t)) 
. 
(26.3.17) 


2Static 
patterns 
can 
also 
be 
considered 
in 
this 
framework 
as 
a 
set 
of 
patterns 
that 
map 
to 
each 
other. 


DRAFT 
March 
9, 
2010 



Learning 
Sequences 


Training Sequencetimeneuron number1020102030405060708090100Max Likelihood1020102030405060708090100Hebb1020102030405060708090100Pseudo Inverse1020102030405060708090100
Figure 
26.3: 
Leftmost 
panel: 
The 
temporally 
highly-
correlated 
training 
sequence 
we 
desire 
to 
store. 
The 
other 
panels 
show 
the 
temporal 
evolution 
of 
the 
network 
after 
initialization 
in 
the 
correct 
starting 
state 
but 
corrupted 
with 
30% 
noise. 
During 
recall, 
deterministic 
updates 
ß 
= 
8 
were 
used. 
The 
Maximum 
Likelihood 
rule 
was 
trained 
using 
10 
batch 
epochs 
with 
. 
=0:1. 
See 
also 


demoHopfield.m 


It 
is 
straightforward 
to 
show 
that 
the 
Hessian 
is 
negative 
denite 
(see 
exercise(244)) 
and 
hence 
the 
likelihood 
has 
a 
single 
global 
maximum. 
To 
increase 
the 
likelihood 
of 
the 
sequence, 
we 
can 
use 
a 
simple 
method 
such 
as 
gradient 
ascent3 


dL 
dL

new 
new 


wij 
= 
wij 
+ 
, 
i 
= 
i 
+ 
. 
(26.3.18)

dwij 
di 


where 


T 
..1T 
..1

dL 
dL 


= 
i(t)vi(t 
+ 
1)vj(t), 
= 
i(t)vi(t 
+ 
1) 
(26.3.19)

dwij 
di 


t=1 
t=1 


The 
learning 
rate 
. 
is 
chosen 
empirically 
to 
be 
suciently 
small 
to 
ensure 
convergence. 
The 
learning 
rule 
equation 
(26.3.19) 
can 
be 
seen 
as 
a 
modied 
Hebb 
learning 
rule, 
the 
basic 
Hebb 
rule 
being 
given 
when 
i(t) 
= 
1. 
As 
learning 
progresses, 
the 
i(t) 
will 
typically 
tend 
to 
values 
close 
to 
either 
1 
or 
0, 
and 
hence 
the 
learning 
rule 
can 
be 
seen 
as 
asymptotically 
equivalent 
to 
making 
an 
update 
only 
in 
the 
case 
of 
disagreement 
(ai(t) 
and 
vi(t 
+ 
1) 
are 
of 
dierent 
signs). 


This 
batch 
training 
procedure 
can 
be 
readily 
converted 
to 
an 
online 
in 
which 
an 
update 
occurs 
immediately 
after 
the 
presentation 
of 
two 
consecutive 
patterns. 


Storage 
capacity 
of 
the 
ML 
Hebb 
rule 


The 
ML 
Hebb 
rule 
is 
capable 
of 
storing 
a 
sequence 
of 
V 
linearly 
independent 
patterns. 
To 
see 
this, 
we 
can 
form 
an 
input-output 
training 
set 
for 
each 
neuron 
i, 
f(v(t);vi(t 
+ 
1));t 
=1;:::;T 
- 
1g. 
Each 
neuron 
has 
an 
associated 
weight 
vector 
wi 
= 
wij;j 
=1;:::;V 
, 
which 
forms 
a 
logistic 
regressor 
or, 
in 
the 
limit 
ß 
= 
1, 
a 
perceptron[132]. 
For 
perfect 
recall 
of 
the 
patterns, 
we 
therefore 
need 
only 
that 
the 
patterns 
on 
the 
sequence 
be 
linearly 
separable. 
This 
will 
be 
the 
case 
if 
the 
patterns 
are 
linearly 
independent, 
regardless 
of 
the 
outputs 
vi(t 
+ 
1);t 
=1;:::;T 
- 
1. 


Relation 
to 
the 
perceptron 
rule 


In 
the 
limit 
that 
the 
activation 
is 
large, 
jaij» 
1 


1 
vi(t 
+ 
1)ai 
< 
0 


i 
˜ 
(26.3.20)

0 
vi(t 
+ 
1)ai 
= 
0 


Provided 
the 
activation 
and 
desired 
next 
output 
are 
the 
same 
sign, 
no 
update 
is 
made 
for 
neuron 
i. 
In 
this 
limit, 
equation 
(26.3.19) 
is 
called 
the 
perceptron 
rule[132, 
80]. 
For 
an 
activation 
a 
that 
is 
close 
to 
the 


3Naturally, 
one 
can 
use 
more 
sophisticated 
methods 
such 
as 
the 
Newton 
method, 
or 
conjugate 
gradients. 
In 
theoretical 
neurobiology 
the 
emphasis 
is 
small 
gradient 
style 
updates 
since 
these 
are 
deemed 
to 
be 
biologically 
more 
plausible. 


DRAFT 
March 
9, 
2010 
479 



Learning 
Sequences 


00.050.10.150.20.250.30.350.40.450.50.50.550.60.650.70.750.80.850.90.951flip probabilityfraction correctsequence length=50Max Likelihoodnoise trained Max Likelihoodperceptron (M=10)
perceptron (M=0)
hebbpseudo inverse
(a)
Figure 
26.4: 
The 
fraction 
of 
neurons 
correct 
for 
the 
nal 
state 
of 
the 
network 
T 
= 
50 
for 
a 
100 
neuron 
Hopeld 
network 
trained 
to 
store 
a 
length 
50 
sequence 
patterns. 
After 
initialization 
in 
the 
correct 
initial 
state 
at 
t 
= 
1, 
the 
Hopeld 
network 
is 
updated 
deterministically, 
with 
a 
randomly 
chosen 
percentage 
of 
the 
neurons 
ipped 
post 
updating. 
The 
correlated 
sequence 
of 
length 
T 
= 
50 
was 
produced 
by 
ipping 
with 
probability 
0.5, 
20% 
of 
the 
previous 
state 
of 
the 
network. 
A 
fraction 
correct 
value 
of 
1 
indicates 
perfect 
recall 
of 
the 
nal 
state, 
and 
a 
value 
of 
0:5 
indicates 
a 
performance 
no 
better 
than 
random 
guessing 
of 
the 
nal 
state. 
For 
maximum 
likelihood 
50 
epochs 
of 
training 
were 
used 
with 
. 
=0:02. 
During 
recall, 
deterministic 
updates 
ß 
= 
1hwere 
used. 
The 
results 
presented 
are 
averages 
over 
5000 
simulations, 
resulting 
in 
standard 
errors 
of 
the 
order 
of 
the 
symbol 
sizes. 


decision 
boundary, 
a 
small 
change 
can 
lead 
to 
a 
dierent 
sign 
of 
the 
neural 
ring. 
To 
guard 
against 
this 
it 
is 
common 
to 
include 
a 
stability 
criterion 




1 
vi(t 
+ 
1)ai 
<M 


i 
=(26.3.21)

0 
vi(t 
+ 
1)ai 
hM 


where 
M 
is 
an 
empirically 
chosen 
positive 
threshold. 


Example 
108 
(Storing 
a 
correlated 
sequence). 
In 
g(26.3) 
we 
consider 
storage 
of 
a 
highly-correlated 
temporal 
sequence 
of 
length 
T 
= 
20 
of 
100 
neurons 
using 
the 
three 
learning 
rules: 
Hebb, 
Maximum 
Likelihood 
and 
Pseudo 
Inverse. 
The 
sequence 
is 
chosen 
to 
be 
highly 
correlated, 
which 
constitutes 
a 
dicult 
learning 
task. 
The 
thresholds 
i 
are 
set 
to 
zero 
throughout 
to 
facilitate 
comparison. 
The 
initial 
state 
of 
the 
training 
sequence, 
corrupted 
by 
30% 
noise 
is 
presented 
to 
the 
trained 
networks, 
and 
we 
desire 
that 
the 
training 
sequence 
will 
be 
generated 
from 
this 
initial 
noisy 
state. 
Whilst 
the 
Hebb 
rule 
is 
operating 
in 
a 
feasible 
limit 
for 
uncorrelated 
patterns, 
the 
strong 
correlations 
in 
this 
training 
sequence 
entails 
poor 
results. 
The 
PI 
rule 
is 
capable 
of 
storing 
a 
sequence 
of 
length 
100 
yet 
is 
not 
robust 
to 
perturbations 
from 
the 
correct 
initial 
state. 
The 
Maximum 
Likelihood 
rule 
performs 
well 
after 
a 
small 
amount 
of 
training. 


Stochastic 
interpretation 


By 
straightforward 
manipulations, 
the 
weight 
update 
rule 
in 
equation 
(26.3.19) 
can 
be 
written 
as 


T 
..1

X

dL 
1 


dwij 
=
2 
vi(t 
+ 
1) 
..hvi(t 
+ 
1)ip(vi(t+1)jai(t)) 
vj(t) 
(26.3.22) 
t=1 


A 
stochastic, 
online 
learning 
rule 
is 
therefore 


wij(t)= 
. 
(vi(t 
+ 
1) 
..hv~i(t 
+ 
1)) 
vj(t) 
(26.3.23) 


where 
~vi(t 
+ 
1) 
is 
1 
with 
probability 
(ai(t)), 
and 
..1 
otherwise. 
Provided 
that 
the 
learning 
rate 
. 
is 
small, 
this 
stochastic 
updating 
will 
approximate 
the 
learning 
rule 
(26.3.18,26.3.19). 


Example 
109 
(Recalling 
sequences 
under 
perpetual 
noise). 
We 
compare 
the 
performance 
of 
the 
Maximum 
Likelihood 
learning 
rule 
(with 
zero 
thresholds 
) 
with 
the 
standard 
Hebb, 
Pseudo 
Inverse, 
and 
Perceptron 
rule 
for 
learning 
a 
single 
temporal 
sequence. 
The 
network 
is 
initialized 
to 
a 
noise 
corrupted 


DRAFT 
March 
9, 
2010 



Learning 
Sequences 



(a) 
(b) 
Figure 
26.5: 
(a): 
Original 
T 
= 
25 
binary 
video 
sequence 
on 
a 
set 
of 
81 
× 
111 
= 
8991 
neurons. 
(b): 
The 
reconstructions 
beginning 
from 
a 
20% 
noise 
perturbed 
initial 
state. 
Every 
odd 
time 
reconstruction 
is 
also 
randomly 
perturbed. 
Despite 
the 
high 
level 
of 
noise 
the 
basis 
of 
attraction 
of 
the 
pattern 
sequence 
is 
very 
broad 
and 
the 
patterns 
immediately 
fall 
back 
close 
to 
the 
pattern 
sequence 
even 
after 
a 
single 
timestep. 


version 
of 
the 
correct 
initial 
state 
v(t 
= 
1) 
from 
the 
training 
sequence. 
The 
dynamics 
is 
then 
run 
(at 
ß 
= 
1) 
for 
the 
same 
number 
of 
steps 
as 
the 
length 
of 
the 
training 
sequence, 
and 
the 
fraction 
of 
bits 
of 
the 
recalled 
nal 
state 
which 
are 
the 
same 
as 
the 
training 
sequence 
nal 
state 
v(T 
) 
is 
measured, 
g(26.4). 
At 
each 
stage 
in 
the 
dynamics 
(except 
the 
last), 
the 
state 
of 
the 
network 
was 
corrupted 
with 
noise 
by 
ipping 
each 
neuron 
state 
with 
the 
specied 
ip 
probability. 
The 
training 
sequences 
are 
produced 
by 
starting 
from 
a 
random 
initial 
state, 
v(1), 
and 
then 
choosing 
at 
random 
20% 
percent 
of 
the 
neurons 
to 
ip, 
each 
of 
the 
chosen 
neurons 
being 
ipped 
with 
probability 
0.5, 
giving 
a 
random 
training 
sequence 
with 
a 
high 
degree 
of 
temporal 
correlation. 


The 
standard 
Hebb 
rule 
performs 
relatively 
poorly, 
particularly 
for 
small 
ip 
rates, 
whilst 
the 
other 
methods 
perform 
relatively 
well, 
being 
robust 
at 
small 
ip 
rates. 
As 
the 
ip 
rate 
increases, 
the 
pseudo 
inverse 
rule 
becomes 
unstable, 
especially 
for 
the 
longer 
temporal 
sequence 
which 
places 
more 
demands 
on 
the 
network. 
The 
perceptron 
rule 
can 
perform 
as 
well 
as 
the 
Maximum 
Likelihood 
rule, 
although 
its 
performance 
is 
critically 
dependent 
on 
an 
appropriate 
choice 
of 
the 
threshold 
M. 
The 
results 
for 
M 
= 
0 
Perceptron 
training 
are 
poor 
for 
small 
ip 
rates. 
An 
advantage 
of 
the 
Maximum 
Likelihood 
rule 
is 
that 
it 
performs 
well 
without 
the 
need 
for 
ne 
tuning 
of 
parameters. 
In 
all 
cases, 
batch 
training 
was 
used. 


An 
example 
for 
a 
larger 
network 
is 
given 
in 
g(26.5) 
which 
consists 
of 
highly 
correlated 
sequences. 
For 
such 
short 
sequences 
the 
basin 
of 
attraction 
is 
very 
large 
and 
the 
video 
sequence 
can 
be 
stored 
robustly. 


26.3.2 
Multiple 
sequences 
The 
previous 
section 
detailed 
how 
to 
train 
a 
Hopeld 
network 
for 
a 
single 
temporal 
sequence. 
We 
now 
address 
the 
learning 
a 
set 
of 
sequences 
fVn;n 
=1;:::;Ng. 
If 
we 
assume 
that 
the 
sequences 
are 
independent, 
the 
log 
likelihood 
of 
a 
set 
of 
sequences 
is 
the 
sum 
of 
the 
individual 
sequences. 
The 
gradient 
is 
given 
by 


dL 
dwij 
= 
ß 
NN 
n=1 
T 
..1N 
t=1 
n 
i 
(t)v 
n 
i 
(t 
+ 
1)v 
n 
j 
(t), 
dL 
di 
= 
ß 
NN 
n=1 
T 
..1N 
t=1 
n 
i 
(t)v 
n 
i 
(t 
+ 
1) 
(26.3.24) 
where 
n 
i 
(t) 
= 
1 
- 
ß 
(v 
n 
i 
(t 
+ 
1)a 
n 
i 
(t)) 
, 
a 
n 
i 
(t) 
= 
i 
+ 
N 
j 
wijv 
n 
j 
(t) 
(26.3.25) 


The 
log 
likelihood 
remains 
convex 
since 
it 
is 
the 
sum 
of 
convex 
functions, 
so 
that 
the 
standard 
gradient 
based 
learning 
algorithms 
can 
be 
used 
here 
as 
well. 


DRAFT 
March 
9, 
2010 



Tractable 
Continuous 
Latent 
Variable 
Models 


26.3.3 
Boolean 
networks 
The 
Hopeld 
network 
is 
one 
particular 
parameterisation 
of 
the 
table 
p(vi(t 
+1) 
= 
1jv(t)). 
However, 
less 
constrained 
parameters 
may 
be 
considered 
– 
indeed 
one 
could 
consider 
the 
fully 
unconstrained 
case 
in 
which 
each 
neuron 
i 
would 
have 
an 
associated 
2V 
parental 
states. 
This 
exponentially 
large 
number 
of 
states 
is 
impractical 
and 
an 
interesting 
restriction 
is 
to 
consider 
that 
each 
neuron 
has 
only 
K 
parents, 
so 
that 
each 
table 
contains 
2K 
entries. 
Learning 
the 
table 
parameters 
by 
Maximum 
Likelihood 
is 
straightforward 
since 
the 
log 
likelihood 
is 
a 
convex 
function 
of 
the 
table 
entries. 
Hence, 
for 
given 
any 
sequence 
(or 
set 
of 
sequences) 
one 
may 
readily 
nd 
parameters 
that 
maximise 
the 
sequence 
reconstruction 
probability. 
The 
Maximum 
Likelihood 
method 
also 
produces 
large 
basins 
of 
attraction 
for 
the 
associated 
stochastic 
dynamical 
system. 
Such 
models 
are 
of 
potential 
interest 
in 
Articial 
Life 
and 
Random 
Boolean 
networks 
in 
which 
emergent 
macroscopic 
behaviour 
appears 
from 
local 
update 
rules[158]. 


26.3.4 
Sequence 
disambiguation 
A 
limitation 
of 
rst 
order 
networks 
dened 
on 
visible 
variables 
alone 
(such 
as 
the 
Hopeld 
network) 
is 
that 
the 
observation 
transition 
p(vt+1jvt 
= 
v) 
is 
the 
same 
every 
time 
the 
joint 
state 
v 
is 
encountered. 
This 
means 
that 
if 
the 
sequence 
contains 
a 
subsequence 
such 
as 
a, 
b, 
a, 
c 
this 
cannot 
be 
recalled 
with 
high 
probability 
since 
a 
transitions 
to 
dierent 
states, 
depending 
on 
time. 
Whilst 
one 
could 
attempt 
to 
resolve 
this 
sequence 
disambiguation 
problem 
using 
a 
higher 
order 
Markov 
model 
to 
account 
for 
a 
longer 
temporal 
context, 
we 
would 
lose 
biological 
plausibility. 
Using 
latent 
variables 
is 
an 
alternative 
way 
to 
sequence 
disambiguation. 
In 
the 
Hopeld 
model 
the 
recall 
capacity 
can 
be 
increased 
using 
latent 
variables 
by 
make 
a 
sequencing 
in 
the 
joint 
latent-visible 
space 
that 
is 
linearly 
independent, 
even 
if 
the 
visible 
variable 
sequence 
alone 
is 
not. 
In 
section(26.4) 
we 
discuss 
a 
general 
method 
that 
extends 
dynamic 
Bayes 
networks 
dened 
on 
visible 
variables 
alone, 
such 
as 
the 
Hopeld 
network, 
to 
include 
continuous 
non-linearly 
updating 
latent 
variables, 
without 
requiring 
additional 
approximations. 


26.4 
Tractable 
Continuous 
Latent 
Variable 
Models 
A 
dynamic 
Bayes 
network 
with 
latent 
variables 
takes 
the 
form 


T 
..1

T 


p(v(1 
: 
T 
);h(1 
: 
T 
)) 
= 
p(v(1))p(h(1)jv(1)) 
p(v(t 
+1)jv(t);h(t))p(h(t 
+1)jv(t);v(t 
+1);h(t)) 
(26.4.1) 


t=1 


As 
we 
saw 
in 
chapter(23), 
provided 
all 
hidden 
variables 
are 
discrete, 
inference 
in 
these 
models 
is 
straightforward. 
However, 
in 
many 
physical 
systems 
it 
is 
more 
natural 
to 
assume 
continuous 
h(t). 
In 
chapter(24) 
we 
saw 
that 
one 
such 
tractable 
continuous 
h(t) 
model 
is 
given 
by 
linear 
Gaussian 
transitions 
and 
emissions 
-the 
LDS. 
Whilst 
this 
is 
useful, 
we 
cannot 
represent 
non-linear 
changes 
in 
the 
latent 
process 
using 
an 
LDS 
alone. 
The 
Switching 
LDS 
of 
chapter(25) 
is 
able 
to 
model 
non-linear 
continuous 
dynamics 
(via 
switching) 
although 
we 
saw 
that 
this 
leads 
to 
computational 
diculties. 
For 
computational 
reasons 
we 
therefore 
seem 
limited 
to 
either 
purely 
discrete 
h 
(with 
no 
limitation 
on 
the 
discrete 
transitions) 
or 
purely 
continuous 
h 
(but 
be 
forced 
to 
use 
simple 
linear 
dynamics). 
Is 
there 
a 
way 
to 
have 
a 
continuous 
state 
with 
non-linear 
dynamics 
for 
which 
posterior 
inference 
remains 
tractable? 
The 
answer 
is 
yes, 
provided 
that 
we 
assume 
the 
hidden 
transitions 
are 
deterministic[14]. 
When 
conditioned 
on 
the 
visible 
variables, 
this 
renders 
the 
hidden 
unit 
distribution 
trivial. 
This 
allows 
the 
consideration 
of 
rich 
non-linear 
dynamics 
in 
the 
hidden 
space 
if 
required. 


26.4.1 
Deterministic 
latent 
variables 
Consider 
a 
Belief 
Network 
dened 
on 
a 
sequence 
of 
visible 
variables 
v1:T 
. 
To 
enrich 
the 
model 
we 
include 
additional 
continuous 
latent 
variables 
h1:T 
that 
will 
follow 
a 
non-linear 
Markov 
transition. 
To 
retain 
tractability 
of 
inference, 
we 
constrain 
the 
latent 
dynamics 
to 
be 
deterministic, 
described 
by 


p(h(t 
+ 
1)jv(t 
+ 
1);v(t);h(t)) 
= 
d 
(h(t 
+ 
1) 
- 
f 
(v(t 
+ 
1);v(t);h(t);h)) 
(26.4.2) 


Here 
(x) 
represents 
the 
Dirac 
delta 
function 
for 
continuous 
hidden 
variables. 
The 
(possibly 
non-linear) 
function 
f 
parameterises 
the 
CPT. 
Whilst 
the 
restriction 
to 
deterministic 
CPTs 
appears 
severe, 
the 
model 


482 
DRAFT 
March 
9, 
2010 



Tractable 
Continuous 
Latent 
Variable 
Models 


v(1)v(2)v(t)
h(1)h(2)h(t)
h(1)h(2)h(t)
v(1)v(2)v(t)
(a) 
(b) 
(c) 
Figure 
26.6: 
(a): 
A 
rst 
order 
Dynamic 
Bayesian 
Network 
with 
deterministic 
hidden 
CPTs 
(represented 
by 
diamonds) 
that 
is, 
the 
hidden 
node 
is 
certainly 
in 
a 
single 
state, 
determined 
by 
its 
parents. 
(b): 
Conditioning 
on 
the 
visible 
variables 
forms 
a 
directed 
chain 
in 
the 
hidden 
space 
which 
is 
deterministic. 
Hidden 
unit 
inference 
can 
be 
achieved 
by 
forward 
propagation 
alone. 
(c): 
Integrating 
out 
hidden 
variables 
gives 
a 
cascade 
style 
directed 
visible 
graph 
which 
so 
that 
each 
v(t) 
depends 
on 
all 
v(1 
: 
t 
- 
1). 


retains 
some 
attractive 
features: 
The 
marginal 
p(v(1 
: 
T 
)) 
is 
non-Markovian, 
coupling 
all 
the 
variables 
in 
the 
sequence, 
see 
g(26.6c), 
whilst 
hidden 
unit 
inference 
p(h(1 
: 
T 
)jv(1 
: 
T 
)) 
is 
deterministic, 
as 
illustrated 
in 
g(26.6b). 


The 
adjustable 
parameters 
of 
the 
hidden 
and 
visible 
CPTs 
are 
represented 
by 
h 
and 
v 
respectively. 
For 
learning, 
the 
log 
likelihood 
of 
a 
single 
training 
sequence 
V 
is 


T 
..1

T 


L(v;hjV) 
= 
log 
p(v(1)jv) 
+ 
log 
p(v(t 
+ 
1)jv(t);h(t);v) 
(26.4.3) 


t=1 


where 
the 
hidden 
unit 
values 
are 
calculated 
recursively 
using 


h(t 
+1) 
= 
f 
(v(t 
+ 
1);v(t);h(t);h) 
(26.4.4) 


To 
maximise 
the 
log 
likelihood 
using 
gradient 
techniques 
we 
need 
to 
the 
derivatives 
with 
respect 
to 
the 
model 
parameters. 
These 
can 
be 
calculated 
as 
follows: 


T 
..1

X

dL. 
. 


= 
log 
p(v(1)jv) 
+ 
log 
p(v(t 
+ 
1)jv(t);h(t);v) 
(26.4.5)

dv 
@v 
@v 


t=1 


T 
..1

X

dL. 
dh(t)

= 
log 
p(v(t 
+ 
1)jv(t);h(t);v)(26.4.6)

dh 
@h(t) 
dh

t=1 


dh(t) 
@f(t) 
@f(t) 
dh(t 
- 
1)

= 
+ 
(26.4.7)

dh 
@h 
@h(t 
- 
1) 
dh 


where 


f(t) 
= 
f(v(t);v(t 
- 
1);h(t 
- 
1);h) 
(26.4.8) 


Hence 
the 
derivatives 
can 
be 
calculated 
by 
deterministic 
forward 
propagation 
of 
errors 
alone. 
The 
case 
of 
training 
multiple 
independently 
generated 
sequences 
Vn;n 
=1;:::;N 
is 
a 
straightforward 
extension. 


26.4.2 
An 
augmented 
Hopeld 
network 
To 
make 
the 
deterministic 
latent 
variable 
model 
more 
explicit, 
we 
consider 
the 
case 
of 
continuous 
hidden 
units 
and 
discrete, 
binary 
visible 
units, 
vi(t) 
. 
f..1, 
1g. 
In 
particular, 
we 
restrict 
attention 
to 
the 
Hopeld 
model 
augmented 
with 
latent 
variables 
that 
have 
a 
simple 
linear 
dynamics 
(see 
exercise(245) 
for 
a 
nonlinear 
extension): 


h(t 
+1) 
= 
2(Ah(t)+ 
Bv(t)) 
- 
1 
deterministic 
latent 
transition 
(26.4.9) 


Vv 
p(v(t 
+ 
1)jv(t), 
h(t)) 
= 
i=1 
s 
(vi(t 
+ 
1)i(t)) 
, 
(t) 
= 
Ch(t) 
+ 
Dv(t) 
(26.4.10) 
DRAFT 
March 
9, 
2010 
483 



Neural 
Models 


This 
model 
generalises 
a 
recurrent 
stochastic 
heteroassociative 
Hopeld 
network[132] 
to 
include 
deterministic 
hidden 
units 
dependent 
on 
past 
network 
states. 
The 
parameters 
of 
the 
model 
are 
A, 
B, 
C, 
D. 
For 
gradient 
based 
training 
we 
require 
the 
derivatives 
with 
respect 
to 
each 
of 
these 
parameters. 
The 
derivative 
of 
the 
log 
likelihood 
for 
a 
generic 
parameter 
. 
is 


X

d
L 
=i(t) 
d
i(t) 
(26.4.11)

d. 
d. 


i 


where 


0. 


i(t) 
= 
@1 
- 
(vi(t 
+ 
1))i(t)AXvi(t 
+ 
1) 
(26.4.12) 
j 


This 
gives 
(where 
all 
indices 
are 
summed 
over 
the 
dimensions 
of 
the 
quantities 
they 
relate 
to): 


X

d
i(t)=Cij 
d
hj(t) 
(26.4.13)
dAß 
dA

j 


X

d
i(t)=Cij 
d
hj(t) 
(26.4.14)
dBß 
dB

j 


d
i(t)= 
ih(t) 
(26.4.15)
dCß 


d
i(t)= 
iv(t) 
(26.4.16)
dDß 


X

dd 


hi(t 
+1) 
= 
2i0
(t 
+ 
1)Aij 
hj(t)+ 
ih(t) 
(26.4.17)
dAß 
dA

j 


X

dd 


hi(t 
+1) 
= 
2i0
(t 
+ 
1)Aij 
hj(t)+ 
iv(t) 
(26.4.18)
dBß 
dB

j 


0(t) 
= 
(hi(t)) 
(1 
- 
(hi(t))) 
(26.4.19)

i

If 
we 
assume 
that 
h(1) 
is 
a 
given 
xed 
value 
(say 
0), 
we 
can 
compute 
the 
derivatives 
recursively 
by 
forward 
propagation. 
Gradient 
based 
training 
for 
this 
augmented 
Hopeld 
network 
is 
therefore 
straightforward 
to 
implement. 
This 
model 
extends 
the 
power 
of 
the 
original 
Hopeld 
model, 
being 
capable 
of 
resolving 
ambiguous 
transitions 
in 
sequences 
such 
as 
a, 
b, 
a, 
c, 
see 
example(110) 
and 
demoHopfieldLatent.m. 
In 
terms 
of 
a 
dynamic 
system, 
the 
learned 
network 
is 
an 
attractor 
with 
the 
training 
sequence 
as 
a 
stable 
point 
and 
demonstrates 
that 
such 
models 
are 
capable 
of 
learning 
attractor 
recurrent 
networks 
more 
powerful 
than 
those 
without 
hidden 
units. 


Example 
110 
(Sequence 
disambiguation). 
The 
sequence 
in 
g(26.7a) 
contains 
repeated 
patterns 
and 
therefore 
cannot 
be 
reliably 
recalled 
with 
a 
rst 
order 
model 
containing 
visible 
variables 
alone. 
To 
deal 
with 
this 
we 
consider 
a 
Hopeld 
network 
with 
3 
visible 
units 
and 
7 
additional 
hidden 
units 
with 
deterministic 
(linear) 
latent 
dynamics. 
The 
model 
was 
trained 
with 
gradient 
ascent 
to 
maximise 
the 
likelihood 
of 
the 
binary 
sequence 
in 
g(26.7a). 
As 
shown 
in 
g(26.7b), 
the 
learned 
network 
is 
capable 
of 
recalling 
the 
sequence 
correctly, 
even 
when 
initialised 
in 
an 
incorrect 
state, 
having 
no 
diculty 
with 
the 
fact 
that 
the 
sequence 
transitions 
are 
ambiguous. 


26.5 
Neural 
Models 
The 
tractable 
deterministic 
latent 
variable 
model 
introduced 
in 
section(26.4) 
presents 
an 
opportunity 
to 
extend 
models 
such 
as 
the 
Hopeld 
network 
to 
include 
more 
biologically 
realistic 
processes 
without 
losing 
computational 
tractability. 
First 
we 
discuss 
a 
general 
framework 
for 
learning 
in 
a 
class 
of 
neural 
models[15, 
223], 
this 
being 
a 
special 
case 
of 
the 
deterministic 
latent 
variable 
models[14] 
and 
a 
generalisation 
of 
the 
spike-response 
model 
of 
theoretical 
neurobiology[108]. 


484 
DRAFT 
March 
9, 
2010 



Neural 
Models 



Figure 
26.7: 
(a): 
The 
training 
sequence 
consists 
of 
a 
random 
set 
of 
vectors 
(V 
= 
3) 


(a) 
over 
T 
= 
10 
time 
steps. 
(b): 
The 
reconstruction 
using 
H 
= 
7 
hidden 
units. 
The 
initial 
state 
v(t 
= 
1) 
for 
the 
recalled 
sequence 
was 
set 
to 
the 
correct 
initial 
training 
value 
albeit 
with 
one 
of 
the 
values 
ipped. 
Note 
that 
the 
method 
is 
capable 
of 
sequence 
disambiguation 
in 
the 
sense 
that 
the 
transitions 
of 
the 
form 
a, 
b;:::, 
a, 
c 
(b) 
can 
be 
recalled. 


26.5.1 
Stochastically 
spiking 
neurons 
We 
assume 
that 
neuron 
i 
res 
depending 
on 
the 
membrane 
potential 
ai(t) 
through 


p(vi(t 
+1) 
= 
1jv(t), 
h(t)) 
= 
p(vi(t 
+1) 
= 
1jai(t)) 
(26.5.1) 
To 
be 
specic, 
we 
take 
throughout 


p(vi(t 
+1) 
= 
1jai(t)) 
= 
s 
(ai(t)) 
(26.5.2) 
Here 
we 
to 
dene 
the 
quiescent 
state 
as 
vi(t 
+ 
1) 
= 
0, 
so 
that 


p(vi(t 
+ 
1)jai(t)) 
= 
s 
((2vi(t 
+ 
1) 
- 
1)ai(t)) 
(26.5.3) 
The 
choice 
of 
the 
sigmoid 
function 
(x) 
is 
not 
fundamental 
and 
is 
chosen 
merely 
for 
analytical 
convenience. 
The 
log-likelihood 
of 
a 
sequence 
of 
visible 
states 
V 
is 


T 
..1V

XX

L 
= 
log 
s 
((2vi(t 
+ 
1) 
- 
1)ai(t)) 
(26.5.4) 
t=1 
i=1 


and 
the 
gradient 
of 
the 
log-likelihood 
is 
then 


T 
..1

X

dL 
dai(t)

=(vi(t 
+ 
1) 
- 
(ai(t))) 
(26.5.5)

dwij 
dwij

t=1 


where 
we 
used 
the 
fact 
that 
vi 
2f0, 
1g. 
Here 
wij 
are 
parameters 
of 
the 
membrane 
potential 
(see 
below). 
We 
take 
equation 
(26.5.5) 
as 
common 
in 
the 
following 
models 
in 
which 
the 
membrane 
potential 
ai(t) 
is 
described 
with 
increasing 
sophistication. 


26.5.2 
Hopeld 
membrane 
potential 
As 
a 
rst 
step, 
we 
show 
how 
the 
Hopeld 
network 
training, 
as 
described 
in 
section(26.3.1), 
can 
be 
recovered 
as 
a 
special 
case 
of 
the 
above 
framework. 
The 
Hopeld 
membrane 
potential 
is 


V

X

ai(t) 
= 
wijvj(t) 
- 
bi 
(26.5.6) 
j=1 


where 
wij 
characterizes 
the 
ecacy 
of 
information 
transmission 
from 
neuron 
j 
to 
neuron 
i, 
and 
bi 
is 
a 
threshold. 
Applying 
the 
Maximum 
Likelihood 
framework 
to 
this 
model 
to 
learn 
a 
temporal 
sequence 
V 
by 
adjustment 
of 
the 
parameters 
wij 
(the 
bi 
are 
xed 
for 
simplicity), 
we 
obtain 
the 
(batch) 
learning 
rule 
(using 
dai=dwij 
= 
vj(t) 
in 
equation 
(26.5.4)) 


T 
..1

X

dL 
dL

new 


wij 
= 
wij 
+ 
, 
=(vi(t 
+ 
1) 
- 
(ai(t))) 
vj(t), 
(26.5.7)
dwij 
dwij 


t=1 


where 
the 
learning 
rate 
. 
is 
chosen 
empirically 
to 
be 
suciently 
small 
to 
ensure 
convergence. 
Equation(26.5.7) 
matches 
equation 
(26.3.19) 
(which 
uses 
the 
1 
encoding). 


DRAFT 
March 
9, 
2010 



Neural 
Models 


neuron numberOriginalt10205101520253035404550Reconstructiont10205101520253035404550x valuest10205101520253035404550Hebb Reconstructiont10205101520253035404550
Figure 
26.8: 
Learning 
with 
depression 
: 
U 
=0:5, 
t 
= 
5, 
t 
= 
1, 
. 
=0:25. 
Despite 
the 
apparent 
complexity 
of 
the 
dynamics, 
learning 
appropriate 
neural 
connection 
weights 
is 
straightforward 
using 
Maximum 
Likelihood. 
The 
reconstruction 
using 
the 
standard 
Hebb 
rule 
by 
contrast 
is 
poor[15]. 


26.5.3 
Dynamic 
synapses 
In 
more 
realistic 
synaptic 
models, 
neurotransmitter 
generation 
depends 
on 
a 
nite 
rate 
of 
cell 
subcomponent 
production, 
and 
the 
quantity 
of 
vesicles 
released 
is 
aected 
by 
the 
history 
of 
ring[1]. 
Loosely 
speaking, 
when 
a 
neuron 
res 
it 
releases 
a 
chemical 
substance 
from 
a 
local 
reservoir, 
this 
reservoir 
being 
relled 
at 
a 
lower 
rate 
than 
the 
neuron 
can 
re. 
If 
the 
neuron 
res 
continually, 
its 
ability 
to 
continue 
ring 
weakens 
since 
the 
reservoir 
of 
release 
chemical 
is 
depleted. 
This 
can 
be 
accounted 
for 
by 
using 
a 


depression 
mechanism 
that 
aects 
the 
membrane 
potential 
ai(t) 
= 
wijxj(t)vj(t) 
(26.5.8) 
for 
depression 
factors 
xj(t) 
. 
[0, 
1]. 
A 
simple 
dynamics 
for 
these 
depression 
factors 
is[280] 
xj(t 
+ 
1) 
= 
xj(t) 
+ 
t1 
- 
xj(t) 
t 
- 
Uxj(t)vj(t)(26.5.9) 


where 
t, 
t 
, 
and 
U 
represent 
time 
scales, 
recovery 
times 
and 
spiking 
eect 
parameters 
respectively. 
Note 
that 
these 
depression 
factor 
dynamics 
are 
exactly 
of 
the 
form 
of 
deterministic 
hidden 
variables. 


It 
is 
straightforward 
to 
include 
these 
dynamic 
synapses 
in 
a 
principled 
way 
using 
the 
Maximum 
Likelihood 
learning 
framework. 
For 
the 
Hopeld 
potential, 
the 
learning 
dynamics 
is 
simply 
given 
by 
equations 
(26.5.5,26.5.9), 
with 


dai(t) 


= 
xj(t)vj(t) 
(26.5.10)
dwij 


Example 
111 
(Learning 
with 
depression). 
In 
g(26.4) 
we 
demonstrate 
learning 
a 
random 
temporal 
sequence 
of 
20 
time 
steps 
for 
an 
assembly 
of 
50 
neurons 
with 
dynamic 
depressive 
synapses. 
After 
learning 
wij 
the 
trained 
network 
is 
initialised 
in 
the 
rst 
state 
of 
the 
training 
sequence. 
The 
remaining 
states 
of 
the 
sequence 
were 
then 
correctly 
recalled 
by 
iteration 
of 
the 
learned 
model. 
The 
corresponding 
generated 
factors 
xi(t) 
are 
also 
plotted. 
For 
comparison, 
we 
plot 
the 
results 
of 
using 
the 
dynamics 
having 
set 
the 
wij 
using 
the 
temporal 
Hebb 
rule, 
equation 
(26.3.1). 
The 
poor 
performance 
of 
the 
correlation 
based 
Hebb 
rule 
demonstrates 
the 
necessity, 
in 
general, 
to 
couple 
a 
dynamical 
system 
with 
an 
appropriate 
learning 
mechanism. 


26.5.4 
Leaky 
integrate 
and 
re 
models 
Leaky 
integrate 
and 
re 
models 
move 
a 
step 
further 
towards 
biological 
realism 
in 
which 
the 
membrane 
potential 
increments 
if 
it 
receives 
an 
excitatory 
stimulus 
(wij 
> 
0), 
and 
decrements 
if 
it 
receives 
an 
inhibitory 
stimulus 
(wij 
< 
0). 
After 
ring, 
the 
membrane 
potential 
is 
reset 
to 
a 
low 
value 
below 
the 
ring 
threshold, 
and 
thereafter 
steadily 
increases 
to 
a 
resting 
level 
(see 
for 
example 
[62, 
108]). 
A 
model 
that 
incorporates 
such 
eects 
is 


01

wijvj(t)+ 
rest 
(1 
- 
)A(1 
- 
vi(t 
- 
1)) 
+ 
vi(t 
- 
1)fired 


ai(t)= 
@ai(t 
- 
1) 
+ 
(26.5.11) 
j 


DRAFT 
March 
9, 
2010 



Exercises 


Since 
vi 
2f0, 
1g, 
if 
neuron 
i 
res 
at 
time 
t 
..61 
the 
potential 
is 
reset 
to 
fired 
at 
time 
t. 
Similarly, 
with 
no 
synaptic 
input, 
the 
potential 
equilibrates 
to 
rest 
with 
time 
constant 
..1/ 
log 
[15]. 


Despite 
the 
increase 
in 
complexity 
of 
the 
membrane 
potential 
over 
the 
Hopeld 
case, 
deriving 
appropriate 
learning 
dynamics 
for 
this 
new 
system 
is 
straightforward 
since, 
as 
before, 
the 
hidden 
variables 
(here 
the 
membrane 
potentials) 
update 
in 
a 
deterministic 
fashion. 
The 
potential 
derivatives 
are 


dai(t) 
dai(t 
..61)

= 
(1 
..6vi(t 
..61)) 
a 
+ 
vj(t) 
(26.5.12)

dwij 
dwij 


dai(t=1)

By 
initialising 
the 
derivative 
= 
0, 
equations 
(26.5.5,26.5.11,26.5.12) 
dene 
a 
rst 
order 
recursion 


dwij 
for 
the 
gradient 
which 
can 
be 
used 
to 
adapt 
wij 
in 
the 
usual 
manner 
wij 
 6wij 
+ 
dL=dwij. 
We 
could 
also 
apply 
synaptic 
dynamics 
to 
this 
case 
by 
replacing 
the 
term 
vj(t) 
in 
equation 
(26.5.12) 
by 
xj(t)vj(t). 


Although 
a 
detailed 
discussion 
of 
the 
properties 
of 
the 
neuronal 
responses 
for 
networks 
trained 
in 
this 
way 
is 
beyond 
the 
scope 
of 
these 
notes, 
an 
interesting 
consequence 
of 
the 
learning 
rule 
equation 
(26.5.12) 
is 
a 
spike 
time 
dependent 
learning 
window 
in 
qualitative 
agreement 
with 
experimental 
results[223, 
186]. 


In 
summary, 
provided 
one 
deals 
with 
deterministic 
latent 
dynamics, 
essentially 
arbitrarily 
complex 
spatiotemporal 
patterns 
may 
potentially 
be 
learned, 
and 
generated 
under 
cued 
retrieval, 
for 
very 
complex 
neural 
dynamics. 
The 
spike-response 
model 
[108] 
can 
be 
seen 
as 
a 
special 
case 
of 
the 
deterministic 
latent 
variable 
model 
in 
which 
the 
latent 
variables 
have 
been 
explicitly 
integrated 
out. 


26.6 
Code 
demoHopfield.m: 
Demo 
of 
Hopeld 
sequence 
learning 
HebbML.m: 
Gradient 
ascent 
training 
of 
a 
set 
of 
sequences 
using 
Max 
Likelihood 
HopfieldHiddenNL.m: 
Hopeld 
network 
with 
additional 
non-linear 
latent 
variables 
demoHopfieldLatent.m: 
demo 
of 
Hopeld 
net 
with 
deterministic 
latent 
variables 
HopfieldHiddenLikNL.m: 
Hopeld 
Net 
with 
hidden 
variables 
sequence 
likelihood 


26.7 
Exercises 
Exercise 
241. 
Consider 
a 
very 
large 
Hopeld 
network 
V 
61 
used 
to 
store 
a 
single 
temporal 
sequence 
of 
length 
v(1 
: 
T 
), 
T 
6V 
. 
In 
this 
case 
the 
weight 
matrix 
w 
may 
be 
dicult 
to 
store. 
Explain 
how 
to 
justify 
the 
assumption 


T 
..1

T 


wij 
= 
ui(t)vi(t 
+ 
1)vj(t) 
(26.7.1) 
t=1 


where 
ui(t) 
are 
the 
dual 
parameters 
and 
derive 
an 
update 
rule 
for 
the 
dual 
parameters 
u. 


Exercise 
242. 
A 
Hopeld 
network 
is 
used 
to 
store 
a 
raw 
uncompressed 
binary 
video 
sequence. 
Each 
image 
in 
the 
sequence 
contains 
106 
binary 
pixels. 
At 
a 
rate 
of 
10 
frames 
per 
second, 
how 
many 
hours 
of 
video 
can 
106 
neurons 
store? 


Exercise 
243. 
Derive 
the 
update 
equation 
(26.3.22). 


Exercise 
244. 
Show 
that 
the 
Hessian 
equation 
(26.3.16) 
is 
negative 
denite. 
That 
is 


T 
d2L 
xijxkl 
60 
(26.7.2)
dwijdwkl 


i;j;k;l 


for 
any 
x6

=0. 


DRAFT 
March 
9, 
2010 



Exercises 


Exercise 
245. 
For 
the 
augmented 
Hopeld 
network 
of 
section(26.4.2),with 
latent 
dynamics 


0. 


hi(t 
+1) 
= 
2s 
@Aijhj(t)+ 
Bijvj(t)AX- 
1 
(26.7.3) 
j 


derive 
the 
derivative 
recursions 
described 
in 
section(26.4.2). 


DRAFT 
March 
9, 
2010 



Part 
V 
Approximate 
Inference 


489 



CHAPTER 
27 


Sampling 


27.1 
Introduction 
1

Sampling 
concerns 
drawing 
realisations 
x;:::, 
xL 
of 
a 
variable 
x 
from 
a 
distribution 
p(x). 
For 
a 
discrete 
variable 
x, 
in 
the 
limit 
of 
a 
large 
number 
of 
samples, 
the 
fraction 
of 
samples 
in 
state 
x 
tends 
to 
p(x 
= 
x). 
That 
is, 


X

1 
Lhl 
i

lim 
Ix 
= 
x= 
p(x 
= 
x) 
(27.1.1)

L!8 
L 


l=1 


In 
the 
continuous 
case, 
one 
can 
consider 
a 
small 
region 
. 
such 
that 
the 
probability 
that 
the 
samples 
ccupy 
. 
tends 
to 
the 
integral 
of 
p(x) 
over 
. 
In 
other 
words, 
the 
relative 
frequency 
x 
2h. 
tends 
to

o 
o
R


p(x). 
Given 
a 
nite 
set 
of 
samples, 
one 
can 
then 
approximate 
expectations 
using

x2. 


X

1 
L

hf(x)ihf(x 
l) 
hfˆ 
(27.1.2)

p(x) 


L 


l=1 


This 
approximation 
holds 
for 
both 
discrete 
and 
continuous 
variables. 
Provided 
the 
samples 
are 
indeed 
from 
p(x), 
then 
the 
average 
of 
the 
approximation 
is 


DE
LDE

X


fˆ 
=
1 
f(x 
l)=hf(x)i(27.1.3)

lp(x)

L 
p(x=xl)

l=1 


The 
variance 
of 
the 
approximation 
is 


DE
DE2 




f^2 
..hfˆ 
=
1 
f2(x)..hf(x)i2 
(27.1.4)

p(x) 
p(x)

L 


Hence 
the 
mean 
of 
the 
approximation 
is 
the 
exact 
mean 
of 
f 
and 
the 
variance 
of 
the 
approximation 
scales 
inversely 
with 
the 
number 
of 
samples. 
In 
principle, 
therefore, 
provided 
the 
samples 
are 
independently 
drawn 
from 
p(x), 
only 
a 
small 
number 
of 
samples 
is 
required 
to 
accurately 
estimate 
this 
mean. 
Importantly, 
this 
result 
is 
independent 
of 
the 
dimension 
of 
x. 
However, 
the 
critical 
diculty 
is 
in 
actually 
generating 
independent 
samples 
from 
p(x). 
Drawing 
samples 
from 
high-dimensional 
distributions 
is 
generally 
dicult 
and 
few 
guarantees 
exist 
to 
ensure 
that 
in 
a 
practical 
timeframe 
the 
samples 
produced 
are 
representative 
enough 
such 
that 
expectations 
can 
be 
approximated 
accurately. 
There 
are 
many 
dierent 
sampling 
algorithms, 
all 
of 
which 
`work 
in 
principle', 
but 
each 
`working 
in 
practice’ 
only 
when 
the 
distribution 
satises 
particular 
properties[112]. 
Before 
we 
develop 
schemes 
for 
multi-variate 
distributions, 
we 
consider 
the 
univariate 
case. 


491 



Introduction 


Figure 
27.1: 
A 
representation 
of 
the 
discrete 
distribution 
equation 
(27.1.5). 
The 
unit 
interval 
from 
0 
to 
1 
is 
partitioned 
in 
parts 
whose 
lengths 
are 
equal 
to 
0:6, 
0:1 
and 
0:3. 


1 
× 
2 
3 


27.1.1 
Univariate 
sampling 
In 
the 
following, 
we 
assume 
that 
a 
random 
number 
generator 
exists 
which 
is 
able 
to 
produce 
a 
value 
uniformly 
at 
random 
from 
the 
unit 
interval 
[0, 
1]. 
We 
will 
make 
use 
of 
this 
uniform 
random 
number 
generator 
to 
draw 
samples 
from 
non-uniform 
distributions. 


Discrete 
case 


Consider 
the 
one 
dimensional 
discrete 
distribution 
p(x) 
where 
dom(x)= 
f1, 
2, 
3g, 
with 


8X

<X0:6 
x 
=1 
p(x)= 
0:1 
x 
= 
2 
(27.1.5)

:X

0:3 
x 
=3 
This 
represents 
a 
partitioning 
of 
the 
unit 
interval 
[0, 
1] 
in 
which 
the 
interval 
[0, 
0:6] 
has 
been 
labelled 
as 
state 
1, 
[0:6, 
0:7] 
as 
state 
2, 
and 
[0:7, 
1:0] 
as 
state 
3, 
g(27.1). 
If 
we 
were 
to 
drop 
a 
point 
× 
anywhere 
at 
random, 
uniformly 
in 
the 
interval 
[0, 
1], 
the 
chance 
that 
× 
would 
land 
in 
interval 
1 
is 
0.6, 
and 
the 
chance 
that 
it 
would 
be 
in 
interval 
2 
is 
0.1 
and 
similarly, 
for 
interval 
3, 
0.3. 
This 
therefore 
denes 
for 
us 
a 
valid 
sampling 
procedure 
for 
discrete 
one-dimensional 
distributions 
as 
described 
in 
algorithm(24). 


In 
our 
example, 
we 
have 
(c0;c1;c2;c3) 
= 
(0, 
0:6, 
0:7, 
1). 
We 
then 
draw 
a 
sample 
uniformly 
from 
[0, 
1], 
say 
u 
=0:66. 
Then 
the 
sampled 
state 
would 
be 
state 
2, 
since 
this 
is 
in 
the 
interval 
(c1;c2]. 


Sampling 
from 
a 
discrete 
univariate 
distribution 
is 
straightforward 
since 
computing 
the 
cumulant 
takes 
only 
O 
(K) 
steps 
for 
a 
K 
state 
discrete 
variable. 


Continuous 
case 


In 
the 
following 
we 
assume 
that 
a 
method 
exists 
to 
generate 
samples 
from 
the 
uniform 
distribution 
U 
(x| 
[0, 
1]). 
Intuitively, 
the 
generalisation 
of 
the 
discrete 
case 
to 
the 
continuous 
case 
is 
clear. 
First 
we 
calculate 
the 
cumulant 
density 
function 


y 


C(y)= 
p(x)dx 
(27.1.6) 


..8 


Then 
we 
sample 
u 
uniformly 
from 
[0, 
1], 
and 
obtain 
the 
corresponding 
sample 
x 
by 
solving 
C(x)= 
u 
. 
x 
= 
C..1(u). 
Formally, 
therefore, 
sampling 
of 
a 
continuous 
univariate 
variable 
is 
straightforward 
provided 
we 
can 
compute 
the 
integral 
of 
the 
corresponding 
probability 
density 
function. 


Algorithm 
24 
Sampling 
from 
a 
univariate 
discrete 
distribution 
p 
with 
K 
states. 


1: 
Label 
the 
K 
states 
as 
i 
=1;:::;K, 
with 
associated 
probabilities 
pi. 
2: 
Calculate 
the 
cumulant 
X

ci 
=pj 


ji 


and 
set 
c0 
=0. 


3: 
Draw 
a 
value 
u 
uniformly 
at 
random 
from 
the 
unit 
interval 
[0, 
1]. 
4: 
Find 
that 
i 
for 
which 
ci..1 
<u 
= 
ci. 
5: 
Return 
state 
i 
as 
a 
sample 
from 
p. 
DRAFT 
March 
9, 
2010 



Introduction 


12302468
1230200400600
Figure 
27.2: 
Histograms 
of 
the 
samples 
from 
the 
three 
state 
distribution 
p(x)= 
f0:6, 
0:1, 
0:3g. 
(a): 
20 
samples. 
(b): 
1000 
samples. 
As 
the 
number 
of 
sam


(a) 
(b) 
ples 
increases, 
the 
relative 
frequency 
of 
the 
samples 
tends 
to 
the 
distribution 
p(x). 
For 
special 
distributions, 
such 
as 
Gaussians, 
numerically 
ecient 
alternative 
procedures 
exist, 
usually 
based 
on 
co-ordinate 
transformations, 
see 
exercise(246). 


27.1.2 
Multi-variate 
sampling 
One 
way 
to 
generalise 
the 
one 
dimensional 
discrete 
case 
to 
a 
higher 
dimensional 
distribution 
p(x1;:::;xn) 
is 
to 
translate 
this 
into 
an 
equivalent 
one-dimensional 
distribution. 
This 
can 
be 
achieved 
by 
enumerating 
all 
the 
possible 
joint 
states 
(x1;:::;xn), 
giving 
each 
a 
unique 
integer 
i 
from 
1 
to 
the 
total 
number 
of 
states, 
and 
constructing 
a 
univariate 
distribution 
with 
probability 
p(i)= 
p(x) 
for 
i 
corresponding 
to 
the 
multivariate 
state 
x. 
This 
then 
transforms 
the 
multi-dimensional 
distribution 
into 
an 
equivalent 
one-dimensional 
distribution, 
and 
sampling 
can 
be 
achieved 
as 
before. 
In 
general, 
of 
course, 
this 
procedure 
is 
impractical 
since 
the 
number 
of 
states 
will 
grow 
exponentially 
with 
the 
number 
of 
variables 
x1;:::;xn. 


An 
alternative 
exact 
approach 
would 
be 
to 
capitalise 
on 
the 
relation 


p(x1;x2)= 
p(x2jx1)p(x1) 
(27.1.7) 


We 
can 
sample 
from 
the 
joint 
distribution 
p(x1;x2) 
by 
rst 
sampling 
a 
state 
for 
x1 
from 
the 
one-dimensional 
p(x1), 
and 
then, 
with 
x1 
clamped 
to 
this 
state, 
sampling 
a 
state 
for 
x2 
from 
the 
one-dimensional 
p(x2jx1). 
It 
is 
clear 
how 
to 
generalise 
this 
to 
more 
variables 
by 
using 
a 
cascade 
decomposition: 


p(x1;:::;xn)= 
p(xnjxn..1;:::;x1)p(xn..1jxn..2;:::;x1) 
:::;p(x2jx1)p(x1) 
(27.1.8) 


However, 
in 
order 
to 
apply 
this 
technique, 
we 
need 
to 
know 
the 
conditionals 
p(xijxi..1;:::;x1). 
Unless 
these 
are 
explicitly 
given 
we 
need 
to 
compute 
these 
from 
the 
joint 
distribution 
p(x1;:::;xn). 
Such 
conditionals 
will, 
in 
general, 
require 
the 
summation 
over 
an 
exponential 
number 
of 
states 
and, 
except 
for 
small 
n, 
generally 
also 
be 
impractical. 
For 
Belief 
Networks, 
however, 
by 
construction 
the 
conditionals 
are 
specied 
so 
that 
this 
technique 
becomes 
practical, 
as 
we 
discuss 
in 
section(27.2). 


Drawing 
samples 
from 
a 
multi-variate 
distribution 
is 
in 
general 
therefore 
a 
complex 
task 
and 
one 
seeks 
to 
exploit 
any 
structural 
properties 
of 
the 
distribution 
to 
make 
this 
computationally 
more 
feasible. 
A 
common 
approach 
is 
to 
seek 
to 
transform 
the 
distribution 
into 
a 
product 
of 
lower 
dimensional 
distributions. 
A 
classic 
example 
of 
this 
is 
sampling 
from 
a 
multi-variate 
Gaussian, 
which 
can 
be 
reduced 
to 
sampling 
from 
a 
set 
of 
univariate 
Gaussians 
by 
a 
suitable 
coordinate 
transformation, 
as 
discussed 
in 
example(112). 


Example 
112 
(Sampling 
from 
a 
multi-variate 
Gaussian). 
Our 
interest 
is 
to 
draw 
a 
sample 
from 
the 
multi-variate 
Gaussian 
p(x)= 
N 
(x 


m, 
S). 
For 
a 
general 
covariance 
matrix 
S, 
p(x) 
does 
not 
factorise 
into 
a 
product 
of 
univariate 
distributions. 
However, 
consider 
the 
transformation 


y 
= 
C..1 
(x 
- 
m) 
(27.1.9) 


where 
C 
is 
chosen 
so 
that 
CCT 
= 
S. 
Since 
this 
is 
a 
linear 
transformation, 
y 
is 
also 
Gaussian 
distributed 
with 
mean 







hy) 
=C..1 
(x 
- 
m)= 
C..1hxi- 
m= 
C..1 
(m 
- 
m)= 
0 
(27.1.10)
p(x) 
p(x) 


Since 
the 
mean 
of 
y 
is 
zero, 
the 
covariance 
is 
given 
by

DE 
E

T

yy= 
C..1(x 
- 
m)(x 
- 
m)TC..T 
= 
C..1SC..T 
= 
C..1CCTC..T 
= 
I 
(27.1.11) 


p(x) 
p(x) 


DRAFT 
March 
9, 
2010 



Ancestral 
Sampling 


x1x2x3x4x5x6
Figure 
27.3: 
An 
ancestral 
Belief 
Network 
without 
any 
evidential 
variables. 
To 
sample 
from 
this 
distribution, 
we 
draw 
a 
sample 
from 
variable 
1, 
and 
then 
variables, 
2,. 
. 
. 
,6 
in 
order. 


Hence 


Y

p(y)= 
N 
(y 


0, 
I)=N 
(yi 


0, 
1) 
(27.1.12) 


i 


Hence 
a 
sample 
from 
y 
can 
be 
obtained 
by 
independently 
drawing 
a 
sample 
from 
each 
of 
the 
univariate 
zero 
mean 
unit 
variance 
Gaussians. 
Given 
a 
sample 
for 
y, 
a 
sample 
for 
x 
is 
obtained 
using 


x 
= 
Cy 
+ 
m 
(27.1.13) 


Drawing 
samples 
from 
a 
univariate 
Gaussian 
is 
a 
well-studied 
topic, 
with 
a 
popular 
method 
being 
the 
Box-Muller 
technique, 
exercise(246). 


27.2 
Ancestral 
Sampling 
Belief 
Networks 
take 
the 
general 
form: 


Y

p(x)=p(xijpa 
(xi)) 
(27.2.1) 
i 


where 
each 
of 
the 
conditional 
distributions 
p(xijpa 
(xi)) 
is 
known. 
Provided 
that 
no 
variables 
are 
evidential, 
we 
can 
sample 
from 
this 
distribution 
in 
a 
straightforward 
manner. 
For 
convenience, 
we 
rst 
rename 
the 
variable 
indices 
so 
that 
parent 
variables 
always 
come 
before 
their 
children 
(ancestral 
ordering), 
for 
example 
(see 
g(27.3)) 


p(x1;:::;x6)= 
p(x1)p(x2)p(x3jx1;x2)p(x4jx3)p(x5jx3)p(x6jx4;x5) 
(27.2.2) 


One 
can 
sample 
rst 
from 
those 
nodes 
that 
do 
not 
have 
any 
parents 
(here, 
x1 
and 
x2). 
Given 
these 
values, 
one 
can 
then 
sample 
x3, 
and 
then 
x4 
and 
x5 
and 
nally 
x6. 
Despite 
the 
presence 
of 
loops 
in 
the 
graph, 
such 
a 
forward 
sampling 
procedure 
is 
straightforward. 
This 
procedure 
holds 
for 
both 
discrete 
and 
continuous 
variables. 


If 
one 
attempted 
to 
carry 
out 
an 
exact 
inference 
scheme 
using 
moralisation 
and 
triangulation, 
in 
more 
complex 
multiply 
connected 
graphs, 
cliques 
can 
become 
very 
large. 
However, 
regardless 
of 
the 
loop 
structure, 
ancestral 
sampling 
is 
straightforward. 


Ancestral 
or 
`forward’ 
sampling 
is 
a 
case 
of 
perfect 
sampling 
(also 
termed 
exact 
sampling) 
since 
each 
sample 
is 
indeed 
drawn 
from 
the 
required 
distribution. 
This 
is 
in 
contrast 
to 
Markov 
Chain 
Monte 
Carlo 
methods 
sections(27.3,27.4) 
for 
which 
the 
samples 
are 
from 
p(x) 
only 
in 
the 
limit 
of 
a 
large 
number 
of 


iterations. 
27.2.1 
Dealing 
with 
evidence 
How 
can 
we 
sample 
from 
a 
distribution 
in 
which 
certain 
variables 
xE 
are 
clamped 
to 
evidential 
states? 
Formally 
we 
need 
to 
sample 
from 
p(xnE 
jxE 
) 
= 
p(xnE 
, 
xE 
) 
p(xE 
) 
(27.2.3) 
494 
DRAFT 
March 
9, 
2010 



Gibbs 
Sampling 


Figure 
27.4: 
The 
Markov 
blanket 
of 
x4. 
To 
draw 
a 
sample 
from 
p(x4jxn4) 
we 
clamp 
x1;x2;x3;x5;x7 
into 
their 
evidential 
states 
and 
draw 
a 
sample 
from 
p(x4jx1;x2)p(x6jx3;x4)p(x7jx4;x5)=Z 
where 
Z 
is 
a 
normalisation 
constant. 


x1x2
x3x4x5
x6x7
If 
an 
evidential 
variable 
xi 
has 
no 
parents, 
then 
one 
can 
simply 
set 
the 
variable 
into 
this 
state 
and 
continue 
forward 
sampling 
as 
before. 
For 
example, 
to 
compute 
a 
sample 
from 
p(x1;x3;x4;x5;x6jx2) 
dened 
in 
equation 
(27.2.2), 
one 
simply 
clamps 
the 
x2 
into 
its 
evidential 
state 
and 
continues 
forward 
sampling. 
The 
reason 
this 
is 
straightforward 
is 
that 
conditioning 
on 
x2 
merely 
denes 
a 
new 
distribution 
on 
a 
subset 
of 
the 
variables, 
for 
which 
the 
distribution 
is 
immediately 
known. 


On 
the 
other 
hand, 
consider 
sampling 
from 
p(x1;x2;x3;x4;x5jx6). 
Using 
Bayes’ 
rule, 
we 
have 


p(x1)p(x2)p(x3jx1;x2)p(x4jx3)p(x5jx3)p(x6jx4;x5)

p(x1;x2;x3;x4;x5jx6)= 
P(27.2.4) 


p(x1)p(x2)p(x3jx1;x2)p(x4jx3)p(x5jx3)p(x6jx4;x5)

x1;x2;x3;x4;x5 


The 
conditioning 
on 
x6 
means 
that 
the 
structure 
of 
the 
distribution 
on 
the 
non-evidential 
variables 
changes 


– 
for 
example 
x4 
and 
x5 
become 
coupled. 
One 
could 
attempt 
to 
work 
out 
an 
equivalent 
new 
forward 
sampling 
structure, 
(see 
exercise(247)) 
although 
generally 
this 
will 
be 
as 
complex 
as 
running 
an 
exact 
inference 
approach. 
An 
alternative 
is 
to 
proceed 
with 
forward 
sampling 
from 
the 
non-evidential 
distribution, 
and 
then 
discard 
any 
samples 
which 
do 
not 
match 
the 
evidential 
states. 
This 
is 
generally 
not 
recommended 
since 
the 


QP

e

probability 
that 
a 
sample 
from 
p(x) 
will 
be 
consistent 
with 
the 
evidence 
is 
roughly 
O 
(1/ 
dim 
x) 
where 


ii 


e

dim 
xis 
the 
number 
of 
states 
of 
evidential 
variable 
i. 
In 
principle 
one 
can 
ease 
this 
eect 
by 
discarding 


i 


the 
sample 
as 
soon 
as 
any 
variable 
state 
is 
inconsistent 
with 
the 
evidence. 
Nevertheless, 
the 
number 
of 
re-starts 
required 
to 
obtain 
a 
valid 
sample 
would 
on 
average 
be 
very 
large. 


27.2.2 
Perfect 
sampling 
for 
a 
Markov 
network 
For 
a 
Markov 
network 
we 
can 
draw 
exact 
samples 
by 
forming 
an 
equivalent 
directed 
representation 
of 
the 
graph, 
see 
section(6.8), 
and 
subsequently 
using 
ancestral 
sampling 
on 
this 
directed 
graph. 
This 
is 
achieved 
by 
rst 
choosing 
a 
root 
clique 
and 
then 
consistently 
orienting 
edges 
away 
from 
this 
clique. 
An 
exact 
sample 
can 
then 
be 
drawn 
from 
the 
Markov 
network 
by 
rst 
sampling 
from 
the 
root 
clique 
and 
then 
recursively 
from 
the 
children 
of 
this 
clique. 
See 
potsample.m, 
JTsample.m 
and 
demoJTreeSample.m. 


27.3 
Gibbs 
Sampling 
The 
ineciency 
of 
methods 
such 
as 
ancestral 
sampling 
under 
evidence, 
motivates 
alternative 
techniques. 
An 
important 
and 
widespread 
technique 
is 
Gibbs 
sampling 
which 
is 
generally 
straightforward 
to 
implement. 


No 
evidence 


1

Assume 
we 
have 
a 
joint 
sample 
state 
xfrom 
the 
multivariate 
distribution 
p(x). 
We 
then 
consider 
a 
particular 
variable, 
xi. 
Using 
Bayes’ 
rule 
we 
may 
write 


p(x)= 
p(xijx1;:::;xi..1;xi+1;:::;xn)p(x1;:::;xi..1;xi+1;:::;xn) 
(27.3.1) 


111 
1

Given 
a 
joint 
initial 
state 
x1, 
from 
which 
we 
can 
read 
off 
the 
`parental’ 
state 
x1;:::;xi..1;xi+1;:::;x, 
we 


n

2

can 
then 
draw 
a 
sample 
xfrom

i 


111 
1 


p(xijx1;:::, 
x 
i+1;:::, 
x 
) 
= 
p(xijxni) 
(27.3.2)

i..1, 
x 
n

DRAFT 
March 
9, 
2010 
495 



x1x2
x3x4
x1x2
x4
x1x2
x3x4
x1x2
x4
Gibbs 
Sampling 


Figure 
27.5: 
(a): 
A 
toy 
`intractable’ 
distribution. 
Gibbs 
sampling 
by 
conditioning 
on 
all 
variables 
except 
one 
leads 
to 
a 
simple 
univariate 
conditional 
distribution. 
(b): 
Conditioning 
on 
x3 
yields 
a 
new 
distribution 
that 
is 
singly-connected, 
for 
which 
exact 


(a) 
(b) 
sampling 
is 
straightforward. 


We 
assume 
this 
distribution 
is 
easy 
to 
sample 
from 
since 
it 
is 
univariate. 
We 
call 
this 
new 
joint 
sample 
(in 


..X

21121 
1

which 
only 
xi 
has 
been 
updated) 
x= 
x1;:::, 
xi..1, 
xi 
, 
xi+1;:::, 
x. 
One 
then 
selects 
another 
variable 
xj

n 


1

to 
sample 
and, 
by 
continuing 
this 
procedure, 
generates 
a 
set 
x;:::, 
xL 
of 
samples 
in 
which 
each 
xl+1 
diers 
from 
xl 
in 
only 
a 
single 
component. 
The 
reason 
this 
is 
valid 
sampling 
scheme 
is 
outlined 
in 
section(27.3.1). 


For 
a 
Belief 
Network, 
the 
conditional 
p(xijxni) 
is 
dened 
by 
the 
Markov 
blanket 
of 
xi: 


Y

1 


p(xijxni)= 
p(xijpa 
(xi))p(xjjpa 
(xj)) 
(27.3.3)

Z 


j2ch(i) 


see 
for 
example, 
g(27.4). 
This 
means 
that 
only 
the 
parent 
and 
parents 
of 
children 
states 
are 
required 
in 
forming 
the 
sample 
update. 
The 
normalisation 
constant 
for 
this 
univariate 
distribution 
is 
straightforward 
to 
work 
out 
from 
the 
requirement: 


XY

Z 
=p(xijpa 
(xi))p(xjjpa 
(xj)) 
(27.3.4) 


xi 
j2ch(i) 


In 
the 
case 
of 
a 
continuous 
variable 
xi 
the 
summation 
above 
is 
replaced 
with 
integration. 


Evidence 


Evidence 
is 
readily 
dealt 
with 
by 
clamping 
for 
all 
samples 
the 
evidential 
variables 
into 
their 
evidential 
states. 
There 
is 
also 
no 
need 
to 
sample 
for 
these 
variables, 
since 
their 
states 
are 
known. 


27.3.1 
Gibbs 
sampling 
as 
a 
Markov 
chain 
In 
Gibbs 
sampling 
we 
have 
a 
sample 
of 
the 
joint 
variables 
xl 
at 
stage 
l. 
Based 
on 
this 
we 
produce 
a 
new 
l+1

joint 
sample 
x. 
This 
means 
that 
we 
can 
write 
Gibbs 
sampling 
as 
a 
procedure 
that 
draws 
from 


x 
l+1 
~ 
q(x 
l+1jx 
l) 
(27.3.5) 


for 
some 
distribution 
q(xl+1jxl). 
If 
we 
choose 
the 
variable 
to 
update, 
xi, 
at 
random 
from 
a 
distribution 
q(i), 
then 
Gibbs 
sampling 
corresponds 
to 
drawing 
samples 
using 
the 
Markov 
transition 




XY

q(x 
l+1jx 
l)=q(x 
l+1jx 
l;i)q(i);q(x 
l+1jx 
l;i)= 
p(x 
l+1jx 
l 
)x 
l+1 
;x 
l 
(27.3.6)

i 
nijj
ij=i

6

with 
q(i) 
> 
0. 
Our 
interest 
is 
to 
show 
that 
the 
stationary 
distribution 
of 
q(x0jx) 
is 
p(x). 
We 
carry 
this 
out 
assuming 
x 
is 
continuous 
– 
the 
discrete 
case 
is 
analogous:

ZZ

X

q(x0jx)p(x)=q(i)q(x0jxni)p(x) 
(27.3.7) 


x 
ix 


Z

XY..X

=q(i)x0j;xj 
p(xi0
jxni)p(xi;xni) 
(27.3.8) 
x

ij=6
i 


Z

X

000

=q(i)p(xijx)p(xi;x) 
(27.3.9)

nini
xi

i

XX

0\ 
0

=q(i)p(xijx 
)p(x 
)=q(i)p(x 
0)= 
p(x 
0) 
(27.3.10)

nini
ii 


DRAFT 
March 
9, 
2010 



Gibbs 
Sampling 


1x2x
Figure 
27.6: 
A 
two 
dimensional 
distribution 
for 
which 
Gibbs 
sampling 
fails. 
The 
distribution 
has 
mass 
only 
in 
the 
shaded 
quadrants. 
Gibbs 
sampling 


ll 
l

proceeds 
from 
the 
lth 
sample 
state 
(x1, 
x) 
and 
then 
sampling 
from 
p(x2jx),

21

l+1 
l+1 
l+1 
l

which 
we 
write 
(x 
, 
x 
) 
where 
x 
= 
x1. 
One 
then 
continues 
with 
a 


12 
1 


l+1

sample 
from 
p(x1jx2 
= 
x 
), 
etc. 
If 
we 
start 
in 
the 
lower 
left 
quadrant 


2 


and 
proceed 
this 
way, 
the 
upper 
right 
region 
is 
never 
explored. 


Hence, 
as 
long 
as 
we 
continue 
to 
draw 
samples 
according 
to 
the 
distribution 
q(x0jx), 
in 
the 
limit 
of 
a 
large 
number 
of 
samples 
we 
will 
ultimately 
tend 
to 
draw 
samples 
from 
p(x). 
Any 
distribution 
q(i) 
> 
0 
suces 
so 
visiting 
all 
variables 
equally 
often 
is 
also 
a 
valid 
choice. 
Technically, 
we 
also 
require 
that 
q(x0jx) 
has 
p(x) 
as 
its 
equilibrium 
distribution, 
so 
that 
no 
matter 
in 
which 
state 
we 
start, 
we 
always 
converge 
to 
p(x); 
see 
g(27.6) 
for 
a 
discussion 
of 
this 
issue. 


27.3.2 
Structured 
Gibbs 
sampling 
One 
can 
extend 
Gibbs 
sampling 
by 
using 
conditioning 
to 
reveal 
a 
tractable 
distribution 
on 
the 
remaining 
variables. 
For 
example, 
consider 
the 
simple 
distribution, 
g(27.5a) 


p(x1;x2;x3;x4)= 
(x1;x2)(x2;x3)(x3;x4)(x4;x1)(x1;x3) 
(27.3.11) 


In 
single-site 
Gibbs 
sampling 
we 
would 
condition 
on 
three 
of 
the 
four 
variables, 
and 
sample 
from 
the 
remaining 
variable. 
For 
example 


p(x1jx2, 
x3, 
x4) 
. 
(x1, 
x2)(x4;x1)(x1, 
x3) 
(27.3.12) 


However, 
we 
may 
use 
more 
limited 
conditioning 
as 
long 
as 
the 
conditioned 
distribution 
is 
easy 
to 
sample 


from. 
In 
the 
case 
of 
equation 
(27.3.11) 
we 
can 
condition 
on 
x3 
alone 
to 
give 
p(x1, 
x2, 
x4jx3) 
. 
(x1, 
x2)(x2, 
x3)(x3, 
x4)(x4, 
x1)(x1, 
x3) 
This 
can 
be 
written 
as 
a 
modied 
distribution, 
g(27.5b) 
(27.3.13) 
p(x1, 
x2, 
x4jx3) 
. 
0(x1, 
x2)0(x4, 
x1) 
(27.3.14) 


As 
a 
distribution 
on 
x1;x2;x4 
this 
is 
a 
singly-connected 
linear 
chain 
from 
which 
samples 
can 
be 
drawn 
exactly. 
A 
simple 
approach 
is 
compute 
the 
normalisation 
constant 
by 
any 
of 
the 
standard 
techniques, 
for 
example, 
using 
the 
factor 
graph 
method. 
One 
may 
then 
convert 
this 
undirected 
linear 
chain 
to 
a 
directed 
graph, 
and 
use 
ancestral 
sampling. 
These 
operations 
are 
linear 
in 
the 
number 
of 
variables 
in 
the 
conditioned 
distribution. 
Alternatively, 
one 
may 
form 
a 
junction 
tree 
from 
a 
set 
of 
potentials, 
choose 
a 
root 
and 
then 
form 
a 
set 
chain 
by 
reabsorption 
on 
the 
junction 
tree. 
Ancestral 
sampling 
can 
then 
be 
performed 
on 
the 
resulting 
oriented 
clique 
tree. 
This 
is 
the 
approach 
taken 
in 
GibbsSample.m. 


In 
the 
above 
example 
one 
can 
also 
reveal 
a 
tractable 
distribution 
by 
conditioning 
on 
x1, 


p(x3;x2;x4jx1) 
. 
(x1;x2)(x2;x3)(x3;x4)(x4, 
x1)(x1;x3) 
(27.3.15) 


and 
then 
draw 
a 
sample 
of 
x2;x3;x4 
from 
this 
distribution. 
A 
valid 
sampling 
procedure 
is 
then 
to 
draw 
a 
sample 
x1;x2;x4 
from 
equation 
(27.3.13) 
and 
then 
a 
sample 
x3;x2;x4 
from 
equation 
(27.3.15). 
These 
two 
steps 
are 
then 
iterated. 
Note 
that 
x2 
and 
x4 
are 
not 
constrained 
to 
be 
equal 
to 
their 
values 
in 
the 
previous 
sample. 
This 
procedure 
is 
generally 
to 
be 
preferred 
to 
the 
single-site 
Gibbs 
updating 
since 
the 
samples 
are 
less 
correlated 
from 
one 
sample 
to 
the 
next. 


See 
demoGibbsSample.m 
for 
a 
comparison 
of 
unstructured 
and 
structured 
sampling 
from 
a 
set 
of 
potentials. 


DRAFT 
March 
9, 
2010 



Gibbs 
Sampling 


-4-3-2-101234-3-2-10123
-3-2-10123-2-1.5-1-0.500.511.52
(a) 
(b) 
Figure 
27.7: 
Two 
hundred 
Gibbs 
samples 
for 
a 
two 
dimensional 
Gaussian. 
At 
each 
stage 
only 
a 
single 
component 
is 
updated. 
(a): 
For 
a 
Gaussian 
with 
low 
correlation, 
Gibbs 
sampling 
can 
move 
through 
the 
likely 
regions 
eectively. 
(b): 
For 
a 
strongly 
correlated 
Gaussian, 
Gibbs 
sampling 
is 
less 
eective 
and 
does 
not 
rapidly 
explore 
the 
likely 
regions, 
see 
demoGibbsGauss.m. 


27.3.3 
Remarks 
If 
the 
initial 
sample 
x1 
is 
in 
a 
part 
of 
the 
state 
space 
that 
is 
very 
unlikely 
then 
it 
may 
take 
some 
time 
for 
the 
samples 
to 
become 
representative, 
as 
only 
a 
single 
component 
of 
x 
is 
updated 
at 
each 
iteration. 
This 
motivates 
a 
so-called 
burn 
in 
stage 
in 
which 
the 
initial 
samples 
are 
discarded. 


In 
single 
site 
Gibbs 
sampling 
there 
will 
be 
a 
high 
degree 
of 
correlation 
in 
any 
two 
successive 
samples, 
since 
only 
one 
variable 
(in 
the 
single-site 
updating 
version) 
is 
updated 
at 
each 
stage. 
An 
ideal 
`perfect’ 
sampling 
scheme 
would 
draw 
each 
x 
`at 
random’ 
from 
p(x) 
– 
clearly, 
in 
general, 
two 
such 
perfect 
samples 
will 
not 
possess 
the 
same 
degree 
of 
correlation 
as 
those 
from 
Gibbs 
sampling. 
This 
motivates 
subsampling 


KK+10 
K+20

in 
which, 
say, 
every 
10th, 
sample 
x, 
x, 
x;:::, 
is 
taken, 
and 
the 
rest 
discarded. 


Due 
to 
its 
simplicity, 
Gibbs 
sampling 
is 
one 
of 
the 
most 
popular 
sampling 
methods 
and 
is 
particularly 
convenient 
when 
applied 
to 
Belief 
Networks, 
due 
to 
the 
Markov 
blanket 
property1 
. 
Gibbs 
sampling 
is 
a 
special 
case 
of 
the 
MCMC 
framework 
and, 
as 
with 
all 
MCMC 
methods, 
one 
should 
bear 
in 
mind 
that 
convergence 
can 
be 
a 
major 
issue 
– 
that 
is, 
answering 
questions 
such 
as 
`how 
many 
samples 
are 
needed 
to 
be 
reasonably 
sure 
that 
my 
sample 
estimate 
p(x5) 
is 
accurate?’ 
is 
dicult. 
Despite 
mathematical 
results 
for 
special 
cases, 
general 
rules 
of 
thumb 
and 
awareness 
on 
behalf 
of 
the 
user 
are 
required 
to 
monitor 
the 
eciency 
of 
the 
sampling. 


Gibbs 
sampling 
assumes 
that 
we 
can 
move 
throughout 
the 
space 
eectively 
by 
only 
single 
co-ordinate 
updates. 
We 
also 
require 
that 
every 
state 
can 
be 
visited 
innitely 
often. 
In 
g(27.6), 
we 
show 
a 
case 
in 
which 
the 
two 
dimensional 
continuous 
distribution 
has 
mass 
only 
in 
the 
lower 
left 
and 
upper 
right 
regions. 
In 
that 
case, 
if 
we 
start 
in 
the 
lower 
left 
region, 
we 
will 
always 
remain 
there, 
and 
never 
explore 
the 
upper 
right 
region. 
This 
problem 
occurs 
when 
two 
regions 
which 
are 
not 
connected 
by 
a 
`probable’ 
Gibbs 
path. 


Gibbs 
sampling 
becomes 
a 
perfect 
sampler 
when 
the 
distribution 
is 
factorised 
– 
that 
is 
the 
variables 
are 
independent. 
This 
suggests 
that 
in 
general 
Gibbs 
sampling 
will 
be 
less 
eective 
when 
variables 
are 
strongly 
correlated. 
For 
example, 
if 
we 
consider 
Gibbs 
sampling 
from 
a 
strongly 
correlated 
two 
variable 
Gaussian 
distribution, 
then 
updates 
will 
move 
very 
slowly 
in 
space, 
g(27.7). 


1The 
BUGS 
package 
www.mrc-bsu.cam.ac.uk/bugs 
is 
general 
purpose 
software 
for 
sampling 
from 
Belief 
Networks. 


DRAFT 
March 
9, 
2010 



Markov 
Chain 
Monte 
Carlo 
(MCMC) 


27.4 
Markov 
Chain 
Monte 
Carlo 
(MCMC) 
We 
assume 
we 
have 
a 
multivariate 
distribution 
in 
the 
form 


1 


p(x)= 
p 
* 
(x) 
(27.4.1)

Z 


where 
Z 
is 
the 
normalisation 
constant 
of 
the 
distribution 
and 
p 
* 
(x) 
is 
the 
unnormalised 
distribution. 
We 


R

assume 
we 
are 
able 
to 
evaluate 
p 
* 
(x 
= 
x), 
for 
any 
state 
x, 
but 
not 
Z, 
since 
Z 
=p 
* 
(x) 
is 
an 
intractable 


x 


high 
dimensional 
summation/integration. 


The 
idea 
in 
MCMC 
sampling 
is 
to 
sample, 
not 
directly 
from 
p(x), 
but 
from 
a 
dierent 
distribution 
such 
that, 
in 
the 
limit 
of 
a 
large 
number 
of 
samples, 
eectively 
the 
samples 
will 
be 
from 
p(x). 
To 
achieve 
this 
we 
forward 
sample 
from 
a 
Markov 
transition 
whose 
stationary 
distribution 
is 
equal 
to 
p(x). 


27.4.1 
Markov 
chains 
Consider 
the 
conditional 
distribution 
q(xl+1jxl). 
If 
we 
are 
given 
an 
initial 
sample 
x1, 
then 
we 
can 
re


12L

cursively 
generate 
samples 
x;x;:::;x. 
After 
a 
long 
time 
L 
» 
1, 
(and 
provided 
the 
Markov 
chain 
is 
`irreducible', 
meaning 
that 
we 
can 
eventually 
get 
from 
any 
state 
to 
any 
other 
state) 
the 
samples 
are 
from 
the 
stationary 
distribution 
q8 
(x) 
which 
is 
dened 
as 
(for 
a 
continuous 
variable) 


q8 
(x 
/ 
)= 
q(x 
/ 
jx)q8 
(x) 
(27.4.2) 


x 


The 
condition 
for 
a 
discrete 
variable 
is 
analogous 
on 
replacing 
integration 
with 
summation. 
The 
idea 
in 
MCMC 
is, 
for 
a 
given 
distribution 
p(x), 
to 
nd 
a 
transition 
q(x/ 
jx) 
which 
has 
p(x) 
as 
its 
stationary 
distribution. 
If 
we 
can 
do 
so, 
then 
we 
can 
draw 
samples 
from 
the 
Markov 
Chain 
by 
forward 
sampling 
and 
take 
these 
as 
samples 
from 
p(x). 


Note 
that 
for 
every 
distribution 
p(x) 
there 
will 
be 
more 
than 
one 
transition 
q(x/ 
jx) 
with 
p(x) 
as 
its 
stationary 
distribution. 
This 
is 
why 
there 
are 
very 
many 
dierent 
MCMC 
sampling 
methods, 
each 
with 
dierent 
characteristics 
and 
varying 
suitability 
for 
the 
particular 
distribution 
at 
hand. 


Detailed 
balance 


How 
do 
we 
construct 
a 
transition 
q(x/ 
jx) 
with 
given 
p(x) 
as 
its 
stationary 
distribution? 
This 
problem 
can 
be 
simplied 
if 
we 
consider 
special 
transitions 
that 
satisfy 
the 
detailed 
balance 
condition. 
If 
we 
are 
given 
the 
marginal 
distribution 
p(x), 
the 
detailed 
balance 
condition 
for 
a 
transition 
q 
is 


q(x/ 
jx) 
p(x/ 
) 


0

= 
, 
8x, 
x(27.4.3) 


q(xjx/ 
) 
p(x) 


Under 
this 
we 
see 


q(x/ 
jx)p(x)= 
q(xjx/ 
)p(x/ 
)= 
p(x/ 
) 
(27.4.4) 


xx 


so 
that 
p(x) 
is 
the 
stationary 
distribution 
of 
q(x/ 
jx). 
The 
detailed 
balance 
requirement 
can 
make 
the 
process 
of 
constructing 
a 
suitable 
transition 
easier 
since 
only 
the 
relative 
value 
of 
p(x/ 
) 
to 
p(x) 
is 
required 
in 
equation 
(27.4.3), 
and 
not 
the 
absolute 
value 
of 
p(x) 
or 
p(x/ 
). 


27.4.2 
Metropolis-Hastings 
sampling 
Consider 
the 
following 
transition 


q(x/ 
jx)= 
q~(x/ 
jx)f(x/ 
;x)+ 
(x/ 
;x)1 
- 
q~(x0/ 
jx)f(x0/ 
;x) 
(27.4.5) 


00

x

DRAFT 
March 
9, 
2010 
499 



Markov 
Chain 
Monte 
Carlo 
(MCMC) 


Algorithm 
25 
Metropolis-Hastings 
MCMC 
sampling. 


1: 
Choose 
a 
starting 
point 
x1 
. 
2: 
for 
i 
=2 
to 
L 
do 
3: 
Draw 
a 
candidate 
sample 
xcand 
from 
the 
proposal 
q~(x/ 
jxl- 
1). 
l..1| 
xcand)p(xcand)

q~(x

4: 
Let 
a 
= 
cand| 
xl..1)p(xl..1)

q~(x

l 
cand

5: 
if 
a 
= 
1 
then 
x= 
xI 
Accept 
the 
candidate 
6: 
else 
7: 
draw 
a 
random 
value 
u 
uniformly 
from 
the 
unit 
interval 
[0, 
1]. 
l 
cand

8: 
if 
u<a 
then 
x= 
xI 
Accept 
the 
candidate 
9: 
else 
ll- 
1

10: 
x= 
xI 
Reject 
the 
candidate 
11: 
end 
if 
12: 
end 
if 
13: 
end 
for 
0

where 
~q(x/ 
jx) 
is 
a 
so-called 
proposal 
distribution 
and 
0 
<f(x;x) 
= 
1 
a 
positive 
function. 
This 
denes 
a 
valid 
distribution 
q(x/ 
jx) 
since 
it 
is 
non-negative 
and

ZZZ

q(x/ 
jx)=q~(x/ 
jx)f(x/ 
;x)+1 
..q~(x0/ 
jx)f(x0/ 
;x) 
= 
1 
(27.4.6) 


0/ 
00

xxx

Our 
interest 
is 
to 
set 
f(x, 
x/ 
) 
such 
that 
the 
stationary 
distribution 
of 
q(x/ 
jx) 
is 
equal 
to 
p(x) 
for 
any 
proposal 
~q(x/ 
jx). 
That 
is 


Z

p(x/ 
)=q(x/ 
jx)p(x) 
(27.4.7) 


x

ZZ

=q~(x/ 
jx)f(x/ 
;x)p(x)+ 
p(x/ 
)1 
..q~(x0/ 
jx/ 
)f(x0/ 
;x/ 
)(27.4.8) 


00

xx

00

In 
order 
that 
this 
holds, 
we 
require 
(changing 
the 
integral 
variable 
from 
xto 
x)

ZZ

q~(x/ 
jx)f(x/ 
;x)p(x)=q~(xjx/ 
)f(x, 
x/ 
)p(x/ 
) 
(27.4.9) 


xx 


Now 
consider 
the 
Metropolis-Hastings 
acceptance 
function 




q~(xjx/ 
)p(x/ 
) 
q~(xjx/ 
)p 
* 
(x/ 
)

0

f(x;x)=min1, 
= 
min1, 
(27.4.10) 


q~(x/ 
jx)p(x)q~(x/ 
jx)p 
* 
(x)

which 
is 
dened 
for 
all 
x, 
x/ 
and 
has 
the 
detailed 
balance 
property 


..

f(x/ 
;x)~q(x/ 
jx)p(x) 
= 
minq~(x/ 
jx)p(x);q~(xjx/ 
)p(x/ 
)(27.4.11)

..

= 
minq~(xjx/ 
)p(x/ 
);q~(x/ 
jx)p(x)= 
f(x, 
x/ 
)~q(xjx/ 
)p(x/ 
) 
(27.4.12) 


0

Hence 
the 
function 
f(x;x) 
as 
dened 
above 
ensures 
equation 
(27.4.9) 
holds 
and 
that 
q(x/ 
jx) 
has 
p(x) 
as 
its 
stationary 
distribution. 


How 
do 
we 
sample 
from 
q(x/ 
jx)? 
Equation(27.4.5) 
can 
be 
interpreted 
as 
a 
mixture 
of 
two 
distributions, 


R

one 
proportional 
to 
~q(x/ 
jx)f(x/ 
;x) 
and 
the 
other 
(x/ 
;x) 
with 
mixture 
coecient 
1 
..0/ 
q~(x0/ 
jx)f(x0/ 
;x).

x

0

To 
draw 
a 
sample 
from 
this, 
we 
draw 
a 
sample 
from 
~q(x/ 
jx) 
and 
accept 
this 
with 
probability 
f(x;x). 
Since 
drawing 
from 
~q(x/ 
jx) 
and 
accepting 
are 
performed 
independently, 
the 
probability 
of 
accepting 
the 
drawn 


0

candidate 
is 
the 
product 
of 
these 
probabilities, 
namely 
~q(x/ 
jx)f(x;x). 
Otherwise 
the 
candidate 
is 
rejected 


0

and 
we 
take 
the 
sample 
x= 
x. 
Using 
the 
properties 
of 
the 
acceptance 
function, 
equation 
(27.4.10), 
the 
following 
is 
equivalent 
to 
deciding 
on 
accepting/rejecting 
the 
candidate. 
If 


q~(xjx/ 
)p 
* 
(x/ 
) 
>q~(x/ 
jx)p 
* 
(x) 
(27.4.13) 


DRAFT 
March 
9, 
2010 



Auxiliary 
Variable 
Methods 



Figure 
27.8: 
Metropolis-Hastings 
samples 
from 
a 
bivariate 
distribution 
p(x1;x2) 
using 
a 
proposal 
~q(x0jx)= 
N 
(x/ 


x, 
I). 
We 
also 
plot 
the 
iso-probability 
contours 
of 


p. 
Although 
p(x) 
is 
multimodal, 
the 
dimensionality 
is 
low 
enough 
and 
the 
modes 
suciently 
close 
such 
that 
a 
simple 
Gaussian 
proposal 
distribution 
is 
able 
to 
bridge 
the 
two 
modes. 
In 
higher 
dimensions, 
such 
multi-modality 
is 
more 
problematic. 
See 
demoMetropolis.m 
we 
accept 
the 
sample 
from 
~q(x0jx). 
Otherwise 
we 
accept 
the 
sample 
x/ 
from 
q(x0jx) 
with 
probability 
q~(xjx0)p 
(x0)=q~(x0jx)p 
(x). 
If 
we 
reject 
the 
candidate 
we 
take 
x/ 
= 
x. 


Note 
that 
if 
the 
candidate 
x/ 
is 
rejected, 
we 
take 
the 
original 
x 
as 
the 
new 
sample. 
Hence 
at 
each 
iteration 
of 
the 
algorithm 
produces 
a 
sample 
– 
either 
a 
copy 
of 
the 
current 
sample, 
or 
the 
candidate 
sample. 
A 
rough 
rule 
of 
thumb 
is 
to 
choose 
a 
proposal 
distribution 
for 
which 
the 
acceptance 
rate 
is 
between 
50% 
and 
85%[105]. 


Gaussian 
proposal 
distribution 


A 
common 
proposal 
distribution 
for 
multivariate 
x 
(writing 
explicitly 
as 
a 
vector) 
is 


..

1

- 
(x0..x)2

0

q~(x0jx)= 
Nx

x;2I. 
e 
22 
(27.4.14) 


for 
which 
~q(x0jx)= 
q~(xjx0) 
and 
the 
acceptance 
criterion 
becomes 




p 
(x0)

0

f(x, 
x)=min1, 
(27.4.15) 


p 
(x)

If 
the 
unnormalised 
probability 
of 
the 
candidate 
state 
is 
higher 
than 
the 
current 
state, 
we 
therefore 
accept 
the 
candidate. 
Otherwise, 
if 
the 
unnormalised 
probability 
of 
the 
candidate 
state 
is 
lower 
than 
the 
current 
state, 
we 
accept 
the 
candidate 
only 
with 
probability 
p 
(x0)=p(x). 
If 
the 
candidate 
is 
rejected, 
the 
new 
sample 
is 
taken 
to 
be 
a 
copy 
of 
the 
previous 
sample 
x. 
See 
g(27.8) 
for 
a 
demonstration. 


In 
high 
dimensions 
it 
is 
unlikely 
that 
a 
random 
candidate 
sampled 
from 
a 
Gaussian 
will 
result 
in 
a 
candidate 
probability 
higher 
than 
the 
current 
value, 
exercise(249). 
Because 
of 
this, 
only 
very 
small 
jumps 
(2 
small) 
are 
likely 
to 
be 
accepted. 
This 
limits 
the 
speed 
at 
which 
we 
explore 
the 
space 
x. 


This 
acceptance 
function 
above 
highlights 
that 
sampling 
is 
dierent 
from 
nding 
the 
optimum. 
Provided 
x/ 
has 
a 
higher 
probability 
than 
x, 
we 
accept 
x0. 
However, 
we 
also 
accept 
(with 
a 
specied 
acceptance 
probability) 
candidates 
that 
have 
also 
a 
lower 
probability 
than 
the 
current 
sample. 


27.5 
Auxiliary 
Variable 
Methods 
A 
practical 
concern 
in 
MCMC 
methods 
is 
ensuring 
that 
one 
moves 
eectively 
through 
the 
signicant 
probability 
regions 
of 
the 
distribution. 
For 
methods 
such 
as 
Metropolis-Hastings 
with 
local 
proposal 
distributions 
(local 
in 
the 
sense 
they 
are 
unlikely 
to 
propose 
a 
candidate 
far 
from 
the 
current 
sample), 
if 
the 
target 
distribution 
has 
isolated 
islands 
of 
high 
density, 
then 
the 
likelihood 
that 
we 
would 
be 
able 
to 
move 
from 
one 
island 
to 
the 
other 
is 
very 
small. 
If 
we 
attempt 
to 
make 
the 
proposal 
less 
local 
by 
using 
one 
with 
a 
high 
variance 
the 
chance 
then 
of 
landing 
at 
random 
on 
a 
high 
density 
island 
is 
remote. 
Auxiliary 
variable 
methods 
use 
additional 
dimensions 
to 
exploration 
and 
in 
certain 
cases 
to 
provide 
a 
bridge 
between 


DRAFT 
March 
9, 
2010 
501 



Auxiliary 
Variable 
Methods 


-5-4-3-2-101234500.511.522.53x 10-3
(a) 
(b) 
(c) 
Figure 
27.9: 
Hybrid 
Monte 
Carlo. 
(a): 
Multi-modal 
distribution 
p(x) 
for 
which 
we 
desire 
samples. 
(b): 
HMC 
forms 
the 
joint 
distibution 
p(x)p(y) 
where 
p(y) 
is 
Gaussian. 
(c): 
Starting 
from 
the 
point 
x, 
we 
rst 
draw 
a 
y 
from 
the 
Gaussian 
p(y), 
giving 
a 
point 
(x, 
y), 
green 
line. 
Then 
we 
use 
Hamiltonian 
dynamics 
(white 
line) 
to 
traverse 
the 
distribution 
at 
roughly 
constant 
energy 
for 
a 
xed 
number 
of 
steps, 
giving 


000

x;y. 
We 
accept 
this 
point 
if 
H(x;y0) 
>H(x, 
y0) 
and 
make 
the 
new 
sample 
x/ 
(red 
line). 
Otherwise 
this 
0

candidate 
is 
accepted 
with 
probability 
exp(H(x;y0) 
- 
H(x, 
y0)). 
If 
rejected 
the 
new 
sample 
x/ 
is 
taken 
as 
a 
copy 
of 
x. 


isolated 
high 
density 
islands. 


Consider 
drawing 
samples 
from 
p(x) 
where 
x 
is 
a 
high-dimensional 
vector. 
For 
an 
auxiliary 
variable 
y 
we 
introduce 
a 
distribution 
p(yjx), 
to 
form 
the 
joint 
distribution 


p(x, 
y)= 
p(yjx)p(x) 
(27.5.1) 


l

If 
we 
draw 
samples 
(x;yl) 
from 
this 
joint 
distribution 
then 
a 
valid 
set 
of 
samples 
from 
p(x) 
is 
given 
by 
taking 
the 
xl 
alone. 
If 
we 
sampled 
x 
directly 
from 
p(x) 
and 
then 
y 
from 
p(yjx), 
introducing 
y 
is 
pointless 
since 
there 
is 
no 
eect 
on 
the 
x 
sampling 
procedure. 
In 
order 
for 
this 
to 
be 
useful, 
therefore, 
the 
auxiliary 
variable 
must 
inuence 
how 
we 
sample 
x. 
Below 
we 
discuss 
some 
of 
the 
common 
auxiliary 
variable 
schemes. 


27.5.1 
Hybrid 
Monte 
Carlo 
Hybrid 
MC 
is 
a 
method 
for 
continuous 
variables 
that 
aims 
to 
make 
non-local 
jumps 
in 
the 
samples 
and, 
in 
so 
doing, 
to 
jump 
potentially 
from 
one 
mode 
to 
another. 
We 
dene 
the 
distribution 
from 
which 
we 
wish 
to 
sample 
as 


1 


Hx(x)

p(x)= 
e 
(27.5.2)
Zx 


for 
some 
given 
`Hamiltonian’ 
Hx(x) 
(this 
is 
just 
a 
potential). 
We 
then 
dene 
another, 
`easy’ 
distribution 
from 
which 
we 
can 
readily 
generate 
samples, 


1 


Hy(y)

p(y)= 
e 
(27.5.3)
Zy 


so 
that 
the 
joint 
distribution 
is 
given 
by 


11

Hx(x)+Hy(y) 
H(x;y)

p(x, 
y)= 
p(x)p(y)= 
e 
= 
e 
(27.5.4)

ZZ 


In 
the 
standard 
form 
of 
the 
algorithm, 
a 
multi-dimensional 
Gaussian 
is 
chosen 
for 
the 
auxiliary 
distribution 
with 
dim 
y 
= 
dim 
x, 
so 
that 


1 
T

Hy(y)= 
- 
yy 
(27.5.5)

2

The 
HMC 
algorithm 
rst 
draws 
from 
p(y) 
and 
subsequently 
from 
p(x, 
y). 
For 
a 
Gaussian 
p(y), 
sampling 
from 
is 
straightforward. 
In 
the 
next,`dynamic’ 
step, 
a 
sample 
is 
drawn 
from 
p(x, 
y) 
using 
a 
Metropolis 


DRAFT 
March 
9, 
2010 



Auxiliary 
Variable 
Methods 


MCMC 
sampler. 
The 
idea 
is 
to 
go 
from 
one 
point 
of 
the 
space 
x, 
y 
to 
a 
new 
point 
x0, 
y/ 
that 
is 
a 
non-trivial 
distance 
from 
x, 
y 
and 
which 
will 
be 
accepted 
with 
a 
high 
probability. 
The 
candidate 
(x0, 
y0) 
will 
have 
a 
good 
chance 
to 
be 
accepted 
if 
H(x0, 
y0) 
is 
close 
to 
H(x, 
y) 
– 
this 
can 
be 
achieved 
by 
following 
a 
contour 
of 
equal 
`energy’ 
H, 
as 
described 
in 
the 
next 
section. 


Hamiltonian 
dynamics 


We 
wish 
to 
make 
an 
update 
x/ 
= 
x 
+x, 
y/ 
= 
y 
+y 
for 
small 
x 
and 
y 
such 
that 
the 
Hamiltonian 
H(x, 
y) 
= 
Hx(x)+ 
Hy(y) 
is 
conserved, 


0

H(x, 
y0)= 
H(x, 
y) 
(27.5.6) 


We 
can 
satisfy 
this 
(up 
to 
rst 
order) 
by 
considering 
the 
Taylor 
expansion 


0

H(x, 
y0)= 
H(x 
+x, 
y 
+y) 


....

˜ 
H(x)+xTrxH(x, 
y)+ 
H(y)+yTryH(x, 
y)+ 
Ojxj2+ 
Ojyj2(27.5.7) 


Conservation, 
up 
to 
rst 
order, 
therefore 
requires 


xTrxH(x, 
y)+yTryH(x, 
y) 
= 
0 
(27.5.8) 


This 
is 
a 
single 
scalar 
requirement, 
and 
there 
are 
therefore 
many 
dierent 
solutions 
for 
x 
and 
y 
that 
satisfy 
this 
single 
condition. 
It 
is 
customary 
to 
use 
Hamiltonian 
dynamics, 
which 
correspond 
to 
the 
setting: 


x 
= 
ryH(x, 
y)y 
= 
..rxH(x, 
y) 
(27.5.9) 


where 
E 
is 
a 
small 
value 
to 
ensure 
that 
the 
Taylor 
expansion 
is 
accurate. 
Hence 


x(t 
+1) 
= 
x(t)+ 
ryHy(y) 
y(t 
+1) 
= 
y(t) 
- 
rxHx(x) 
(27.5.10) 


For 
the 
HMC 
method, 
H(x, 
y)= 
Hx(x)+ 
Hy(y), 
so 
that 
rxH(x, 
y)= 
rxHx(x) 
and 
ryH(x, 
y)= 
ryHy(y) 
For 
the 
Gaussian 
case, 
ryHy(y)= 
..y 
so 
that 


x(t 
+1) 
= 
x(t) 
- 
yy(t 
+1) 
= 
y(t) 
- 
rxH(x) 
(27.5.11) 


There 
are 
specic 
ways 
to 
implement 
the 
Hamiltonian 
dynamics 
called 
Leapfrog 
discretisation 
that 
are 
more 
accurate 
than 
the 
simple 
time-discretisation 
used 
above, 
and 
we 
refer 
the 
reader 
to 
[205] 
for 
details. 


In 
order 
to 
make 
a 
symmetric 
proposal 
distribution, 
at 
the 
start 
of 
the 
dynamic 
step, 
we 
choose 
E 
=+0 
or 
E 
= 
..0 
uniformly. 
This 
means 
that 
there 
is 
the 
same 
chance 
that 
we 
go 
back 
to 
the 
point 
x, 
y 
starting 
from 
x0, 
y0, 
as 
vice 
versa. 
We 
then 
follow 
the 
Hamiltonian 
dynamics 
for 
many 
time 
steps 
(usually 
of 
the 
order 
of 
several 
hundred) 
to 
reach 
a 
candidate 
point 
x0, 
y0. 
If 
the 
Hamiltonian 
dynamics 
is 
numerically 
accurate, 
H(x0, 
y0) 
will 
have 
roughly 
the 
same 
value 
as 
H(x, 
y). 
We 
then 
do 
a 
Metropolis 
step, 
and 
accept 
the 
point 
x0, 
y/ 
if 
H(x0, 
y0) 
>H(x, 
y) 
and 
otherwise 
accept 
it 
with 
probability 
exp(H(x0, 
y0) 
- 
H(x, 
y)). 
If 
rejected, 
we 
take 
the 
initial 
point 
x, 
y 
as 
the 
sample. 
Combined 
with 
the 
p(y) 
sample 
step, 
we 
then 
have 
the 
general 
procedure 
as 
described 
in 
algorithm(26). 


In 
HMC 
we 
use 
not 
just 
the 
potential 
Hx(x) 
to 
dene 
candidate 
samples, 
but 
the 
gradient 
of 
Hx(x) 
as 
well. 
An 
intuitive 
explanation 
for 
the 
success 
of 
the 
algorithm 
is 
that 
it 
is 
less 
myopic 
than 
straightforward 
Metropolis 
since 
the 
gradient 
enables 
the 
algorithm 
to 
feel 
its 
way 
to 
other 
regions 
of 
high 
probability 
by 
contouring 
paths 
in 
the 
augmented 
space. 
One 
can 
also 
view 
the 
auxiliary 
variables 
as 
momentum 
variables 
– 
it 
is 
as 
if 
the 
sample 
has 
now 
a 
momentum 
which 
can 
carry 
it 
through 
the 
low-density 
x-
regions. 
Provided 
this 
momentum 
is 
high 
enough, 
we 
can 
escape 
local 
regions 
of 
signicant 
probability, 
see 
g(27.9). 


DRAFT 
March 
9, 
2010 



Auxiliary 
Variable 
Methods 


Algorithm 
26 
Hybrid 
Monte 
Carlo 
sampling 


1: 
Start 
from 
x 
2: 
for 
i 
=1 
to 
L 
do 
3: 
Draw 
a 
new 
sample 
y 
from 
p(y). 
4: 
Choose 
a 
random 
(forwards 
or 
backwards) 
trajectory 
direction. 
5: 
Starting 
from 
x, 
y, 
follow 
Hamiltonian 
dynamics 
for 
a 
xed 
number 
of 
steps, 
giving 
a 
candidate 
x0, 
y0. 
6: 
Accept 
x0, 
y/ 
if 
H(x0, 
y0) 
>H(x, 
y), 
otherwise 
accept 
it 
with 
probability 
exp(H(x0, 
y0) 
- 
H(x, 
y)). 
7: 
If 
rejected, 
we 
take 
the 
sample 
as 
x, 
y. 
8: 
end 
for 
(a) 
(b) 
(c) 
Q

Figure 
27.10: 
Swendson-Wang 
updating 
for 
p(x) 
/ij 
exp 
I[xi 
= 
xj]. 
(a): 
Current 
sample 
of 
states 
(here 
on 
a 
nearest 
neighbour 
lattice). 
(b): 
Like 
coloured 
neighbours 
are 
bonded 
together 
with 
probability 
1..e.., 
forming 
clusters 
of 
variables. 
(c): 
Each 
cluster 
is 
given 
a 
random 
colour, 
forming 
the 
new 
sample. 


27.5.2 
Swendson-Wang 
Originally, 
the 
SW 
method 
was 
introduced 
to 
alleviate 
the 
problems 
encountered 
in 
sampling 
from 
Ising 
Models 
close 
to 
their 
critical 
temperature[268]. 
At 
this 
point 
large 
islands 
of 
same-state 
variables 
form 
so 
that 
strong 
correlations 
appear 
in 
the 
distribution 
– 
the 
scenario 
under 
which, 
for 
example, 
Gibbs 
sampling 
is 
not 
well 
suited. 
The 
method 
has 
been 
generalised 
to 
other 
models[88], 
although 
here 
we 
outline 
the 
procedure 
for 
the 
Ising 
model 
only, 
referring 
the 
reader 
to 
more 
specialised 
text 
for 
the 
extensions 
[37]. 
See 
also 
[198] 
for 
the 
use 
of 
auxiliary 
variables 
in 
perfect 
sampling. 


The 
Ising 
model 
with 
no 
external 
elds 
is 
dened 
on 
variables 
x 
=(x1;:::;xn), 
xi 
2f0, 
1} 
and 
takes 
the 
form 


Y

1 


I[xi=xj 
]

p(x)= 
e 
(27.5.12)

Z

ij 




which 
means 
that 
this 
is 
a 
pairwise 
Markov 
network 
with 
a 
potential 
contribution 
eif 
neighbouring 
nodes 
i 
and 
j 
on 
a 
square 
lattice 
are 
in 
the 
same 
state, 
and 
a 
contribution 
1 
otherwise. 
We 
assume 
that 
> 
0 
which 
encourages 
neighbours 
to 
be 
in 
the 
same 
state. 
The 
lattice 
based 
neighbourhood 
structure 
makes 
this 
dicult 
to 
sample 
from, 
and 
especially 
when 
ß 
˜ 
0:9 
which 
encourages 
large 
scale 
islands 
of 
same-state 
variables 
to 
form. 


The 
aim 
is 
to 
remove 
the 
problematic 
terms 
eI[xi=xj 
] 
by 
the 
use 
of 
the 
auxiliary 
`bond’ 
variables, 
yij, 
one 
for 
each 
edge 
on 
the 
lattice, 
making 
the 
conditional 
p(xjy) 
easy 
to 
sample 
from. 
This 
is 
given 
by 


Y

I[xi=xj 
]

p(xjy) 
. 
p(yjx)p(x) 
. 
p(yjx)e 
(27.5.13) 


ij 


I[xi

Using 
p(yjx) 
we 
can 
cancel 
the 
terms 
e=xj 
] 
by 
setting 


hi

YY

1 


I[xi=xj 
]

p(yjx)=p(yijjxi;xj)=I0 
<yij 
<e(27.5.14) 
zij

ijij 




=xj 
]

where 
I0 
<yij 
<eI[xidenotes 
a 
uniform 
distribution 
between 
0 
and 
eI[xi=xj 
]; 
zij 
is 
the 
normalisa-

DRAFT 
March 
9, 
2010 



Auxiliary 
Variable 
Methods 



Figure 
27.11: 
Ten 
successive 
samples 
from 
a 
25 
625 
Ising 
model 
p(x) 
/6exp 
ij 
I[xi 
= 
xj] 
, 
with 
ß 
=0:88, 
close 
to 
the 
critical 
temperature. 
The 
Swendson-Wang 
procedure 
is 
used. 
Starting 
in 
a 
random 
initial 
conguration, 
the 
samples 
quickly 
move 
away 
from 
this 
initial 
state, 
with 
the 
characteristic 
long-
range 
correlations 
of 
the 
variables 
seen 
close 
to 
the 
critical 
temperature. 


I[xi=xj 
]

tion 
constant 
zij 
= 
e. 
Hence 


p(xjy) 
/6p(yjx)p(x) 
(27.5.15) 


hi

Y

1 


I[xi=xj 
]I[xi=xj 
]

/I0 
<yij 
<ee 
(27.5.16)

I[xi=xj 
]

e

ij 


hi

Y

I[xi=xj 
]

/I0 
<yij 
<e(27.5.17) 
ij 


Let's 
assume 
that 
we 
have 
a 
sample 
fyijg. 
If 
yij 
> 
1, 
then 
to 
draw 
a 
sample 
from 
p(xjy), 
we 
must 
have 
1 
<eI[xi=xj 
], 
which 
means 
that 
xi 
and 
xj 
are 
constrained 
to 
be 
in 
the 
same 
state. 
Otherwise, 
if 
yij 
< 
1, 
then 
this 
introduces 
no 
extra 
constraint 
on 
xi 
and 
xj. 
Hence, 
wherever 
yij 
> 
1, 
we 
bond 
xi 
and 
xj 
to 
be 
in 
the 
same 
state. 


To 
sample 
from 
the 
bond 
variables 
p(yijjxi;xj) 
consider 
rst 
the 
situation 
that 
xi 
and 
xj 
are 
in 
the 


..YY



same 
state. 
Then 
p(yijjxi 
= 
xj)= 
Uyijj60;e. 
Similarly 
when 
xi 
and 
xj 
are 
in 
dierent 
states, 
p(yijjxi6

= 
xj)= 
U 
(yijj6[0, 
1]). 
A 
bond 
will 
occur 
if 
yij 
> 
1, 
which 
occurs 
with 
probability 


Z8 
hi

1 
eß 
..61 
..ß 


p(yij 
> 
1jxi 
= 
xj)=I0 
<yij 
<e= 
=1 
..6e 
(27.5.18)

ß 


yij 
=1 
zij 
e

..

Hence, 
if 
xi 
= 
xj, 
we 
bind 
xi 
and 
xj 
to 
be 
in 
the 
same 
state 
with 
probability 
1 
..6e. 
On 
the 
other 
hand 
if 
xi 
and 
xj 
are 
in 
dierent 
states, 
yij 
is 
uniformly 
distributed 
between 
0 
and 
1. 


After 
doing 
this 
for 
all 
the 
xi 
and 
xj 
pairs, 
we 
will 
end 
up 
with 
a 
graph 
in 
which 
we 
have 
clusters 
of 
like-state 
bonded 
variables. 
The 
algorithm 
simply 
chooses 
a 
random 
state 
for 
each 
cluster 
– 
that 
is, 
with 
probability 
0.5 
all 
variables 
in 
the 
cluster 
are 
in 
state 
1. 
The 
algorithm 
is 
described 
below, 
see 
g(27.10): 


Algorithm 
27 
Swendson-Wang 
sampling 


11

1: 
Start 
from 
a 
random 
conguration 
of 
all 
x1;:::;x.
n

2: 
for 
l 
=1 
to 
L 
do 
3: 
for 
i, 
j 
in 
the 
edge 
set 
do 
..

4: 
If 
xi 
= 
xj, 
we 
bond 
variables 
xi 
and 
xj 
with 
probability 
1 
..6e. 
5: 
end 
for 
6: 
For 
each 
cluster 
formed 
from 
the 
above, 
set 
the 
state 
of 
the 
cluster 
uniformly 
at 
random. 
ll

7: 
This 
gives 
a 
new 
joint 
conguration 
x1;:::;x.
n

8: 
end 
for 
This 
technique 
has 
found 
application 
in 
spatial 
statistics, 
particularly 
image 
restoration[134]. 


27.5.3 
Slice 
sampling 
Slice 
sampling[207] 
is 
an 
auxiliary 
variable 
technique 
that 
aims 
to 
overcome 
some 
of 
the 
diculties 
in 
choosing 
an 
appropriate 
`length 
scale’ 
in 
methods 
such 
as 
Metropolis 
sampling. 
The 
brief 
discussion 


DRAFT 
March 
9, 
2010 



Importance 
Sampling 



Figure 
27.12: 
The 
full 
slice 
for 
a 
given 
y. 
Ideally 
slice 
sampling 
would 
draw 
an 
x 
sample 
from 
anywhere 
on 
the 
full 
slice 
(green). 
In 
general 
this 
is 
intractable 
for 
a 
complex 
distribution 
and 
a 
local 
approximate 
slice 
is 
formed 
instead, 
see 
g(27.13). 


1

here 
follows 
that 
presented 
in 
[183] 
and 
[42]. 
We 
want 
to 
draw 
samples 
from 
p(x)= 
p 
(x) 
where 
the 


Z 


normalisation 
constant 
Z 
is 
unknown. 
By 
introducing 
the 
auxiliary 
variable 
y 
and 
dening 
the 
distribution 




1=Z 
for 
0 
= 
y 
= 
p 
(x)

p(x, 
y) 
=(27.5.19)

0 
otherwise 


we 
have

ZZ

p 
(x) 


11 


p(x, 
y)dy 
=dy 
= 
p 
(x)= 
p(x) 
(27.5.20)

ZZ

0 


which 
shows 
that 
the 
marginal 
of 
p(x, 
y) 
over 
y 
is 
equal 
to 
the 
distribution 
we 
wish 
to 
draw 
samples 
from. 
Hence 
if 
we 
draw 
samples 
from 
p(x, 
y), 
we 
can 
ignore 
the 
y 
samples 
and 
we 
will 
have 
a 
valid 
sampling 
scheme 
for 
p(x). 


To 
draw 
from 
p(x, 
y) 
we 
use 
Gibbs 
sampling, 
rst 
drawing 
from 
p(yjx) 
and 
then 
from 
p(xjy). 
Drawing 
a 
sample 
from 
p(yjx) 
means 
that 
we 
draw 
a 
value 
y 
from 
the 
uniform 
distribution 
U 
(y| 
[0;p 
(x)]). 


Given 
a 
sample 
y, 
one 
then 
draws 
a 
sample 
x 
from 
p(xjy). 
Using 
p(xjy) 
. 
p(x, 
y) 
we 
see 
that 
p(xjy) 
is 
the 
distribution 
over 
x 
such 
that 
p 
(x) 
>y: 


p(xjy) 
. 
I[p 
(x) 
>y] 
(27.5.21) 


For 
a 
given 
y, 
we 
call 
the 
x 
that 
satisfy 
this 
a 
`slice', 
g(27.12). 
Computing 
the 
normalisation 
of 
this 
distribution 
is 
in 
general 
non-trivial 
since 
we 
would 
in 
principle 
need 
to 
search 
over 
all 
x 
to 
nd 
those 
for 
which 
p 
(x) 
>y. 
Ideally 
we 
would 
like 
to 
get 
as 
much 
of 
the 
slice 
as 
feasible, 
since 
this 
will 
improve 
the 
mixing 
of 
the 
chain. 
If 
we 
concentrate 
on 
the 
part 
of 
the 
slice 
only 
very 
local 
to 
the 
current 
x, 
then 
the 
samples 
move 
through 
the 
space 
very 
slowly. 
If 
we 
attempt 
to 
guess 
at 
random 
a 
point 
a 
long 
way 
from 
x 
and 
check 
if 
is 
in 
the 
slice, 
this 
will 
be 
mostly 
unsuccessful. 
The 
happy 
compromise 
presented 
in 
algorithm(28)[207] 
and 
described 
in 
g(27.13) 
determines 
an 
appropriate 
local 
slice 
by 
adjusting 
the 
left 
and 
right 
regions. 
The 
technique 
is 
to 
start 
from 
the 
current 
x 
and 
attempt 
to 
nd 
the 
largest 
local 
slice 
by 
incrementally 
widening 
the 
candidate 
slice. 
Once 
we 
have 
the 
largest 
potential 
slice 
we 
attempt 
to 
sample 
from 
this. 
If 
the 
sample 
point 
within 
the 
local 
slice 
is 
in 
fact 
not 
in 
the 
slice, 
this 
is 
rejected 
and 
the 
slice 
is 
shrunk. 


This 
describes 
a 
procedure 
for 
sampling 
from 
a 
univariate 
distribution 
p 
(x). 
To 
sample 
from 
a 
multivariate 
distribution 
p(x), 
single 
variable 
Gibbs 
sampling 
can 
be 
used 
to 
sample 
from 
p(xjjxnj), 
repeatedly 
choosing 
a 
new 
variable 
xj 
to 
sample. 


27.6 
Importance 
Sampling 
Importance 
sampling 
is 
a 
technique 
to 
approximate 
averages 
with 
respect 
to 
an 
intractable 
distribution 
p(x). 
The 
term 
`sampling’ 
is 
arguably 
a 
misnomer 
since 
the 
method 
does 
not 
attempt 
to 
draw 
samples 
from 
p(x). 
Rather 
the 
method 
draws 
samples 
from 
a 
simpler 
importance 
distribution 
q(x) 
and 
then 
reweights 
them 
such 
that 
averages 
with 
respect 
to 
p(x) 
can 
be 
approximated 
using 
the 
samples 
from 
q(x). 
Consider 


R

(x)

p(x)= 
p 
where 
p 
(x) 
can 
be 
evaluated 
but 
Z 
=p 
(x) 
is 
an 
intractable 
normalisation 
constant. 
The 


Zx 


average 
of 
f(x) 
with 
respect 
to 
p(x) 
is 
given 
by

ZRR(x)

f(x)p 
(x)x 
f(x)p
q(x) 
q(x)

x

f(x)p(x)=R=R(27.6.1) 


p 
(x) 
p 
(x) 
q(x)

x 


x 


xq(x) 


DRAFT 
March 
9, 
2010 



Importance 
Sampling 



(a) 
(b) 
(c) 
(d) 
Figure 
27.13: 
(a): 
For 
the 
current 
sample 
x, 
a 
point 
y 
is 
sampled 
between 
0 
and 
p 
(x), 
giving 
a 
point 
(x, 
y) 
(black 
circle). 
Then 
an 
interval 
of 
width 
w 
is 
placed 
around 
x, 
the 
blue 
bar. 
The 
ends 
of 
the 
bar 
denote 
if 
the 
point 
is 
in 
the 
slice 
(green) 
or 
out 
of 
the 
slice 
(red). 
(b): 
The 
interval 
is 
increased 
until 
it 
hits 
a 
point 
out 
of 
the 
slice. 
(c): 
Given 
an 
interval 
a 
sample 
x' 
is 
taken 
uniformly 
in 
the 
interval. 
If 
the 
candidate 
x' 
is 
not 
in 
the 
slice 
(red), 
p(x0) 
<y, 
the 
candidate 
is 
rejected 
and 
the 
interval 
is 
shrunk. 
(d): 
The 
sampling 
from 
the 
interval 
is 
repeated 
until 
a 
candidate 
is 
in 
the 
slice 
(green), 
and 
is 
subsequently 
accepted. 


1

Let 
x;:::;xL 
be 
samples 
from 
q(x), 
then 
we 
can 
approximate 
the 
average 
by 


ZPPLl)p 
(xl) 
L
f(xX

l=1 
q(xl)

l

f(x)p(x) 
= 
f(x 
l)w 
(27.6.2)

PLp 
(xl)

x 


l=1 
q(xl) 
l=1 


where 
we 
dene 
the 
normalised 
importance 
weights 


L

p 
(xl)=q(xl)XP

l 


w 
= 
PL 
, 
with 
w 
l 
= 
1 
(27.6.3) 
(xl)=q(xl)

l=1 
p 
l=1 


In 
principle, 
reweighing 
the 
samples 
from 
q 
will 
give 
the 
correct 
result 
for 
the 
average 
with 
respect 
to 


p. 
Since 
the 
weight 
is 
a 
measure 
of 
how 
well 
q 
matches 
p, 
there 
will 
typically 
be 
only 
a 
single 
dominant 
weight. 
This 
is 
particularly 
evident 
in 
high 
dimensions 
there 
will 
typically 
only 
be 
one 
dominant 
weight 
with 
value 
close 
to 
1, 
and 
the 
rest 
will 
be 
zero, 
particularly 
if 
the 
sampling 
distribution 
q 
is 
not 
well 
matched 
to 
p. 
As 
an 
indication 
of 
this 
eect, 
consider 
a 
D-dimensional 
multivariate 
x 
with 
two 
samples 
x1 
and 
x2 
. 
For 
simplicity 
we 
assume 
that 
both 
p 
and 
q 
are 
factorised 
over 
their 
variables. 
The 
associated 
unnormalised 
importance 
weights 
are 
then 
DD

12

YPp 
(x) 
YPp 
(x)

1 
d2 
d

w~= 
;w~= 
(27.6.4)

12

q(x) 
q(x)

dd

d=1 
d=1 


If 
we 
assume 
the 
the 
match 
between 
q 
and 
p 
better 
is 
worse 
by 
a 
factor 
a 
= 
1 
in 
each 
of 
the 
dimensions 
at 
x2 
than 
x1 
then 


w~2 
..

= 
OD(27.6.5) 


w~1 


12

so 
that 
importance 
weight 
at 
wwill 
exponentially 
dominate 
w. 
A 
method 
that 
can 
help 
address 
this 
weight 
dominance 
is 
resampling. 
Given 
the 
weight 
distribution 
w1;:::;wL, 
one 
draws 
a 
set 
of 
L 
sample 
indices. 
This 
new 
set 
of 
indices 
will 
almost 
certainly 
contain 
repeats 
since 
any 
of 
the 
original 
low-weight 
samples 
will 
most 
likely 
not 
be 
included. 
The 
weight 
of 
each 
of 
these 
new 
samples 
is 
set 
uniformly 
to 
1=L. 
This 
procedure 
helps 
select 
only 
the 
`ttest’ 
of 
the 
samples 
and 
is 
known 
as 
Sampling 
Importance 
Resampling[235]. 


DRAFT 
March 
9, 
2010 
507 



Importance 
Sampling 


Algorithm 
28 
Slice 
Sampling 
(univariate). 


1: 
Choose 
a 
starting 
point 
x1 
and 
step 
size 
w. 
2: 
for 
i 
=1 
to 
L 
do 
..

3: 
Draw 
a 
vertical 
coordinate 
y 
uniformly 
from 
the 
interval0;p 
(xi). 
4: 
Create 
a 
horizontal 
interval 
(xleft;xright) 
that 
contains 
xi 
as 
follows: 
5: 
Draw 
r 
~ 
U 
(r| 
(0, 
1)) 
6: 
xleft 
= 
xi 
- 
rw, 
xright 
= 
xi 
+ 
(1 
- 
r)wI 
Create 
an 
initial 
interval 
7: 
while 
p 
(xleft) 
>y 
do 
8: 
xleft 
= 
xleft 
- 
wI 
Step 
out 
left 
9: 
end 
while 
10: 
while 
p 
(xright) 
>y 
do 
11: 
xright 
= 
xright 
+ 
wI 
Step 
out 
right 
12: 
end 
while 
13: 
accept 
= 
false 
14: 
while 
accept 
= 
false 
do 
15: 
draw 
a 
random 
value 
x' 
uniformly 
from 
the 
unit 
interval 
(xleft;xright). 
16: 
if 
p 
(x0) 
>y 
then 
17: 
accept 
= 
true 
I 
Found 
a 
valid 
sample 
18: 
else 
19: 
modify 
the 
interval 
(xleft;xright) 
as 
follows: 
0

20: 
if 
x>xi 
then 
21: 
xright 
= 
x' 
I 
Shrinking 
22: 
else 
0

23: 
= 
x
xleft 


24: 
end 
if 
25: 
end 
if 
26: 
end 
while 
i+1 
0

27: 
x= 
x
28: 
end 
for 
27.6.1 
Sequential 
importance 
sampling 
One 
can 
apply 
importance 
sampling 
to 
temporal 
distributions 
p(x1:t) 
for 
which 
the 
importance 
distribution 
samples 
from 
q(x1:t) 
are 
paths. 
In 
many 
applications 
such 
as 
tracking, 
one 
wishes 
to 
update 
ones 
beliefs 
as 
time 
increases 
and, 
as 
such, 
is 
required 
to 
resample 
and 
then 
reweight 
the 
whole 
path. 
For 
distributions 
p(x1:t) 
with 
a 
Markov 
structure, 
one 
would 
expect 
that 
a 
local 
update 
is 
possible, 
without 
needing 
to 
deal 
with 
the 
previous 
path. 
To 
show 
this, 
consider 
the 
unnormalised 
importance 
weights 
for 
a 
sample 
path 


l

x

1:t 
lll 
l

p 
(x) 
p 
(x) 
p 
(x) 
p 
(x)

l 
1:t1:t..11:tl 
1

w~= 
= 
;w~= 
(27.6.6)

t 
1

l 
llll 
l

q(x) 
q(x) 
p 
(x)q(xjx)q(x)

1:t1:t..11:t..1t1:t..11

We 
can 
recursively 
dene 
the 
un-normalised 
weights 
using 


ll 


w~= 
w~t..1t
l 
, 
t> 
1 
(27.6.7)

t 


where 


l

p 
(x)

1:t
l 
= 
(27.6.8)

t 


l 
ll

p 
(x)q(xjx)

1:t..1t1:t..1

This 
means 
that 
in 
SIS 
we 
need 
only 
dene 
the 
conditional 
importance 
distribution 
q(xtjx1:t..1). 
The 
ideal 
setting 
of 
the 
sequential 
importance 
distribution 
is 
q 
= 
p 
and 
q(xtjx1:t..1)= 
p(xtjx1:t..1), 
although 
this 
choice 
is 
impractical 
in 
most 
cases. 


508 
DRAFT 
March 
9, 
2010 



Importance 
Sampling 


v1v2v3v4
h1h2h3h4
Figure 
27.14: 
A 
Dynamic 
Bayesian 
Network. 
In 
many 
applications 
of 
interest, 
the 
emission 
distribution 
p(vtjht) 
is 
non-Gaussian, 
leading 
to 
the 
formal 
intractability 
of 
ltering/smoothing. 


For 
Dynamic 
Bayes 
Networks, 
equation 
(27.6.8) 
will 
simplify 
considerably. 
For 
example 
consider 
distributions 
with 
a 
Hidden 
Markov 
independence 
structure, 


t

YP

p(v1:t;h1:t)= 
p(v1jh1)p(h1) 
p(vtjht)p(htjht..1) 
(27.6.9) 
t=2 


where 
v1:t 
are 
observations 
and 
h1:t 
are 
the 
random 
variables. 
A 
cancelation 
of 
terms 
in 
the 
numerator 
and 
denominator 
occurs, 
leaving 
simply 


p(vtjhl 
)p(hl 
jhl 
)

ttt..1

l 
= 
(27.6.10)

t 


q(hl 
jhl 
)

t1:t

Sequential 
importance 
sampling 
is 
also 
known 
as 
particle 
ltering. 
Particularly 
in 
cases 
where 
the 
transition 
is 
easy 
to 
sample 
from, 
a 
common 
sequential 
importance 
distribution 
is 
q(htjh1:t..1)= 
p(htjht..1) 
(27.6.11) 
in 
which 
case, 
from 
equation 
(27.6.8), 
l 
= 
p(vtjht) 
and 
the 
unnormalised 
weights 
are 
recursively 
dened 


t 


by 


ll 


w~= 
w~t..1p(vtjhl) 
(27.6.12)

tt

A 
drawback 
of 
this 
procedure 
is 
that 
after 
a 
small 
number 
of 
iterations 
only 
very 
few 
particle 
weights 
will 
be 
signicantly 
non-zero 
due 
to 
the 
mismatch 
between 
the 
importance 
distribution 
q 
and 
the 
target 
distribution 
p. 
This 
can 
be 
addressed 
using 
resampling, 
as 
described 
in 
section(27.6)[82]. 


27.6.2 
Particle 
ltering 
as 
an 
approximate 
forward 
pass 
Particle 
ltering 
can 
be 
viewed 
as 
an 
approximation 
to 
the 
exact 
ltering 
recursion. 
Using 
. 
to 
represent 
the 
ltered 
distribution, 


(ht) 
. 
p(htjv1:t) 
(27.6.13) 


the 
exact 
ltering 
recursion 
is 


Z

(ht) 
. 
p(vtjht)p(htjht..1)(ht..1) 
(27.6.14) 
ht..1 


A 
PF 
can 
be 
viewed 
as 
an 
approximation 
of 
equation 
(27.6.14) 
in 
which 
the 
message 
(ht..1) 
is 
approximated 
by 
a 
sum 
of 
-peaks: 


L

XP

l

(ht..1) 
˜ 
wt..1ht..1;hl 
(27.6.15)

t..1 
l=1 


PL

ll

where 
ware 
the 
normalised 
importance 
weights= 
1, 
and 
hl 
are 
the 
particles 
. 
In 
other 


t..1 
l=1 
wt..1 
t..1 


words, 
the 
. 
message 
is 
represented 
as 
a 
weighted 
mixture 
of 
delta-spikes 
where 
the 
weight 
and 
position 
of 
the 
spikes 
are 
the 
parameters 
of 
the 
distribution. 
Using 
equation 
(27.6.15) 
in 
equation 
(27.6.14), 
we 
have 


L

XP

(ht)= 
1 
p(vtjht) 
p(htjhl 
)w 
l 
(27.6.16)

t..1t..1

Z 


l=1 


DRAFT 
March 
9, 
2010 



Importance 
Sampling 


The 
constant 
Z 
is 
used 
to 
normalise 
the 
distribution 
(ht). 
Although 
(ht..1) 
was 
a 
simple 
sum 
of 
delta 
peaks, 
in 
general 
(ht) 
will 
not 
be 
– 
the 
delta-peaks 
get 
`broadened’ 
by 
the 
hidden-to-hidden 
and 
hidden-
to-observation 
factors. 
Our 
task 
is 
then 
to 
approximate 
(ht) 
as 
a 
new 
sum 
of 
delta-peaks. 
Below 
we 
discuss 
a 
method 
to 
achieve 
this 
for 
which 
explicit 
knowledge 
of 
the 
normalisation 
Z 
is 
not 
required. 
This 
is 
useful 
since 
in 
many 
tracking 
applications 
the 
normalisation 
of 
the 
emission 
p(vtjht) 
is 
unknown. 


A 
Monte-Carlo 
sampling 
approximation 


A 
simple 
approach 
to 
forming 
an 
approximate 
mixture-of-delta 
functions 
representation 
of 
equation 


(27.6.16) 
is 
to 
generate 
a 
set 
of 
sample 
points 
using 
importance 
sampling. 
That 
is 
we 
generate 
a 
set 
of 
samples 
h1;:::;hL 
from 
some 
importance 
distribution 
q(ht) 
which 
gives 
the 
unnormalised 
importance 
tt 


weights 


p(vtjhl)PL 
jhl0)wl0

l 
tl0=1 
p(ht
l 
t..1t..1 


w~= 
(27.6.17)

t 


q(hl 
)

t

Dening 
the 
normalised 
weights: 


l 


l 
w~t

wt 
= 
Pl0(27.6.18) 


w~

l0t 


we 
obtain 
an 
approximation 


L

XP

l

(ht) 
˜ 
wht;hl 
(27.6.19)

tt
l=1 


Ideally 
one 
would 
use 
the 
importance 
distribution 
that 
makes 
the 
importance 
weights 
unity, 
namely 


L

XP

l 


q(ht) 
. 
p(vtjht) 
p(htjhl 
)w 
(27.6.20)

t..1t..1 
l=1 


However, 
this 
is 
often 
dicult 
to 
sample 
from 
directly 
due 
to 
the 
unknown 
normalisation 
of 
the 
emission 
p(vtjht). 
A 
simpler 
alternative 
is 
to 
sample 
from 
the 
transition 
mixture: 


L

XP

l 


q(ht)= 
p(htjhl 
)w 
(27.6.21)

t..1t..1 
l=1 


1 
L

To 
do 
so, 
one 
rst 
samples 
a 
component 
l* 
from 
the 
histogram 
with 
weights 
from 
wt..1;:::;wt..1. 
Given 
this 
sample 
index, 
say 
l, 
one 
then 
draws 
a 
sample 
from 
p(htjhl0). 
In 
this 
case 
the 
un-normalised 
weights 


t..1

become 
simply 


l 


w~= 
p(vtjhl 
) 
(27.6.22)

tt

This 
Forward-Sampling-Resampling 
procedure 
is 
used 
in 
demoParticleFilter.m 
and 
in 
the 
following 
toy 
example. 


Example 
113 
(A 
toy 
face-tracking 
example). 
At 
time 
t 
a 
binary 
face 
template 
is 
in 
a 
location 
ht, 
which 
describes 
the 
upper-left 
corner 
of 
the 
template 
using 
a 
two-dimensional 
vector. 
At 
time 
t 
= 
1 
the 
position 
of 
the 
face 
is 
known, 
see 
g(27.15a). 
The 
face 
template 
is 
known. 
In 
subsequent 
times 
the 
face 
moves 
randomly 
according 
to 


ht 
= 
ht..1 
+ 
t 
(27.6.23) 


where 
t 
N 
(t 


0, 
I) 
is 
a 
two 
dimensional 
zero 
mean 
unit 
covariance 
noise 
vector. 
In 
addition, 
a 
fraction 
of 
the 
binary 
pixels 
in 
the 
whole 
image 
are 
selected 
at 
random 
and 
their 
states 
ipped. 
The 
aim 


DRAFT 
March 
9, 
2010 



Importance 
Sampling 


Figure 
27.15: 
Tracking 
an 
object 
with 
a 
particle 
lter 
containing 
50 
particles. 
The 
small 
circles 
are 
the 
particles, 
scaled 
by 
their 
weights. 
The 
correct 
corner 
position 
of 
the 
face 
is 
given 
by 
the 
`', 
the 
ltered 
average 
by 
the 
large 
circle 
`o', 
and 
the 
most 
likely 
particle 
by 
`+'. 
(a): 
Initial 
position 
of 
the 
face 
without 
noise 
and 
corresponding 
weights 
of 
the 
particles. 
(b): 
Face 
with 
noisy 
background 
and 
the 
tracked 
corner 
position 
after 
20 
timesteps. 
The 
Forward-
Sampling-Resampling 
PF 
method 
is 


used 
to 
maintain 
a 
healthy 
proportion 
of 
non-zero 
weights. 
See 
demoParticleFilter.m 


is 
to 
try 
to 
track 
the 
upper-left 
corner 
of 
the 
face 
through 
time. 


We 
need 
to 
dene 
the 
emission 
distribution 
p(vtjht) 
on 
the 
binary 
pixels 
with 
vi 
2f0, 
1g. 
Consider 
the 
following 
compatibility 
function 


T

(vt, 
ht)= 
vv~(ht) 
(27.6.24)

t 


where 
v~(ht) 
is 
the 
vector 
representing 
the 
image 
with 
a 
clean 
face 
placed 
at 
position 
ht. 
This 
measures 
the 
overlap 
between 
the 
face 
template 
and 
the 
noisy 
image 
restricted 
to 
the 
template 
pixels. 
The 
compatibility 
function 
is 
maximal 
when 
the 
observed 
image 
vt 
has 
the 
face 
placed 
at 
position 
ht. 
We 
can 
therefore 
tentatively 
dene 


p(vtjht) 
. 
(vt, 
ht) 
(27.6.25) 


A 
subtlety 
is 
that 
ht 
is 
continuous, 
and 
in 
the 
compatibility 
function 
we 
rst 
map 
ht 
to 
the 
nearest 
integer 
pixel 
representation. 
We 
have 
not 
specied 
the 
normalisation 
constant 
of 
this 
distribution, 
which 
fortunately 
this 
is 
not 
required 
by 
the 
particle 
ltering 
algorithm. 
In 
g(27.15a) 
50 
particles 
are 
used 
to 
track 
the 
face. 
The 
particles 
are 
plotted 
along 
with 
their 
corresponding 
weights. 
For 
each 
t> 
1, 
5% 
of 
the 
pixels 
are 
selected 
at 
random 
in 
the 
image 
and 
their 
states 
ipped. 
Using 
the 
Forward-Sampling-
Resampling 
method 
we 
can 
successfully 
track 
the 
face 
despite 
the 
presence 
of 
the 
background 
clutter. 


Real 
tracking 
applications 
involve 
complex 
issues, 
including 
tracking 
multiple 
objects, 
transformations 
of 
the 
object 
(scaling, 
rotation, 
morphology 
changes). 
Nevertheless, 
the 
principles 
are 
largely 
the 
same 
and 
many 
tracking 
applications 
work 
by 
seeking 
simple 
compatibility 
functions, 
often 
based 
on 
the 
colour 
histogram 
in 
a 
template. 
Indeed, 
tracking 
objects 
in 
complex 
environments 
was 
one 
of 
the 
original 
applications 
of 
particle 
lters 
[140]. 


DRAFT 
March 
9, 
2010 


510152025303540510152025303540102030405000.010.020.030.040.050.060.07particle weights
(a)
510152025303540510152025303540102030405000.010.020.030.040.050.060.070.080.09particle weights
(b)

Exercises 


27.7 
Code 
potsample.m: 
Exact 
sample 
from 
a 
set 
of 
potentials 
ancestralsample.m: 
Ancestral 
sample 
from 
a 
Belief 
Net 
JTsample.m: 
Sampling 
from 
a 
consistent 
Junction 
Tree 
GibbsSample.m: 
Gibbs 
sampling 
from 
a 
set 
of 
potentials 
demoMetropolis.m: 
Demo 
of 
Metropolis 
sampling 
for 
a 
bimodal 
distribution 
metropolis.m: 
Metropolis 
sample 
logp.m: 
Log 
of 
a 
bimodal 
distribution 
demoParticleFilter.m: 
Demo 
Particle 
Filtering 
(Forward-Sampling-Resampling 
method) 
placeobject.m: 
Place 
an 
object 
in 
a 
grid 
compat.m: 
Compatibility 
function 
demoSampleHMM.m: 
Naive 
Gibbs 
sampling 
for 
a 
HMM 


27.8 
Exercises 
Exercise 
246 
(Box-Muller 
method). 
Let 
x1 
~ 
U 
(x1| 
[0, 
1]), 
x2 
~ 
U 
(x2| 
[0, 
1]) 
and 


pp

y1 
=..2 
log 
x1 
cos 
2x2, 
y2 
=..2 
log 
x2 
sin 
2x2 
(27.8.1) 
Show 
that 
Zp(y1, 
y2) 
=p(y1jx1, 
x2)p(y2jx1, 
x2)p(x1)p(x2)dx1dx2 
= 
N 
(y1 
0, 
1) 
N 
(y2 
0, 
1) 
(27.8.2) 
and 
suggest 
an 
algorithm 
to 
sample 
from 
a 
univariate 
normal 
distribution. 
Exercise 
247. 
Consider 
the 
distribution 
p(x1, 
. 
. 
. 
, 
x6) 
= 
p(x1)p(x2)p(x3jx1, 
x2)p(x4jx3)p(x5jx3)p(x6jx4, 
x5) 
(27.8.3) 


For 
x5 
xed 
in 
a 
given 
state 
x5, 
write 
down 
a 
distribution 
on 
the 
remaining 
variables 
p0(x1;x2;x3;x4;x6) 
and 
explain 
how 
forward 
(ancestral) 
sampling 
can 
be 
carried 
out 
for 
this 
new 
distribution. 


Exercise 
248. 
Consider 
an 
Ising 
model 
on 
an 
M 
× 
M 
square 
lattice 
with 
nearest 
neighbour 
interactions: 


X

p(x) 
. 
exp 
I[xi 
= 
xj] 
(27.8.4) 
ij 


Now 
consider 
the 
M 
× 
M 
grid 
as 
a 
checkerboard, 
and 
give 
each 
white 
square 
a 
label 
wi, 
and 
each 
black 
square 
a 
label 
bj, 
so 
that 
each 
square 
is 
associated 
with 
a 
particular 
variable. 
Show 
that 


p(b1;b2;:::, 
jw1;w2;:::)= 
p(b1jw1;w2;:::)p(b2jw1;w2;:::) 
::. 
(27.8.5) 


That 
is, 
conditioned 
on 
the 
white 
variables, 
the 
black 
variables 
are 
independent. 
The 
converse 
is 
also 
true, 
that 
conditioned 
on 
the 
black 
variables, 
the 
white 
variables 
are 
independent. 
Explain 
how 
this 
can 
be 
exploited 
by 
a 
Gibbs 
sampling 
procedure. 
This 
procedure 
is 
known 
as 
checkerboard 
or 
black 
and 
white 
sampling. 


Exercise 
249. 
Consider 
the 
symmetric 
Gaussian 
proposal 
distribution 


..



0

q~(x0jx)= 
Nx

x;2I(27.8.6)

q 


and 
the 
target 
distribution 


..



p(x)= 
Nx 


0;2I(27.8.7)

p

where 
dim 
x 
= 
N. 
Show 
that



p(x0) 
N2 


q

log 
= 
- 
(27.8.8) 


p(x)22 


q~(x0jx) 
p 


Discuss 
how 
this 
result 
relates 
to 
the 
probability 
of 
accepting 
a 
Metropolis-Hastings 
update 
under 
a 
Gaussian 
proposal 
distribution 
in 
high-dimensions. 


DRAFT 
March 
9, 
2010 



Exercises 


Exercise 
250. 
The 
le 
demoSampleHMM.m 
performs 
naive 
Gibbs 
sampling 
of 
the 
posterior 
p(h1:T 
jv1:T 
) 
for 
a 
HMM. 
At 
each 
Gibbs 
update 
a 
single 
variable 
ht 
is 
chosen, 
with 
the 
remaining 
h 
variables 
clamped. 
The 
procedure 
starts 
from 
t 
=1 
and 
sweeps 
forwards 
through 
time. 
When 
the 
end 
time 
t 
= 
T 
is 
reached, 
the 
joint 
state 
h1:T 
is 
taken 
as 
a 
sample 
from 
the 
posterior. 
The 
parameter 
. 
controls 
how 
deterministic 
the 
hidden 
transition 
matrix 
p(htjht..1) 
will 
be. 
Adjust 
demoSampleHMM.m 
to 
run 
100 
times, 
each 
time 
for 
the 
same 
, 
computing 
a 
mean 
absolute 
error 
over 
these 
100 
runs. 
Then 
repeat 
this 
for 
. 
=0:1, 
1, 
10, 
20. 
Discuss 
why 
the 
performance 
of 
this 
Gibbs 
sampling 
routine 
deteriorates 
with 
increasing 
. 


DRAFT 
March 
9, 
2010 



Exercises 


DRAFT 
March 
9, 
2010 



CHAPTER 
28 


Deterministic 
Approximate 
Inference 


28.1 
Introduction 
Deterministic 
approximate 
inference 
methods 
are 
an 
alternative 
to 
the 
stochastic 
techniques 
discussed 
in 
chapter(27). 
Whilst 
stochastic 
methods 
are 
powerful 
and 
often 
generally 
applicable, 
they 
nevertheless 
produce 
sample 
estimates 
of 
a 
quantity. 
Even 
if 
we 
are 
able 
to 
perform 
perfect 
sampling, 
we 
would 
still 
only 
obtain 
an 
approximate 
result 
due 
to 
the 
inherent 
uncertainty 
introduced 
by 
sampling. 
Furthermore, 
in 
practice, 
drawing 
exact 
samples 
is 
typically 
computationally 
intractable 
and 
assessing 
the 
quality 
of 
the 
sample 
estimates 
is 
dicult. 
In 
this 
chapter 
we 
discuss 
some 
alternative 
deterministic 
approximate 
inference 
schemes. 
The 
rst, 
Laplace's 
method, 
is 
a 
simple 
perturbation 
technique. 
The 
second 
class 
of 
methods 
are 
those 
that 
produce 
rigorous 
bounds 
on 
quantities 
of 
interest. 
Such 
methods 
are 
interesting 
since 
they 
provide 
certain 
knowledge 
– 
it 
may 
be 
sucient, 
for 
example, 
to 
show 
that 
a 
marginal 
probability 
is 
greater 
than 
0.1 
in 
order 
to 
make 
an 
informed 
decision. 
A 
further 
class 
of 
methods 
are 
the 
consistency 
methods, 
such 
as 
loopy 
belief 
propagation. 
Such 
methods 
have 
revolutionised 
certain 
elds, 
including 
error 
correction[183], 
providing 
performance 
unobtainable 
from 
sampling 
based 
procedures. 


It 
is 
important 
to 
bear 
in 
mind 
that 
no 
single 
approximation 
technique, 
deterministic 
or 
stochastic, 
is 
going 
to 
beat 
all 
others 
on 
all 
problems, 
given 
the 
same 
computational 
resources. 
In 
this 
sense, 
insight 
as 
to 
the 
properties 
of 
the 
approximation 
method 
used 
is 
useful 
in 
matching 
an 
approximation 
method 
to 
the 
problem 
at 
hand. 


28.2 
The 
Laplace 
approximation 
Consider 
a 
distribution 
on 
a 
continuous 
variable 
of 
the 
form 
1 


..E(x)

p(x)= 
e 
(28.2.1)

Z 
The 
Laplace 
method 
makes 
a 
Gaussian 
approximation 
of 
p(x) 
based 
on 
a 
local 
perturbation 
expansion 


around 
a 
mode 
x 
* 
. 
First 
we 
nd 
the 
mode 
numerically, 
giving 
x 
* 
= 
argmin 
E(x) 
(28.2.2) 
x 
Then 
a 
Taylor 
expansion 
up 
to 
second 
order 
around 
this 
mode 
gives 
E(x) 
˜ 
E(x 
) 
+ 
(x 
- 
x 
)T 
rEjx 
* 
+ 
1 
2 
(x 
- 
x 
)T 
H 
(x 
- 
x 
) 
(28.2.3) 


where 
H 
= 
rrE(x)jx 
* 
is 
the 
Hessian 
evaluated 
at 
the 
mode. 
At 
the 
mode, 
rEjx 
* 
= 
0, 
and 
an 
approximation 
of 
the 
distribution 
is 
given 
by 
the 
Gaussian 




1 
..

- 
1 
(x..x 
)TH(x..x 
) 


* 


p 
(x)= 
Z* 
e 
2 
= 
Nx 


x 
, 
H..1(28.2.4) 


515 



Properties 
of 
Kullback-Leibler 
Variational 
Inference 


Figure 
28.1: 
Fitting 
a 
mixture 
of 
Gaussians 
p(x) 
(blue) 
with 
a 
single 
Gaussian. 
The 
green 
curve 
minimises 
KL(qjp) 
corresponding 
to 
tting 
a 
local 
model. 
The 
red 
curve 
minimises 
KL(pjq) 
corresponding 
to 
moment 
matching. 


-30-20-10010203000.050.10.150.20.250.30.350.4
p

which 
has 
mean 
x 
* 
and 
covariance 
H..1, 
with 
Z* 
=det 
(2H..1). 
Similarly, 
we 
can 
use 
the 
above 
expansion 
to 
estimate 
the 
integral

ZZ

p

..E(x) 
..E(x 
)- 
1 
(x..x 
)TH(x..x 
) 
..E(x 
)

ee 
2 
= 
e 
det 
(2H..1) 
(28.2.5) 


xx 


Although 
the 
Laplace 
approximation 
ts 
a 
Gaussian 
to 
a 
distribution, 
it 
is 
not 
necessarily 
the 
`best’ 
Gaussian 
approximation. 
As 
we'll 
see 
below, 
other 
criteria, 
such 
as 
based 
on 
minimal 
KL 
divergence 
between 
p(x) 
and 
a 
Gaussian 
approximation 
may 
be 
more 
appropriate, 
depending 
on 
the 
context. 
A 
benet 
of 
Laplace's 
method 
is 
its 
relative 
speed 
and 
simplicity 
compared 
with 
other 
approximate 
inference 
techniques. 


28.3 
Properties 
of 
Kullback-Leibler 
Variational 
Inference 
Variational 
methods 
can 
be 
used 
to 
approximate 
a 
complex 
distribution 
p(x) 
by 
a 
simpler 
distribution 
q(x). 
Given 
a 
denition 
of 
discrepancy 
between 
an 
approximation 
q(x) 
to 
p(x), 
any 
free 
parameters 
of 
q(x) 
are 
then 
set 
by 
minimising 
the 
discrepancy. 


A 
particularly 
popular 
measure 
of 
the 
discrepancy 
between 
an 
approximation 
q(x) 
and 
the 
intractable 
distribution 
p(x) 
is 
the 
Kullback-Leibler 
divergence 


KL(qjp)= 
hlog 
qi..hlog 
pi(28.3.1)

qq 


It 
is 
straightforward 
to 
show 
that 
KL(qjp) 
= 
0 
and 
is 
zero 
if 
and 
only 
if 
the 
distributions 
p 
and 
q 
are 
identical, 
see 
section(8.8). 
Note 
that 
whilst 
the 
KL 
divergence 
cannot 
be 
negative, 
there 
is 
no 
upper 
bound 
on 
the 
value 
it 
can 
potentially 
take 
so 
that 
the 
discrepancy 
can 
be 
`innitely’ 
bad. 


28.3.1 
Bounding 
the 
normalisation 
constant 
For 
a 
distribution 
of 
the 
form 


1 


p(x)= 
e 
(x) 
(28.3.2)

Z 


we 
have 


KL(qjp)= 
hlog 
q(x)i..hlog 
p(x)i= 
hlog 
q(x)i..h(x)i+ 
log 
Z 
(28.3.3)

q(x) 
q(x) 
q(x) 
q(x) 


Since 
KL(qjp) 
= 
0 
this 
immediately 
gives 
the 
bound 


log 
Z 
..hlog 
q(x)i+ 
h(x)i(28.3.4)

q(x) 
q(x)

|{z}|{z}p

entropy 
energy 


which 
is 
called 
the 
`free 
energy’ 
bound 
in 
the 
physics 
community[236]. 
Using 
the 
notation 
Hq 
for 
the 
entropy 
of 
q, 
we 
can 
write 
the 
bound 
more 
compactly 
as 


log 
Z 
= 
Hq 
+ 
h(x)iq(x) 
(28.3.5) 


The 
KL(qjp) 
method 
provides 
therefore 
a 
lower 
bound 
on 
the 
normalisation 
constant. 
For 
some 
models 
it 
is 
possible 
(using 
alternative 
methods, 
see 
for 
example 
[287] 
and 
exercise(258)) 
to 
also 
form 
an 
upper 


516 
DRAFT 
March 
9, 
2010 



Properties 
of 
Kullback-Leibler 
Variational 
Inference 


bound 
on 
the 
normalisation 
constant. 
With 
both 
an 
upper 
and 
lower 
bound 
on 
the 
normalisation 
terms, 
we 
are 
able 
to 
bracket 
marginals 
l 
= 
p(xi) 
= 
u, 
see 
exercise(261). 
The 
tightness 
of 
the 
resulting 
bracket 
gives 
an 
indication 
as 
to 
how 
tight 
the 
bounding 
procedures 
are. 
Even 
in 
cases 
where 
the 
resulting 
bracket 
is 
weak 
– 
for 
example 
it 
might 
be 
that 
the 
result 
is 
that 
0:1 
<p(cancer 
= 
true) 
< 
0:99, 
this 
may 
be 
sucient 
for 
decision 
making 
purposes 
since 
the 
probability 
of 
cancer 
is 
suciently 
large 
to 
merit 
action. 


28.3.2 
Bounding 
the 
marginal 
likelihood 
In 
Bayesian 
modelling 
the 
likelihood 
of 
the 
model 
M 
with 
parameters 
. 
generating 
data 
D 
is 
given 
by 


Z

p(DjM)=p(Dj, 
M) 
p(jM) 
(28.3.6) 


. 
' 
{z}|{z" 


likelihood 
prior 


This 
quantity 
is 
fundamental 
to 
model 
comparison. 
However, 
in 
cases 
where 
. 
is 
high-dimensional, 
the 
integral 
over 
. 
is 
dicult 
to 
perform. 
Using 
Bayes’ 
rule, 


p(Dj, 
M)p(jM) 


p(jD, 
M) 
= 
(28.3.7) 


p(DjM) 


and 
considering 


KL(q()jp(jD, 
M)) 
= 
hlog 
q()i..hlog 
p(jD, 
M)i(28.3.8)

q() 
q() 


= 
hlog 
q()i..hlog 
p(Dj, 
M)p(jMi+ 
log 
p(DjM) 
(28.3.9)

q() 
q() 


the 
non-negativity 
of 
the 
Kullback-Leibler 
divergence 
gives 
the 
bound 


log 
p(DjM) 
..hlog 
q()i+ 
hlog 
p(Dj, 
M)p(jMi(28.3.10)

q() 
q() 


This 
bound 
holds 
for 
any 
distribution 
q() 
and 
saturates 
when 
q()= 
p(jD, 
M). 
Since 
using 
the 
optimal 
setting 
is 
assumed 
computationally 
intractable, 
the 
idea 
in 
variational 
bounding 
is 
to 
choose 
a 
distribution 
family 
for 
q() 
for 
which 
the 
bound 
is 
computationally 
tractable 
(for 
example 
factorised, 
or 
Gaussian), 
and 
then 
maximise 
the 
bound 
with 
respect 
to 
any 
free 
parameters 
of 
q(). 
The 
resulting 
bound 
then 
can 
be 
used 
as 
a 
surrogate 
for 
the 
exact 
marginal 
likelihood 
in 
model 
comparison. 


28.3.3 
Gaussian 
approximations 
using 
KL 
divergence 
Minimising 
KL(qjp) 


Using 
a 
simple 
approximation 
q(x) 
of 
a 
more 
complex 
distribution 
p(x) 
by 
minimising 
KL(qjp) 
tends 
to 
give 
a 
solution 
for 
q(x) 
that 
focuses 
on 
a 
local 
mode 
of 
p(x), 
thereby 
underestimating 
the 
variance 
of 
p(x). 
To 
show 
this, 
consider 
approximating 
a 
mixture 
of 
two 
Gaussians 
with 
equal 
variance 
2 
, 


..



1 
....

p(x)= 
Nx 


- 
, 
2+ 
Nx 


, 
2(28.3.11)

2

see 
g(28.1), 
with 
a 
single 
Gaussian 


..



2

q(x)= 
Nx 


m, 
s 
(28.3.12) 


We 
wish 
to 
nd 
the 
optimal 
m, 
s2 
that 
minimise 


KL(qjp)= 
hlog 
q(x)i..hlog 
p(x)i(28.3.13)

q(x) 
q(x) 


If 
we 
consider 
the 
case 
that 
the 
two 
Gaussian 
components 
of 
p(x) 
are 
well 
separated, 
µ 
» 
, 
then 
setting 
q(x) 
to 
be 
centred 
on 
the 
left 
mode 
at 
..µ 
the 
Gaussian 
q(x) 
only 
has 
appreciable 
mass 
close 
to 
.., 
so 
that 
the 
second 
mode 
at 
µ 
has 
negligible 
contribution 
to 
the 
Kullback-Leibler 
divergence. 
In 
this 
sense 
one 
can 
approximate 
p(x) 
˜ 
1 
q(x), 
so 
that 


2 


KL(qjp) 
hlog 
q(x)i..hlog 
p(x)i= 
log 
2 
(28.3.14)

q(x) 
q(x) 


DRAFT 
March 
9, 
2010 



Properties 
of 
Kullback-Leibler 
Variational 
Inference 



Figure 
28.2: 
A 
planar 
pairwise 
Markov 
random 
eld 
on 
a 
set 
of 
variables 
x1;:::;x25,
Q


representing 
a 
distribution 
of 
the 
formij 
(xi;xj). 
In 
statistical 
physics 
such 
lattice 
models 
include 
the 
Ising 
model 
on 
binary 
`spin’ 
variables 
xi 
2f+1, 
..1} 
with 
wij 
xixj

(xi;xj)= 
e. 
On 
the 
other 
hand, 
setting 
m 
= 
0, 
which 
is 
the 
correct 
mean 
of 
the 
distribution 
p(x), 
very 
little 
of 
the 


2

mass 
of 
the 
mixture 
is 
captured 
unless 
sis 
large, 
giving 
a 
poor 
t 
and 
large 
KL 
divergence. 
Another 
way 
to 
view 
this 
is 
to 
consider 
KL(qjp)= 
hlog 
q(x)=p(x)i; 
provided 
q 
is 
close 
to 
p 
around 
where 
q 
has 


q(x)

signicant 
mass, 
the 
ratio 
q(x)=p(x) 
will 
be 
order 
1 
and 
the 
KL 
divergence 
small. 
Setting 
m 
= 
0 
means 
that 
q(x)=p(x) 
is 
large 
where 
q 
has 
signicant 
mass, 
and 
is 
therefore 
a 
poor 
t. 
The 
optimal 
solution 
in 
this 
case 
is 
to 
place 
the 
Gaussian 
close 
to 
a 
single 
mode. 
Note, 
however, 
that 
for 
two 
modes 
that 
are 
less 
well-separated, 
the 
optimal 
solution 
will 
not 
necessarily 
be 
to 
place 
the 
Gaussian 
around 
a 
local 
mode. 
In 
general, 
the 
optimal 
Gaussian 
t 
needs 
to 
be 
determined 
numerically 
– 
that 
is, 
there 
is 
no 
closed 
form 
solution 
to 
nding 
the 
optimal 
mean 
and 
(co)variance 
parameters. 


Minimising 
KL(pjq) 


For 
tting 
a 
Gaussian 
q 
to 
p 
based 
on 
KL(pjq), 
we 
have 


KL(pjq)= 
hlog 
p(x)i..hlog 
q(x)i(28.3.15)

p(x) 
p(x)

DE

= 
- 
1(x 
- 
m)2- 
1 
log 
det..2+ 
const. 
(28.3.16)

22p(x) 
2 


Minimising 
this 
with 
respect 
to 
m 
and 
2 
we 
obtain: 





m 
= 
hxi2 
=(x 
- 
m)2(28.3.17)

p(x) 
, 
p(x) 


so 
that 
the 
optimal 
Gaussian 
t 
matches 
the 
rst 
and 
second 
moments 
of 
p(x). 


In 
the 
case 
of 
g(28.1), 
the 
mean 
of 
p(x) 
is 
a 
zero, 
and 
the 
variance 
of 
p(x) 
is 
large. 
This 
solution 
is 
therefore 
dramatically 
dierent 
from 
that 
produced 
by 
tting 
the 
Gaussian 
using 
KL(qjp). 
The 
t 
found 
using 
KL(qjp) 
focusses 
on 
making 
q 
t 
p 
locally 
well, 
whereas 
KL(pjq) 
focusses 
on 
making 
q 
t 
p 
well 
to 
the 
global 
statistics 
of 
the 
distribution 
(possibly 
at 
the 
expense 
of 
a 
good 
local 
match). 


28.3.4 
Moment 
matching 
properties 
of 
minimising 
KL(pjq) 
Q

For 
simplicity, 
consider 
a 
factorised 
approximation 
q(x)=i 
q(xi). 
Then 


X

KL(pjq)= 
hlog 
p(x)i..hlog 
q(xi)) 
(28.3.18)

p(x) 
p(xi) 
i 


The 
rst 
entropic 
term 
is 
independent 
of 
q(x) 
so 
that 
up 
to 
a 
constant 
independent 
of 
q(x), 
the 
above 
is

X

KL(p(xi)jq(xi)) 
(28.3.19) 
i 


so 
that 
optimally 
q(xi)= 
p(xi). 
That 
is, 
the 
optimal 
factorised 
approximation 
is 
to 
set 
the 
factors 
of 
q(xi) 
to 
the 
marginals 
of 
p(xi), 
exercise(265). 
For 
any 
approximating 
distribution 
in 
the 
exponential 
family, 
minimising 
KL(pjq) 
corresponds 
to 
moment 
matching, 
see 
exercise(264). 
In 
practice, 
one 
generally 
cannot 
compute 
the 
moments 
of 
p(x) 
(since 
the 
distribution 
p(x) 
is 
considered 
`intractable'), 
so 
that 
tting 
q 
to 
p 
based 
only 
on 
KL(pjq) 
does 
not 
itself 
lead 
to 
a 
practical 
algorithm 
for 
approximate 
inference. 
Nevertheless, 
as 
we 
will 
see, 
it 
is 
a 
useful 
subroutine 
for 
local 
approximations, 
in 
particular 
Expectation 
Propagation. 


518 
DRAFT 
March 
9, 
2010 



Variational 
Bounding 
Using 
KL(qjp) 


28.4 
Variational 
Bounding 
Using 
KL(qjp) 
In 
this 
section 
we 
discuss 
how 
to 
t 
a 
distribution 
q(x) 
from 
some 
assumed 
family 
to 
an 
`intractable’ 
distribution 
p(x). 
As 
we 
saw 
above 
for 
the 
case 
of 
tting 
Gaussians, 
the 
optimal 
q 
needs 
to 
be 
found 
numerically. 
This 
itself 
can 
be 
a 
complex 
task 
(indeed, 
formally 
this 
can 
be 
just 
as 
dicult 
as 
performing 
inference 
directly 
with 
the 
intractable 
p) 
and 
the 
reader 
may 
wonder 
why 
we 
trade 
a 
dicult 
inference 
task 
for 
a 
potentially 
dicult 
optimisation 
problem. 
The 
general 
idea 
is 
that 
the 
optimisation 
problem 
has 
some 
local 
smoothness 
properties 
that 
enable 
one 
to 
rapidly 
nd 
a 
reasonable 
optimum 
based 
on 
generic 
optimisation 
methods. 
To 
make 
these 
ideas 
more 
concrete, 
we 
discuss 
a 
particular 
case 
of 
tting 
q 
to 
a 
formally 
intractable 
p 
in 
section(28.4.1) 
below. 


28.4.1 
Pairwise 
Markov 
random 
eld 
A 
canonical 
`intractable’ 
distribution 
is 
the 
pairwise 
Markov 
Random 
Field 
dened 
on 
binary 
variables 
xi 
2f+1, 
..1g, 
i 
=1;:::;D, 


PP

1 


p(x)= 
ei;j 
wij 
xixj 
+i 
bixi 
(28.4.1)

Z(w, 
b)

Here 
the 
`partition 
function’ 
Z(w, 
b) 
ensures 
normalisation, 


XPP

Z(w, 
b)=ei;j 
wij 
xixj 
+i 
bixi 
(28.4.2) 


x 


22

Since 
x= 
1, 
the 
terms 
wiixare 
constant 
and 
without 
loss 
of 
generality 
we 
may 
set 
wii 
to 
zero1 
. 
This

ii 


gives 
an 
undirected 
distribution 
with 
connection 
geometry 
dened 
by 
the 
weights 
w. 
In 
practice, 
the 
weights 
often 
dene 
local 
interactions 
on 
a 
lattice, 
see 
g(28.2). 
A 
case 
for 
which 
inference 
in 
this 
model 
is 
required 
is 
given 
in 
example(114). 


Example 
114 
(Bayesian 
image 
denoising). 
Consider 
a 
binary 
image, 
where 
x 
describes 
the 
state 
of 
the 
clean 
pixels 
(1 
encoding). 
We 
assume 
a 
noisy 
pixel 
generating 
process 
that 
takes 
each 
clean 
pixel 
xi 
and 
ips 
its 
binary 
state: 


Y

yixi

p(yjx)=p(yijxi);p(yijxi) 
. 
e(28.4.3) 


i 


The 
probability 
that 
yi 
and 
xi 
are 
in 
the 
same 
state 
is 
e=(e. 
+ 
e..). 
Our 
interest 
is 
to 
the 
posterior 
distribution 
on 
clean 
pixels 
p(xjy). 
In 
order 
to 
do 
this 
we 
need 
to 
make 
an 
assumption 
as 
to 
what 
clean 
images 
look 
like. 
We 
do 
this 
using 
a 
MRF 


P

ij 
wij 
xixj

p(x) 
. 
e(28.4.4) 


for 
some 
settings 
of 
wij 
> 
0, 
with 
i 
~ 
j 
indicating 
that 
i 
and 
j 
are 
neighbours. 
This 
encodes 
the 
assumption 
that 
clean 
images 
tend 
to 
have 
neighbouring 
pixels 
in 
the 
same 
state. 
An 
isolated 
pixel 
in 
a 
dierent 
state 
to 
its 
neighbours 
is 
unlikely 
under 
this 
prior. 
We 
now 
have 
the 
joint 
distribution 


Y

p(x, 
y)= 
p(x)p(yijxi) 
(28.4.5) 
i 


see 
g(28.3), 
from 
which 
the 
posterior 
is 
given 
by 


PP

p(yjx)p(x)

ij 
wij 
xixj 
+yixi

i

p(xjy)= 
P. 
e(28.4.6) 


p(yjx)p(x)

x 


Quantities 
such 
as 
the 
MAP 
state 
(most 
a 
posteriori 
probable 
image), 
marginals 
p(xijy) 
and 
the 
normalisation 
constant 
are 
of 
interest. 


1Whilst 
inference 
with 
a 
general 
MRF 
is 
formally 
computationally 
intractable 
(no 
exact 
polynomial 
time 
methods 
are 
known), 
two 
celebrated 
results 
that 
we 
mention 
in 
passing 
are 
that 
for 
the 
planar 
MRF 
model 
with 
pure 
interactions 
(b 
= 
0), 
the 
partition 
function 
is 
computable 
in 
polynomial 
time[157, 
95, 
177, 
115, 
243], 
as 
is 
the 
MAP 
state 
for 
attractive 
planar 
Ising 
models 
w> 
0 
[121], 
see 
section(28.8). 


DRAFT 
March 
9, 
2010 
519 



Variational 
Bounding 
Using 
KL(qjp) 



Figure 
28.3: 
A 
distribution 
on 
pixels. 
The 
lled 
nodes 
indicate 
observed 
noisy 
pixels, 
the 
unshaded 
nodes 
a 
Markov 
Random 
Field 
on 
latent 
clean 
pixels. 
The 
task 
is 
to 
infer 
the 
clean 
pixels 
given 
the 
noisy 
pixels. 
The 
MRF 
encourages 
the 
posterior 
distribution 
on 
the 
clean 
pixels 
to 
contain 
neighbouring 
pixels 
in 
the 
same 
state. 
Kullback-Leibler 
based 
methods 


For 
the 
MRF 
we 
have 


XX

KL(qjp)= 
hlog 
qi..wij 
hxixji..bi 
hxii+ 
log 
Z 
60 
(28.4.7)

q 
qq 
ij 
i 


Rewriting, 
this 
gives 
a 
bound 
on 
the 
log-partition 
function 


XX

log 
Z 
..hlog 
qi+wij 
hxixji+bi 
hxii(28.4.8)

q 
qq

|{z}Xij 
i 
entropy 
|{zq 
energy 


The 
bound 
saturates 
when 
q 
= 
p. 
However, 
this 
is 
of 
little 
help 
since 
we 
cannot 
compute 
the 
averages 
of 
variables 
with 
respect 
to 
this 
intractable 
distribution 
hxixji, 
hxii. 
The 
idea 
of 
a 
variational 
method 
is 


pp 


to 
assume 
a 
simpler 
`tractable’ 
distribution 
q 
for 
which 
these 
averages 
can 
be 
computed, 
along 
with 
the 
entropy 
of 
q. 
Minimising 
the 
KL 
divergence 
with 
respect 
to 
any 
free 
parameters 
of 
q(x) 
is 
then 
equivalent 
to 
maximising 
the 
lower 
bound 
on 
the 
log 
partition 
function. 


Factorised 
approximation 


A 
simple 
`naive’ 
assumption 
is 
the 
fully 
factorised 
distribution 


q(x)= 
qi(xi) 
(28.4.9) 
i 


The 
graphical 
model 
of 
this 
approximation 
is 
given 
in 
g(28.4a). 
In 
this 
case 


XXX

log 
Z 
..hlog 
qii+wij 
hxixji+bi 
hxii(28.4.10)

qi 
q(xi;xj 
) 
q(xi) 
iij 
i 


For 
a 
factorised 
distribution 
and 
bearing 
in 
mind 
that 
xi 
2f+1, 
..1g, 


1 
i 
= 
j

hxixji6= 
(28.4.11)

hxiihxji6i=6
j 
For 
a 
binary 
variable, 
one 
may 
use 
the 
convenient 
parametrization 


i

e

qi(xi 
= 
1) 
= 
(28.4.12) 


ei 
+ 
e..i 


so 
that 


hxii6qi 
= 
+1 
6q(xi 
= 
1) 
..61 
6q(xi 
= 
..1) 
= 
tanh(i) 
(28.4.13) 
Figure 
28.4: 
(a): 
Naive 
Mean 
Field 
approximation 



Q

q(x)=i 
qi(xi). 
(b): 
A 
spanning 
tree 
approximation. 
(c): 
A 
decomposable 
(hypertree) 
approximation.


(a) 
(b) 
(c) 
DRAFT 
March 
9, 
2010 



Variational 
Bounding 
Using 
KL(qjp) 
This 
gives 
the 
following 
lower 
bound 
on 
the 
log 
partition 
function: 


XXX

log 
Z 
B() 
H(i)+wij 
tanh(i) 
tanh(j)+bi 
tanh(i) 
(28.4.14) 
ii6=ji 


where 
H(i) 
is 
the 
binary 
entropy 
of 
a 
distribution 
parameterised 
according 
to 
equation 
(28.4.12): 




. 
..i

H(i) 
= 
logei 
+ 
e 
..6i 
tanh(i) 
(28.4.15) 


Finding 
the 
best 
factorised 
approximation 
in 
the 
minimal 
Kullback-Liebler 
divergence 
sense 
then 
corresponds 
to 
maximising 
the 
bound 
B() 
with 
respect 
to 
the 
variational 
parameters 
. 


The 
bound 
B, 
equation 
(28.4.14), 
is 
generally 
non-convex 
in 
. 
and 
riddled 
with 
local 
optima. 
Finding 
the 
globally 
optimal 
. 
is 
therefore 
typically 
a 
computationally 
hard 
problem. 
It 
seems 
that 
we 
have 
simply 
replaced 
a 
computationally 
hard 
problem 
of 
computing 
log 
Z 
by 
an 
equally 
hard 
computational 
problem 
of 
maximising 
B(). 
Indeed, 
the 
`graphical 
structure’ 
of 
this 
optimisation 
problem 
matches 
exactly 
that 
of 
the 
original 
MRF. 
However, 
the 
hope 
is 
that 
by 
transforming 
a 
dicult 
discrete 
summation 
into 
a 
continuous 
optimisation 
problem, 
we 
will 
be 
able 
to 
bring 
to 
the 
table 
techniques 
of 
continuous 
variable 
numerical 
optimisation 
to 
nd 
a 
good 
approximation. 
A 
particularly 
simple 
optimisation 
technique 
is 
to 
dierentiate 
the 
bound 
equation 
(28.4.14) 
and 
equate 
to 
zero. 
Straightforward 
algebra 
leads 
to 
requirement 
that 
the 
optimal 
solution 
satises 
the 
equations 


X

i 
= 
bi 
+wij 
tanh(j), 
8i 
(28.4.16) 
j=i

6

One 
may 
show 
that 
updating 
any 
i 
according 
to 
equation 
(28.4.16) 
increases 
B(). 
This 
is 
called 
asyc
hronous 
updating 
and 
is 
guaranteed 
to 
lead 
to 
a 
(local) 
minimum 
of 
the 
KL 
divergence, 
section(28.4.3). 


Once 
a 
converged 
solution 
has 
been 
identied, 
in 
addition 
to 
a 
bound 
on 
log 
Z, 
we 
can 
approximate 


hxii6= 
tanh(i) 
(28.4.17)

hxii6pq 
Validity 
of 
the 
factorised 
approximation 


When 
might 
one 
expect 
such 
a 
naive 
factorised 
approximation 
to 
work 
well? 
Clearly, 
if 
wij 
is 
very 
small 
for 
i6

= 
j, 
the 
distribution 
p 
will 
be 
eectively 
factorised. 
However, 
a 
more 
interesting 
case 
is 
when 
each 
variable 
xi 
has 
many 
neighbours. 
It 
is 
useful 
to 
write 
the 
MRF 
as 


PPPP

11 
1 
1 


ij 
wij 
xixj 
Di 
xij 
wij 
xj 
Di 
xizi

p(x)= 
e= 
e 
D= 
e 
(28.4.18)

ZZ 
Z 


where 
the 
local 
`elds’ 
are 
dened 
as 


X

1 


zi 
6wijxj 
(28.4.19)

D

j 


We 
now 
invoke 
a 
circular 
(but 
self-consistent) 
argument: 
Let's 
assume 
that 
p(x) 
is 
factorised. 
Then 
for 


PX

zi, 
each 
of 
the 
terms 
xj 
in 
the 
summation 
j 
wijxj 
is 
independent. 
Provided 
the 
wij 
are 
not 
strongly 
correlated 
the 
conditions 
of 
validity 
of 
the 
central 
limit 
theorem 
hold[122], 
and 
each 
zi 
will 
be 
Gaussian 
distributed. 
Assuming 
that 
each 
wij 
is 
O 
(1), 
the 
mean 
of 
zi 
is 


X

1 


hzii6= 
wij 
hxji6= 
O 
(1) 
(28.4.20)

D

j 


The 
variance 
is

X


1 
D

22 


zi..hzii2 
= 
D2 
wik1 
..hxki2= 
O 
(1=D) 
(28.4.21) 


k=1 


DRAFT 
March 
9, 
2010 



Variational 
Bounding 
Using 
KL(qjp) 


Hence 
the 
variance 
of 
the 
eld 
zi 
is 
much 
smaller 
than 
its 
mean 
value. 
As 
D 
increases 
the 
uctuations 
around 
the 
mean 
diminish, 
and 
we 
may 
write 


PY

1 


Di 
xihzii

p(x) 
˜ 
ep(xi) 
(28.4.22)

Z 


i 


The 
assumption 
that 
p 
is 
approximately 
factorised 
is 
therefore 
self-consistent 
in 
the 
limit 
of 
MRFs 
with 
a 
large 
number 
of 
neighbours. 
Hence 
the 
factorised 
approximation 
would 
appear 
to 
be 
reasonable 
in 
the 
extreme 
limits 
(i) 
a 
very 
weakly 
connected 
system 
wij 
˜ 
0, 
or 
(ii) 
a 
large 
densely 
connected 
system. 


The 
fully 
factorised 
approximation 
is 
also 
called 
the 
Naive 
Mean 
Field 
theory 
since 
for 
the 
MRF 
case 
it 
assumes 
that 
we 
can 
replace 
the 
eect 
of 
the 
neighbours 
by 
a 
mean 
of 
the 
eld 
at 
each 
site. 


28.4.2 
General 
mean 
eld 
equations 
For 
a 
general 
intractable 
distribution 
p(x) 
on 
discrete 
or 
continuous 
x, 
the 
KL 
divergence 
between 
a 
factorised 
approximation 
q(x) 
and 
p(x) 
is 


YX

KL 
q(xi)jp(x)=hlog 
q(xi)) 
..hlog 
p(x)iQ(28.4.23)

q(xi) 
i 
q(xi) 
ii 


Isolating 
the 
dependency 
of 
the 
above 
on 
a 
single 
factor 
q(xi) 
we 
have 


DE

hlog 
q(xi)) 
..hlog 
p(x)iQ(28.4.24) 


=i 
q(xj 
)

q(xi) 
j6q(xi) 


Up 
to 
a 
normalisation 
constant, 
this 
is 
therefore 
the 
KL 
divergence 
between 
q(xi) 
and 
a 
distribution 




proportional 
to 
exphlog 
p(x)iQso 
that 
the 
optimal 
setting 
for 
q(xi) 
satises 


j6=i 
q(xj 
)



q(xi) 
. 
exphlog 
p(x)iQ(28.4.25) 


=i 
q(xj 
)

j6

These 
are 
known 
as 
the 
mean-eld 
equations 
and 
dene 
a 
new 
approximation 
factor 
in 
terms 
of 
the 
previous 
approximation 
factors. 
Note 
that 
if 
the 
normalisation 
constant 
of 
p(x) 
is 
unknown, 
this 
presents 
no 
problem 
since 
this 
constant 
is 
simply 
absorbed 
into 
the 
normalisation 
of 
the 
factors 
q(xi). 
In 
other 
words 
one 
may 
replace 
p(x) 
with 
the 
unnormalised 
p 
(x) 
in 
equation 
(28.4.25). 
Beginning 
with 
an 
initial 
randomly 
chosen 
set 
of 
distributions 
q(xi), 
the 
mean-eld 
equations 
are 
iterated 
until 
convergence. 
Asynchronous 
updating 
is 
guaranteed 
to 
decrease 
the 
KL 
divergence 
at 
each 
stage, 
section(28.4.3). 


28.4.3 
Asynchronous 
updating 
guarantees 
approximation 
improvement 
For 
a 
factorised 
variational 
approximation 
equation 
(28.4.23), 
we 
claim 
that 
each 
update 
equation 
(28.4.25) 
reduces 
the 
Kullback-Leibler 
approximation 
error. 
To 
show 
this 
we 
write 
a 
single 
updated 
distribution 
as 


1

new 


qi 
= 
Zi 
exp 
hlog 
p(x)iQj6=i 
qj
old 
(28.4.26) 


The 
joint 
distribution 
under 
this 
single 
update 
is 


Y

new 
new 
old 


q 
= 
qiqj 
(28.4.27) 
j=i

6

Our 
interest 
is 
the 
change 
in 
the 
approximation 
error 
under 
this 
single 
mean-eld 
update: 




. 
= 
KL(q 
newjp) 
- 
KLq 
oldjp(28.4.28) 


Using 


DEDE

X

new 
old 


KL(q 
newjp)= 
hlog 
q 
inew 
+log 
q 
..hlog 
p(x)iQold 
(28.4.29)

iqj
qold 
=i 
q
qnew 
ji 


ij6j

j6

=i

DRAFT 
March 
9, 
2010 



Variational 
Bounding 
Using 
KL(qjp) 


x1x2
x3x4
x1x2
x3x4
Figure 
28.5: 
(a): 
A 
toy 
`intractable’ 
distribution. 
(b): 
A 
structured 
singly-connected 
approximation. 


(a) 
(b) 
and 
dening 
the 
un-normalised 
distribution 


* 
new 
q 
(xi) 
= 
exp 
hlog 
p(x)iQold 
= 
Ziq 
(28.4.30)

i 
j6=i 
qj 
i 


then 


DEDEDE

new 
old 


= 
hlog 
qi 
iqnew 
..log 
qi..hlog 
piQold 
+hlog 
piQold 
(28.4.31)

i 
old 
j6=i 
qjnew 
j6=i 
qjold

qqq

ii 
i

DE

* 
old 
* 


= 
hlog 
qiiqnew 
- 
log 
Zi 
..log 
qi..hlog 
qi 
iqnew 
+ 
hlog 
qiiqold 
(28.4.32)

old

i 
ii

q

DEi 


old 
* 


= 
- 
log 
Zi 
..log 
qi+ 
hlog 
qi 
iqold 
(28.4.33)

old 


i 



qi 
old 
new


= 
..KLq 
jq 
= 
0 
(28.4.34)

ii

Hence 




KLq 
newjp= 
KLq 
oldjp(28.4.35) 


so 
that 
updating 
a 
single 
component 
of 
q 
at 
a 
time 
is 
guaranteed 
to 
improve 
the 
approximation. 
Note 
that 
this 
result 
is 
quite 
general, 
holding 
for 
any 
distribution 
p(x). 
In 
the 
case 
of 
a 
Markov 
network 
the 
guaranteed 
approximation 
improvement 
is 
equivalent 
to 
a 
guaranteed 
increase 
(strictly 
speaking 
a 
non-decrease) 
in 
the 
lower 
bound 
on 
the 
partition 
function. 


28.4.4 
Intractable 
energy 
Whilst 
the 
fully 
factorised 
approximation 
is 
rather 
severe, 
even 
this 
may 
not 
be 
enough 
to 
render 
the 
mean-
eld 
equations 
tractably 
implementable. 
To 
do 
so 
we 
need 
to 
be 
able 
to 
compute 
hlog 
p 
(x)iQFor 


j6=i 
q(xj 
). 


some 
models 
of 
interest 
this 
is 
still 
not 
possible 
and 
additional 
approximations 
are 
required. 


Example 
115 
(`Intractable’ 
mean-eld 
approximation). 
Consider 
the 
posterior 
distribution 
from 
a 
Relevance 
Vector 
Machine 
classication 
problem, 
section(18.2.4): 




Y

..

T

p(wjD) 
/Nw 


0;s 
2Icnwxn(28.4.36) 


n 


..

The 
terms 
cnwTxnrender 
p(wjD) 
non-Gaussian. 
We 
can 
nd 
a 
Gaussian 
approximation 
q(w)= 
N 
(w 


, 
) 
by 
minimising 
the 
Kullback-Leibler 
divergence 


KL(q(w)jp(wjD)) 
= 
hlog 
q(w)i..hlog 
p(wjD)i(28.4.37)

q(w) 
q(w) 


The 
entropic 
term 
is 
straightforward 
since 
this 
is 
the 
negative 
entropy 
of 
a 
Gaussian. 
However, 
we 
also 
require 
the 
`energy’ 
which 
includes 
a 
contribution

DE

Txn

log 
cnw

(28.4.38) 
N 
(w 


;) 


There 
is 
no 
closed 
form 
expression 
for 
this. 
One 
approach 
is 
to 
use 
additional 
variational 
approximations, 
[141, 
238]. 
Another 
approach 
is 
to 
recognise 
that 
the 
multi-variate 
average 
can 
be 
reduced 
to 
a 
uni-variate 
Gaussian 
average:

DE

TxnTxn

log 
cnw

= 
hlog 
s 
(cna)iN 
(a 


m 
= 
µ 
;2 
=(xn)T 
xn 
(28.4.39)

m;2) 
, 


N 
(w 


;) 


DRAFT 
March 
9, 
2010 



Mutual 
Information 
Maximisation 
: 
A 
KL 
Variational 
Approach 


and 
the 
uni-variate 
Gaussian 
average 
can 
be 
carried 
out 
using 
quadrature. 
This 
approach 
was 
used 
in 
[22] 
to 
approximate 
the 
posterior 
distribution 
of 
Bayesian 
Neural 
Networks. 


28.4.5 
Structured 
variational 
approximation 
One 
can 
extend 
the 
factorised 
KL 
variational 
approximation 
by 
using 
non-factorised 
q(x)[239, 
24]. 
Those 
for 
which 
averages 
of 
the 
variables 
can 
be 
computed 
in 
linear 
time 
include 
spanning 
trees, 
g(28.4b) 
and 
decomposable 
graphs 
g(28.4c). 
For 
example, 
for 
the 
distribution 


1 


p(x1;x2;x3;x4)= 
(x1;x2)(x2;x3)(x3;x4)(x4;x1)(x1;x3) 
(28.4.40)

Z 


a 
tractable 
q 
distribution 
would 
be, 
g(28.5) 


1 


~

q(x1;x2;x3;x4)= 
~(x1;x2)~(x1;x3)~(x1;x4) 
(28.4.41)

Z 


In 
this 
case 
we 
have 


X

KL(qjp)= 
Hq(x1;x2)+ 
Hq(x1;x3)+ 
Hq(x1;x4) 
- 
3Hq(x1)+hlog 
(xi;xj)i(28.4.42)

q(xi;xj 
) 
ij 


Since 
q 
is 
singly-connected, 
computing 
the 
marginals 
and 
entropy 
is 
straightforward 
(since 
the 
entropy 
requires 
only 
pairwise 
marginals 
on 
graph 
neighbours). 


More 
generally 
one 
can 
exploit 
any 
structural 
approximation 
with 
an 
arbitrary 
hypertree 
width 
w 
by 
use 
of 
the 
junction 
tree 
algorithm 
in 
combination 
with 
the 
KL 
divergence. 
However, 
the 
computational 
expense 
increases 
exponentially 
with 
the 
hypertree 
width[294]. 


28.5 
Mutual 
Information 
Maximisation 
: 
A 
KL 
Variational 
Approach 
Here 
we 
take 
a 
short 
interlude 
here 
to 
demonstrate 
an 
application 
of 
the 
Kullback-Leibler 
variational 
approach 
in 
information 
theory. 
A 
fundamental 
goal 
is 
to 
maximise 
information 
transfer, 
measured 
by 
the 


(see 
also 
denition(87)) 
I(X, 
Y 
) 
= 
H(X) 
- 
H(XjY 
) 
(28.5.1) 
where 
the 
and 
are 
dened 
H(X) 
= 
- 
hlog 
p(x)ip(x) 
, 
H(XjY 
) 
= 
- 
hlog 
p(xjy)ip(x;y) 
(28.5.2) 


Here 
we 
are 
interested 
in 
the 
situation 
in 
which 
p(x) 
is 
xed, 
but 
p(yjx, 
) 
has 
adjustable 
parameters 
. 
that 
we 
wish 
to 
set 
in 
order 
to 
maximise 
I(X, 
Y 
). 
In 
this 
case 
H(X) 
is 
constant 
and 
the 
optimisation 
problem 
is 
equivalent 
to 
minimising 
the 
conditional 
entropy 
H(XjY 
). 
Unfortunately, 
in 
many 
cases 
of 
practical 
interest 
H(XjY 
) 
is 
computationally 
intractable. 
See 
example(116) 
below 
for 
a 
motivating 
example. 
We 
discuss 
in 
section(28.5.1) 
a 
general 
procedure 
based 
on 
the 
Kullback-Leibler 
divergence 
to 
approximately 
maximise 
the 
mutual 
information. 


Example 
116. 
Consider 
a 
neural 
transmission 
system 
in 
which 
xi 
2f0, 
1} 
denotes 
an 
emitting 
neuron 
in 
a 
non-ring 
state 
(0) 
or 
ring 
state 
(1), 
and 
yj 
2f0, 
1} 
a 
receiving 
neuron. 
If 
each 
receiving 
neuron 
res 
independently, 
depending 
only 
on 
the 
emitting 
neurons, 
we 
have 


Y

p(yjx)=p(yijx) 
(28.5.3) 


i 


DRAFT 
March 
9, 
2010 



Mutual 
Information 
Maximisation 
: 
A 
KL 
Variational 
Approach 


x1x2x3x4
y1y2y3y4
Figure 
28.6: 
An 
information 
transfer 
problem. 
For 
a 
xed 
distri


Q

bution 
(x)=i 
p(xi) 
and 
parameterised 
distributions 
p(yjjx)= 


..p 
pX

wi 
Tx 
, 
nd 
the 
optimal 
parameters 
wi 
that 
maximise 
the 
mutual 
information 
between 
the 
variables 
x 
and 
y. 
Such 
considerations 
are 
popular 
in 
theoretical 
neuroscience 
and 
aim 
to 
understand 
how 
the 
receptive 
elds 
wi 
of 
a 
neuron 
relate 
to 
the 
statistics 
of 
the 
environment 
p(x). 


Algorithm 
29 
IM 
algorithm 


1: 
Choose 
a 
class 
of 
approximating 
distributions 
Q 
(for 
example 
factorised) 
2: 
Initialise 
the 
parameters 
. 
3: 
repeat 
4: 
For 
xed 
q(xjy), 
nd 
new 
= 
argmax 
~I(X, 
Y 
) 
(28.5.6) 
. 


5: 
For 
xed 
, 
~

q 
new(xjy) 
= 
argmax 
I(X, 
Y 
) 
(28.5.7) 


q(xjy)2Q 


where 
Q 
is 
a 
chosen 
class 
of 
distributions. 


6: 
until 
converged 
where 
for 
example 
we 
could 
use 




T 


p(yi 
=1jx)= 
wi 
x(28.5.4) 


If 
we 
make 
the 
simple 
assumption 
that 
emitting 
neurons 
re 
independently, 


Y

p(x)=p(xi) 
(28.5.5) 
i 


then 
for 
p(xjy) 
all 
components 
of 
the 
x 
variable 
are 
dependent, 
see 
g(28.6). 
This 
denes 
a 
complex 
high-dimensional 
p(x, 
y) 
for 
which 
the 
conditional 
entropy 
is 
typically 
intractable. 


28.5.1 
The 
information 
maximisation 
algorithm 
Consider 


KL(p(xjy)jq(xjy)) 
= 
0 
(28.5.8) 


This 
immediately 
gives 
a 
bound

XX

p(xjy) 
log 
p(xjy) 
..p(xjy) 
log 
q(xjy) 
= 
0 
(28.5.9) 


xx 


Multiplying 
both 
sides 
by 
p(y), 
we 
obtain 


XX

)p(y)p(xjy) 
log 
p(xjy) 
p(x, 
y) 
log 
q(xjy) 
(28.5.10) 
x;y 
x;y 


From 
the 
denition, 
the 
left 
of 
the 
above 
is 
..H(XjY 
). 
Hence 


I(X, 
Y 
) 
= 
H(X)+ 
hlog 
q(xjy)i= 
I~(X, 
Y 
) 
(28.5.11)

p(x;y) 


DRAFT 
March 
9, 
2010 



Loopy 
Belief 
Propagation 


d
ab
c
ef
Figure 
28.7: 
Classical 
Belief 
Propagation 
can 
be 
derived 
by 
considering 
how 
to 
compute 
the 
marginal 
of 
a 
variable 
on 
a 
MRF. 
In 
this 
case 
the 
marginal 
p(d) 
depends 
on 
messages 
transmitted 
via 
the 
neighbours 
of 
d. 
By 
dening 
local 
messages 
on 
the 
links 
of 
the 
graph, 
a 
recursive 
algorithm 
for 
computing 
all 
marginals 
can 
be 
derived, 
see 
text. 


From 
this 
lower 
bound 
on 
the 
mutual 
information 
we 
arrive 
at 
the 
information 
maximisation 
(IM) 
algorithm[20]. 
Given 
a 
distribution 
p(x) 
and 
a 
parameterised 
distribution 
p(yjx, 
), 
we 
seek 
to 
maximise 
I~(X, 
Y 
) 
with 
respect 
to 
. 
A 
co-ordinate 
wise 
optimisation 
procedure 
is 
presented 
in 
algorithm(29). 
The 
Blahut-Arimoto 
algorithm 
in 
information 
theory 
(see 
for 
example 
[184]) 
is 
a 
special 
case 
in 
which 
the 
optimal 
decoder 


q(xjy) 
. 
p(yjx, 
)p(x) 
(28.5.12) 


is 
used. 
In 
applications 
where 
the 
Blahut-Arimoto 
algorithm 
is 
intractable 
to 
implement, 
the 
IM 
algorithm 
can 
provide 
an 
alternative 
by 
restricting 
q 
to 
a 
tractable 
family 
of 
distributions 
(tractable 
in 
the 
sense 
that 
the 
lower 
bound 
can 
be 
computed). 
The 
Blahut-Arimoto 
algorithm 
is 
analogous 
to 
the 
EM 
algorithm 
for 
Maximum 
Likelihood 
and 
guarantees 
a 
non-decrease 
of 
the 
mutual 
information 
at 
each 
stage 
of 
the 
update. 
Similarly, 
the 
IM 
procedure 
is 
analogous 
to 
a 
Generalised 
EM 
procedure 
and 
each 
step 
of 
the 
procedure 
cannot 
decrease 
the 
lower 
bound 
on 
the 
mutual 
information. 


28.5.2 
Linear 
Gaussian 
decoder 
A 
special 
case 
of 
the 
IM 
framework 
is 
to 
use 
a 
linear 
Gaussian 
decoder 


q(xjy)= 
N 
(x 


Uy, 
) 
. 
log 
q(xjy)=(x 
- 
Uy)T 
..1 
(x 
- 
Uy) 
(28.5.13) 


Plugging 
this 
into 
the 
MI 
bound, 
equation 
(28.5.11), 
and 
optimising 
with 
respect 
to 
, 
and 
U, 
we 
obtain 


DEDEDE..1 


TT

S 
=(x 
- 
Uy)(x 
- 
Uy)T, 
U 
=xyyy(28.5.14) 


where 
h) 
= 
hiUsing 
this 
in 
the 
MI 
bound 
we 
obtain 


p(x;y). 


DEDEDE..1DE

1 
TTTT

I(X, 
Y 
) 
= 
H(X) 
- 
log 
detxx..xyyyyx+ 
const. 
(28.5.15)

2 


Up 
to 
irrelevant 
constants, 
this 
is 
equivalent 
to 
Linsker's 
as-if-Gaussian 
approximation 
to 
the 
mutual 
information 
[176]. 
One 
can 
therefore 
view 
Linsker's 
approach 
as 
a 
special 
case 
of 
the 
IM 
algorithm 
restricted 
to 
linear-Gaussian 
decoders. 
In 
principle, 
one 
can 
therefore 
improve 
on 
Linsker's 
method 
by 
considering 
more 
powerful 
non-linear-Gaussian 
decoders. 
Applications 
of 
this 
technique 
to 
Neural 
systems 
are 
discussed 
in 
[20]. 


28.6 
Loopy 
Belief 
Propagation 
Belief 
Propagation 
is 
a 
technique 
for 
exact 
inference 
of 
marginals 
p(xi) 
for 
singly-connected 
distributions 
p(x). 
There 
are 
dierent 
formulations 
of 
BP, 
the 
most 
modern 
treatment 
being 
the 
sum-product 
algorithm 
on 
the 
corresponding 
factor 
graph, 
as 
described 
in 
section(5.1.2). 
An 
important 
observation 
is 
that 
the 
algorithm 
is 
purely 
local 
– 
the 
updates 
are 
unaware 
of 
the 
global 
structure 
of 
the 
graph, 
depending 
only 
on 
the 
local 
neighbourhood 
structure. 
This 
means 
that 
even 
if 
the 
graph 
is 
multiply-connected 
(it 
is 
loopy) 
one 
can 
still 
apply 
the 
algorithm 
and 
`see 
what 
happens'. 
Provided 
the 
loops 
in 
the 
graph 
are 
relatively 
long, 
one 
may 
hope 
that 
running 
`loopy’ 
BP 
will 
converge 
to 
a 
good 
approximation 
of 
the 
true 
marginals. 
In 
general, 
this 
cannot 
be 
guaranteed; 
however, 
when 
the 
method 
converges, 
the 
results 
can 
be 
surprisingly 
accurate. 


In 
the 
following 
we 
will 
show 
how 
loopy 
BP 
can 
also 
be 
motivated 
by 
a 
variational 
objective. 
To 
do 
so, 
the 
most 
natural 
connection 
is 
with 
the 
classical 
BP 
algorithm 
(rather 
than 
the 
factor 
graph 
sum-product) 
algorithm. 
For 
this 
reason 
we 
briey 
describe 
below 
the 
classical 
BP 
approach. 


526 
DRAFT 
March 
9, 
2010 



Loopy 
Belief 
Propagation 


x2 


x1!)
)
xi 
(xixi!xj 
(xj) 


(x

x!

2

Figure 
28.8: 
Loopy 
Belief 
Propagation. 
Once 
a 
node 
has 
received 
incoming 
messages 
from 
all 
neighbours 
(excluding 
the 
one 
it 
wants 
to 


send 
a 
message 
to), 
it 
may 
send 
an 
outgoing 
message 
to 
a 
neighbour: 


xj

x1 


xi 




x

3

!

i(

x

X

xi!xj 
(xj)=f 
(xi;xj)x1!xi 
(xi) 
x2!xi 
(xi) 
x3!xi 
(xi) 


xi 


x3
28.6.1 
Classical 
BP 
on 
an 
undirected 
graph 
BP 
can 
be 
derived 
by 
considering 
how 
to 
calculate 
a 
marginal 
in 
terms 
of 
messages 
on 
an 
undirected 


PX

graph. 
Consider 
calculating 
the 
marginal 
p(d)= 
a;b;c;e;f 
p(a, 
b, 
c, 
d, 
e, 
f) 
for 
the 
pairwise 
Markov 
net-

PX

work 
in 
g(28.7). 
We 
denote 
both 
a 
node 
and 
its 
state 
by 
the 
same 
symbol, 
so 
that 
f 
(d, 
b) 
denotes 


b 


summation 
over 
the 
states 
of 
the 
variable 
b. 
Carrying 
out 
such 
a 
summation 
results 
in 
a 
`message’ 
b!d 
(d) 
containing 
information 
from 
node 
b 
to 
node 
d. 
In 
order 
to 
compute 
the 
summation 
eciently, 
we 
distribute 
summations 
as 
follows: 


p(d) 
= 
1 
ZXf 
(b, 
d) 
Xf 
(a, 
d) 
Xf 
(d, 
f) 
Xf 
(d, 
e)Xf 
(c, 
e) 
(28.6.1) 
b|X{zX}a|X{zX}f 
|X{zX}
e 
c|X{zX}b!d(d)a!d(d)f!d(d)|X{zX
c!e(e)
}

e!d(d) 


where 
we 
dene 
messages 
n1!n2 
(n2) 
sending 
information 
from 
node 
n1 
to 
node 
n2 
as 
a 
function 
of 
the 
state 
of 
node 
n2. 
In 
general, 
a 
node 
xi 
passes 
a 
message 
to 
node 
xj 
via 


XY

xi!xj 
(xj)=f 
(xi;xj)xk!xi 
(xi) 
(28.6.2) 


xi 
k2ne(i);k=6
j 


This 
algorithm 
is 
equivalent 
to 
the 
sum-product 
algorithm 
provided 
the 
graph 
is 
singly-connected. 


28.6.2 
Loopy 
BP 
as 
a 
variational 
procedure 
A 
variational 
procedure 
that 
corresponds 
to 
loopy 
BP 
can 
be 
derived 
by 
considering 
the 
terms 
of 
a 
standard 
variational 
approximation 
based 
on 
the 
Kullback-Leibler 
divergence 
KL(qjp)[299]. 
For 
a 
pairwise 
MRF 
dened 
on 
potentials 
(xi;xj), 


Y

1 


p(x)= 
(xi;xj) 
(28.6.3)

Z

ij 


and 
approximating 
distribution 
q(x), 
the 
Kullback-Leibler 
bound 
is 


X

log 
Z 
..hlog 
q(x)i+hlog 
(xi;xj)i(28.6.4)

q(x) 
q(x) 
ij

|X{z


energy 


Since

hlog 
(xi;xj)i=hlog 
(xi;xj)i(28.6.5)

q(x) 
q(xi;xj 
) 


each 
contribution 
to 
the 
energy 
depends 
on 
q(x) 
only 
via 
the 
pairwise 
marginals 
q(xi;xj). 
This 
suggests 
that 
these 
marginals 
should 
form 
the 
natural 
parameters 
of 
any 
approximation. 
Can 
we 
then 
nd 
an 
expression 
for 
the 
entropy 
..hlog 
q(x)iq(x) 
in 
terms 
of 
these 
pairwise 
marginals? 
Consider 
a 
case 
in 
which 
the 
required 
marginals 
are 


q(x1;x2);q(x2;x3);q(x3;x4) 
(28.6.6) 


DRAFT 
March 
9, 
2010 



Loopy 
Belief 
Propagation 


Given 
these 
marginals, 
the 
energy 
term 
is 
straightforward 
to 
compute, 
and 
we 
are 
left 
with 
requiring 
only 
the 
entropy 
of 
q. 
Either 
by 
appealing 
to 
the 
junction 
tree 
representation, 
or 
by 
straightforward 
algebra, 
one 
can 
show 
that 
we 
can 
uniquely 
express 
q 
in 
terms 
of 
the 
marginals 
as 


q(x1;x2)q(x2;x3)q(x3;x4) 


q(x) 
= 
(28.6.7) 


q(x2)q(x3) 


An 
intuitive 
way 
to 
arrive 
at 
this 
result 
is 
by 
examining 
the 
numerator 
of 
equation 
(28.6.7). 
The 
variable 
x2 
appears 
twice, 
as 
does 
the 
variable 
x3 
and, 
since 
any 
joint 
distribution 
cannot 
have 
replicated 
variables 
on 
the 
left 
of 
any 
conditioning, 
we 
must 
compensate 
for 
the 
additional 
x2 
and 
x3 
variables 
by 
dividing 
by 
these 
marginals. 
In 
this 
case, 
the 
entropy 
of 
q(x) 
can 
be 
written 
as 


Hq(x)= 
..hlog 
q(x)iq(x) 
= 
Hq(x1;x2)+ 
Hq(x2;x3)+ 
Hq(x3;x4) 
- 
Hq(x2) 
- 
Hq(x3) 
(28.6.8) 


More 
generally, 
chapter(6), 
any 
decomposable 
graph 
can 
be 
represented 
as 


Q

q(Xc)

c 


q(x)=Q(28.6.9) 


q(Xs)

s 


where 
the 
q(Xc) 
are 
the 
marginals 
dened 
on 
cliques 
of 
the 
graph, 
with 
Xc 
being 
the 
variables 
of 
the 
clique, 
and 
the 
q(Xs) 
are 
dened 
on 
the 
separators 
(intersections 
of 
neighbouring 
cliques). 
The 
expression 
for 
the 
entropy 
of 
the 
distribution 
is 
then 
given 
by 
a 
sum 
of 
marginal 
entropies 
minus 
the 
entropy 
of 
the 
marginals 
dened 
on 
the 
separators. 


Bethe 
Free 
energy 


Consider 
now 
a 
MRF 
corresponding 
to 
a 
non-decomposable 
graph, 
for 
example 
the 
4-cycle 


1 


p(x)= 
(x1;x2)(x2;x3)(x3;x4)(x4;x1) 
(28.6.10)

Z 


The 
energy 
requires 
therefore 
that 
we 
know 


q(x1;x2);q(x2;x3);q(x3;x4);q(x4;x1) 
(28.6.11) 


Assuming 
that 
these 
marginals 
are 
given, 
can 
we 
nd 
an 
expression 
for 
the 
entropy 
of 
the 
joint 
distribution 
q(x1;x2;x3;x4) 
in 
terms 
of 
its 
pairwise 
marginals 
q(xi;xj)? 
In 
general 
this 
is 
not 
possible 
since 
the 
graph 
corresponding 
to 
the 
marginals 
contains 
loops 
(so 
that 
the 
junction 
tree 
representation 
would 
result 
in 
cliques 
greater 
than 
size 
2). 
However, 
a 
simple 
`no 
overcounting’ 
approximation 
is 
to 
write 


q(x1;x2)q(x2;x3)q(x3;x4)q(x4;x1) 


q(x) 
˜ 
(28.6.12) 


q(x1)q(x2)q(x3)q(x4) 


subject 
to 
the 
constraints

X

q(xi;xj)= 
q(xj) 
(28.6.13) 


xi 


An 
entropy 
approximation 
using 
this 
representation 
is 
therefore 


4

X

Hq(x) 
˜ 
Hq(x1;x2)+ 
Hq(x2;x3)+ 
Hq(x3;x4)+ 
Hq(x1;x4) 
- 
Hq(xi) 
(28.6.14) 
i=1 


With 
this 
approximation 
the 
log 
partition 
function 
is 
known 
in 
statistical 
physics 
as 
the 
(negative) 
Bethe 
free 
energy. 
Our 
interest 
is 
then 
to 
maximise 
this 
expression 
with 
respect 
to 
the 
parameters 
q(xi;xj)

PX

subject 
to 
marginal 
consistency 
constraints, 
q(xi;xj)= 
q(xj). 
These 
may 
be 
enforced 
using 
Lagrange 


xi 


multipliers. 
One 
can 
write 
the 
Bethe 
free 
energy 
as 


XXXXX

F(q, 
) 
..Hq(xi;xj)+Hq(xi)+hlog 
(xi;xj)i+i;j 
q(xi) 
..q(xi;xj)

q(xi;xj 
) 
ij 
iijijxi 


DRAFT 
March 
9, 
2010 



Loopy 
Belief 
Propagation 


(28.6.15) 
where 
i 
~ 
j 
denotes 
the 
unique 
neighbouring 
edges 
on 
the 
graph 
(each 
edge 
is 
counted 
only 
once). 
This 
is 
no 
longer 
a 
bound 
on 
the 
log 
partition 
function 
since 
the 
entropy 
approximation 
is 
not 
a 
lower 
bound 
on 
the 
true 
entropy. 
The 
task 
is 
now 
to 
maximise 
this 
`approximate 
bound’ 
with 
respect 
to 
the 
parameters, 
namely 
all 
the 
pairwise 
marginals 
q(xi;xj) 
and 
the 
Lagrange 
multipliers 
. 


A 
simple 
scheme 
to 
maximise 
equation 
(28.6.15) 
is 
to 
use 
a 
xed 
point 
iteration 
by 
equating 
the 
derivatives 
of 
the 
Bethe 
free 
energy 
with 
respect 
to 
the 
parameters 
q(xi;xj) 
to 
zero, 
and 
likewise 
for 
the 
Lagrange 
multipliers. 
One 
may 
show 
that 
the 
resulting 
set 
of 
xed 
point 
equations, 
on 
eliminating 
q, 
is 
equivalent 
to 
(undirected) 
Belief 
Propagation[299] 
for 
which, 
in 
general, 
a 
node 
xi 
passes 
a 
message 
to 
node 
xj 
using 


XYxi!xj 
(xj) 
=f 
(xi, 
xj)xk!xi 
(xi) 
(28.6.16) 
xi 
k2ne(i);k6=j 
At 
convergence 
the 
marginal 
p(xi) 
is 
then 
approximated 
by 
Yq(xi) 
/xj 
!xi 
(xi) 
(28.6.17) 
i2ne(j) 


the 
prefactor 
being 
determined 
by 
normalisation. 
For 
a 
singly-connected 
distribution 
p, 
this 
message 
passing 
scheme 
converges 
and 
the 
marginal 
corresponds 
to 
the 
exact 
result. 
For 
multiply 
connected 
(loopy) 
structures, 
running 
this 
loopy 
Belief 
Propagation 
will 
generally 
result 
in 
an 
approximation. 
Naturally, 
we 
can 
dispense 
with 
the 
Bethe 
free 
energy 
if 
desired 
and 
run 
the 
associated 
loopy 
Belief 
Propagation 
algorithm 
directly 
on 
the 
undirected 
graph. 


The 
convergence 
of 
Loopy 
Belief 
Propagation 
which 
can 
be 
heavily 
dependent 
on 
the 
topology 
of 
the 
graph 
and 
also 
the 
message 
updating 
schedule[291, 
201]. 
The 
potential 
benet 
of 
the 
Bethe 
free 
energy 
viewpoint 
is 
that 
it 
gives 
an 
objective 
that 
is 
required 
to 
be 
optimised, 
opening 
up 
the 
possibility 
of 
more 
general 
optimisation 
techniques 
than 
BP. 
The 
so-called 
double-loop 
techniques 
iteratively 
isolate 
convex 
contributions 
to 
the 
Bethe 
Free 
energy, 
interleaved 
with 
concave 
contributions. 
At 
each 
stage, 
the 
resulting 
optimisiations 
can 
be 
carried 
out 
eciently[301, 
133, 
299]. 


Validity 
of 
loopy 
belief 
propagation 


For 
a 
MRF 
which 
has 
a 
loop, 
computationally 
this 
means 
that 
a 
perturbation 
in 
a 
variable 
on 
the 
loop 
eventually 
reverberates 
to 
the 
same 
variable. 
However, 
if 
there 
are 
a 
large 
number 
of 
variables 
in 
the 
loop, 
and 
the 
individual 
neighbouring 
links 
are 
not 
all 
extremely 
strong, 
the 
numerical 
eect 
of 
the 
loop 
is 
small 
in 
the 
sense 
that 
inuence 
of 
the 
variable 
on 
itself 
is 
negligible. 
In 
such 
cases 
one 
would 
expect 
the 
loopy 
BP 
approximation 
to 
be 
accurate. 
An 
area 
of 
particular 
success 
for 
Loopy 
Belief 
Propagation 
inference 
is 
in 
error 
correction 
based 
on 
low 
density 
parity 
check 
codes, 
which 
are 
designed 
to 
have 
this 
long-
loop 
property[183]. 
In 
many 
examples 
of 
practical 
interest 
(for 
example 
an 
MRF 
with 
nearest 
neighbour 
interactions 
on 
a 
lattice), 
however, 
loops 
can 
be 
very 
short. 
In 
such 
cases 
a 
naive 
implementation 
of 
Loopy 
BP 
will 
fail. 
A 
natural 
extension 
is 
to 
cluster 
variables 
to 
alleviate 
some 
of 
the 
issues 
arising 
from 
strong 
local 
dependencies; 
this 
technique 
is 
called 
the 
Kikuchi 
or 
Cluster 
Variation 
method[154]. 
More 
elaborate 
ways 
of 
clustering 
variables 
can 
be 
considered 
using 
region 
graphs[299, 
292]. 


Example 
117. 
The 
le 
demoMFBPGibbs.m 
compares 
the 
performance 
of 
naive 
Mean 
Field 
theory, 
Belief 
Propagation 
and 
unstructured 
Gibbs 
sampling 
on 
marginal 
inference 
in 
a 
pairwise 
Markov 
network 


p(w, 
x, 
y, 
z)= 
wx(w, 
x)wy(w, 
y)wz(w, 
z)xy(x, 
y)xz(x, 
z)yz(y, 
z) 
(28.6.18) 


in 
which 
all 
variables 
take 
6 
states. 
In 
the 
experiment 
the 
tables 
are 
selected 
from 
a 
uniform 
distribution 
raised 
to 
a 
power 
. 
For 
a 
close 
to 
zero, 
all 
the 
tables 
are 
essentially 
at 
and 
therefore 
the 
variables 
become 
independent, 
a 
situation 
for 
which 
MF, 
BP 
and 
Gibbs 
sampling 
are 
ideally 
suited. 
As 
a 
is 
increased 
to 


DRAFT 
March 
9, 
2010 



Expectation 
Propagation 


wxyzoriginal graphwxyzFactorised graph
0500100000.050.10.150.2p distribution
0500100000.20.40.60.81p distribution
(a) 
(b) 
(c) 
Figure 
28.9: 
(a): 
The 
Markov 
network 
(left) 
that 
we 
wish 
to 
approximate 
the 
marginals 
p(w);p(x);p(y);p(z) 
for. 
All 
tables 
are 
drawn 
from 
a 
uniform 
distribution 
raised 
to 
a 
power 
. 
On 
the 
right 
is 
shown 
the 
naive 
mean 
eld 
approximation 
factorised 
structure. 
(b): 
There 
are 
64 
= 
1295 
states 
of 
the 
distribution. 
Shown 
is 
a 
randomly 
sampled 
distribution 
for 
a 
= 
5 
which 
has 
many 
isolated 
peaks, 
suggesting 
the 
distribution 
is 
far 
from 
factorised. 
In 
this 
case 
the 
MF 
and 
Gibbs 
sampling 
approximations 
may 
perform 
poorly. 
(c): 
As 
a 
is 
increased 
to 
25, 
typically 
only 
one 
state 
of 
the 
distribution 
dominates. 
See 
demoMFBPGibbs.m. 


x1
Figure 
28.10: 
(a): 
Multiply-connected 
factor 
graph 
representing 
p(x). 
(b): 
Expectation 
propagation 
approximates 
(a) 
in 
terms 
of 
a 
tractable 
factor 
graph. 
The 
open 
squares 
indicate 
that 
the 
factors 
are 
parameters 
of 
the 
approximation. 
The 
basic 
EP 
approximation 
is 
to 



x2
x2x1
x2x1
x3
x4
x3x4
x3x4
(a) 
(b) 
(c) 
replace 
all 
factors 
in 
p(x) 
by 
product 
factors. 
(c): 
Tree 
structured 
EP. 
5, 
the 
dependencies 
amongst 
the 
variables 
increase 
and 
the 
methods 
perform 
worse, 
especially 
MF 
and 
Gibbs. 
As 
a 
is 
increased 
to 
25, 
the 
distribution 
becomes 
sharply 
peaked 
around 
a 
single 
state, 
such 
that 
the 
posterior 
is 
eectively 
factorised, 
see 
g(28.9). 
This 
suggests 
that 
a 
MF 
approximation 
(and 
also 
Gibbs 
sampling) 
should 
work 
well. 
However, 
nding 
this 
state 
is 
computationally 
dicult 
and 
both 
methods 
often 
get 
stuck 
in 
local 
minima. 
Belief 
propagation 
seems 
less 
susceptible 
to 
being 
trapped 
in 
local 
minima 
in 
this 
regime 
and 
tends 
to 
outperform 
both 
MF 
and 
Gibbs 
sampling. 


28.7 
Expectation 
Propagation 
The 
messages 
in 
schemes 
such 
as 
belief 
propagation 
are 
not 
always 
representable 
in 
a 
compact 
form. 
The 
Switching 
Linear 
Dynamical 
System, 
chapter(25) 
is 
such 
an 
instance, 
in 
which 
the 
messages 
require 
an 
exponential 
amount 
of 
storage. 
This 
limits 
BP 
to 
cases 
such 
as 
discrete 
networks, 
or 
more 
generally 
exponential 
family 
messages. 
Expectation 
Propagation 
extends 
the 
applicability 
of 
BP 
to 
cases 
in 
which 
the 
messages 
are 
not 
in 
the 
exponential 
family 
by 
projecting 
the 
messages 
back 
to 
the 
exponential 
family 
at 
each 
stage. 
This 
projection 
is 
obtained 
by 
using 
a 
Kullback-Leibler 
measure[195, 
246, 
197]. 


Consider 
a 
distribution 
of 
the 
form 


Y

1 


p(x)= 
i(X 
) 
(28.7.1)

Z

i 


In 
EP 
one 
identies 
those 
factors 
i(X 
) 
which, 
if 
replaced 
by 
simpler 
factors 
~i(X 
), 
would 
render 
the 
distribution 
~p(x) 
tractable. 
One 
then 
sets 
any 
free 
parameters 
of 
~i(X 
) 
by 
minimising 
the 
Kullback-

DRAFT 
March 
9, 
2010 



Expectation 
Propagation 


Leibler 
divergence 
KL(pjp~). 
For 
example, 
consider 
a 
pairwise 
MRF 


1 


p(x)= 
1;2(x1;x2)2;3(x2;x3)3;4(x3;x4)4;1(x4;x1) 
(28.7.2)

Z 


with 
Factor 
Graph 
as 
depicted 
in 
g(28.10a). 
If 
we 
replace 
all 
terms 
ij(xi;xj) 
by 
approximate 
factors 
~ij(xi)~ij(xj) 
then 
the 
resulting 
joint 
distribution 
~p 
would 
be 
factorised 
and 
hence 
tractable. 
Since 
the 
variable 
xi 
appears 
in 
more 
than 
one 
term 
from 
p(x), 
we 
need 
to 
index 
the 
approximation 
factors. 
A 
convenient 
way 
to 
do 
this 
is 


~

p~= 
1 
2!1 
(x1) 
~1!2 
(x2) 
~3!2 
(x2) 
~2!3 
(x3) 
~4!3 
(x3) 
~3!4 
(x4) 
~1!4 
(x4) 
~4!1 
(x1) 
(28.7.3)

~

Z 


which 
is 
represented 
in 
g(28.10b). 
The 
idea 
in 
EP 
is 
now 
to 
determine 
the 
optimal 
approximation 
term 
by 
the 
self-consistent 
requirement 
that, 
on 
replacing 
it 
with 
its 
exact 
form, 
there 
is 
no 
dierence 
to 
the 
marginal 
of 
~p. 
Consider 
the 
approximation 
parameters 
~3!2 
(x2) 
and 
~2!3 
(x3). 
To 
set 
these 
we 
rst 
replace 
the 
contribution 
~3!2 
(x2) 
~2!3 
(x3) 
by 
(x2;x3). 
This 
gives 


1 


p~* 
=~~2!1 
(x1) 
~1!2 
(x2) 
(x2;x3)~4!3 
(x3) 
~3!4 
(x4) 
~1!4 
(x4) 
~4!1 
(x1) 
(28.7.4)

Z* 


Now 
consider 
the 
Kullback-Leibler 
divergence 
between 
this 
distribution 
and 
our 
approximation, 


KL(~pjp~) 
=hlog 
~pi~..hlog 
~pi~(28.7.5)

p* 
p* 


Since 
our 
interest 
is 
in 
updating 
~3!2 
(x2) 
and 
~2!3 
(x3), 
we 
isolate 
the 
contribution 
from 
these 
parameters 
in 
the 
Kullback-Leibler 
divergence 
which 
gives 


KL(~pjp~) 
= 
log 
Z~..hlog 
~3!2 
(x2) 
~2!3 
(x3)+ 
const. 
(28.7.6) 
p~(x2;x3) 


Also, 
since 
~p 
is 
factorised, 
up 
to 
a 
constant 
proportionality 
factor, 
the 
dependence 
of 
Z~on 
~3!2 
(x2) 
and 
~

2!3 
(x3) 
is 


XX

Z~/~1!2 
(x2) 
~3!2 
(x2)~2!3 
(x3) 
~4!3 
(x3) 
(28.7.7) 


x2 
x3 


Dierentiating 
the 
Kullback-Leibler 
divergence 
equation 
(28.7.6) 
with 
respect 
to 
~3!2 
(x2) 
and 
equating 
to 
zero, 
we 
obtain 


~1!2 
(x2) 
~3!2 
(x2)
P= 
p~(x2) 
(28.7.8) 


~1!2 
(x2) 
~3!2 
(x2)

x2 


Similarly, 
optimising 
w.r.t. 
~2!3 
(x3) 
gives 


~2!3 
(x3) 
~4!3 
(x3)
P= 
p~(x3) 
(28.7.9) 


~2!3 
(x3) 
~4!3 
(x3)

x3 


These 
equations 
only 
determine 
the 
approximation 
factors 
up 
to 
a 
proportionality 
constant. 
We 
can 
therefore 
write 
the 
optimal 
updates 
as 


p~(x2)

~

3!2 
(x2)= 
z3!2 
(28.7.10)

~

1!2 
(x2) 
and 
p~(x3)

~

2!3 
(x3)= 
z2!3 
(28.7.11)

~

4!3 
(x3) 


DRAFT 
March 
9, 
2010 



Expectation 
Propagation 


where 
z3!2 
and 
z2!3 
are 
proportionality 
terms. 
We 
can 
determine 
the 
proportionalities 
by 
the 
requirement 
that 
the 
term 
approximation 
~3!2 
(x2) 
~2!3 
(x3) 
has 
the 
same 
eect 
on 
the 
normalisation 
of 
~p 
as 
it 
has 
on 
~p. 
That 
is

X

~2!1 
(x1) 
~1!2 
(x2) 
~3!2 
(x2) 
~2!3 
(x3) 
~4!3 
(x3) 
~3!4 
(x4) 
~1!4 
(x4) 
~4!1 
(x1) 


x1;x2;x3;x4 


X

=~2!1 
(x1) 
~1!2 
(x2) 
(x2;x3)~4!3 
(x3) 
~3!4 
(x4) 
~1!4 
(x4) 
~4!1 
(x1) 
(28.7.12) 


x1;x2;x3;x4 


which, 
on 
substituting 
in 
the 
updates 
equation 
(28.7.10) 
and 
equation 
(28.7.11), 
reduces 
to 


* 


z 


z2!3z3!2 
= 
2;3 
(28.7.13) 


z~2;3 


where 


X

p~(x2) 
p~(x3)

˜ 
~

z~2;3 
=1!2 
(x2) 
4!3 
(x3) 
(28.7.14)

~~

1!2 
(x2) 
4!3 
(x3)

x2;x3 


and 
z 
* 
2;3 
=X~1!2 
(x2) 
(x2, 
x3) 
~4!3 
(x3) 
(28.7.15) 
x2;x3 


Any 
choice 
of 
local 
normalisations 
z2!3, 
z3!2 
that 
satises 
equation 
(28.7.13) 
suces 
to 
ensure 
that 
the 
scale 
of 
the 
term 
approximation 
matches. 
For 
example, 
one 
may 
set 


s

* 


z 


z2!3 
= 
z3!2 
=2;3 
(28.7.16) 


z~2;3 


Once 
set, 
an 
approximation 
for 
the 
global 
normalisation 
constant 
of 
p 
is 


Z 
˜ 
Z˜ 
(28.7.17) 


The 
above 
gives 
a 
procedure 
for 
updating 
the 
terms 
~3!2 
(x2) 
and 
~2!3 
(x3). 
One 
then 
chooses 
another 
term 
and 
replaces 
it 
with 
its 
approximation, 
until 
the 
parameters 
of 
the 
approximation 
converge. 
The 
generic 
procedure 
is 
outlined 
in 
algorithm(30). 


Comments 
on 
EP 


• 
For 
the 
MRF 
example 
above, 
EP 
corresponds 
to 
Belief 
Propagation 
(the 
sum-product 
form 
on 
the 
factor 
graph). 
This 
is 
intuitively 
clear 
since 
in 
both 
EP 
and 
BP 
the 
product 
of 
messages 
incoming 
to 
a 
variable 
is 
proportional 
to 
the 
approximation 
of 
the 
marginal 
of 
that 
variable. 
A 
dierence, 
however, 
is 
the 
schedule: 
in 
EP 
all 
messages 
corresponding 
to 
a 
term 
approximation 
are 
updated 
simultaneously 
(in 
the 
above 
~3!2 
(x2) 
and 
~2!3 
(x3)), 
whereas 
in 
BP 
they 
are 
updated 
in 
arbitrary 
order. 
• 
EP 
is 
a 
useful 
extension 
of 
BP 
to 
cases 
in 
which 
the 
BP 
messages 
cannot 
be 
easily 
represented. 
In 
the 
case 
that 
the 
approximating 
distribution 
~p 
is 
in 
the 
exponential 
family, 
the 
minimal 
Kullback-Leibler 


criterion 
equates 
to 
matching 
moments 
of 
the 
approximating 
distribution 
to 
p 
. 
See 
[246] 
for 
a 
more 
detailed 
discussion. 


• 
In 
general 
there 
is 
no 
need 
to 
replace 
all 
terms 
in 
the 
joint 
distribution 
with 
factorised 
approximations. 
One 
only 
needs 
that 
the 
resulting 
approximating 
distribution 
is 
tractable; 
this 
results 
in 
a 
structured 
Expectation 
Propagation 
algorithm, 
see 
g(28.10c). 
• 
EP 
and 
its 
extensions 
are 
closely 
related 
to 
other 
variational 
procedures 
such 
as 
tree-reweighting[287] 
and 
fractional 
EP[295] 
designed 
to 
compensate 
for 
message 
overcounting 
eects. 
DRAFT 
March 
9, 
2010 



MAP 
for 
MRFs 


Q

1

Algorithm 
30 
Expectation 
Propagation: 
approximation 
of 
p(x)= 
i(Xi).

Zi 


1: 
Decide 
on 
a 
set 
of 
terms 
i(Xi) 
to 
replace 
with 
~i(Xi) 
in 
order 
to 
reveal 
a 
tractable 
distribution 
Y

1 


~

p~(x)= 
i(Xi) 
(28.7.18)

~

Z

i 


2: 
Initialise 
the 
all 
parameters 
~i(Xi). 
3: 
repeat 
4: 
Select 
a 
term 
i(Xi) 
from 
p 
to 
update. 
5: 
Replace 
the 
term 
i(Xi) 
by 
the 
tractable 
term 
~i(Xi) 
to 
form 
~* 
= 
QYj 
~j(Xj) 
~i(Xi) 
i(Xi) 
= 
i(Xi)Yj6=i 
~j(Xj) 
(28.7.19) 
6: 
Find 
the 
parameters 
of 
~i(Xi) 
by 
~i(Xi) 
. 
argmin 
~i(Xi) 
KL(~pj~p) 
(28.7.20) 
where 
~p* 
. 
~, 
~p(x) 
/Yi 
~i(Xi) 
(28.7.21) 
7: 
Set 
any 
proportionality 
terms 
of 
~i(Xi) 
by 
requiring
Xx 
i(Xi)Yj6=i 
~j(Xj) 
=XxYj 
~j(Xj) 
(28.7.22) 
8: 
9: 
until 
converged 
return 
~p(x) 
= 
1 
~ZYi 
~i(Xi), 
~Z 
=XxYi 
~i(Xi) 
(28.7.23) 


as 
an 
approximation 
to 
p(x), 
where 
Z~approximates 
the 
normalisation 
constant 
Z. 


28.8 
MAP 
for 
MRFs 
Consider 
a 
pairwise 
MRF 
p(x) 
. 
eE(x) 
with 


XX

0

E(x) 
f 
(xi;xj)+g(xi;x 
) 
(28.8.1)

i 
iji 


where 
i 
~ 
j 
denotes 
neighbouring 
variables. 
Here 
the 
terms 
f 
(xi;xj) 
represent 
pairwise 
interactions. 
The 


0

terms 
g(xi;x) 
represent 
unary 
interactions, 
written 
for 
convenience 
in 
terms 
of 
a 
pairwise 
interaction 
with 


i 


0

a 
xed 
(non-variable) 
x. 
Typically 
the 
term 
f(xi;xj) 
is 
used 
to 
ensure 
that 
neighbouring 
variables 
xi 
and 


00

xj 
are 
in 
similar 
states; 
the 
term 
g(xi;x) 
is 
used 
to 
bias 
xi 
to 
be 
close 
to 
a 
desired 
state 
xSuch 
models 


ii 
. 
have 
application 
in 
areas 
such 
as 
computer 
vision 
and 
image 
restoration 
in 
which 
an 
observed 
noisy 
image 
x0 
is 
to 
be 
cleaned, 
g(28.3). 
To 
do 
so 
we 
seek 
a 
clean 
image 
x 
for 
which 
each 
clean 
pixel 
value 
xi 
is 
close 


0

to 
the 
observed 
noisy 
pixel 
value 
x, 
whilst 
being 
in 
a 
similar 
state 
to 
its 
clean 
neighbours. 


i 


28.8.1 
MAP 
assignment 
The 
MAP 
assignment 
of 
a 
set 
of 
variables 
x1;:::;xD 
corresponds 
to 
that 
joint 
x 
that 
maximises 
E(x). 
For 
a 
general 
graph 
connectivity 
we 
cannot 
naively 
exploit 
dynamic 
programming 
intuitions 
to 
nd 
an 


DRAFT 
March 
9, 
2010 
533 



MAP 
for 
MRFs 


a
b
cd
ef
a
b
cd
ef
Figure 
28.11: 
(a): 
A 
graph 
with 
bidirectional 
weights 
wij 
= 
wji. 
(b): 
A 
graph 
cut 
partitions 
the 
nodes 
into 
two 
groups 
S 
(blue) 
and 
T 
(red). 
The 
weight 
of 
the 
cut 
is 
the 
sum 
of 
the 
edge 
weights 
from 
S 
(blue) 
to 
T 
(red). 
Intuitively, 
it 
is 
clear 
that 
after 
assigning 
nodes 
to 
state 
1 
(for 
blue) 
and 
0 
(red) 
that 
the 
weight 
of 
the 
cut 
corresponds 
to 
the 
summed 
weights 
of 
neighbours 
in 
dierent 
states. 
Here 
we 
highlight 
those 
weight 
contributions. 
The 
non-highlighted 


edges 
do 
not 
contribute 
to 
the 
cut 
weight. 
Note 
that 


(a) 
(b) 
only 
one 
of 
the 
edge 
directions 
contributes 
to 
the 
cut. 


exact 
solution 
since 
the 
graph 
is 
loopy. 
As 
we 
see 
below, 
in 
special 
cases, 
even 
though 
the 
graph 
is 
loopy, 
this 
is 
possible. 
In 
general, 
however, 
approximate 
algorithms 
are 
required. 


A 
simple 
general 
approximate 
solution 
can 
be 
found 
as 
follows: 
rst 
initialise 
all 
x 
at 
random. 
Then 
select 
a 
variable 
xi 
and 
nd 
the 
state 
of 
xi 
that 
maximally 
improves 
E(x), 
keeping 
all 
other 
variables 
xed. 
One 
then 
repeats 
this 
selection 
and 
local 
maximal 
state 
computation 
until 
convergence. 
This 
is 
called 
Iterated 
Conditional 
Modes[36]. 
Due 
to 
the 
Markov 
properties 
its 
clear 
that 
we 
can 
improve 
on 
this 
ICM 
method 
by 
simultaneously 
optimising 
all 
variables 
conditioned 
on 
their 
respective 
Markov 
blankets 
(similar 
to 
the 
approach 
used 
in 
black-white 
sampling). 
Another 
improvement 
is 
to 
update 
only 
a 
subset 
of 
the 
variables, 
where 
the 
subset 
has 
the 
form 
of 
singly-connected 
structure. 
By 
recursively 
clamping 
variables 
to 
reveal 
a 
singly-connected 
structure 
on 
un-clamped 
variables, 
one 
may 
nd 
an 
approximate 
solution 
by 
solving 
a 
sequence 
of 
tractable 
problems. 


Remarkably, 
in 
the 
special 
case 
of 
binary 
variables 
and 
positive 
w 
discussed 
below, 
an 
ecient 
exact 
algorithm 
exists 
for 
nding 
the 
MAP 
state, 
regardless 
of 
the 
topology. 


28.8.2 
Attractive 
binary 
MRFs 
Consider 
nding 
the 
MAP 
of 
a 
MRF 
with 
binary 
variables 
dom(xi)= 
f0, 
1} 
and 
positive 
connections 
wij 
= 
0. 
In 
this 
case 
our 
task 
is 
to 
nd 
the 
assignment 
x 
that 
maximises 


XX

E(x) 
wijI[xi 
= 
xj]+cixi 
(28.8.2) 
iji 


where 
i 
~ 
j 
denotes 
neighbouring 
variables 
and 
real 
ci. 
Note 
that 
for 
binary 
variables 
xi 
2f0, 
1g, 


I[xi 
= 
xj]= 
xixj 
+ 
(1 
- 
xi)(1 
- 
xj) 
(28.8.3) 


For 
this 
particular 
case 
an 
ecient 
MAP 
algorithm 
exists 
for 
arbitrary 
topology 
of 
w[121]. 
The 
algorithm 
rst 
translates 
the 
MAP 
assignment 
problem 
into 
an 
equivalent 
min 
s-t-cut 
problem[39], 
for 
which 
ecient 
algorithms 
exist. 
In 
min 
s-t-cut, 
we 
need 
a 
graph 
with 
positive 
weights 
on 
the 
edges. 
This 
is 
clearly 
satised 


PX

if 
wij 
> 
0, 
although 
the 
bias 
terms 
i 
cixi 
need 
to 
be 
addressed. 


Dealing 
with 
the 
bias 
terms 


To 
translate 
the 
MAP 
assignment 
problem 
to 
a 
min-cut 
problem 
we 
need 
to 
deal 
with 
the 
additional 
linear 


PX

terms 
i 
cixi. 
First 
consider 
the 
eect 
of 
including 
a 
new 
node 
x* 
and 
connecting 
this 
to 
each 
existing 
node 
i 
with 
weight 
ci. 
This 
adds 
then 
a 
term

XX

ciI[xi 
= 
x]=ci 
(xix* 
+ 
(1 
- 
xi) 
(1 
- 
x)) 
(28.8.4) 
ii 


If 
we 
set 
x* 
in 
state 
1, 
this 
will 
then 
add 
terms

X

cixi 
(28.8.5) 
i 


DRAFT 
March 
9, 
2010 



MAP 
for 
MRFs 


a+
b..6
c+d..6
e..6f+
st
Figure 
28.12: 
(a): 
A 
Graph 
with 
bidirectional 
weights 
wij 
= 
wji 
augmented 
with 
a 
source 
node 
s 
and 
sink 
node 
t. 
Each 
node 
has 
a 
corresponding 
bias 
whose 
sign 
is 
indicated. 
The 
source 
node 
is 
linked 
to 
the 
nodes 
corresponding 
to 
positive 
bias, 
and 
the 
nodes 
with 
negative 
bias 
to 
the 
sink. 
(b): 
A 
graph 
cut 
partitions 
the 
nodes 
into 
two 
groups 
S6(blue) 
and 


(a) 
T6(red), 
where 
S6is 
the 
union 
of 
the 
source 
node 
and 
a
b
cd
ef
st
nodes 
in 
state 
1, 
T6is 
the 
union 
of 
the 
sink 
node 
and 
nodes 
in 
state 
0. 
The 
weight 
of 
the 
cut 
is 
the 
sum 
of 
the 
edge 
weights 
from 
S6(blue) 
to 
T6(red). 
The 
red 
lines 
indicate 
contributions 
to 
the 
cut, 
and 
can 
be 
considered 
penalties 
since 
we 
wish 
to 
nd 
the 
minimal 
cut. 
For 
example 
a 
being 
in 
state 
1 
(blue) 
does 
not 
incur 
a 
penalty 
since 
ca 
> 
0; 
on 
the 
other 
hand, 
variable 
f 
being 
in 
state 
0 
(red) 
incurs 
a 
penalty 
since 
cf 
> 
0. 


(b) 
Otherwise, 
if 
we 
set 
x* 
in 
state 
0 
we 
obtain

XX

ci 
(1 
..6xi)= 
..cixi 
+ 
const. 
(28.8.6) 


ii 


Since 
our 
requirement 
is 
that 
we 
need 
the 
weights 
to 
be 
positive 
we 
see 
that 
we 
can 
achieve 
this 
by 
dening 
two 
additional 
nodes. 
We 
dene 
a 
source 
node 
xs, 
set 
to 
state 
1 
and 
connect 
it 
to 
those 
xi 
which 
have 
positive 
ci, 
dening 
wsi 
= 
ci. 
In 
addition 
we 
dene 
a 
sink 
node 
xt 
= 
0 
and 
connect 
all 
nodes 
with 
negative 
ci, 
to 
xt, 
using 
weight 
wit 
= 
..ci, 
(which 
is 
therefore 
positive). 


For 
the 
source 
node 
clamped 
to 
xs 
= 
1 
and 
the 
sink 
node 
to 
xt 
= 
0, 
then 
including 
the 
source 
and 
sink, 
we 
have 


X

E(x)=wijI[xi 
= 
xj]+ 
const. 
(28.8.7) 


ij 


is 
equal 
to 
the 
energy 
function, 
equation 
(28.8.2). 


Denition 
115 
(Graph 
Cut). 
For 
a 
graph 
G 
with 
vertices 
v1;:::;vD, 
and 
weights 
wij 
> 
0 
a 
cut 
is 
a 
partition 
of 
the 
vertices 
into 
two 
disjoint 
groups, 
called 
S6and 
T6. 
The 
weight 
of 
a 
cut 
is 
then 
dened 
as 
the 
sum 
of 
the 
weights 
that 
leave 
S6and 
land 
in 
T6, 
see 
g(28.11). 


The 
weight 
of 
a 
cut 
corresponds 
to 
the 
sum 
of 
weights 
between 
mismatched 
neighbours, 
see 
g(28.11b). 
That 
is, 


X

cut(x)=wijI[xi6
= 
xj] 
(28.8.8) 
ij 


Since 
I[xi6
= 
xj], 
we 
can 
dene 
the 
weight 
of 
the 
cut 
equivalently 
as 


= 
xj]=1 
..6I[xi 


XX

cut(x)=wij 
(1 
..6I[xi 
= 
xj]) 
= 
..wijI[xi 
= 
xj]+ 
const. 
(28.8.9) 
ijij 


DRAFT 
March 
9, 
2010 



MAP 
for 
MRFs 



Figure 
28.13: 
(a): 
Clean 
image. 
(b): 
Noisy 
image. 
(c): 
Restored 
image 
using 
ICM. 
See 
demoMRFclean.m. 


(a) 
(b) 
(c) 
P

so 
that 
the 
minimal 
cut 
assignment 
will 
correspond 
to 
maximisingij 
wijI[xi 
= 
xj]. 
In 
the 
MRF 
case, 
our 
translation 
into 
a 
weighted 
graph 
with 
positive 
interactions 
then 
requires 
that 
we 
identify 
the 
source 
and 
all 
other 
variables 
assigned 
to 
state 
1 
with 
S, 
and 
the 
sink 
and 
all 
variables 
in 
state 
0 
with 
T 
, 
see 
g(28.12). 
A 
fundamental 
result 
is 
that 
the 
min 
s-t-cut 
solution 
corresponds 
to 
the 
max-ow 
solution 
from 
the 
source 
s 
to 
the 
sink 
t 
[39]. 
There 
are 
ecient 
algorithms 
for 
max-ow, 
see 
for 
example 
[47], 


..X

which 
take 
OD3 
operations 
or 
less. 
This 
means 
that 
one 
can 
nd 
the 
exact 
MAP 
assignment 
of 
an 


..X

attractive 
binary 
MRF 
eciently 
in 
OD3 
operations. 
In 
MaxFlow.m 
we 
implement 
the 
Ford-Fulkerson 
(Edmonds-Karp-Dinic 
breadth 
rst 
search 
variant)[87], 
see 
also 
exercise(251). 


Example 
118 
(Analysing 
dirty 
pictures). 
In 
g(28.13) 
we 
present 
a 
noisy 
binary 
y 
image 
that 
we 
wish 
to 
clean. 
To 
do 
so 
we 
use 
an 
objective 


XX

E(x)=wijI[xi 
= 
xj]+I[xi 
= 
yi] 
(28.8.10) 
ij 
i 


The 
variables 
xi, 
i 
=1;:::, 
784 
are 
dened 
on 
a 
2828 
grid 
and 
where 
wij 
= 
10 
if 
xi 
and 
xj 
are 
neighbours 
on 
the 
grid. 
Using 


I[xi 
= 
yi]= 
xi 
(2yi 
- 
1) 
+ 
const. 
(28.8.11) 


we 
have 
a 
standard 
binary 
MRF 
MAP 
problem 
with 
`bias’ 
b 
=2y 
- 
1. 
Once 
can 
then 
nd 
the 
exact 
optimal 
x 
by 
the 
min-cut 
procedure. 
However, 
our 
implementation 
of 
this 
is 
slow 
and 
instead 
we 
use 
the 
simpler 
ICM 
algorithm, 
with 
results 
as 
shown 
in 
g(28.13). 


28.8.3 
Potts 
model 
An 
extension 
of 
the 
previous 
model 
is 
to 
the 
case 
when 
the 
variables 
are 
non-binary, 
which 
is 
termed 
the 
Potts 
model: 


XX

0

E(x)=wijI[xi 
= 
xj]+ciIxi 
= 
xi(28.8.12) 
iji 


0

where 
wij 
> 
0 
and 
the 
xare 
known. 
This 
model 
has 
immediate 
application 
in 
non-binary 
image 
restora


i 


tion, 
and 
also 
in 
clustering 
based 
on 
a 
similarity 
score. 
Whilst 
no 
ecient 
exact 
algorithm 
is 
known, 
a 
useful 
approach 
is 
to 
approximate 
the 
problem 
as 
a 
sequence 
of 
binary 
problems, 
as 
we 
describe 
below. 


Potts 
to 
binary 
MRF 
translation 


Consider 
the 
-expansion 
representation 


old 


xi 
= 
sia 
+ 
(1 
- 
si)x 
(28.8.13)

i 


old 


where 
si 
2f0, 
1} 
and 
for 
a 
given 
a 
2f0, 
1, 
2;:::;Ng. 
This 
restricts 
xi 
to 
be 
either 
the 
state 
xor 
,

i 
depending 
on 
the 
binary 
variable 
si. 
Using 
a 
new 
binary 
variable 
s 
we 
can 
therefore 
restrict 
x 
to 
a 
subpart 


DRAFT 
March 
9, 
2010 



MAP 
for 
MRFs 


Figure28.14:(a):Noisyim-
age.(b):Restoredimage.The
-expansionmethodwasused,
withsuitableinteractionswandbiasctoensurereasonablere-
sults.From[47].
(a) 
(b) 
of 
the 
full 
space 
and 
write 
a 
new 
objective 
function 
in 
terms 
of 
s 
alone: 


XX

E(s)=wijI[si 
= 
sj]+cisi 
+ 
const. 
(28.8.14) 
iji 


for 
wij 
> 
0. 
This 
new 
problem 
is 
of 
the 
form 
of 
an 
attractive 
binary 
MRF 
which 
can 
be 
solved 
exactly 
using 
the 
graph 
cuts 
procedure. 
The 
idea 
is 
then 
to 
choose 
another 
a 
value 
(at 
random) 
and 
then 
nd 
the 
optimal 
s 
for 
the 
new 
. 
In 
this 
way 
we 
are 
guaranteed 
to 
iteratively 
increase 
E. 


For 
a 
given 
a 
and 
xold, 
the 
transformation 
of 
the 
Potts 
model 
objective 
is 
given 
by 
using 
si 
2f0, 
1} 
and 
considering 


hi

old 
old 


I[xi 
= 
xj]= 
Isia 
+ 
(1 
- 
si)xi 
= 
sja 
+ 
(1 
- 
sj)xj

hihihi

old 
old 
old 
old 


= 
(1 
- 
si)(1 
- 
sj)Ix 
= 
xj+ 
(1 
- 
si)sjIxi 
= 
+ 
si(1 
- 
sj)Ixj 
= 
+ 
sisj

i 


= 
sisjuij 
+ 
aisi 
+ 
bjsj 
+ 
const. 
(28.8.15) 


with 


hihihi

old 
old 
old 
old 


uij 
= 
1 
- 
Ixi 
= 
- 
Ixj 
= 
+ 
Ixi 
= 
xj(28.8.16) 


and 
similarly 
dened 
ai, 
bi. 
By 
enumeration 
it 
is 
straightforward 
to 
show 
that 
uij 
is 
either 
0, 
1 
or 
2. 
Using 
the 
mathematical 
identity 


1 


sisj 
=(I[si 
= 
sj]+ 
si 
+ 
sj 
- 
1) 
(28.8.17)

2 


we 
can 
write, 


uij

I[xi 
= 
xj]= 
(I[si 
= 
sj]+ 
si 
+ 
sj)+ 
aisi 
+ 
bjsj 
+ 
const. 
(28.8.18)

2 


Hence 
terms 
wijI[xi 
= 
xj] 
translate 
to 
positive 
interaction 
terms 
I[si 
= 
sj] 
wijuij=2. 
All 
the 
unary 
terms 


are 
easily 
exactly 
mapped 
into 
corresponding 
unary 
terms 
cisi 
for 
cdened 
as 
the 
sum 
of 
all 
unary 
terms 


i 
in 
si. 
This 
shows 
that 
the 
positive 
interaction 
wij 
in 
terms 
of 
the 
original 
variables 
x 
maps 
to 
a 
positive 
interaction 
in 
the 
new 
variables 
s. 
Hence 
we 
can 
nd 
the 
maximal 
state 
of 
s 
using 
a 
graph 
cut 
algorithm. 
A 
related 
(though 
dierent) 
procedure 
is 
outlined 
in 
[48]. 


DRAFT 
March 
9, 
2010 



Exercises 


Example 
119 
(Potts 
model 
for 
image 
reconstruction). 
An 
example 
image 
restoration 
problem 
for 
nearest 
neighbour 
interactions 
on 
a 
pixel 
lattice 
and 
suitably 
chosen 
w, 
c 
is 
given 
in 
g(28.14). 
The 
images 
are 
non-binary 
and 
therefore 
the 
optimal 
MAP 
assignment 
cannot 
be 
computed 
exactly 
in 
an 
ecient 
way. 
The 
alpha-expansion 
technique 
was 
used 
here 
combined 
with 
an 
ecient 
min-cut 
approach, 
see 
[47] 
for 
details. 


28.9 
Further 
Reading 
Approximate 
inference 
is 
a 
highly 
active 
research 
area 
and 
increasingly 
links 
to 
convex 
optimisation[46] 
are 
being 
developed. 
See 
[287] 
for 
a 
general 
overview 
and 
[247] 
for 
recent 
application 
of 
convex 
optimisation 
to 
approximate 
inference 
in 
a 
practical 
machine 
learning 
application. 


28.10 
Code 
LoopyBP.m: 
Loopy 
Belief 
Propagation 
(Factor 
Graph 
formalism) 
demoLoopyBP.m: 
Demo 
of 
loopy 
Belief 
Propagation 
demoMFBPGibbs.m: 
Comparison 
of 
Mean 
Field, 
Belief 
Propagation 
and 
Gibbs 
sampling 


demoMRFclean.m: 
Demo 
of 
analysing 
a 
dirty 
picture 
MaxFlow.m: 
Max-Flow 
Min-Cut 
algorithm 
(Ford-Fulkerson) 
binaryMRFmap.m: 
Optimising 
a 
binary 
MRF 


28.11 
Exercises 
Exercise 
251. 
For 
the 
max-ow-min-cut 
problem, 
under 
the 
convention 
that 
the 
source 
node 
xs 
is 
clamped 
to 
state 
1, 
and 
the 
sink 
node 
xt 
to 
state 
0, 
a 
standard 
min-cut 
algorithm 
returns 
that 
joint 
x 
which 


minimises
Xij 
wijI[xi 
= 
1] 
I[xj 
= 
0] 
(28.11.1) 
Explain 
how 
this 
can 
be 
written 
in 
the 
form
Xij 
~wijI[xi6
= 
xj] 
(28.11.2) 


Exercise 
252. 
Using 
BRMLtoolbox, 
write 
a 
routine 
KLdiv(q,p) 
that 
returns 
the 
Kullback-Leibler 
div
ergence 
between 
two 
discrete 
distributions 
q 
and 
p 
dened 
as 
potentials 
q 
and 
p. 


Exercise 
253. 
The 
le 
p.mat 
contains 
a 
distribution 
p(x, 
y, 
z) 
on 
ternary 
state 
variables. 
Using 
BRML-
toolbox, 
nd 
the 
best 
approximation 
q(x, 
y)q(z) 
that 
minimises 
the 
Kullback-Leibler 
divergence 
KL(qjp) 
and 
state 
the 
value 
of 
the 
minimal 
Kullback-Leibler 
divergence 
for 
the 
optimal 
q. 


Exercise 
254. 
Consider 
the 
pairwise 
MRF 
dened 
on 
a 
2 
62 
lattice, 
as 
given 
in 
pMRF.mat. 
Using 
BRMLtoolbox, 


Q4 


BP 


1. 
Find 
the 
optimal 
fully 
factorised 
approximationby 
Loopy 
Belief 
Propagation, 
based 
on 
the 
i=1 
qi 


factor 
graph 
formalism. 


Q4 


MF 


2. 
Find 
the 
optimal 
fully 
factorised 
approximationby 
solving 
the 
variational 
Mean 
Field 
i=1 
qi 


equations. 


3. 
By 
pure 
enumeration, 
compute 
the 
exact 
marginals 
pi. 
DRAFT 
March 
9, 
2010 



Exercises 


4. 
Averaged 
over 
all 
4 
variables, 
compute 
the 
mean 
expected 
deviation 
in 
the 
marginals 
XX

1 
41 
2

jqi(x 
= 
j) 
- 
pi(x 
= 
j)j

42 


i=1 
j=1 


for 
both 
the 
BP 
and 
MF 
approximations, 
and 
comment 
on 
your 
results. 


Exercise 
255. 
In 
LoopyBP.m 
the 
message 
schedule 
is 
chosen 
at 
random. 
Modify 
the 
routine 
to 
choose 
a 
schedule 
using 
a 
forward-reverse 
elimination 
sequence 
on 
a 
random 
spanning 
tree. 
Exercise 
256 
(Double 
Integration 
Bounds). 
Consider 
a 
bound 


f(x) 
= 
g(x) 
(28.11.3) 


Then 
for 


ZZ

xx 


f~(x) 
f(x)dx, 
g~(x) 
g(x)dx 
(28.11.4) 


aa 


Show 
that: 


1. 
f~(x) 
= 
g~(x), 
for 
x 
= 
a 
(28.11.5) 
2. 
f^(x) 
= 
g^(x) 
for 
all 
x 
(28.11.6) 
where 


ZZ

xx 


f^(x) 
f~(x)dx, 
g^(x) 
g~(x)dx 
(28.11.7) 


aa 


The 
signicance 
is 
that 
this 
double 
integration 
(or 
summation 
in 
the 
case 
of 
discrete 
variables) 
is 
a 
general 
procedure 
for 
generating 
a 
new 
bound 
from 
an 
existing 
bound 
[172]. 


Exercise 
257. 
Starting 
from 


e 
x 
= 
0 
(28.11.8) 


and 
using 
the 
double 
integration 
procedure, 
show 
that 


e 
x 
= 
e 
a(1 
+ 
x 
- 
a) 


1. 
By 
replacing 
x 
. 
sTWs 
for 
s 
2f0, 
1gD, 
and 
a 
. 
hTs 
derive 
a 
bound 
on 
the 
partition 
function 
of 
a 
Boltzmann 
distribution 
X

Z 
=e 
sTWs 
(28.11.9) 
s 


2. 
Show 
that 
this 
bound 
is 
equivalent 
to 
the 
Mean 
Field 
bound 
on 
the 
partition 
function. 
3. 
Discuss 
how 
one 
can 
generate 
tighter 
bounds 
on 
the 
partition 
function 
of 
a 
Boltzmann 
distribution 
by 
further 
application 
of 
the 
double 
integration 
procedure. 
Exercise 
258. 
Consider 
a 
pairwise 
MRF 


1 


xTWx+bTx 


p(x)= 
e 
(28.11.10)

Z 


for 
symmetric 
W. 
Consider 
the 
decomposition 


X

W 
=qiWi;i 
=1;:::;I 
(28.11.11) 
i 


P

where 
0 
= 
qi 
= 
1 
andi 
qi 
=1, 
and 
the 
graph 
of 
each 
matrix 
Wi 
is 
a 
tree. 
Explain 
how 
to 
form 
an 
upper 
bound 
on 
the 
normalisation 
Z 
and 
discuss 
a 
naive 
method 
to 
nd 
the 
tightest 
upper 
bound. 


DRAFT 
March 
9, 
2010 



Exercises 


Exercise 
259. 
Derive 
Linkser's 
bound 
on 
the 
Mutual 
Information, 
equation 
(28.5.15). 
Exercise 
260. 
Consider 
the 
average 
of 
a 
positive 
function 
f(x) 
with 
respect 
to 
a 
distribution 
p(x) 


Z

J 
= 
logp(x)f(x) 
(28.11.12) 


x 


where 
f(x) 
60. 
The 
simplest 
version 
of 
Jensen's 
inequality 
states 
that 


Z

J 
p(x) 
log 
f(x) 
(28.11.13) 


x 


1. 
By 
considering 
a 
distribution 
r(x) 
/6p(x)f(x), 
and 
KL(qjr), 
for 
some 
variational 
distribution 
q(x), 
show 
that 
J 
..KL(q(x)jp(x)) 
+ 
hlog 
f(x)i(28.11.14)

q(x) 


The 
bound 
saturates 
when 
q(x) 
/6p(x)f(x). 
This 
shows 
that 
if 
we 
wish 
to 
approximate 
the 
average 
J, 
the 
optimal 
choice 
for 
the 
approximating 
distribution 
depends 
on 
both 
the 
distribution 
p(x) 
and 
integrand 
f(x). 


2. 
Furthermore, 
show 
that 
J 
..KL(q(x)jp(x)) 
..6KL(q(x)jf(x)) 
..6H(q(x)) 
(28.11.15) 


where 
H(q(x)) 
is 
the 
entropy 
of 
q(x). 
The 
rst 
term 
encourages 
q 
to 
be 
close 
to 
p. 
The 
second 
encourages 
q 
to 
be 
close 
to 
f, 
and 
the 
third 
encourages 
q 
to 
be 
sharply 
peaked. 


Exercise 
261. 
For 
a 
Markov 
Random 
eld 
over 
D 
binary 
variables 
xi 
2f0, 
1g, 
i 
=1;:::;D, 
we 
dene 


1 


p(x)= 
e 
xTWx 
(28.11.16)

Z 


show 
that 
p(xi) 
= 
Zni 
Z 
(28.11.17) 
where 


X

Zni 
e 
xTWx 
(28.11.18) 


x1;:::;xi..1;xi+1;:::;xD 


and 
explain 
why 
a 
bound 
on 
the 
marginal 
p(xi) 
requires 
both 
upper 
and 
lower 
bounds 
on 
partition 
functions. 


Exercise 
262. 
Consider 
a 
directed 
graph 
such 
that 
the 
capacity 
of 
an 
edge 
x 
!6y 
is 
c(x, 
y) 
60. 
The 
ow 
on 
an 
edge 
f(x, 
y) 
60 
must 
not 
exceed 
the 
capacity 
of 
the 
edge. 
The 
aim 
is 
to 
maximise 
the 
ow 
from 
a 


dened 
source 
node 
s 
to 
a 
dened 
sink 
node 
t. 
In 
addition 
ow 
must 
be 
conserved 
such 
that 
for 
any 
node 
other 
than 
the 
source 
or 
sink 
(y6
= 
s, 
t),
XXf(x, 
y) 
=f(y, 
x) 
(28.11.19) 
x 
x 


A 
cut 
is 
dened 
as 
a 
partition 
of 
the 
nodes 
into 
two 
non-overlapping 
sets 
S6and 
T6such 
that 
s 
is 
in 
S6and 
t 
in 
T6. 
Show 
that: 


1. 
The 
net 
ow 
from 
s 
to 
t, 
val(f) 
is 
the 
same 
as 
the 
net 
ow 
from 
S6to 
T6: 
XX

val(f)=f(x, 
y) 
..f(y, 
x) 
(28.11.20) 


x2S;y2T 
y2T 
;x2S 


P

2. 
val(f) 
f(x, 
y) 
namely 
that 
the 
ow 
is 
upper 
bounded 
by 
the 
capacity 
of 
the 
cut. 
x2S;y2T 


DRAFT 
March 
9, 
2010 



Exercises 


The 
max-ow-min-cut 
theorem 
further 
states 
that 
the 
maximal 
ow 
is 
actually 
equal 
to 
the 
capacity 
of 
the 
cut. 


Exercise 
263 
(Potts 
to 
Ising 
translation). 
Consider 
the 
function 
E(x) 
dened 
on 
a 
set 
of 
multistate 
variables 
dom(xi)= 
f0, 
1, 
2;:::;Ng, 


XX

0

E(x)=wijI[xi 
= 
xj]+ciIxi 
= 
xi(28.11.21) 
iji 


0

where 
wij 
> 
0 
and 
observed 
pixel 
states 
xare 
known, 
as 
are 
ci. 
Our 
interest 
is 
to 
nd 
an 
approximate 


i 


maximisation 
of 
E(x). 
Using 
the 
restricted 
parameteristion 


old 


xi 
= 
sia 
+ 
(1 
- 
si)x 
(28.11.22)

i 


where 
si 
2f0, 
1} 
and 
for 
a 
given 
a 
2f0, 
1, 
2;:::;Ng, 
show 
how 
to 
write 
E(x) 
as 
a 
function 
of 
the 
binary 
variables 


XX

E(s)=wijI[si 
= 
sj]+cisi 
+ 
const. 
(28.11.23) 
iji 


for 
wij 
> 
0. 
This 
new 
problem 
is 
of 
the 
form 
of 
an 
attractive 
binary 
MRF 
which 
can 
be 
solved 
exactly 
using 
the 
graph 
cuts 
procedure. 


Exercise 
264. 
Consider 
an 
approximating 
distribution 
in 
the 
exponential 
family, 


1 


T 
g(x)

q(x)= 
e 
(28.11.24)

Z()
We 
wish 
to 
use 
q(x) 
to 
approximate 
a 
distribution 
p(x) 
using 
the 
KL 
divergence 
KL(pjq) 
(28.11.25) 


1. 
Show 
that 
optimally 
hg(x)i= 
hg(x)i(28.11.26)

p(x) 
q(x) 


2. 
Show 
that 
a 
Gaussian 
can 
be 
written 
in 
the 
exponential 
form 
..

1 


Tg(x)

Nx 


, 
2= 
e 
(28.11.27)

Z()
where 
g1(x)= 
x, 
g2(x)= 
x2 
and 
suitably 
chosen 
. 


..



3. 
Hence 
show 
that 
the 
optimal 
Gaussian 
t 
Nx 
, 
2to 
any 
distribution, 
in 
the 
minimal 
KL(pjq) 
sense 
matches 
the 
moments: 





2

µ 
= 
hxip(x) 
;2 
=x 
..hxi2 
(28.11.28)

p(x) 
p(x) 


..



2

Exercise 
265. 
We 
wish 
to 
nd 
a 
Gaussian 
approximation 
q(x)= 
Nx 


m, 
sto 
a 
distribution 
p(x). 
Show 
that 


KL(pjq)= 
..hlog 
q(x)i+ 
const. 
(28.11.29)

p(x) 


Write 
the 
KL 
divergence 
explicitly 
as 
a 
function 
of 
m 
and 
s2 
and 
conrm 
the 
general 
result 
that 
the 
optimal 
m 
and 
s2 
that 
minimise 
KL(pjq) 
are 
given 
by 
setting 
the 
mean 
and 
variance 
of 
q 
to 
those 
of 
p. 


DRAFT 
March 
9, 
2010 



Exercises 


Exercise 
266. 
For 
a 
pairwise 
binary 
Markov 
Random 
Field, 
p 
with 
partition 
function 


XPP

Z(w, 
b)=ei;j 
wij 
xixj 
+i 
bixi 
(28.11.30) 


x 


show 
that 
the 
means 
can 
be 
computed 
using 


. 
@bi 
log 
Z(w, 
b) 
= 
1 
Z(w, 
b)Xx 
xiePij 
wij 
xixj 
+Pi 
bixi 
= 
hxiip 
(28.11.31) 
and 
that 
similarly 
the 
covariance 
is 
given 
by 
hxixji6p 
..6hxii6p 
hxji6p 
= 
@2 
@bi@bj 
log 
Z(w, 
b) 
(28.11.32) 
Exercise 
267. 
The 
naive 
mean 
eld 
theory 
applied 
to 
a 
pairwise 
MRF 
PPp(x) 
/6ei;j 
wij 
xixj 
+i 
bixi 
(28.11.33) 
Q

dom(xi)= 
f0, 
1g, 
gives 
a 
factorised 
approximation 
q(x)=i 
q(xi), 
based 
on 
minimising 
KL(qjp). 
Using 
this 
we 
can 
approximate 


hxixji6hxiihxji6;i6(28.11.34)

= 
j

p 
qq 


To 
produce 
a 
better, 
non-factorised 
approximation 
to 
hxixjiwe 
could 
t 
a 
non-factorised 
q. 
The 
linear-

p

response 
method[153] 
may 
also 
be 
used, 
based 
on 
a 
perturbation 
expansion 
of 
the 
free 
energy. 
Alternatively, 
consider 
the 
relation 


p(xi;xj)= 
p(xijxj)p(xj) 
(28.11.35) 


Show 
that 


hxixji6= 
p(xi 
=1;xj 
= 
1) 
= 
p(xi 
=1jxj 
= 
1)p(xj 
= 
1) 
(28.11.36)

p 


Explain 
how 
to 
use 
a 
modied 
naive 
mean 
eld 
method 
to 
nd 
a 
non-factorised 
approximation 
to 
hxixji. 


p 


Exercise 
268. 
Derive 
the 
EP 
updates 
equation 
(28.7.8) 
and 
equation 
(28.7.9). 


Exercise 
269. 
You 
are 
given 
a 
set 
of 
datapoints 
labelled 
1 
to 
N 
and 
a 
similarity 
`metric’ 
wij 
60, 
i, 
j 
= 
1;:::;N 
which 
denotes 
the 
similarity 
of 
the 
points 
i 
and 
j. 
You 
want 
to 
assign 
each 
datapoint 
to 
a 
cluster 
index 
cn 
2f1;:::;Kg. 
For 
a 
subset 
of 
the 
datapoints 
you 
have 
a 
preference 
for 
the 
cluster 
index. 
Explain 
how 
to 
use 
a 
Potts 
model 
to 
formulate 
an 
objective 
function 
for 
this 
`semi-supervised’ 
clustering 
problem. 


DRAFT 
March 
9, 
2010 



APPENDIX 
A 


Background 
Mathematics 


A.1 
Linear 
Algebra 
A.1.1 
Vector 
algebra 
Let 
x 
denote 
the 
n-dimensional 
column 
vector 
with 
components 


.
. 


BBB
. 


x1 
x2 


. 


. 


. 


xn 


CCC
. 


Denition 
116 
(scalar 
product). 
The 
scalar 
product 
w 
· 
x 
is 
dened 
as: 


X

n

T

w 
· 
x 
= 
wixi 
= 
wx 
(A.1.1) 


i=1 


and 
has 
a 
natural 
geometric 
interpretation 
as: 


w 
· 
x 
= 
jwjjx| 
cos() 
(A.1.2) 


where 
. 
is 
the 
angle 
between 
the 
two 
vectors. 
Thus 
if 
the 
lengths 
of 
two 
vectors 
are 
xed 
their 
inner 
product 
is 
largest 
when 
. 
= 
0, 
whereupon 
one 
vector 
is 
a 
constant 
multiple 
of 
the 
other. 
If 
the 
scalar 
product 
xTy 
= 
0, 
then 
x 
and 
y 
are 
orthogonal 
(they 
are 
a 
right 
angles 
to 
each 
other). 


The 
length 
of 
a 
vector 
is 
denoted 
jxj, 
the 
squared 
length 
is 
given 
by 


T222 
2

jxj2 
= 
xx 
= 
x= 
x1 
+ 
x2 
+ 
· 
+ 
x 
(A.1.3)

n 


A 
unit 
vector 
x 
has 
xTx 
= 
1. 


Denition 
117 
(Linear 
dependence). 
A 
set 
of 
vectors 
x1 
;:::, 
xn 
is 
linearly 
dependent 
if 
there 
exists 
a 
vector 
xj 
that 
can 
be 
expressed 
as 
a 
linear 
combination 
of 
the 
other 
vectors. 
Vice-versa, 
if 
the 
only 
solution 
to 


X

n

i

ix= 
0 
(A.1.4) 


i=1 


543 



Linear 
Algebra 


e*
e
b
aaee*
a
Figure 
A.1: 
Resolving 
a 
vector 
a 
into 
components 
along 
the 
orthogonal 
directions 
e 
and 
e 
* 
. 
The 
projection 
of 
a 
onto 
these 
two 
directions 
are 
lengths 
a 
and 
ß 
along 
the 
directions 
e 
and 


e 
* 


. 


is 
for 
all 
i 
=0;i 
=1;:::;n, 
the 
vectors 
x1 
;:::, 
xn 
are 
linearly 
independent. 


A.1.2 
The 
scalar 
product 
as 
a 
projection 
Suppose 
that 
we 
wish 
to 
resolve 
the 
vector 
a 
into 
its 
components 
along 
the 
orthogonal 
directions 
specied 
by 
the 
unit 
vectors 
e 
and 
e 
* 
. 
That 
is 
je| 
= 
jej* 
= 
1 
and 
e 
· 
e 
* 
= 
0. 
This 
is 
depicted 
in 
g(A.1). 
We 
are 
required 
to 
nd 
the 
scalar 
values 
a 
and 
ß 
such 
that 


* 


a 
= 
e 
+ 
e 
(A.1.5) 


From 
this 
we 
obtain 


* 
* 


a 
· 
e 
= 
e 
· 
e 
+ 
e 
· 
e, 
a 
· 
e 
= 
e 
· 
e 
* 
+ 
e 
· 
e 
(A.1.6) 


From 
the 
orthogonality 
and 
unit 
lengths 
of 
the 
vectors 
e 
and 
e 
, 
this 
becomes 
simply 


* 


a 
· 
e 
= 
, 
a 
· 
e 
= 
ß 
(A.1.7) 


A 
set 
of 
vectors 
is 
orthonormal 
if 
they 
are 
mutually 
orthogonal 
and 
have 
unit 
length. 
This 
means 
that 
we 
can 
write 
the 
vector 
a 
in 
terms 
of 
the 
orthonormal 
components 
e 
and 
e 
* 
as 


* 


a 
=(a 
· 
e) 
e 
+(a 
· 
e 
) 
e 
(A.1.8) 


One 
can 
see 
therefore 
that 
the 
scalar 
product 
between 
a 
and 
e 
projects 
the 
vector 
a 
onto 
the 
(unit) 
direction 
e. 
The 
projection 
of 
a 
vector 
a 
onto 
a 
direction 
specied 
by 
f 
is 
therefore 


a 
· 
f 


f 
(A.1.9)

jfj2 


A.1.3 
Lines 
in 
space 
A 
line 
in 
2 
(or 
more) 
dimensions 
can 
be 
specied 
as 
follows. 
The 
vector 
of 
any 
point 
along 
the 
line 
is 
given, 
for 
some 
s, 
by 
the 
equation 


p 
= 
a 
+ 
su;s 
2R. 
(A.1.10) 


where 
u 
is 
parallel 
to 
the 
line, 
and 
the 
line 
passes 
through 
the 
point 
a, 
see 
g(A.2). 
This 
is 
called 
the 
parametric 
representation 
of 
the 
line. 
An 
alternative 
specication 
can 
be 
given 
by 
realising 
that 
all 
vectors 
along 
the 
line 
are 
orthogonal 
to 
the 
normal 
of 
the 
line, 
n 
(u 
and 
n 
are 
orthonormal). 
That 
is 


(p 
- 
a) 
· 
n 
=0 
. 
p 
· 
n 
= 
a 
· 
n 
(A.1.11) 


If 
the 
vector 
n 
is 
of 
unit 
length, 
the 
right 
hand 
side 
of 
the 
above 
represents 
the 
shortest 
distance 
from 
the 
origin 
to 
the 
line, 
drawn 
by 
the 
dashed 
line 
in 
g(A.2) 
(since 
this 
is 
the 
projection 
of 
a 
onto 
the 
normal 
direction). 


apnu
Figure 
A.2: 
A 
line 
can 
be 
specied 
by 
some 
position 
vector 
on 
the 
line, 
a, 
and 
a 
unit 
vector 
along 
the 
direction 
of 
the 
line, 
u. 
In 
2 
dimensions, 
there 
is 
a 
unique 
direction, 
n, 
perpendicular 
to 
the 
line. 
In 
three 
dimensions, 
the 
vectors 
perpendicular 
to 
the 
direction 
of 
the 
line 
lie 
in 
a 
plane, 
whose 
normal 
vector 
is 
in 
the 
direction 
of 
the 
line, 
u. 


DRAFT 
March 
9, 
2010 



Linear 
Algebra 


anuvp
Figure 
A.3: 
A 
plane 
can 
be 
specied 
by 
a 
point 
in 
the 
plane, 
a 
and 
two, 
non-parallel 
directions 
in 
the 
plane, 
u 
and 
v. 
The 
normal 
to 
the 
plane 
is 
unique, 
and 
in 
the 
same 
direction 
as 
the 
directed 
line 
from 
the 
origin 
to 
the 
nearest 
point 
on 
the 
plane. 


A.1.4 
Planes 
and 
hyperplanes 
A 
line 
is 
a 
one 
dimensional 
hyperplane. 
To 
dene 
a 
two-dimensional 
plane 
(in 
arbitrary 
dimensional 
space) 
one 
may 
specify 
two 
vectors 
u 
and 
v 
that 
lie 
in 
the 
plane 
(they 
need 
not 
be 
mutually 
orthogonal), 
and 
a 
position 
vector 
a 
in 
the 
plane, 
see 
g(A.3). 
Any 
vector 
p 
in 
the 
plane 
can 
then 
be 
written 
as 


p 
= 
a 
+ 
su 
+ 
tv, 
(s, 
t) 
2R. 
(A.1.12) 


An 
alternative 
denition 
is 
given 
by 
considering 
that 
any 
vector 
within 
the 
plane 
must 
be 
orthogonal 
to 
the 
normal 
of 
the 
plane 
n. 


(p 
- 
a) 
· 
n 
=0 
. 
p 
· 
n 
= 
a 
· 
n 
(A.1.13) 


The 
right 
hand 
side 
of 
the 
above 
represents 
the 
shortest 
distance 
from 
the 
origin 
to 
the 
plane, 
drawn 
by 
the 
dashed 
line 
in 
g(A.3). 
The 
advantage 
of 
this 
representation 
is 
that 
it 
has 
the 
same 
form 
as 
a 
line. 
Indeed, 
this 
representation 
of 
(hyper)planes 
is 
independent 
of 
the 
dimension 
of 
the 
space. 
In 
addition, 
only 
two 
vectors 
need 
to 
be 
dened 
– 
a 
point 
in 
the 
plane, 
a, 
and 
the 
normal 
to 
the 
plane 
n. 


A.1.5 
Matrices 
An 
m 
× 
n 
matrix 
A 
is 
a 
collection 
of 
scalar 
m 
× 
n 
values 
arranged 
in 
a 
rectangle 
of 
m 
rows 
and 
n 
columns. 
A 
vector 
can 
be 
considered 
a 
n 
× 
1 
matrix. 
If 
the 
element 
of 
the 
i-th 
row 
and 
j-th 
column 
is 
Aij, 
then 
AT 
denotes 
the 
matrix 
that 
has 
Aji 
there 
instead 
-the 
transpose 
of 
A. 
For 
example 
A 
and 
its 
transpose 
are 
: 


01i01i

234 
246 
A 
= 
@i459 
AiAT 
= 
@i357 
Ai(A.1.14) 
671 
491 


The 
i, 
j 
element 
of 
matrix 
A 
can 
be 
written 
Aij 
or 
in 
cases 
where 
more 
clarity 
is 
required, 
[A]ij 
(for



A..1

exampleij). 


Denition 
118 
(transpose). 
The 
transpose 
BT 
of 
the 
n 
by 
m 
matrix 
B 
is 
the 
m 
by 
n 
matrix 
D 
with 
components

hi

BT= 
Bjk 
; 
k 
=1;:::;m 
j 
=1;:::;n 
. 
(A.1.15) 


kj 


..T

B,BT= 
B 
and 
(AB)T 
= 
BTAT 
. 
If 
the 
shapes 
of 
the 
matrices 
A,B 
and 
C 
are 
such 
that 
it 
makes 
sense 
to 
calculate 
the 
product 
ABC, 
then 


(ABC)T 
= 
CTBTAT 
(A.1.16) 


A 
square 
matrix 
A 
is 
symmetric 
if 
AT 
= 
A. 
A 
square 
matrix 
is 
called 
Hermitian 
if 


A 
= 
AT* 
(A.1.17) 




where 
denotes 
the 
complex 
conjugate 
operator. 
For 
Hermitian 
matrices, 
the 
eigenvectors 
form 
an 
orthogonal 
set, 
with 
real 
eigenvalues. 


DRAFT 
March 
9, 
2010 
545 



Linear 
Algebra 


Denition 
119 
(Matrix 
addition). 
For 
two 
matrix 
A 
and 
B 
of 
the 
same 
size, 
[A 
+ 
B]ij 
=[A]ij 
+[B]ij 
(A.1.18) 


Denition 
120 
(Matrix 
multiplication). 
For 
an 
l 
by 
n 
matrix 
A 
and 
an 
n 
by 
m 
matrix 
B, 
the 
product 
AB 
is 
the 
l 
by 
m 
matrix 
with 
elements 


n

X

[AB]ij 
=[A]ij 
[B]jk 
; 
i 
=1;:::;l 
k 
=1;:::;m. 
(A.1.19) 
j=1 


For 
example



a11 
a12 
x1 
a11x1 
+ 
a12x2

=(A.1.20)
a21 
a22x2a21x2 
+ 
a22x2

Note 
that 
even 
if 
BA 
is 
dened 
as 
well, 
that 
is 
if 
l 
= 
n, 
generally 
BA 
is 
not 
equal 
to 
AB 
(when 
they 
do 
we 
say 
they 
commute). 
The 
matrix 
I 
is 
the 
identity 
matrix, 
necessarily 
square, 
with 
1's 
on 
the 
diagonal 
and 
0's 
everywhere 
else. 
For 
clarity 
we 
may 
also 
write 
Im 
for 
an 
square 
m 
6m 
identity 
matrix. 
Then 
for 
an 
m 
6n 
matrix 
A 


ImA 
= 
AIn 
= 
A 
(A.1.21) 


The 
identity 
matrix 
has 
elements 
[I]ij 
= 
ij 
given 
by 
the 
Kronecker 
delta: 




1 
i 
= 
j

ij 
(A.1.22)

0 
i=6
j 


Denition 
121 
(Trace). 


XX

trace 
(A)=Aii 
=i 
(A.1.23) 
ii 


where 
i 
are 
the 
eigenvalues 
of 
A. 


A.1.6 
Linear 
transformations 
Rotations 


If 
we 
assume 
that 
rotation 
of 
a 
two-dimensional 
vector 
x 
=(x, 
y)T 
can 
be 
accomplished 
by 
matrix 
multiplication 
Rx 
then, 
since 
matrix 
multiplication 
is 
distributive, 
we 
only 
need 
to 
work 
out 
how 
the 
axes 
unit 
vectors 
i 
= 
(1, 
0)T 
and 
j 
= 
(0, 
1)T 
transform 
since 


Rx 
= 
xRi 
+ 
yRj 
(A.1.24) 


The 
unit 
vectors 
i 
and 
j 
under 
rotation 
by 
. 
degrees 
transform 
to 
vectors 


Ri 
=r11 
r21=cos 
. 
sin 
Rj 
=r12 
r22=..6sin 
. 
cos 
(A.1.25) 
546 
DRAFT 
March 
9, 
2010 



Linear 
Algebra 


From 
this, 
one 
can 
simply 
read 
off 
the 
values 
for 
the 
elements 




cos 
. 
- 
sin 


R 
=(A.1.26)

sin 
. 
cos 


A.1.7 
Determinants 
Denition 
122 
(Determinant). 
For 
a 
square 
matrix 
A, 
the 
determinant 
is 
the 
volume 
of 
the 
transformation 
of 
the 
matrix 
A 
(up 
to 
a 
sign 
change). 
That 
is, 
we 
take 
a 
hypercube 
of 
unit 
volume 
and 
map 
each 
vertex 
under 
the 
transformation, 
and 
the 
volume 
of 
the 
resulting 
object 
is 
dened 
as 
the 
determinant. 
Writing 
[A]ij 
= 
aij, 




a11 
a12

det= 
a11a22 
- 
a21a12 
(A.1.27)
a21 
a22

01

a11 
a12 
a13 
det 
@a21 
a22 
a23A= 
a11 
(a22a33 
- 
a23a32) 
- 
a12 
(a21a33 
- 
a31a23)+ 
a13 
(a21a32 
- 
a31a22) 
(A.1.28) 


a31 
a32 
a33 


The 
determinant 
in 
the 
(3 
× 
3) 
case 
has 
the 
form 




a12 
a23 
a21 
a23 
a21 
a22 


a11det- 
a12det+ 
a13det(A.1.29)
a32 
a33a31 
a33a31 
a32

The 
determinant 
of 
the 
(3 
× 
3) 
matrix 
A 
is 
given 
by 
the 
sum 
of 
terms 
(..1)i+1a1idet 
(Ai) 
where 
Ai 
is 
the 
(2 
× 
2) 
matrix 
formed 
from 
A 
by 
removing 
the 
ith 
row 
and 
column. 
This 
form 
of 
the 
determinant 
generalises 
to 
any 
dimension. 
That 
is, 
we 
can 
dene 
the 
determinant 
recursively 
as 
an 
expansion 
along 
the 
top 
row 
of 
determinants 
of 
reduced 
matrices. 
The 
absolute 
value 
of 
the 
determinant 
is 
the 
volume 
of 
the 
transformation. 




detAT= 
det 
(A) 
(A.1.30) 


For 
square 
matrices 
A 
and 
B 
of 
equal 
dimensions, 


..

A..1

det 
(AB) 
= 
det(A) 
det 
(B) 
, 
det 
(I)=1 
. 
det=1=det 
(A) 
(A.1.31) 


For 
any 
matrix 
A 
which 
collapses 
dimensions, 
then 
the 
volume 
of 
the 
transformation 
is 
zero, 
and 
so 
is 
the 
determinant. 
If 
the 
determinant 
is 
zero, 
the 
matrix 
cannot 
be 
invertible 
since 
given 
any 
vector 
x, 
given 
a 
`projection’ 
y 
= 
Ax, 
we 
cannot 
uniquely 
compute 
which 
vector 
x 
was 
projected 
to 
y– 
there 
will 
in 
general 
be 
an 
innite 
number 
of 
solutions. 


Denition 
123 
(Orthogonal 
matrix). 
A 
square 
matrix 
A 
is 
orthogonal 
if 
AAT 
= 
I 
= 
ATA. 
From 
the 
properties 
of 
the 
determinant, 
we 
see 
therefore 
that 
an 
orthogonal 
matrix 
has 
determinant 
1 
and 
hence 
corresponds 
to 
a 
volume 
preserving 
transformation 
– 
i.e. 
a 
rotation. 


Denition 
124 
(Matrix 
rank). 
For 
an 
m 
× 
n 
matrix 
X 
with 
n 
columns, 
each 
written 
as 
an 
m-vector: 




1

X 
=x;:::, 
xn(A.1.32) 


the 
rank 
of 
X 
is 
the 
maximum 
number 
of 
linearly 
independent 
columns 
(or 
equivalently 
rows). 
A 
n 
× 
n 
square 
matrix 
is 
full 
rank 
if 
the 
rank 
is 
n 
and 
the 
matrix 
is 
non-singular. 
Otherwise 
the 
matrix 
is 
reduced 
rank 
and 
is 
singular. 


DRAFT 
March 
9, 
2010 



Linear 
Algebra 


A.1.8 
Matrix 
inversion 
Denition 
125 
(Matrix 
inversion). 
For 
a 
square 
matrix 
A, 
its 
inverse 
satises 


A..1A 
= 
I 
= 
AA..1 
(A.1.33) 
It 
is 
not 
always 
possible 
to 
nd 
a 
matrix 
A..1 
such 
that 
A..1A 
= 
I. 
In 
that 
case, 
we 
call 
the 
matrix 
A 
singular. 
Geometrically, 
singular 
matrices 
correspond 
to 
`projections': 
if 
we 
were 
to 
take 
the 
transform 
of 
each 
of 
the 
vertices 
v 
of 
a 
binary 
hypercube 
Av, 
the 
volume 
of 
the 
transformed 
hypercube 
would 
be 
zero. 
If 
you 
are 
given 
a 
vector 
y 
and 
a 
singular 
transformation, 
A, 
one 
cannot 
uniquely 
identify 
a 
vector 
x 
for 
which 
y 
= 
Ax 
-typically 
there 
will 
be 
a 
whole 
space 
of 
possibilities. 
Provided 
the 
inverses 
matrices 
exist 


(AB)..1 
= 
B..1A..1 
(A.1.34) 
For 
a 
non-square 
matrix 
A 
such 
that 
AAT 
is 
invertible, 
then 
the 
pseudo 
inverse, 
dened 
as 


..1 


A† 
= 
AT 
AAT 
(A.1.35) 


satises 
AA† 
= 
I. 


A.1.9 
Computing 
the 
matrix 
inverse 
For 
a 
2 
× 
2 
matrix, 
it 
is 
straightforward 
to 
work 
out 
for 
a 
general 
matrix, 
the 
explicit 
form 
of 
the 
inverse. 




If 
the 
matrix 
whose 
inverse 
we 
wish 
to 
nd 
is 
A 
=ab 
, 
then 
the 
condition 
for 
the 
inverse 
is

cd



ab 
ef 
10 


=(A.1.36)

cdgh01

Multiplying 
out 
the 
left 
hand 
side, 
we 
obtain 
the 
four 
conditions




ae 
+ 
bg 
af 
+ 
bh 
10 


=(A.1.37)

ce 
+ 
dg 
cf 
+ 
dh01

It 
is 
readily 
veried 
that 
the 
solution 
to 
this 
set 
of 
four 
linear 
equations 
is 
given 
by




ef 
1 
d 
..c 


== 
A..1 
(A.1.38)

ghad 
- 
bc..ba

The 
quantity 
ad 
- 
bc 
is 
the 
determinant 
of 
A. 
There 
are 
many 
ways 
to 
compute 
the 
inverse 
of 
a 
general 
matrix, 
and 
we 
refer 
the 
reader 
to 
more 
specialised 
texts. 


Note 
that, 
if 
one 
wants 
to 
solve 
only 
a 
linear 
system, 
although 
the 
solution 
can 
be 
obtained 
through 
matrix 
inversion, 
this 
should 
not 
be 
use. 
Often, 
one 
needs 
to 
solve 
huge 
dimensional 
linear 
systems 
of 
equations, 
and 
speed 
becomes 
an 
issue. 
These 
equations 
can 
be 
solved 
much 
more 
accurately 
and 
quickly 
using 
elimination 
techniques 
such 
as 
Gaussian 
Elimination. 


A.1.10 
Eigenvalues 
and 
eigenvectors 
The 
eigenvectors 
of 
a 
matrix 
correspond 
to 
the 
natural 
coordinate 
system, 
in 
which 
the 
geometric 
transformation 
represented 
by 
A 
can 
be 
most 
easily 
understood. 


DRAFT 
March 
9, 
2010 



Linear 
Algebra 


Denition 
126 
(Eigenvalues 
and 
Eigenvectors). 
For 
a 
square 
matrix 
A, 
e 
is 
an 
eigenvector 
of 
A 
with 
eigenvalue 
. 
if 


Ae 
= 
e 
(A.1.39) 


n

YX

det 
(A)= 
i 
(A.1.40) 
i=1 


Hence 
a 
matrix 
is 
singular 
if 
it 
has 
a 
zero 
eigenvalue. 
The 
trace 
of 
a 
matrix 
can 
be 
expressed 
as 


X

trace 
(A)=i 
(A.1.41) 
i 


For 
an 
(n 
6n) 
dimensional 
matrix, 
there 
are 
(including 
repetitions) 
n 
eigenvalues, 
each 
with 
a 
corresponding 
eigenvector. 
We 
can 
reform 
equation 
(A.1.39) 
as 


(A 
..6I) 
e 
= 
0 
(A.1.42) 


This 
is 
a 
linear 
equation, 
for 
which 
the 
eigenvector 
e 
and 
eigenvalue 
. 
is 
a 
solution. 
We 
can 
write 
equation 


B..10 
trivially 
satises 
the 
eigen-equation. 
For 
any 
non-trivial 
solution 
to 
the 
problem 
Be 
= 
0, 
we 
therefore 
need 
B 
to 
be 
non-invertible. 
This 
is 
equivalent 
to 
the 
condition 
that 
B 
has 
zero 
determinant. 
Hence 
. 
is 
an 
eigenvalue 
of 
A 
if 


(A.1.42) 
as 
Be 
= 
0, 
where 
B 
6A 
..6I. 
If 
B 
has 
an 
inverse, 
then 
a 
solution 
is 
e 
== 
0, 
which 
det 
(A 
..6I) 
= 
0 
(A.1.43) 


This 
is 
known 
as 
the 
characteristic 
equation. 
This 
determinant 
equation 
will 
be 
a 
polynomial 
of 
degree 
n 
and 
the 
resulting 
equation 
is 
known 
as 
the 
characteristic 
polynomial. 
Once 
we 
have 
found 
an 
eigenvalue, 
the 
corresponding 
eigenvector 
can 
be 
found 
by 
substituting 
this 
value 
for 
. 
in 
equation 
(A.1.39) 
and 
solving 
the 
linear 
equations 
for 
e. 
It 
may 
be 
that 
the 
for 
an 
eigenvalue 
. 
the 
eigenvector 
is 
not 
unique 
and 
there 
is 
a 
space 
of 
corresponding 
vectors. 
Geometrically, 
the 
eigenvectors 
are 
special 
directions 
such 
that 
the 
eect 
of 
the 
transformation 
A 
along 
a 
direction 
e 
is 
simply 
to 
scale 
the 
vector 
e. 
For 
a 
rotation 
matric 
R 
in 
general 
there 
will 
be 
no 
direction 
preserved 
under 
the 
rotation 
so 
that 
the 
eigenvalues 
and 
eigenvectors 
are 
complex 
valued 
(which 
is 
why 
the 
Fourier 
representation, 
which 
corresponds 
to 
representation 
in 
a 
rotated 
basis, 
is 
necessarily 
complex). 


Remark 
11 
(Orthogonality 
of 
eigenvectors 
of 
symmetric 
matrices). 
For 
a 
real 
symmetric 
matric 
A 
= 
AT, 
and 
two 
of 
its 
eigenvectors 
ei 
and 
ej 
of 
A 
are 
orthogonal 
(ei)Tej 
= 
0 
if 
the 
eigenvalues 
i 
and 
j 
are 
dierent. 


The 
above 
can 
be 
shown 
by 
considering: 


i

Aei 
= 
iei 
)6(ej)TAei 
= 
i(ej)Te(A.1.44) 
Since 
A 
is 
symmetric, 
the 
left 
hand 
side 
is 
equivalent 
to 


i 
ii

((ej)TA)ei 
=(Aej)Te= 
j(ej)Tei 
)6i(ej)Te= 
j(ej)Te(A.1.45) 
If 
i6

= 
j, 
this 
condition 
can 
be 
satised 
only 
if 
(ej)Tei 
= 
0, 
namely 
that 
the 
eigenvectors 
are 
orthogonal. 


DRAFT 
March 
9, 
2010 



Linear 
Algebra 


A.1.11 
Matrix 
decompositions 
The 
observation 
that 
the 
eigenvectors 
of 
a 
symmetric 
matrix 
are 
orthogonal 
leads 
directly 
to 
the 
spectral 
decomposition 
formula 
below. 


Denition 
127 
(Spectral 
decomposition). 
A 
symmetric 
matrix 
A 
has 
an 
eigen-decomposition 


n

X

T

A 
= 
ieie(A.1.46)

i 
i=1 


where 
i 
is 
the 
eigenvalue 
of 
eigenvector 
ei 
and 
the 
eigenvectors 
form 
an 
orthogonal 
set,

..T 
..T

ij 
ii

ee= 
ijee(A.1.47) 
In 
matrix 
notation 


A 
= 
EET 
(A.1.48) 
where 
E 
is 
the 
matrix 
of 
eigenvectors 
and 
. 
the 
corresponding 
diagonal 
eigenvalue 
matrix. 
More 
generally, 
for 
a 
square 
non-symmetric 
non-singular 
A 
we 
can 
write 


A 
= 
EE..1 
(A.1.49) 


Denition 
128 
(Singular 
Value 
Decomposition). 
The 
SVD 
decomposition 
of 
a 
n 
6p 
matrix 
X 
is 


X 
= 
USVT 
(A.1.50) 


where 
dim 
U 
= 
nn 
with 
UTU 
= 
In. 
Also 
dim 
V 
= 
pp 
with 
VTV 
= 
Ip. 
The 
matrix 
S 
has 
dim 
S 
= 
np 
with 
zeros 
everywhere 
except 
on 
the 
diagonal 
entries. 
The 
`singular 
values’ 
are 
the 
diagonal 
entries 
[S]ii 
and 
are 
positive. 
The 
singular 
values 
are 
ordered 
so 
that 
the 
upper 
left 
diagonal 
element 
of 
S 
contains 
the 
largest 
singular 
value. 


Quadratic 
forms 


Denition 
129 
(Quadratic 
form). 
xTAx 
+ 
xTb 
(A.1.51) 


Denition 
130 
(Positive 
denite 
matrix). 
A 
symmetric 
matrix 
A, 
with 
the 
property 
that 
xTAx 
60 
for 
any 
vector 
x 
is 
called 
nonnegative 
denite. 
A 
symmetric 
matrix 
A, 
with 
the 
property 
that 
xTAx 
> 
0 
for 
any 
vector 
x6positive 
denite.

= 
0 
is 
called 
A 
positive 
denite 
matrix 
has 
full 
rank 
and 
is 
thus 
invertible. 
Using 
the 
eigen-decomposition 
of 
A, 


XX2 


TTi

i)T

xTAx 
=iyei(ex 
=ixe(A.1.52) 


ii 


which 
is 
greater 
than 
zero 
if 
and 
only 
if 
all 
the 
eigenvalues 
are 
positive. 
Hence 
A 
is 
positive 
denite 
if 
and 
only 
if 
all 
its 
eigenvalues 
are 
positive. 


DRAFT 
March 
9, 
2010 



Multivariate 
Calculus 


A.2 
Matrix 
Identities 
Denition 
131 
(Trace-Log 
formula). 
For 
a 
positive 
denite 
matrix 
A, 


trace 
(log 
A) 
= 
log 
det 
(A) 
(A.2.1) 


Note 
that 
the 
above 
logarithm 
of 
a 
matrix 
is 
not 
the 
element-wise 
logarithm. 
In 
MATLAB 
the 
required 
function 
is 
logm. 
In 
general 
for 
an 
analytic 
function 
f(x), 
f(M) 
is 
dened 
via 
the 
power-series 
expansion 
of 
the 
function. 
On 
the 
right, 
since 
det 
(A) 
is 
a 
scalar, 
the 
logarithm 
is 
the 
standard 
logarithm 
of 
a 
scalar. 


Denition 
132 
(Matrix 
Inversion 
Lemma 
(Woodbury 
formula)). 
Provided 
the 
appropriate 
inverses 
exist: 


..1 
..1 
A 
+ 
UVT 
= 
A..1 
- 
A..1UI 
+ 
VTA..1UVTA..1 
(A.2.2) 


Eigenfunctions

Z

0

K(x;x)a(x)= 
aa(x 
0) 
(A.2.3) 
x 


By 
an 
analogous 
argument 
that 
proves 
the 
theorem 
of 
linear 
algebra 
above, 
the 
eigenfunctions 
are 
orthog


0

onal 
of 
a 
real 
symmetric 
kernel, 
K(x;x)= 
K(x, 
x0) 
are 
orthogonal:

Z

a(x)f 
(x)= 
ab 
(A.2.4)

b 
x 


where 
(x) 
is 
the 
complex 
conjugate 
of 
(x)1 
. 
From 
the 
previous 
results, 
we 
know 
that 
a 
symmetric 
real 
matrix 
K 
must 
have 
a 
decomposition 
in 
terms 
eigenvectors 
with 
positive, 
real 
eigenvalues. 
Since 
this 
is 
to 
be 
true 
for 
any 
dimension 
of 
matrix, 
it 
suggests 
that 
we 
need 
the 
(real 
symmetric) 
kernel 
function 
itself 
to 
have 
a 
decomposition 
(provided 
the 
eigenvalues 
are 
countable) 


X

K(x 
i 
, 
xj) 
=(x 
i)f 
* 
(xj) 
(A.2.5) 
µ 
since 
then
XXXXXyiK(x 
i 
, 
xj)yj 
=yi(x 
i)f 
* 
(xj)yj 
=µ 
(yi(x 
i)) 
(yif 
* 
(x 
i)) 
(A.2.6) 
i;j 
i;j;µ 
µ 
i|X{}
Xi|X{}
X* 
zi 
zi 
which 
is 
greater 
than 
zero 
if 
the 
eigenvalues 
are 
all 
positive 
(since 
for 
complex 
z, 
zz 
* 
= 
0). 
If 
the 


eigenvalues 
are 
uncountable 
(which 
happens 
when 
the 
domain 
of 
the 
kernel 
is 
unbounded), 
the 
appropriate 
decomposition 
is 


Z

i 
ij

K(x 
;xj)=(s)(x 
;s)f 
(x;s)ds 
(A.2.7) 


A.3 
Multivariate 
Calculus 
1This 
denition 
of 
the 
inner 
product 
is 
useful, 
and 
particularly 
natural 
in 
the 
context 
of 
translation 
invariant 
kernels. 
We 
are 
free 
to 
dene 
the 
inner 
product, 
but 
this 
conjugate 
form 
is 
often 
the 
most 
useful. 


DRAFT 
March 
9, 
2010 
551 



Multivariate 
Calculus 


Figure 
A.4: 
Interpreting 
the 
gradient. 
The 
ellipses 
are 
contours 
of 
constant 
function 
value, 
f 
= 
const. 
At 
any 
point 
x, 
the 
gradient 
vector 
rf(x) 
points 
along 
the 
direction 
of 
maximal 
increase 
of 
the 
function. 


x1x2f(x)
Denition 
133 
(Partial 
derivative). 
Consider 
a 
function 
of 
n 
variables, 
f(x1;x2;:::;xn) 
or 
f(x). 
The 
partial 
derivative 
of 
f 
w.r.t. 
x1 
at 
x 
* 
is 
dened 
as 
the 
following 
limit 
(when 
it 
exists) 


@f 


@x1

 


= 


lim 




f(x1 
+ 
h, 
x* 
2;:::;x 
) 
- 
f(x 
)

n

* 
h!0 
h 
x=x 


(A.3.1) 
The 
gradient 
vector 
of 
f 
will 
be 
denoted 
by 
rf 
or 
g 


.


.


@f 


@x1 


. 


. 


. 


@f 
@xn 


B. 


C.


rf(x) 
= 
g(x) 
= 


(A.3.2) 
A.3.1 
Interpreting 
the 
gradient 
vector 
Consider 
a 
function 
f(x) 
that 
depends 
on 
a 
vector 
x. 
We 
are 
interested 
in 
how 
the 
function 
changes 
when 
the 
vector 
x 
changes 
by 
a 
small 
amount 
: 
x 
. 
x 
+ 
, 
where 
d 
is 
a 
vector 
whose 
length 
is 
very 
small. 
According 
to 
a 
Taylor 
expansion, 
the 
function 
f 
will 
change 
to 


 


@f 


 


 


2

f 
(x 
+ 
)= 
f(x)+


(A.3.3)
i 
+ 
O

@xi

i 


We 
can 
interpret 
the 
summation 
above 
as 
the 
scalar 
product 
between 
the 
vector 
rf 
with 
components 


@f 


[rf]i 
= 
@xi 
and 
. 


 


 


f 
(x 
+ 
)= 
f(x)+(rf)Td 
+ 
O

2

(A.3.4) 
The 
gradient 
points 
along 
the 
direction 
in 
which 
the 
function 
increases 
most 
rapidly. 
Why? 
Consider 
a 
direction 
p^(a 
unit 
length 
vector). 
Then 
a 
displacement, 
d 
units 
along 
this 
direction 
changes 
the 
function 
value 
to 


f(x 
+ 
p^) 
˜ 
f(x)+ 
rf(x) 
· 
p^(A.3.5) 
The 
direction 
p^for 
which 
the 
function 
has 
the 
largest 
change 
is 
that 
which 
maximises 
the 
overlap 


rf(x) 
· 
p^= 
jrf(x)jjp^| 
cos 
. 
= 
jrf(x)| 
cos 
. 
(A.3.6) 
where 
. 
is 
the 
angle 
between 
p^and 
rf(x). 
The 
overlap 
is 
maximised 
when 
. 
= 
0, 
giving 
p^= 
rf(x)=jrf(x)j. 
Hence, 
the 
direction 
along 
which 
the 
function 
changes 
the 
most 
rapidly 
is 
along 
rf(x). 


A.3.2 
Higher 
derivatives 
The 
`rst 
derivative’ 
of 
a 
function 
of 
n 
variables 
is 
an 
n-vector; 
the 
`second-derivative’ 
of 
an 
n-variable 
function 
is 
dened 
by 
the 
n2 
partial 
derivatives 
of 
the 
n 
rst 
partial 
derivatives 
w.r.t. 
the 
n 
variables 


. 
@xi 
@f 
@xj 
i 
= 
1, 
. 
. 
. 
, 
n; 
j 
= 
1, 
. 
. 
. 
, 
n 
(A.3.7) 
552 
DRAFT 
March 
9, 
2010 



Multivariate 
Calculus 


which 
is 
usually 
written 


@2f@2f 


;i6
= 
j 
;i 
= 
j 
(A.3.8)

2

@xi@xj 
@xi 
If 
the 
partial 
derivatives 
@f=@xi, 
@f=@xj 
and 
@2f=@xi@xj 
are 
continuous, 
then 
@2f=@xi@xj 
exists 
and 
@2f=@xi@xj 
= 
@2f=@xj@xi 
. 
(A.3.9) 


2

These 
nsecond 
partial 
derivatives 
are 
represented 
by 
a 
square, 
symmetric 
matrix 
called 
the 
Hessian 
matrix 
of 
f(x). 


.


. 


@2f 
@2f 
@x1
2 
. 
. 
. 
@x1@xn 
. 
. 
. 
. 
. 
. 
@2f 
@2f 
@x1@xn 
. 
. 
. 
@xn 
2 


BB
. 


CC
.


Hf 
(x)= 


(A.3.10) 
A.3.3 
Chain 
rule 
Consider 
f(x1;:::;xn). 
Now 
let 
each 
xj 
be 
parameterized 
by 
u1;:::;um, 
i.e. 
xj 
= 
xj(u1;:::;um). 
What 
is 
@f=@u? 


X

n

@xj

j=1 


@f 


= 
f(x1 
+ 
x1;:::;xn 
+ 
xn) 
..6f(x1;:::;xn)= 


xj 
+ 
higher 
order 
terms 


f 


m

@u

=1 


X

@xj 


ua 
+ 
higher 
order 
terms 


xj 
= 


So 


nm

@xj 
@u

j=1 
=1 


X

Therefore 


X

@f 
@xj 


ua 
+ 
higher 
order 
terms 
(A.3.11)

f 
= 


Denition 
134 
(Chain 
rule). 


X

n

@xj 
@u

j=1 


@f 


= 


@ua 


@f 


@xj 


(A.3.12) 
or 
in 
vector 
notation 
@@x(u)

f(x(u)) 
= 
rfT(x(u))(A.3.13)

@ua 
@ua 


Denition 
135 
(Directional 
derivative). 
Assume 
f 
is 
dierentiable. 
We 
dene 
the 
scalar 
directional 
derivative 
(Dvf)(x 
) 
of 
f 
in 
a 
direction 
v 
at 
a 
point 
x 
* 
. 
Let 
x 
= 
x 
* 
+ 
hv, 
Then 


(Dvf)(x 


)= 


d 


dh 


f(x 


* 
+ 
hv)

 


= 


m 


vj 


@f 


@xj

 


= 
rfT

v 


(A.3.14) 
h=0 


j 


* 


x=x 


A.3.4 
Matrix 
calculus 
DRAFT 
March 
9, 
2010 
553 



Inequalities 


Denition 
136 
(Derivative 
of 
a 
matrix 
trace). 
For 
matrices 
A, 
and 
B 


. 
trace 
(AB) 
hBT 
(A.3.15)

@A

Denition 
137 
(Derivative 
of 
log 
det 
(A)). 


..

. 
log 
det 
(A)= 
@trace 
(log 
A) 
= 
traceA..1@A(A.3.16) 
So 
that 


. 
log 
det 
(A)= 
A..T 
(A.3.17)

@A 


Denition 
138 
(Derivative 
of 
a 
matrix 
inverse). 
For 
an 
invertible 
matrix 
A, 
@A..1 
..A..T@AA..1 
(A.3.18) 


A.4 
Inequalities 
A.4.1 
Convexity 
Denition 
139 
(Convex 
function). 
A 
function 
f(x) 
is 
dened 
as 
convex 
if 
for 
any 
x, 
y 
and 
0 
h. 
h1 


f(x 
+ 
(1 
..h)y) 
hf(x) 
+ 
(1 
..h)f(y) 
(A.4.1) 
If 
..f(x) 
is 
convex, 
f(x) 
is 
called 
concave. 


An 
intuitive 
picture 
of 
a 
convex 
function 
is 
to 
consider 
rst 
the 
quantity 
x 
+ 
(1 
..h)y. 
As 
we 
vary 
. 
from 
0 
to 
1, 
this 
traces 
points 
between 
x 
(. 
= 
0) 
and 
y 
(. 
= 
1). 
Hence 
for 
. 
= 
0 
we 
start 
at 
the 
point 
x, 
f(x) 
and 
as 
. 
increase 
trace 
a 
straight 
line 
towards 
the 
point 
y, 
f(y) 
at 
. 
= 
1. 
Convexity 
states 
that 
the 
function 
f 
always 
lies 
below 
this 
straight 
line. 
Geometrically 
this 
means 
that 
the 
function 
f(x) 
is 
always 
always 
increasing 
(never 
non-decreasing). 
Hence 
if 
d2f(x)=dx2 
> 
0 
the 
function 
is 
convex. 


As 
an 
example, 
the 
function 
log 
x 
is 
concave 
since 
its 
second 
derivative 
is 
negative: 
d 
1 
d2 
1

log 
x 
= 
, 
log 
x 
= 
..h(A.4.2)

dx 
xdx2 
x2 


A.4.2 
Jensen's 
inequality 
For 
a 
convex 
function 
f(x), 
it 
follows 
directly 
from 
the 
denition 
of 
convexity 
that 
f(hxi) 
hf(x)i(A.4.3)

p(x)p(x) 


for 
any 
distribution 
p(x). 


554 
DRAFT 
March 
9, 
2010 



Gradient 
Descent 


A.5 
Optimisation 
A.5.1 
Critical 
points 
When 
all 
rst-order 
partial 
derivatives 
at 
a 
point 
are 
zero 
(i.e. 
rf 
= 
0) 
then 
the 
point 
is 
said 
to 
be 
a 
stationary 
or 
critical 
point. 
Can 
be 
a 
minimum, 
maximum 
or 
saddle 
point. 


Necessary 
rst-order 
condition 
for 
a 
minimum 
There 
is 
a 
minimum 
of 
f 
at 
x 
* 
if 
f(x 
) 
= 
f(x) 
for 
all 
x 
suciently 
close 
to 
x 
* 
. 
Let 
x 
= 
x 
* 
+ 
hv 
for 
small 
h 
and 
some 
direction 
v. 
Then 
by 
a 
Taylor 
expansion, 
for 
small 
h, 


f(x 
* 
+ 
hv)= 
f(x 
)+ 
hrfTv 
+ 
O(h2) 
(A.5.1) 
and 
thus 
for 
a 
minimum 


hrfTv 
+ 
O(h2) 
= 
0 
(A.5.2) 
Choosing 
v 
to 
be 
..rf 
the 
condition 
becomes 


..hrfTrf 
+ 
O(h2) 
= 
0 
(A.5.3) 
and 
is 
violated 
for 
small 
positive 
h 
unless 
jrfj2 
= 
rfTrf 
= 
0. 
So 
x 
* 
can 
only 
be 
a 
local 
minimum 
if 
jrf(x 
)| 
= 
0, 
i.e. 
if 
rf(x 
)= 
0. 


Necessary 
second-order 
condition 
for 
a 
minimum 


At 
a 
stationary 
point 
rf 
= 
0. 
Hence 
the 
Taylor 
expansion 
is 
given 
by 


f(x 
* 
+ 
hv)= 
f(x 
)+ 
h2vTHf 
v 
+ 
O(h3) 
(A.5.4) 
Thus 
the 
minimum 
condition 
requires 
that 
vTHf 
v 
= 
0, 
i.e. 
the 
Hessian 
is 
non-negative 
denite. 


Denition 
140 
(Conditions 
for 
a 
minimum). 
Sucient 
conditions 
for 
a 
minimum 
at 
x 
* 
are 
(i) 
rf(x 
)=0 
and 
(ii) 
Hf 
(x 
) 
is 
positive 
denite. 


For 
a 
quadratic 
function 
f(x)= 
1 
xTAx..bTx+ 
c, 
with 
symmetric 
A 
the 
necessary 
condition 
rf(x 
)= 
0

2 


reads: 


Ax* 
- 
b 
= 
0 
(A.5.5) 
If 
A 
is 
invertible 
this 
equation 
has 
the 
unique 
solution 
x 
* 
= 
A..1b. 
If 
A 
is 
positive 
denite, 
x 
* 
is 
a 
minimum. 


A.6 
Gradient 
Descent 
Almost 
all 
of 
the 
search 
techniques 
that 
we 
consider 
are 
iterative, 
i.e. 
we 
proceed 
towards 
the 
minimum 
x 
* 
by 
a 
sequence 
of 
steps. 
On 
the 
kth 
step 
we 
take 
a 
step 
of 
length 
k 
in 
the 
direction 
pk, 


xk+1 
= 
xk 
+ 
kpk 
(A.6.1) 


The 
length 
of 
the 
step 
can 
either 
be 
chosen 
using 
prior 
knowledge, 
or 
by 
carrying 
out 
a 
line 
search 
in 
the 
direction 
pk. 
It 
is 
the 
way 
that 
pk 
is 
chosen 
that 
tends 
to 
distinguish 
the 
dierent 
methods 
of 
multivariate 
optimization 
that 
we 
will 
discuss. 


DRAFT 
March 
9, 
2010 
555 



Gradient 
Descent 


We 
shall 
assume 
that 
we 
can 
analytically 
evaluate 
the 
gradient 
of 
f 
and 
will 
often 
use 
the 
shorthand 
notation 


gk 
=rf(xk) 
. 
(A.6.2) 
Typically 
we 
will 
want 
to 
choose 
pk 
using 
only 
gradient 
information; 
for 
large 
problems 
it 
can 
be 
very 
expensive 
to 
compute 
the 
Hessian, 
and 
this 
can 
also 
require 
a 
large 
amount 
of 
storage. 
Consider 
the 
change 
of 
variables 
x 
= 
My. 
Then 


g(y)= 
f(x)= 
f(My) 
(A.6.3) 
and 


X

@g 
@f 
@xi 


=(A.6.4)

@yi 
@xi 
@yj

j 


so 
that

ryg 
= 
Mrxf 
(A.6.5) 


Then 
the 
change 
in 
g 
is 
dierent 
from 
the 
change 
in 
f, 
even 
though 
the 
only 
dierence 
between 
the 
two 
functions 
is 
the 
coordinate 
system. 
This 
unfortunate 
sensitivity 
to 
the 
parameterisation 
of 
the 
function 
is 
partially 
addressed 
in 
rst 
order 
methods 
such 
as 
gradient 
descent 
by 
the 
natural 
gradient 
which 
uses 
a 
prefactor 
designed 
to 
compensate 
for 
some 
of 
the 
lost 
invariance. 
We 
refer 
the 
reader 
to 
[7] 
for 
a 
description 
of 
this 
method. 


A.6.1 
Gradient 
descent 
with 
xed 
stepsize 
Locally, 
if 
we 
are 
at 
point 
xk, 
we 
can 
decrease 
f(x) 
by 
taking 
a 
step 
in 
the 
direction 
..g(x). 
If 
we 
make 
the 
update 
equation 


xk+1 
= 
xk 
..rgk 
(A.6.6) 


then 
we 
are 
doing 
gradient 
descent 
with 
xed 
stepsize 
. 
If 
. 
is 
non-innitesimal, 
it 
is 
always 
possible 
that 
we 
will 
step 
over 
the 
true 
minimum. 
Making 
. 
very 
small 
guards 
against 
this, 
but 
means 
that 
the 
optimization 
process 
will 
take 
a 
very 
long 
time 
to 
reach 
a 
minimum. 


To 
see 
why 
gradient 
descent 
works, 
consider 
the 
general 
update 


xk+1 
= 
xk 
+ 
pk 
(A.6.7) 
For 
small 
a 
we 
can 
expand 
f 
around 
xk 
using 
Taylor's 
theorem: 


T

f(xk 
+ 
kpk) 
rf(xk)+ 
kgk 
pk 
. 
(A.6.8) 
With 
pk 
= 
..gk 
and 
for 
small 
positive 
k, 
we 
see 
a 
guaranteed 
reduction: 


f(xk 
+ 
kpk) 
rf(xk) 
..rkjjgkjj2 
. 
(A.6.9) 


A.6.2 
Gradient 
descent 
with 
momentum 
A 
simple 
idea 
that 
can 
improve 
convergence 
of 
gradient 
descent 
is 
to 
include 
at 
each 
iteration 
a 
proportion 
of 
the 
change 
from 
the 
previous 
iteration. 
one 
uses 


xk+1 
= 
... 
@E 
@x 
+ 
xk 
(A.6.10) 
where 
a 
is 
the 
momentum 
coecient. 
556 
DRAFT 
March 
9, 
2010 



Multivariate 
Minimization: 
Quadratic 
functions 


Figure 
A.5: 
Optimisation 
using 
line 
search 
along 
steepest 
descent 
directions. 
Rushing 
off 
following 
the 
steepest 
way 
downhill 
from 
a 
point 
(and 
continuing 
for 
a 
nite 
time 
in 
that 
direction) 
doesn't 
always 
result 
in 
the 
fastest 
way 
to 
get 
to 
the 
bottom! 



A.6.3 
Gradient 
descent 
with 
line 
searches 
An 
extension 
to 
the 
idea 
of 
gradient 
descent 
is 
to 
choose 
the 
direction 
of 
steepest 
descent, 
as 
indicated 
by 
the 
gradient 
g, 
but 
to 
calculate 
the 
value 
of 
the 
step 
to 
take 
which 
most 
reduces 
the 
value 
of 
E 
when 
moving 
in 
that 
direction. 
This 
involves 
solving 
the 
one-dimensional 
problem 
of 
minimizing 
E(xk 
- 
gk) 
with 
respect 
to 
, 
and 
is 
known 
as 
a 
line 
search. 
That 
step 
is 
then 
taken 
and 
the 
process 
repeated 
again. 


Finding 
the 
size 
of 
the 
step 
takes 
a 
little 
work; 
for 
example, 
you 
might 
nd 
three 
points 
along 
the 
line 
such 
that 
the 
error 
at 
the 
intermediate 
point 
is 
less 
than 
at 
the 
other 
two, 
so 
that 
there 
is 
some 
minimum 
along 
the 
line 
lies 
between 
the 
rst 
and 
second 
or 
between 
the 
second 
and 
third, 
and 
some 
kind 
of 
interval-
halving 
approach 
can 
then 
be 
used 
to 
nd 
it. 
(The 
minimum 
found 
in 
this 
way, 
just 
as 
with 
any 
sort 
of 
gradient-descent 
algorithm, 
may 
not 
be 
a 
global 
minimum 
of 
course.) 
There 
are 
several 
variants 
of 
this 
theme. 
Notice 
that 
if 
the 
step 
size 
is 
chosen 
to 
reduce 
E 
as 
much 
as 
it 
can 
in 
that 
direction, 
then 
no 
further 
improvement 
in 
E 
can 
be 
made 
by 
moving 
in 
that 
direction 
for 
the 
moment. 
Thus 
the 
next 
step 
will 
have 
no 
component 
in 
that 
direction; 
that 
is, 
the 
next 
step 
will 
be 
at 
right 
angles 
to 
the 
one 
just 
taken. 
This 
can 
lead 
to 
zig-zag 
type 
behaviour 
in 
the 
optimisation, 
see 
g(A.5). 


A.6.4 
Exact 
line 
search 
condition 
At 
the 
k-th 
step, 
we 
chose 
k 
to 
minimize 
f(xk 
+ 
kpk). 
So 
setting 
F 
()= 
f(xk 
+ 
pk), 
at 
this 
step 
we 
solve 
the 
one-dimensional 
minimization 
problem 
for 
F 
(). 
Thus 
our 
choice 
of 
k 
= 
* 
will 
satisfy 
F 
0(k) 
= 
0. 
Now 


F 
0(k) 
dd 


==

F 
(k 
+ 
h)jh=0 
f(xk 
+ 
kpk 
+ 
hpk)jh=0

dh 
dh 


d 


= 
f(xk+1 
+ 
hpk)jh=0 
=(Dpk 
f)(xk+1)= 
rfT(xk+1)pk 
(A.6.11)

dh 


So 
F 
0(k) 
= 
0 
means 
the 
directional 
derivative 
in 
the 
search 
direction 
must 
vanish 
at 
the 
new 
point 
and 
this 
gives 
the 
Exact 
Line 
Search 
Condition: 


0= 
g. 
(A.6.12)

k
T 
+1pk 


For 
a 
quadratic 
function 
f(x)= 
1 
xTAx..bTx+c, 
with 
symmetric 
, 
we 
can 
use 
the 
condition 
to 
analytically 


2 


calculate 
k. 
Since 
rf(xk+1)= 
Axk 
+ 
kApk 
- 
b 
= 
rf(xk)+ 
kApk 
we 
nd 


pT 


k 
gk

k 
= 
- 
. 
(A.6.13) 


pTApk

k 


A.7 
Multivariate 
Minimization: 
Quadratic 
functions 
The 
goal 
of 
this 
section 
is 
to 
derive 
ecient 
algorithms 
for 
minimizing 
multivariate 
quadratic 
functions. 
We 
shall 
begin 
by 
summarizing 
some 
properties 
of 
quadratic 
functions, 
and 
as 
byproduct 
obtain 
an 
ecient 
method 
for 
checking 
whether 
a 
symmetric 
matrix 
is 
positive 
denite. 


A.7.1 
Minimising 
quadratic 
functions 
using 
line 
search 
Consider 
minimising 
the 
quadratic 
function 


1 


f(x)= 
xTAx 
- 
bTx 
+ 
c 
(A.7.1)

2

DRAFT 
March 
9, 
2010 
557 



Multivariate 
Minimization: 
Quadratic 
functions 


where 
A 
is 
positive 
denite 
and 
symmetric. 
(If 
A 
is 
not 
symmetric, 
consider 
instead 
the 
symmetrised 
matrix 
(A 
+ 
AT)=2, 
which 
gives 
the 
same 
function 
f). 
Although 
we 
know 
where 
the 
minimum 
of 
this 
function 
is, 
just 
using 
linear 
algebra, 
we 
wish 
to 
use 
this 
function 
as 
a 
toy 
model 
for 
more 
complex 
functions 
which 
however 
locally 
look 
approximately 
quadratic. 
One 
approach 
is 
to 
search 
along 
a 
particular 
direction 
p, 
and 
nd 
a 
minimum 
along 
this 
direction. 
We 
can 
then 
search 
for 
a 
deeper 
minima 
by 
looking 
in 
dierent 
directions. 
That 
is, 
we 
can 
search 
along 
a 
line 
x0 
+ 
p 
such 
that 
the 
function 
attains 
a 
minimum. 
That 
is, 
the 
directional 
derivative 
is 
zero 
along 
this 
line, 
This 
has 
solution, 


. 
=..b 
..6Ax06p 
pTAp 
6..rf(x0) 
6p 
pTAp 
(A.7.2) 


Now 
we've 
found 
the 
minimum 
along 
the 
line 
through 
x0 
with 
direction 
p. 
But 
how 
should 
we 
choose 
the 


line 
search 
direction 
p? 
It 
would 
seem 
sensible 
to 
choose 
successive 
line 
search 
directions 
p 
according 
to 
pnew 


= 
..rf(x 
), 
so 
that 
each 
time 
we 
minimise 
the 
function 
along 
the 
line 
of 
steepest 
descent. 
However, 
this 
is 
far 
from 
the 
optimal 
choice 
in 
the 
case 
of 
minimising 
quadratic 
functions. 
A 
much 
better 
set 
of 
search 
directions 
are 
those 
dened 
by 
the 
vectors 
conjugate 
to 
A. 


If 
the 
matrix 
A 
were 
diagonal, 
then 
the 
minimisation 
is 
straightforward 
and 
can 
be 
carried 
out 
independently 
for 
each 
dimension. 
If 
we 
could 
nd 
an 
invertible 
matrix 
P 
with 
the 
property 
that 
PTAP 
is 
diagonal 
then 
the 
solution 
is 
easy 
since 
for 


1 


f(x^) 
= 
x^TPTAPx^..6bTPx^+ 
c 
(A.7.3)

2 


with 
x 
= 
Px^, 
we 
can 
compute 
the 
minimum 
for 
each 
dimension 
of 
x^separately 
and 
then 
retransform 
to 
nd 
x 
* 
= 
Px^* 
. 


Denition 
141 
(Conjugate 
vectors). 
The 
vectors 
pi, 
i 
=1;:::;k 
are 
called 
conjugate 
to 
the 
matrix 
A, 
if 
and 
only 
if 
for 
i, 
j 
=1;:::;k 
and 
i6

= 
j: 


TT

pApj 
= 
0 
and 
pApi 
> 
0 
. 
(A.7.4)

ii 


The 
two 
conditions 
guarantee 
that 
conjugate 
vectors 
are 
linearly 
independent: 
Assume 
that 


ki..1k

XXX

0 
= 
jpj 
= 
jpj 
+ 
ipi 
+ 
jpj 
(A.7.5) 
j=1 
j=1 
j=i+1 


Now 
multiplying 
from 
the 
left 
with 
pTA 
yields 
0 
= 
ipTApi. 
So 
i 
is 
zero 
since 
we 
know 
that 
pTApi 
> 
0.

ii 
i 
As 
we 
can 
make 
this 
argument 
for 
any 
i 
=1;:::;k, 
all 
of 
the 
i 
must 
be 
zero. 


A.7.2 
Gram-Schmidt 
construction 
of 
conjugate 
vectors 
Let 
P 
=(p1, 
p2;:::, 
pk), 
where 
the 
columns 
are 
formed 
from 
A-conjugate 
vectors 
and 
note 
that 
we 
start 
with 
an 
n 
by 
k 
matrix, 
k 
6n. 
The 
reason 
for 
this 
is 
that 
we 
are 
aiming 
at 
an 
incremental 
procedure, 
where 
columns 
are 
successively 
added 
to 
P. 
Since 
(PTAP)ij 
= 
pTApj 
the 
matrix 
PTAP 
will 
be 
diagonal 


i 


if 
pTApj 
= 
0 
for 
i6
= 
j. 
Assume 
we 
already 
have 
k 
conjugate 
vectors 
p1;:::, 
pk 
and 
let 
v 
be 
a 
vector 


i 


which 
is 
linearly 
independent 
of 
p1;:::, 
pk. 
We 
then 
set 


k

XpTAv 
pk+1 
= 
v 
..6j 
pj 
(A.7.6) 


j=1 
pT 
j 
Apj 


for 
which 
it 
is 
clear 
that 
the 
vectors 
p1;:::, 
pk+1 
are 
conjugate 
if 
A 
is 
positive 
denite. 
Using 
the 
Gram-
Schmidt 
procedure 
we 
can 
construct 
n 
conjugate 
vectors 
for 
a 
positive 
denite 
matrix 
in 
the 
following 
way. 
We 
start 
with 
n 
linearly 
independent 
vectors 
u1;:::, 
un, 
we 
might 
chose 
ui 
= 
ei, 
the 
unit 
vector 
in 


558 
DRAFT 
March 
9, 
2010 



Multivariate 
Minimization: 
Quadratic 
functions 


the 
ith 
direction. 
We 
then 
set 
p1 
= 
u1 
and 
use 
(A.7.6) 
to 
compute 
p2 
from 
p1 
and 
v 
= 
u2. 
Next 
we 
set 
v 
= 
u3 
and 
compute 
p3 
from 
p1, 
p2 
and 
v. 
Continuing 
in 
this 
manner 
we 
obtain 
n 
conjugate 
vectors. 
Note 
that 
at 
each 
stage 
of 
the 
procedure 
the 
vectors 
u1;:::, 
uk 
span 
the 
same 
subspace 
as 
the 
vectors 
p1;:::, 
pk. 
What 
is 
going 
to 
happen 
if 
A 
is 
not 
positive 
denite? 
If 
we 
could 
nd 
n 
conjugate 
vectors, 
A 
would 
be 
positive 
denite, 
and 
so 
at 
some 
point 
k 
the 
Gram-Schmidt 
procedure 
must 
break 
down. 
This 
will 
happen 
if 
pTApk 
= 
0. 
So 
by 
trying 
out 
the 
Gram-Schmidt 
procedure, 
we 
can 
in 
fact 
nd 
out 
whether 


k 


a 
matrix 
is 
positive 
denite. 


A.7.3 
The 
conjugate 
vectors 
algorithm 
1

Let 
us 
assume 
that 
when 
minimising 
f(x)= 
xTAx 
- 
bTx 
+ 
c 
we 
rst 
construct 
n 
vectors 
p1;:::, 
pn

2 


conjugate 
to 
A 
which 
we 
use 
as 
our 
search 
directions. 
So 


xk+1 
= 
xk 
+ 
kpk 
. 
(A.7.7) 


At 
each 
step 
we 
chose 
k 
by 
an 
exact 
line 
search, 
thus 


pT

gk

k

k 
= 
- 
. 
(A.7.8) 


pTApk

k 


This 
conjugate 
vectors 
algorithm, 
has 
the 
geometrical 
interpretation 
that 
not 
only 
is 
the 
directional 
derivative 
zero 
at 
the 
new 
point 
along 
the 
direction 
pk, 
it 
is 
zero 
along 
all 
the 
previous 
search 
directions 
p1;:::, 
pk). 


Theorem 
1 
(Luenberger 
expanding 
subspace 
theorem). 


Rn

Let 
fpign 
be 
a 
sequence 
of 
vectors 
in 
conjugate 
to 
the 
(positive 
denite) 
matrix 
A 
and

i=1 


f(x)= 
1 
xTAx 
- 
bTx 
+ 
c. 
Then 
for 
any 
x1 
the 
sequence 
fxk} 
generated 
according 
to 
(A.7.7) 
and 
(A.7.8) 


2 


has 
the 
property 
that 
the 
directional 
derivative 
of 
f 
in 
the 
direction 
pi 
vanishes 
at 
the 
point 
xk+1 
if 
i 
= 
k; 


i.e. 
Dpi 
f(xk+1) 
= 
0. 
Proof: 
For 
i 
= 
k, 
we 
can 
write 
xk+1 
as: 


k

k 


xk+1 
= 
xi+1 
+ 
jpj 
. 
(A.7.9) 
j=i+1 


Since 
rf(x)= 
Ax 
- 
b 
we 
have 


kk

Xk 


rf(xk+1)= 
Axk+1 
- 
b 
= 
Axi+1 
- 
b 
+ 
A 
jpj 
= 
rf(xi+1)+ 
jApj 
(A.7.10) 
j=i+1 
j=i+1 


So 


kk

Xk 


TTT 
T

Dpi 
f(xk+1)= 
pi 
rf(xk+1)= 
pi 
rf(xi+1)+ 
jpApj 
=(Dpi 
f)(xi+1)+ 
jpApj 
(A.7.11)

ii 
j=i+1 
j=i+1 


Now 
(Dxi+1 
f)(pi) 
= 
0 
since 
the 
point 
xi+1 
was 
obtained 
by 
an 
exact 
line 
search 
in 
the 
direction 
pi. 
But 
all 
of 
the 
terms 
in 
the 
sum 
over 
j 
also 
vanish 
since 
j>i 
and 
pTApj 
= 
0 
by 
conjugacy. 
So 
(Dpif 
)(xk+1) 
= 
0. 


i 


The 
subspace 
theorem 
shows, 
that 
because 
we 
use 
conjugate 
vectors, 
optimizing 
in 
the 
direction 
pk, 
does 
not 
spoil 
the 
optimality 
w.r.t. 
to 
the 
previous 
search 
directions. 
In 
particular 
after 
having 
carried 
out 
n 
steps 
of 
the 
algorithm 
we 
have 
(Dxn+1 
f)(pi)= 
rfT(xn+1)pi 
= 
0, 
for 
i 
=1;:::;n. 
The 
n 
equations 
can 
be 
written 
in 
a 
more 
compact 
form 
as: 


rfT(xn+1)(p1, 
p2;:::, 
pn)= 
0 
. 
(A.7.12) 


DRAFT 
March 
9, 
2010 
559 



Multivariate 
Minimization: 
Quadratic 
functions 


The 
square 
matrix 
P 
=(p1, 
p2;::. 
pn) 
is 
invertible 
since 
the 
pi 
are 
conjugate, 
so 
rf(xn+1)= 
0: 
The 
point 
xn+1 
is 
the 
minimum 
x 
* 
of 
the 
quadratic 
function 
f. 
So 
in 
contrast 
to 
gradient 
descent, 
for 
a 
quadratic 
function 
the 
conjugate 
vectors 
algorithm 
converges 
in 
a 
nite 
number 
of 
steps. 


A.7.4 
The 
conjugate 
gradients 
algorithm 
The 
conjugate 
gradients 
algorithm 
is 
a 
special 
case 
of 
the 
conjugate 
vectors 
algorithm, 
in 
which 
the 
Gram-
Schmidt 
procedure 
becomes 
very 
simple. 
We 
do 
not 
use 
a 
predetermined 
set 
of 
conjugate 
vectors 
but 
construct 
these 
`on-the-y'. 
After 
k-steps 
of 
the 
conjugate 
vectors 
algorithm 
we 
need 
to 
construct 
a 
vector 
pk+1 
which 
is 
conjugate 
to 
p1;:::, 
pk. 
This 
could 
be 
done 
by 
applying 
the 
Gram-Schmidt 
procedure 
to 
any 
vector 
v 
which 
is 
linearly 
independent 
of 
the 
vectors 
p1;:::, 
pk. 
In 
the 
conjugate 
gradients 
algorithm 
one 
makes 
the 
special 
choice 
v 
= 
..rf(xk+1). 
By 
the 
subspace 
theorem 
the 
gradient 
at 
the 
new 
point 
xk+1 
is 
orthogonal 
to 
pi, 
i 
=1;:::;k. 
So 
rf(xk+1) 
is 
linearly 
independent 
of 
p1;:::, 
pk 
and 
a 
valid 
choice 
for 
v, 
unless 
rf(xk+1)= 
0. 
In 
the 
latter 
case 
xk+1 
is 
our 
minimum 
and 
we 
are 
done, 
and 
from 
now 
on 
we 
assume 
that 
rf(xk+1)6Using 
the 
notation 
gk 


= 
0.= 
rf(xk), 
the 
equation 
for 
the 
new 
search 
direction 
given 
by 
the 
Gram-Schmidt 
procedure 
is: 


pk+1 
= 
..gk+1 
+ 


k

X

i=1 


p

i 


p

T

Agk+1 
Api 


(A.7.13)
pi 
:

T 


i 


T 


T

T 


k+1gk+1. 


gk+1gk+1 


and 
in 
particular 
k+16

= 
0. 
We 
now 
want 
to 
show 
that 
because 
we 
have 
been 
using 
the 
conjugate 
gradients 
algorithm 
at 
the 
previous 
steps 
as 
well, 
in 
equation 
(A.7.13) 
all 
terms 
but 
the 
last 
in 
the 
sum 
over 
i 
vanish. 
We 
shall 
assume 
that 
k> 
0 
since 
in 
the 
rst 
step 
(k 
= 
0) 
we 
just 
set 
p1 
= 
..g1. 
First 
note 
that 


gi+1 
..6gi 
= 
Axi+1 
..6b 
..6(Axi 
..6b)= 
A(xi+1 
..6xi)= 
iApi 
(A.7.15) 


and 
since 
i6

= 
0: 


Api 
=(gi+1 
..6gi)=i. 
(A.7.16) 
So 
in 
equation 
(A.7.13): 


T 


k+1gi+1 
..6gk+1gi)=i 


T

Since 
gk+1 
is 
orthogonal 
to 
pi, 
i 
=1;:::;k, 
by 
the 
subspace 
theorem 
we 
have 
pk+1gk+1 


So


= 
..g


k+1 
can 
be 
written 
as 


(A.7.14)
k+1 
= 


;

Apk+1 


T

p


k+1

Tpi 


T

T

(gi+1 
..6gi)=i 
=(g

(A.7.17)
Agk+1 


Api 


= 
g


= 
g


k+1

k+1

T

T 


Since 
the 
pi 
where 
obtained 
by 
applying 
the 
Gram-Schmidt 
procedure 
to 
the 
gradients 
gi, 
the 
subspace 
k+1pi 
= 
0, 
implies, 
also 
g

theorem 
g


k+1gi 
= 
0 
for 
i 
=1;:::;k. 
This 
shows 
that 


0 
if1 
6i<k 


T 


T 
T 


k+1gi+1 
..6gk+1gi)=i 


Hence 
equation 
(A.7.13) 
simplies 
to 


g

T

Agk+1 
=(g

(A.7.18)
=


p


i 


g


T 


k+1

gk+1=k 
if 
i 
= 
k 


(A.7.19)
pk+1 
= 
..gk+1 
+ 


p


T 


k 


k+1gk+1=k 


pk 
. 


Apk 


This 
can 
be 
brought 
into 
an 
even 
simpler 
form 
by 
applying 
equation 
(A.7.14) 
to 
k: 


T

g

k+1

p

T

Apk 
g

k+1
gk 
g

T

p

k 


g

gk+1 
Apk 


gk+1 
gk 


pk 


(A.7.20)
pk+1 


= 
..gk+1 
+ 


= 
..gk+1 
+

pk

T

T

T 


k 


k 


k 


We 
shall 
write 
this 
in 
the 
form 


T

gk+1gk+1 


DRAFT 
March 
9, 
2010 


where 
k 


(A.7.21)
= 
..gk+1 
+ 
kpk 


=


pk+1 


.


T

g


gk

k 



Multivariate 
Minimization: 
Quadratic 
functions 


Algorithm 
31 
Conjugate 
Gradients 
for 
minimising 
a 
function 
f(x) 


1: 
k 
=1 
2: 
Choose 
x1. 
3: 
p1 
= 
..g1 
4: 
while 
gk=6
0 
do 
5: 
k 
= 
argmin 
f(xk 
+ 
kpk) 
I 
Line 
Search 
k 


6: 
xk+1 
:= 
xk 
+ 
kpk 
7: 
k 
:= 
gT 
gk+1=(gTgk)
k+1k 


8: 
pk+1 
:= 
..gk+1 
+ 
kpk 
9: 
k 
= 
k 
+1 
10: 
end 
while 
The 
formula 
(A.7.21) 
for 
k 
is 
due 
to 
Fletcher 
and 
Reeves. 
Since 
the 
gradients 
are 
orthogonal, 
k 
can 
also 
be 
written 
as 


gT 


k+1(gk+1 
..rgk)

k 
= 
, 
(A.7.22) 


gT 


k 
gk 


this 
is 
the 
Polak-Ribiere 
formula. 
The 
choice 
between 
the 
two 
expression 
for 
k 
can 
be 
of 
some 
importance 
if 
f 
is 
not 
quadratic. 


A.7.5 
Newton's 
method 
Consider 
a 
function 
f(x) 
that 
we 
wish 
to 
nd 
the 
minimum 
of. 
A 
Taylor 
expansion 
up 
to 
second 
order 
gives 


1 


f(x 
+ 
)= 
f(x)+ 
Trf 
+ 
TH. 
+ 
O(jj3) 
(A.7.23)

2

The 
matrix 
H 
is 
the 
Hessian. 
Dierentiating 
the 
right 
hand 
side 
with 
respect 
to 
. 
(or, 
equivalently, 
completing 
the 
square), 
we 
nd 
that 
the 
right 
hand 
side 
has 
its 
lowest 
value 
when

rf 
= 
H. 
)r. 
= 
H..1rf 
(A.7.24) 


Hence, 
an 
optimisation 
routine 
to 
minimise 
E 
is 
given 
by 
the 
Newton 
update 


xk+1 
= 
xk 
..rH..1rf 
(A.7.25) 


A 
benet 
of 
Newton 
method 
over 
gradient 
descent 
is 
that 
the 
decrease 
in 
the 
objective 
function 
is 
invariant 
under 
a 
linear 
change 
of 
co-ordinates, 
y 
= 
Mx. 


A.7.6 
Quasi-Newton 
methods 
For 
large-scale 
problems 
the 
inversion 
of 
the 
Hessian 
is 
computationally 
demanding, 
especially 
if 
the 
matrix 
is 
close 
to 
singular. 
An 
alternative 
is 
to 
set 
up 
the 
iteration 


xk+1 
= 
xk 
..rkSkgk. 
(A.7.26) 


This 
is 
a 
very 
general 
form; 
if 
Sk 
= 
A..1 
then 
we 
have 
Newton's 
method, 
while 
if 
Sk 
= 
I 
we 
have 
steepest 
descent. 
In 
general 
it 
would 
seem 
to 
be 
a 
good 
idea 
to 
choose 
Sk 
to 
be 
an 
approximation 
to 
the 
inverse 
Hessian. 
Also 
note 
that 
it 
is 
important 
that 
Sk 
be 
positive 
denite 
so 
that 
for 
small 
k 
we 
obtain 
a 
descent 
method. 
The 
idea 
behind 
most 
quasi-Newton 
methods 
is 
to 
try 
to 
construct 
an 
approximate 
inverse 
Hessian 
H~
k 
using 
information 
gathered 
as 
the 
descent 
progresses, 
and 
to 
set 
Sk 
= 
H~
k. 
As 
we 
have 
seen, 
for 
a 
quadratic 
optimization 
problem 
we 
have 
the 
relationship 


gk+1 
..rgk 
= 
A(xk+1 
..rxk) 
(A.7.27) 


DRAFT 
March 
9, 
2010 
561 



Constrained 
Optimisation 
using 
Lagrange 
Multipliers 


Algorithm 
32 
Quasi-Newton 
for 
minimising 
a 
function 
f(x) 


1: 
k 
=1 
2: 
Choose 
x1 
~

3: 
H1 
= 
I 
4: 
while 
gk=6
0 
do 
5: 
pk 
= 
..H~
kgk 
6: 
k 
= 
argmin 
f(xk 
+ 
kpk) 
I 
Line 
Search 
k 


7: 
xk+1 
:= 
xk 
+ 
kpk 
8: 
sk 
= 
xk+1 
..6xk, 
yk 
= 
gk+1 
..6gk, 
and 
update 
H~
k+1 
9: 
k 
= 
k 
+1 
10: 
end 
while 
Dening 
sk 
= 
xk+1 
..6xk 
and 
yk 
= 
gk+1 
..6gk 
(A.7.28) 


T 


we 
see 
that 
equation 
A.7.27 
becomes 


yk 
= 
Ask 
(A.7.29) 
It 
is 
reasonable 
to 
demand 
that 


T 


~

Hk+1yi 
= 
si 
1 
6i 
6k 
(A.7.30) 
After 
n 
linearly 
independent 
steps 
we 
would 
then 
have 
H~
n+1 
= 
A..1 
. 
For 
k<n 
there 
are 
an 
innity 
of 


T 


solutions 
for 
H~
k+1 
satisfying 
equation 
A.7.30. 
A 
popular 
choice 
is 
the 
Broyden-Fletcher-Goldfarb-Shanno 
(or 
BFGS) 
update, 
given 
by 


 !

~

yHkyk 
skssky

kkk 
k 


T

H~
k 
+ 
H~
kyks

~~

Hk+1 
= 
Hk 
+

1+ 


- 


(A.7.31) 
y


T 


k 


sks

T 


k 


T 


k 


yk

yk 
s

This 
is 
a 
rank-2 
correction 
to 
H~
k 
constructed 
from 
the 
vectors 
sk 
and 
H~
kyk. 
The 
direction 
vectors 
p1, 
p2;:::, 
pk, 
pk 
= 
..H~
kgk, 
produced 
by 
the 
algorithm 
obey 


p

~

Hk+1Api 


T 
i 


Apj 
=0 
1 
6i<j 
6k 
(A.7.32) 


= 
pi 


1 
6i 
6k 
(A.7.33) 


Equation 
A.7.33 
is 
called 
the 
hereditary 
property. 
In 
our 
notation 
sk 
= 
kpk, 
and 
as 
the 
's 
are 
non-zero, 
equation 
A.7.32 
can 
also 
be 
written 
as 


T

s


Asj 
=0 
1 
6i<j 
6k 
(A.7.34)

i 


Since 
the 
pk's 
are 
A-conjugate 
and 
since 
we 
successively 
minimize 
f 
in 
these 
directions, 
we 
see 
that 
the 
BFGS 
algorithm 
is 
a 
conjugate 
direction 
method; 
with 
the 
choice 
of 
H1 
= 
I 
it 
is 
in 
fact 
the 
conjugate 
gradient 
method. 
Note 
that 
the 
storage 
requirements 
for 
Quasi 
Newton 
methods 
scale 
quadratically 
with 
the 
number 
of 
variables, 
and 
hence 
tends 
to 
be 
used 
for 
smaller 
problems. 
Limited 
memory 
BFGS 
reduces 
the 
storage 
by 
only 
using 
the 
l 
latest 
updates 
in 
computing 
the 
approximate 
Hessian 
inverse, 
equation 
(A.7.31). 
In 
contrast, 
the 
memory 
requirements 
for 
pure 
Conjugate 
Gradient 
methods 
scale 
only 
linearly 
with 
the 
dimension 
of 
x. 


A.7 
Constrained 
Optimisation 
using 
Lagrange 
Multipliers 
Single 
constraint 


Consider 
rst 
the 
problem 
of 
minimising 
f(x) 
subject 
to 
a 
single 
constraint 
c(x) 
= 
0. 
Imagine 
that 
we 
have 
already 
identied 
an 
x 
that 
satises 
the 
constraint, 
that 
is 
c(x) 
= 
0. 
How 
can 
we 
tell 
if 
this 
x 
minimises 


562 
DRAFT 
March 
9, 
2010 



Constrained 
Optimisation 
using 
Lagrange 
Multipliers 


the 
function 
f? 
We 
are 
only 
allowed 
to 
search 
for 
lower 
function 
values 
around 
this 
x 
in 
directions 
which 
are 
consistent 
with 
the 
constraint. 
For 
a 
small 
change 
, 
the 
change 
in 
the 
constraint 
is, 


c(x 
+ 
) 
˜ 
c(x)+ 
d 
rc(x) 
(A.7.1) 


Hence, 
in 
order 
that 
the 
constraint 
remains 
satised, 
we 
can 
only 
search 
in 
a 
direction 
such 
that 
rc(x)= 
0, 
that 
is 
in 
directions 
d 
that 
are 
orthogonal 
to 
rc(x). 
So, 
let 
us 
explore 
the 
change 
in 
f 
along 
a 
direction 
d 
where 
d 
rc(x) 
= 
0, 


f(x 
+ 
) 
˜ 
f(x)+ 
rf(x) 
· 
. 
(A.7.2) 


Since 
we 
are 
looking 
for 
a 
point 
x 
that 
minimises 
the 
function 
f, 
we 
require 
x 
to 
be 
a 
stationary 
point, 
rf(x) 
· 
d 
= 
0. 
Thus 
d 
must 
be 
orthogonal 
to 
both 
rf(x) 
and 
rc(x). 
Since 
we 
wish 
to 
constrain 
d 
as 
little 
as 
possible, 
the 
most 
freedom 
is 
given 
by 
enforcing 
rf(x) 
to 
be 
parallel 
to 
rc(x), 
so 
that 


rf(x)= 
rc(x) 
(A.7.3) 


for 
some 
. 
. 
R. 
To 
solve 
the 
optimisation 
problem 
therefore, 
we 
look 
for 
a 
point 
x 
such 
that 
rf(x)= 
rc(x), 
for 
some 
, 
and 
for 
which 
c(x) 
= 
0. 
An 
alternative 
formulation 
of 
this 
dual 
requirement 
is 
to 
look 
for 
x 
and 
. 
that 
jointly 
minimise 
the 
Lagrangian 


L(x;)= 
f(x) 
- 
c(x) 
(A.7.4) 


Dierentiating 
with 
respect 
to 
x, 
we 
get 
the 
requirement 
rf(x)= 
rc(x), 
and 
dierentiating 
with 
respect 
to 
, 
we 
get 
that 
c(x) 
= 
0. 


Multiple 
constraints 


Consider 
the 
problem 
of 
optimising 
f(x) 
subject 
to 
the 
constraints 
ci(x)=0;i 
=1;:::;r 
< 
n, 
where 
n 
is 
the 
dimensionality 
of 
the 
space. 
Denote 
by 
S 
the 
n 
- 
r 
dimensional 
subspace 
of 
x 
which 
obeys 
the 
constraints. 
Assume 
that 
x 
* 
is 
such 
an 
optimum. 
As 
in 
the 
unconstrained 
case, 
we 
consider 
perturbations 
v 
to 
x 
, 
but 
now 
such 
that 
v 
lies 
in 
S 


ci(x 
* 
+ 
hv)= 
ci(x 
)+ 
vTrci(x 
)+ 
O(h2) 
(A.7.5) 


Let 
a 
* 
= 
rci(x 
). 
Thus 
for 
the 
perturbation 
to 
stay 
within 
S, 
we 
require 
that 
vTa 
* 
= 
0 
for 
all 
i 
=1;:::;r.

ii 


Let 
A* 
be 
the 
matrix 
whose 
columns 
are 
a 
* 
1, 
a2* 
;:::, 
a 
* 
. 
Then 
this 
condition 
can 
be 
rewritten 
as 
A* 
v 
= 
0.

r

We 
also 
require 
for 
a 
local 
optimum 
that 
vTrf 
= 
0 
for 
all 
v 
in 
S. 
We 
see 
that 
rf 
must 
be 
orthogonal 
to 
v, 
and 
that 
v 
must 
be 
orthogonal 
to 
the 
a 
's. 
This 
can 
be 
achieved 
by 
forcing 
rf 
to 
be 
a 
linear 


i 


combination 
of 
the 
a 
's, 
i.e.

i 
r


X



rf 
= 
. 
* 
(A.7.6)

i 
ai 
i=1 


Geometrically 
this 
says 
that 
the 
gradient 
vector 
is 
normal 
to 
the 
tangent 
plane 
to 
S 
at 
x 
. 
These 
conditions 
give 
rise 
to 
the 
method 
of 
Lagrange 
multipliers 
for 
optimisation 
problems 
with 
equality 
constraints. 
The 
method 
requires 
nding 
x 
* 
and 
. 
* 
which 
solve 
the 
equations 


X

rf 
=ai(x)i 
(A.7.7) 
i 
ci(x)=0 
for 
i 
=1;:::;r 
(A.7.8) 


There 
are 
n 
+ 
r 
equations 
and 
n 
+ 
r 
unknowns, 
so 
the 
system 
is 
well-determined. 
However, 
the 
system 
is 
nonlinear 
(in 
x) 
in 
general, 
and 
so 
may 
not 
be 
easy 
to 
solve. 
We 
can 
restate 
these 
conditions 
by 
introducing 
the 
Lagrangian 
function 


X

L(x, 
)= 
f(x) 
..ici(x). 
(A.7.9) 


i 


The 
partial 
derivatives 
of 
L 
with 
respect 
to 
x 
and 
. 
reproduce 
equations 
A.7.7 
and 
A.7.8. 
Hence 
a 
necessary 
condition 
for 
a 
local 
minimizer 
is 
that 
x 
* 
, 
. 
* 
is 
a 
stationary 
point 
of 
the 
Lagrangian 
function. 
Note 
that 
this 
stationary 
point 
is 
not 
a 
minimum 
but 
a 
saddle 
point, 
as 
L 
depends 
linearly 
on 
. 
We 
have 
given 
rst-order 
necessary 
and 
sucient 
conditions 
for 
a 
local 
optimum. 
To 
show 
that 
this 
optimum 
is 
a 
local 
minimum, 
we 
would 
need 
to 
consider 
second-order 
conditions, 
analogous 
to 
the 
positive 
deniteness 
of 
the 
Hessian 
in 
the 
unconstrained 
case; 
this 
can 
be 
done, 
but 
will 
not 
be 
considered 
here. 


DRAFT 
March 
9, 
2010 
563 



Constrained 
Optimisation 
using 
Lagrange 
Multipliers 


DRAFT 
March 
9, 
2010 



Bibliography 


[1] 
L. 
F. 
Abbott, 
J. 
A. 
Varela, 
K. 
Sen, 
and 
S. 
B. 
Nelson. 
Synaptic 
Depression 
and 
Cortical 
Gain 
Control. 
Science, 
275:220{223, 
1997. 
[2] 
D. 
H. 
Ackley, 
G. 
E. 
Hinton, 
and 
T. 
J. 
Sejnowski. 
A 
Learning 
Algorithm 
for 
Boltzmann 
Machines. 
Cognitive 
Science, 
9:147{169, 
1985. 
[3] 
R. 
P. 
Adams 
and 
D. 
J. 
C. 
MacKay. 
Bayesian 
Online 
Changepoint 
Detection. 
Cavendish 
laboratory, 
department 
of 
physics, 
University 
of 
Cambridge, 
Cambridge, 
UK, 
2006. 
arXiv:0710.3742v1 
[stat.ML]. 
[4] 
E. 
Airoldi, 
D. 
Blei, 
E. 
Xing, 
and 
S. 
Fienberg. 
A 
latent 
mixed 
membership 
model 
for 
relational 
data. 
In 
LinkKDD 
'05: 
Proceedings 
of 
the 
3rd 
international 
workshop 
on 
Link 
discovery, 
pages 
82{89, 
New 
York, 
NY, 
USA, 
2005. 
ACM. 
[5] 
E. 
M. 
Airoldi, 
D. 
M. 
Blei, 
S. 
E. 
Fienberg, 
and 
E. 
P. 
Xing. 
Mixed 
membership 
stochastic 
blockmodels. 
Journal 
of 
Machine 
Learning 
Research, 
9:1981{2014, 
2008. 
[6] 
D. 
L. 
Alspach 
and 
H. 
W. 
Sorenson. 
Nonlinear 
Bayesian 
Estimation 
Using 
Gaussian 
Sum 
Approximations. 
IEEE 
Transactions 
on 
Automatic 
Control, 
17(4):439{448, 
1972. 
[7] 
S-i. 
Amari. 
Natural 
Gradient 
Works 
Eciently 
in 
Learning. 
Neural 
Computation, 
10(2):251{276, 
1998. 
[8] 
S-i. 
Amari. 
Natural 
Gradient 
Learning 
for 
Over 
and 
Under-Complete 
Bases 
in 
ICA. 
Neural 
Computation, 
11:1875{1883, 
1999. 
[9] 
I. 
Androutsopoulos, 
J. 
Koutsias, 
K. 
V. 
Chandrinos, 
and 
C. 
D. 
Spyropoulos. 
An 
experimental 
comparison 
of 
naive 
bayesian 
and 
keyword-based 
anti-spam 
ltering 
with 
personal 
e-mail 
messages. 
In 
Proceedings 
of 
the 
23rd 
annual 
international 
ACM 
SIGIR 
conference 
on 
Research 
and 
development 
in 
information 
retrieval, 
pages 
160{167, 
New 
York, 
NY, 
USA, 
2000. 
ACM. 
[10] 
S. 
Arora 
and 
C. 
Lund. 
Hardness 
of 
approximations. 
In 
Approximation 
algorithms 
for 
NP-hard 
problems, 
pages 
399{446. 
PWS 
Publishing 
Co., 
Boston, 
MA, 
USA, 
1997. 
[11] 
F. 
R. 
Bach 
and 
M. 
I. 
Jordan. 
Thin 
junction 
trees. 
In 
T. 
G. 
Dietterich, 
S. 
Becker, 
and 
Z. 
Ghahramani, 
editors, 
Advances 
in 
Neural 
Information 
Processing 
Systems 
(NIPS), 
number 
14, 
pages 
569{576, 
Cambridge, 
MA, 
2001. 
MIT 
Press. 
[12] 
F. 
R. 
Bach 
and 
M. 
I. 
Jordan. 
A 
probabilistic 
interpretation 
of 
canonical 
correlation 
analysis. 
Computer 
Science 
Division 
and 
Department 
of 
Statistics 
688, 
University 
of 
California 
Berkeley, 
Berkeley, 
USA, 
2005. 
[13] 
Y. 
Bar-Shalom 
and 
Xiao-Rong 
Li. 
Estimation 
and 
Tracking 
: 
Principles, 
Techniques 
and 
Software. 
Artech 
House, 
Norwood, 
MA, 
1998. 
[14] 
D. 
Barber. 
Dynamic 
Bayesian 
Networks 
with 
Deterministic 
Tables. 
In 
S. 
Becker, 
S. 
Thrun, 
and 
K. 
Obermayer, 
editors, 
Advances 
in 
Neural 
Information 
Processing 
Systems 
(NIPS), 
number 
15, 
pages 
713{720, 
Cambridge, 
MA, 
2003. 
MIT 
Press. 
[15] 
D. 
Barber. 
Learning 
in 
Spiking 
Neural 
Assemblies. 
In 
S. 
Becker, 
S. 
Thrun, 
and 
K. 
Obermayer, 
editors, 
Advances 
in 
Neural 
Information 
Processing 
Systems 
(NIPS), 
number 
15, 
pages 
149{156, 
Cambridge, 
MA, 
2003. 
MIT 
Press. 
565 



BIBLIOGRAPHY 
BIBLIOGRAPHY 


[16] 
D. 
Barber. 
Are 
two 
Classiers 
performing 
equally? 
A 
treatment 
using 
Bayesian 
Hypothesis 
Testing. 
IDIAPRR 
57, 
IDIAP, 
Rue 
de 
Simplon 
4, 
Martigny, 
CH-1920, 
Switerland, 
May 
2004. 
IDIAP-RR 
04-57. 
[17] 
D. 
Barber. 
Expectation 
Correction 
for 
smoothing 
in 
Switching 
Linear 
Gaussian 
State 
Space 
models. 
Journal 
of 
Machine 
Learning 
Research, 
7:2515{2540, 
2006. 
[18] 
D. 
Barber. 
Clique 
Matrices 
for 
Statistical 
Graph 
Decomposition 
and 
Parameterising 
Restricted 
Positive 
Denite 
Matrices. 
In 
D. 
A. 
McAllester 
and 
P. 
Myllymaki, 
editors, 
Uncertainty 
in 
Articial 
Intelligence, 
number 
24, 
pages 
26{33, 
Corvallis, 
Oregon, 
USA, 
2008. 
AUAI 
press. 
[19] 
D. 
Barber 
and 
F. 
V. 
Agakov. 
Correlated 
sequence 
learning 
in 
a 
network 
of 
spiking 
neurons 
using 
maximum 
likelihood. 
Informatics 
Research 
Reports 
EDI-INF-RR-0149, 
Edinburgh 
University, 
2002. 
[20] 
D. 
Barber 
and 
F.V. 
Agakov. 
The 
IM 
Algorithm: 
A 
variational 
approach 
to 
Information 
Maximization. 
In 
Advances 
in 
Neural 
Information 
Processing 
Systems 
(NIPS), 
number 
16, 
2004. 
[21] 
D. 
Barber 
and 
C. 
M. 
Bishop. 
Bayesian 
Model 
Comparison 
by 
Monte 
Carlo 
Chaining. 
In 
M. 
C. 
Mozer, 
M. 
I. 
Jordan, 
and 
T. 
Petsche, 
editors, 
Advances 
in 
Neural 
Information 
Processing 
Systems 
(NIPS), 
number 
9, 
pages 
333{339, 
Cambridge, 
MA, 
1997. 
MIT 
Press. 
[22] 
D. 
Barber 
and 
C. 
M. 
Bishop. 
Ensemble 
Learning 
in 
Bayesian 
Neural 
Networks. 
In 
Neural 
Networks 
and 
Machine 
Learning, 
pages 
215{237. 
Springer, 
1998. 
[23] 
D. 
Barber 
and 
S. 
Chiappa. 
Unied 
Inference 
for 
Variational 
Bayesian 
Linear 
Gaussian 
State-Space 
Models. 
In 
B. 
Scholkopf, 
J. 
Platt, 
and 
T. 
Homan, 
editors, 
Advances 
in 
Neural 
Information 
Processing 
Systems 
(NIPS), 
number 
19, 
pages 
81{88, 
Cambridge, 
MA, 
2007. 
MIT 
Press. 
[24] 
D. 
Barber 
and 
W. 
Wiegerinck. 
Tractable 
Variational 
Structures 
for 
Approximating 
Graphical 
Models. 
In 
M. 
S. 
Kearns, 
S. 
A. 
Solla, 
and 
D. 
A. 
Cohn, 
editors, 
Advances 
in 
Neural 
Information 
Processing 
Systems 
(NIPS), 
number 
11, 
pages 
183{189, 
Cambridge, 
MA, 
1999. 
MIT 
Press. 
[25] 
D. 
Barber 
and 
C. 
K. 
I. 
Williams. 
Gaussian 
processes 
for 
Bayesian 
classication 
via 
hybrid 
Monte 
Carlo. 
In 
M. 
C. 
Mozer, 
M. 
I. 
Jordan, 
and 
T. 
Petsche, 
editors, 
Advances 
in 
Neural 
Information 
Processing 
Systems 
NIPS 
9, 
pages 
340{346, 
Cambridge, 
MA, 
1997. 
MIT 
Press. 
[26] 
R. 
J. 
Baxter. 
Exactly 
solved 
models 
in 
statistical 
mechanics. 
Academic 
Press, 
1982. 
[27] 
M. 
J. 
Beal, 
F. 
Falciani, 
Z. 
Ghahramani, 
C. 
Rangel, 
and 
D. 
L. 
Wild. 
A 
Bayesian 
approach 
to 
reconstructing 
genetic 
regulatory 
networks 
with 
hidden 
factors. 
Bioinformatics, 
(21):349{356, 
2005. 
[28] 
A. 
Becker 
and 
D. 
Geiger. 
A 
suciently 
fast 
algorithm 
for 
nding 
close 
to 
optimal 
clique 
trees. 
Articial 
Intelligence, 
125(1-2):3{17, 
2001. 
[29] 
A. 
J. 
Bell 
and 
T. 
J. 
Sejnowski. 
An 
Information-Maximization 
Approach 
to 
Blind 
Separation 
and 
Blind 
Deconvolution. 
Neural 
Computation, 
7(6):1129{1159, 
1995. 
[30] 
R. 
E. 
Bellman. 
Dynamic 
Programming. 
Princeton 
University 
Press, 
Princeton, 
NJ, 
1957. 
Paperback 
edition 
by 
Dover 
Publications 
(2003). 
[31] 
Y. 
Bengio 
and 
P. 
Frasconi. 
Input-Output 
HMMs 
for 
sequence 
processing. 
IEEE 
Trans. 
Neural 
Networks, 
(7):1231{1249, 
1996. 
[32] 
A. 
L. 
Berger, 
S. 
D. 
Della 
Pietra, 
and 
V. 
J. 
D. 
Della 
Pietra. 
A 
maximum 
entropy 
approach 
to 
natural 
language 
processing. 
Computational 
Linguistics, 
22(1):39{71, 
1996. 
[33] 
J. 
O. 
Berger. 
Statistical 
Decision 
Theory 
and 
Bayesian 
Analysis. 
Springer, 
second 
edition, 
1985. 
[34] 
D. 
P. 
Bertsekas. 
Dynamic 
Programming 
and 
Optimal 
Control. 
Athena 
Scientic, 
second 
edition, 
2000. 
[35] 
J. 
Besag. 
Spatial 
Interactions 
and 
the 
Statistical 
Analysis 
of 
Lattice 
Systems. 
Journal 
of 
the 
Royal 
Statistical 
Society, 
Series 
B, 
36(2):192{236, 
1974. 
[36] 
J. 
Besag. 
On 
the 
statistical 
analysis 
of 
dirty 
pictures. 
Journal 
of 
the 
Royal 
Statistical 
Society, 
Series 
B, 
48:259{302, 
1986. 
[37] 
J. 
Besag 
and 
P. 
Green. 
Spatial 
statistics 
and 
Bayesian 
computation. 
Journal 
of 
the 
Royal 
Statistical 
Society, 
Series 
B, 
55:25{37, 
1993. 
DRAFT 
March 
9, 
2010 



BIBLIOGRAPHY 
BIBLIOGRAPHY 


[38] 
G. 
J. 
Bierman. 
Measurement 
updating 
using 
the 
U-D 
factorization. 
Automatica, 
12:375{382, 
1976. 
[39] 
N. 
L. 
Biggs. 
Discrete 
Mathematics. 
Oxford 
University 
Press, 
1990. 
[40] 
K. 
Binder 
and 
A. 
P. 
Young. 
Spin 
glasses: 
Experimental 
facts, 
theoretical 
concepts, 
and 
open 
questions. 
Rev. 
Mod. 
Phys., 
58(4):801{976, 
Oct 
1986. 
[41] 
C. 
M. 
Bishop. 
Neural 
Networks 
for 
Pattern 
Recognition. 
Oxford 
University 
Press, 
1995. 
[42] 
C. 
M. 
Bishop. 
Pattern 
Recognition 
and 
Machine 
Learning. 
Springer, 
2006. 
[43] 
C. 
M. 
Bishop 
and 
M. 
Svensen. 
Bayesian 
hierarchical 
mixtures 
of 
experts. 
In 
U. 
Kjaerulff 
and 
C. 
Meek, 
editors, 
Proceedings 
Nineteenth 
Conference 
on 
Uncertainty 
in 
Articial 
Intelligence, 
pages 
57{64. 
Morgan 
Kaufmann, 
2003. 
[44] 
D. 
Blei, 
A. 
Ng, 
and 
M. 
Jordan. 
Latent 
Dirichlet 
allocation. 
Journal 
of 
machine 
Learning 
Research, 
(3):993– 
1022, 
2003. 
[45] 
R. 
R. 
Bouckaert. 
Bayesian 
belief 
networks: 
from 
construction 
to 
inference. 
PhD 
thesis, 
University 
of 
Utrecht, 
1995. 
[46] 
S. 
Boyd 
and 
L. 
Vandenberghe. 
Convex 
Optimization. 
Cambridge 
University 
Press, 
2004. 
[47] 
Y. 
Boykov 
and 
V. 
Kolmogorov. 
An 
experimental 
comparison 
of 
min-cut/max-ow 
algorithms 
for 
energy 
minimization 
in 
vision. 
IEEE 
Trans. 
Pattern 
Anal. 
Mach. 
Intell., 
26(9):1124{1137, 
2004. 
[48] 
Y. 
Boykov, 
O. 
Veksler, 
and 
R. 
Zabih. 
Fast 
approximate 
energy 
minimization 
via 
graph 
cuts. 
IEEE 
Trans. 
Pattern 
Anal. 
Mach. 
Intell., 
23:1222{1239, 
2001. 
[49] 
M. 
Brand. 
Incremental 
singular 
value 
decomposition 
of 
uncertain 
data 
with 
missing 
values. 
In 
European 
Conference 
on 
Computer 
Vision 
(ECCV), 
pages 
707{720, 
2002. 
[50] 
J. 
Breese 
and 
D. 
Heckerman. 
Decision-theoretic 
troubleshooting: 
A 
framework 
for 
repair 
and 
experiment. 
In 
E. 
Horvitz 
and 
F. 
Jensen, 
editors, 
Uncertainty 
in 
Articial 
Intelligence, 
number 
12, 
pages 
124{132, 
San 
Francisco, 
CA, 
1996. 
Morgan 
Kaufmann. 
[51] 
H. 
Bunke 
and 
T. 
Caelli. 
Hidden 
Markov 
models: 
applications 
in 
computer 
vision. 
Machine 
Perception 
and 
Articial 
Intelligence. 
World 
Scientic 
Publishing 
Co., 
Inc., 
River 
Edge, 
NJ, 
USA, 
2001. 
[52] 
W. 
Buntine. 
Theory 
renement 
on 
Bayesian 
networks. 
In 
Uncertainty 
in 
Articial 
Intelligence, 
number 
7, 
pages 
52{60, 
San 
Francisco, 
CA, 
1991. 
Morgan 
Kaufmann. 
[53] 
A. 
Cano 
and 
S. 
Moral. 
Advances 
in 
Intelligent 
Computing 
– 
IPMU 
1994, 
chapter 
Heuristic 
Algorithms 
for 
the 
Triangulation 
of 
Graphs, 
pages 
98{107. 
Number 
945 
in 
Lectures 
Notes 
in 
Computer 
Sciences. 
Springer-Verlag, 
1995. 
[54] 
O. 
Cappe, 
E. 
Moulines, 
and 
T. 
Ryden. 
Inference 
in 
Hidden 
Markov 
Models. 
Springer, 
New 
York, 
2005. 
[55] 
E. 
Castillo, 
J. 
M. 
Gutierrez, 
and 
A. 
S. 
Hadi. 
Expert 
Systems 
and 
Probabilistic 
Network 
Models. 
Springer, 
1997. 
[56] 
A. 
T. 
Cemgil. 
Bayesian 
Inference 
in 
Non-negative 
Matrix 
Factorisation 
Models. 
Technical 
Report 
CUED/FINFENG/
TR.609, 
University 
of 
Cambridge, 
July 
2008. 
[57] 
A. 
T. 
Cemgil, 
B. 
Kappen, 
and 
D. 
Barber. 
A 
Generative 
Model 
for 
Music 
Transcription. 
IEEE 
Transactions 
on 
Audio, 
Speech 
and 
Language 
Processing, 
14(2):679{694, 
2006. 
[58] 
H. 
S. 
Chang, 
M. 
C. 
Fu, 
J. 
Hu, 
and 
S. 
I. 
Marcus. 
Simulation-based 
Algorithms 
for 
Markov 
Decision 
Processes. 
Springer, 
2007. 
[59] 
S. 
Chiappa 
and 
D. 
Barber. 
Bayesian 
Linear 
Gaussian 
State 
Space 
Models 
for 
Biosignal 
Decomposition. 
Signal 
Processing 
Letters, 
14(4):267{270, 
2007. 
[60] 
S. 
Chib 
and 
M. 
Dueker. 
Non-Markovian 
Regime 
Switching 
with 
Endogenous 
States 
and 
Time-Varying 
State 
Strengths. 
Econometric 
Society 
2004 
North 
American 
Summer 
Meetings 
600, 
Econometric 
Society, 
August 
2004. 
[61] 
C. 
K. 
Chow 
and 
C. 
N. 
Liu. 
Approximating 
discrete 
probability 
distributions 
with 
dependence 
trees. 
IEEE 
Transactions 
on 
Information 
Theory, 
14(3):462{467, 
1968. 
DRAFT 
March 
9, 
2010 



BIBLIOGRAPHY 
BIBLIOGRAPHY 


[62] 
P. 
S. 
Churchland 
and 
T. 
J. 
Sejnowski. 
The 
Computational 
Brain. 
MIT 
Press, 
Cambridge, 
MA, 
USA, 
1994. 
[63] 
D. 
Cohn 
and 
H. 
Chang. 
Learning 
to 
probabilistically 
identify 
authoritative 
documents. 
In 
P. 
Langley, 
editor, 
International 
Conference 
on 
Machine 
Learning, 
number 
17, 
pages 
167{174. 
Morgan 
Kaufmann, 
2000. 
[64] 
D. 
Cohn 
and 
T. 
Hofmann. 
The 
Missing 
Link 
-A 
Probabilistic 
Model 
of 
Document 
Content 
and 
Hypertext 
Connectivity. 
Number 
13, 
pages 
430{436, 
Cambridge, 
MA, 
2001. 
MIT 
Press. 
[65] 
A. 
C. 
C. 
Coolen, 
R. 
Kuhn, 
and 
P. 
Sollich. 
Theory 
of 
Neural 
Information 
Processing 
Systems. 
Oxford 
University 
Press, 
2005. 
[66] 
G. 
F. 
Cooper 
and 
E. 
Herskovits. 
A 
Bayesian 
Method 
for 
the 
Induction 
of 
Probabilistic 
Networks 
from 
Data. 
Machine 
Learning, 
9(4):309{347, 
1992. 
[67] 
A. 
Corduneanu 
and 
C. 
M. 
Bishop. 
Variational 
Bayesian 
Model 
Selection 
for 
Mixture 
Distributions. 
In 
T. 
Jaakkola 
and 
T. 
Richardson, 
editors, 
Artifcial 
Intelligence 
and 
Statistics, 
pages 
27{34. 
Morgan 
Kaufmann, 
2001. 
[68] 
M. 
T. 
Cover 
and 
J. 
A. 
Thomas. 
Elements 
of 
Information 
Theory. 
Wiley, 
1991. 
[69] 
R. 
G. 
Cowell, 
A. 
P. 
Dawid, 
S. 
L. 
Lauritzen, 
and 
D. 
J. 
Spiegelhalter. 
Probabilistic 
Networks 
and 
Expert 
Systems. 
Springer, 
1999. 
[70] 
D. 
R. 
Cox 
and 
N. 
Wermuth. 
Multivariate 
Dependencies. 
Chapman 
and 
Hall, 
1996. 
[71] 
N. 
Cristianini 
and 
J. 
Shawe-Taylor. 
An 
Introduction 
To 
Support 
Vector 
Machines. 
Cambridge 
University 
Press, 
2000. 
[72] 
P. 
Dangauthier, 
R. 
Herbrich, 
T. 
Minka, 
and 
T. 
Graepel. 
Trueskill 
through 
time: 
Revisiting 
the 
history 
of 
chess. 
In 
B. 
Scholkopf, 
J. 
Platt, 
and 
T. 
Homan, 
editors, 
Advances 
in 
Neural 
Information 
Processing 
Systems 
(NIPS), 
number 
19, 
pages 
569{576, 
Cambridge, 
MA, 
2007. 
MIT 
Press. 
[73] 
H. 
A. 
David. 
The 
method 
of 
paired 
comparisons. 
Oxford 
University 
Press, 
New 
York, 
1988. 
[74] 
A. 
P. 
Dawid. 
Inuence 
diagrams 
for 
causal 
modelling 
and 
inference. 
International 
Statistical 
Review, 
70:161– 
189, 
2002. 
[75] 
A. 
P. 
Dawid 
and 
S. 
L. 
Lauritzen. 
Hyper 
Markov 
Laws 
in 
the 
Statistical 
Analysis 
of 
Decomposable 
Graphical 
Models. 
Annals 
of 
Statistics, 
21(3):1272{1317, 
1993. 
[76] 
P. 
Dayan 
and 
L.F. 
Abbott. 
Theoretical 
Neuroscience. 
MIT 
Press, 
2001. 
[77] 
P. 
Dayan 
and 
G. 
E. 
Hinton. 
Using 
Expectation-Maximization 
for 
Reinforcement 
Learning. 
Neural 
Computat
ion, 
9:271{278, 
1997. 
[78] 
T. 
De 
Bie, 
N. 
Cristianini, 
and 
R. 
Rosipal. 
Handbook 
of 
Geometric 
Computing 
: 
Applications 
in 
Pattern 
Recognition, 
Computer 
Vision, 
Neuralcomputing, 
and 
Robotics, 
chapter 
Eigenproblems 
in 
Pattern 
Recognition. 
Springer-Verlag, 
2005. 
[79] 
R. 
Dechter. 
Bucket 
Elimination: 
A 
unifying 
framework 
for 
probabilistic 
inference 
algorithms. 
In 
E. 
Horvitz 
and 
F. 
Jensen, 
editors, 
Uncertainty 
in 
Articial 
Intelligence, 
pages 
211{219, 
San 
Francisco, 
CA, 
1996. 
Morgan 
Kaufmann. 
[80] 
S. 
Diederich 
and 
M. 
Opper. 
Learning 
of 
Correlated 
Patterns 
in 
Spin-Glass 
Networks 
by 
Local 
Learning 
Rules. 
Physical 
Review 
Letters, 
58(9):949{952, 
1986. 
[81] 
R. 
Diestel. 
Graph 
Theory. 
Springer, 
2005. 
[82] 
A. 
Doucet 
and 
A. 
M. 
Johansen. 
A 
Tutorial 
on 
Particle 
Filtering 
and 
Smoothing: 
Fifteen 
years 
later. 
In 
D. 
Crisan 
and 
B. 
Rozovsky, 
editors, 
Oxford 
Handbook 
of 
Nonlinear 
Filtering. 
Oxford 
University 
Press, 
2009. 
[83] 
R. 
O. 
Duda, 
P. 
E. 
Hart, 
and 
D. 
G. 
Stork. 
Pattern 
Classication. 
Wiley-Interscience 
Publication, 
2000. 
[84] 
R. 
Durbin, 
S. 
R. 
Eddy, 
A. 
Krogh, 
and 
G. 
Mitchison. 
Biological 
Sequence 
Analysis 
: 
Probabilistic 
Models 
of 
Proteins 
and 
Nucleic 
Acids. 
Cambridge 
University 
Press, 
1999. 
[85] 
A. 
During, 
A. 
C. 
C. 
Coolen, 
and 
D. 
Sherrington. 
Phase 
diagram 
and 
storage 
capacity 
of 
sequence 
processing 
neural 
networks. 
Journal 
of 
Physics 
A, 
31:8607{8621, 
1998. 
DRAFT 
March 
9, 
2010 



BIBLIOGRAPHY 
BIBLIOGRAPHY 


[86] 
J. 
M. 
Gutierrez 
E. 
Castillo 
and 
A. 
S. 
Hadi. 
Expert 
Systems 
and 
Probabilistic 
Network 
Models. 
Springer 
Verlag, 
1997. 
[87] 
J. 
Edmonds 
and 
R. 
M. 
Karp. 
Theoretical 
improvements 
in 
algorithmic 
eciency 
for 
network 
ow 
problems. 
Journal 
of 
the 
ACM, 
19(2):248{264, 
1972. 
[88] 
R. 
Edwards 
and 
A. 
Sokal. 
Generalization 
of 
the 
fortium-kasteleyn-swendson-wang 
representation 
and 
monte 
carlo 
algorithm. 
Physical 
Review 
D, 
38:2009{2012, 
1988. 
[89] 
A. 
E. 
Elo. 
The 
rating 
of 
chess 
players, 
past 
and 
present. 
Arco, 
New 
York, 
second 
edition, 
1986. 
[90] 
Y. 
Ephraim 
and 
W. 
J. 
J. 
Roberts. 
Revisiting 
autoregressive 
hidden 
Markov 
modeling 
of 
speech 
signals. 
IEEE 
Signal 
Processing 
Letters, 
12(2):166{169, 
February 
2005. 
[91] 
E. 
Erosheva, 
S. 
Fienberg, 
and 
J. 
Laerty. 
Mixed 
membership 
models 
of 
scientic 
publications. 
In 
Proceedings 
of 
the 
National 
Academy 
of 
Sciences, 
volume 
101, 
pages 
5220{5227, 
2004. 
[92] 
R-E. 
Fan, 
P-H. 
Chen, 
and 
C-J. 
Lin. 
Working 
Set 
Selection 
Using 
Second 
Order 
Information 
for 
Training 
Support 
Vector 
Machines. 
Journal 
of 
Machine 
Learning 
Research, 
6:1889{1918, 
2005. 
[93] 
P. 
Fearnhead. 
Exact 
and 
Ecient 
Bayesian 
inference 
for 
multiple 
changepoint 
problems. 
Technical 
report, 
Deptartment 
of 
Mathematics 
and 
Statistics, 
Lancaster 
University, 
2003. 
[94] 
G. 
H. 
Fischer 
and 
I. 
W. 
Molenaar. 
Rasch 
Models: 
Foundations, 
Recent 
Developments, 
and 
Applications. 
Springer, 
New 
York, 
1995. 
[95] 
M. 
E. 
Fisher. 
Statistical 
Mechanics 
of 
Dimers 
on 
a 
Plane 
Lattice. 
Physical 
Review, 
124:1664{1672, 
1961. 
[96] 
B. 
Frey. 
Extending 
Factor 
Graphs 
as 
to 
Unify 
Directed 
and 
Undirected 
Graphical 
Models. 
In 
C. 
Meek 
and 
U. 
Kjrul, 
editors, 
Uncertainty 
in 
Articial 
Intelligence, 
number 
19, 
pages 
257{264. 
Morgan 
Kaufmann, 
2003. 
[97] 
N. 
Friedman, 
D. 
Geiger, 
and 
M. 
Goldszmidt. 
Bayesian 
Network 
Classiers. 
Machine 
Learning, 
29:131{163, 
1997. 
[98] 
S. 
Fruhwirth-Schnatter. 
Finite 
Mixture 
and 
Markov 
Switching 
Models. 
Springer, 
2006. 
[99] 
M. 
Frydenberg. 
The 
chain 
graph 
Markov 
property. 
Scandanavian 
Journal 
of 
Statistics, 
17:333{353, 
1990. 
[100] 
T. 
Furmston 
and 
D. 
Barber. 
Solving 
deterministic 
policy 
(PO)MDPs 
using 
Expectation-Maximisation 
and 
Antifreeze. 
In 
E. 
Suzuki 
and 
M. 
Sebag, 
editors, 
European 
Conference 
on 
Machine 
Learning 
and 
Principles 
and 
Practice 
of 
Knowledge 
discovery 
in 
Databases, 
September 
2009. 
Workshop 
on 
Learning 
and 
Data 
Mining 
for 
Robotics. 
[101] 
A. 
Galka, 
O. 
Yamashita, 
T. 
Ozaki, 
R. 
Biscay, 
and 
P. 
Valdes-Sosa. 
A 
solution 
to 
the 
dynamical 
inverse 
problem 
of 
EEG 
generation 
using 
spatiotemporal 
Kalman 
ltering. 
NeuroImage, 
(23):435{453, 
2004. 
[102] 
P. 
Gandhi, 
F. 
Bromberg, 
and 
D. 
Margaritis. 
Learning 
markov 
network 
structure 
using 
few 
independence 
tests. 
In 
Proceedings 
of 
the 
SIAM 
International 
Conference 
on 
Data 
Mining, 
pages 
680{691, 
2008. 
[103] 
M. 
R. 
Garey 
and 
D. 
S. 
Johnson. 
Computers 
and 
Intractability, 
A 
Guide 
to 
the 
Theory 
of 
NP-Completeness. 
W.H. 
Freeman 
and 
Company, 
New 
York, 
1979. 
[104] 
A. 
Gelb. 
Applied 
optimal 
estimation. 
MIT 
press, 
1974. 
[105] 
A. 
Gelman, 
G. 
O. 
Roberts, 
and 
W. 
R. 
Gilks. 
Ecient 
Metropolis 
jumping 
rules. 
In 
J. 
O. 
Bernardo, 
J. 
M. 
Berger, 
A. 
P. 
Dawid, 
and 
A. 
F. 
M. 
Smith, 
editors, 
Bayesian 
Statistics, 
volume 
5, 
pages 
599{607. 
Oxford 
University 
Press, 
1996. 
[106] 
S. 
Geman 
and 
D. 
Geman. 
Stochastic 
relaxation, 
Gibbs 
distributions, 
and 
the 
Bayesian 
restoration 
of 
images. 
In 
Readings 
in 
uncertain 
reasoning, 
pages 
452{472, 
San 
Francisco, 
CA, 
USA, 
1990. 
Morgan 
Kaufmann 
Publishers 
Inc. 
[107] 
M. 
G. 
Genton. 
Classes 
of 
kernels 
for 
machine 
learning: 
A 
statistics 
perspective. 
Journal 
of 
Machine 
Learning 
Research, 
2:299{312, 
2001. 
[108] 
W. 
Gerstner 
and 
W. 
M. 
Kistler. 
Spiking 
Neuron 
Models. 
Cambridge 
University 
Press, 
2002. 
DRAFT 
March 
9, 
2010 



BIBLIOGRAPHY 
BIBLIOGRAPHY 


[109] 
Z. 
Ghahramani 
and 
M. 
J. 
Beal. 
Variational 
Inference 
for 
Bayesian 
Mixtures 
of 
Factor 
Analysers. 
In 
S. 
A. 
Solla, 
T. 
K. 
Leen, 
and 
K-R. 
Muller, 
editors, 
Advances 
in 
Neural 
Information 
Processing 
Systems 
(NIPS), 
number 
12, 
pages 
449{455, 
Cambridge, 
MA, 
2000. 
MIT 
Press. 
[110] 
Z. 
Ghahramani 
and 
G. 
E. 
Hinton. 
Variational 
learning 
for 
switching 
state-space 
models. 
Neural 
Computation, 
12(4):963{996, 
1998. 
[111] 
A. 
Gibbons. 
Algorithmic 
Graph 
Theory. 
Cambridge 
University 
Press, 
1991. 
[112] 
W. 
R. 
Gilks, 
S. 
Richardson, 
and 
D. 
J. 
Spiegelhalter. 
Markov 
chain 
Monte 
Carlo 
in 
practice. 
Chapman 
& 
Hall, 
1996. 
[113] 
M. 
Girolami 
and 
A. 
Kaban. 
On 
an 
equivalence 
between 
PLSI 
and 
LDA. 
In 
Proceedings 
of 
the 
26th 
annual 
international 
ACM 
SIGIR 
conference 
on 
Research 
and 
development 
in 
information 
retrieval, 
pages 
433{434, 
New 
York, 
NY, 
USA, 
2003. 
ACM 
Press. 
[114] 
M. 
E. 
Glickman. 
Parameter 
estimation 
in 
large 
dynamic 
paired 
comparison 
experiments. 
Applied 
Statistics, 
48:377{394, 
1999. 
[115] 
A. 
Globerson 
and 
T. 
Jaakkola. 
Approximate 
inference 
using 
planar 
graph 
decomposition. 
In 
B. 
Scholkopf, 
J. 
Platt, 
and 
T. 
Homan, 
editors, 
Advances 
in 
Neural 
Information 
Processing 
Systems 
(NIPS), 
number 
19, 
pages 
473{480, 
Cambridge, 
MA, 
2007. 
MIT 
Press. 
[116] 
D. 
Goldberg, 
D. 
Nichols, 
B. 
M. 
Oki, 
and 
D. 
Terry. 
Using 
collaborative 
ltering 
to 
weave 
an 
information 
tapestry. 
Communications 
ACM, 
35:61{70, 
1992. 
[117] 
G. 
H. 
Golub 
and 
C. 
F. 
van 
Loan. 
Matrix 
Computations. 
Johns 
Hopkins 
University 
Press, 
3rd 
edition, 
1996. 
[118] 
M. 
C. 
Golumbic 
and 
I. 
Ben-Arroyo 
Hartman. 
Graph 
Theory, 
Combinatorics, 
and 
Algorithms. 
Springer-Verlag, 
2005. 
[119] 
C. 
Goutis. 
A 
graphical 
method 
for 
solving 
a 
decision 
analysis 
problem. 
IEEE 
Transactions 
on 
Systems, 
Man 
and 
Cybernetics, 
25:1181{1193, 
1995. 
[120] 
P. 
J. 
Green 
and 
B. 
W. 
Silverman. 
Nonparametric 
Regression 
and 
Generalized 
Linear 
Models, 
volume 
58 
of 
Monographs 
on 
Statistics 
and 
Applied 
Probability. 
Chapman 
and 
Hall, 
1994. 
[121] 
D. 
M. 
Greig, 
B. 
T. 
Porteous, 
and 
A. 
H. 
Seheult. 
Exact 
maximum 
a 
posteriori 
estimation 
for 
binary 
images. 
Journal 
of 
the 
Royal 
Statistical 
Society, 
Series 
B, 
2:271{279, 
1989. 
[122] 
G. 
Grimmett 
and 
D. 
Stirzaker. 
Probability 
and 
Random 
Processes. 
Oxford 
University 
Press, 
second 
edition, 
1992. 
[123] 
S. 
F. 
Gull. 
Bayesian 
data 
analysis: 
straight-line 
tting. 
In 
J. 
Skilling, 
editor, 
Maximum 
entropy 
and 
Bayesian 
methods 
(Cambridge 
1988), 
pages 
511{518. 
Kluwer, 
1989. 
[124] 
A. 
K. 
Gupta 
and 
D. 
K. 
Nagar. 
Matrix 
Variate 
Distributions. 
Chapman 
and 
Hall/CRC, 
Boca 
Raton, 
Florida 
USA, 
1999. 
[125] 
D. 
J. 
Hand 
and 
K. 
Yu. 
Idiot's 
Bayes|Not 
So 
Stupid 
After 
All? 
International 
Statistical 
Review, 
69(3):385{398, 
2001. 
[126] 
D. 
R. 
Hardoon, 
S. 
Szedmak, 
and 
J. 
Shawe-Taylor. 
Canonical 
Correlation 
Analysis: 
An 
Overview 
with 
Application 
to 
Learning 
Methods. 
Neural 
Computation, 
16(12):2639{2664, 
2004. 
[127] 
D. 
O. 
Hebb. 
The 
organization 
of 
behavior. 
Wiley, 
New 
York, 
1949. 
[128] 
D. 
Heckerman. 
A 
Tutorial 
on 
Learning 
With 
Bayesian 
Networks. 
Technical 
Report 
MSR-TR-95-06, 
Microsoft 
Research, 
Redmond, 
WA, 
March 
1996. 
Revised 
November 
1996. 
[129] 
D. 
Heckerman, 
D. 
Geiger, 
and 
D. 
Chickering. 
Learning 
Bayesian 
Networks: 
The 
Combination 
of 
Knowledge 
and 
Statistical 
Data. 
Machine 
Learning, 
20(3):197{243, 
1995. 
[130] 
R. 
Herbrich, 
T. 
Minka, 
and 
T. 
Graepel. 
TrueSkillTM: 
A 
Bayesian 
Skill 
Rating 
System. 
In 
B. 
Scholkopf, 
J. 
Platt, 
and 
T. 
Homan, 
editors, 
Advances 
in 
Neural 
Information 
Processing 
Systems 
(NIPS), 
number 
19, 
pages 
569{576, 
Cambridge, 
MA, 
2007. 
MIT 
Press. 
[131] 
H. 
Hermansky. 
Should 
recognizers 
have 
ears? 
Speech 
Communication, 
25:3{27, 
1998. 
DRAFT 
March 
9, 
2010 



BIBLIOGRAPHY 
BIBLIOGRAPHY 


[132] 
J. 
Hertz, 
A. 
Krogh, 
and 
R. 
Palmer. 
Introduction 
to 
the 
theory 
of 
Neural 
Computation. 
Addison-Wesley, 
1991. 
[133] 
T. 
Heskes. 
Convexity 
arguments 
for 
ecient 
minimization 
of 
the 
Bethe 
and 
Kikuchi 
free 
energies. 
Journal 
of 
Articial 
Intelligence 
Research, 
26:153{190, 
2006. 
[134] 
D. 
M. 
Higdon. 
Auxiliary 
variable 
methods 
for 
Markov 
chain 
Monte 
Carlo 
with 
applications. 
Journal 
of 
the 
American 
Statistical 
Association, 
93(442):585{595, 
1998. 
[135] 
G. 
E. 
Hinton 
and 
R. 
R. 
Salakhutdinov. 
Reducing 
the 
dimensionality 
of 
data 
with 
neural 
networks. 
Science, 
(313):504{507, 
2006. 
[136] 
T. 
Hofmann, 
J. 
Puzicha, 
and 
M. 
I. 
Jordan. 
Learning 
from 
dyadic 
data. 
In 
M. 
S. 
Kearns, 
S. 
A. 
Solla, 
and 
D. 
A. 
Cohn, 
editors, 
Advances 
in 
Neural 
Information 
Processing 
Systems 
(NIPS), 
pages 
466{472, 
Cambridge, 
MA, 
1999. 
MIT 
Press. 
[137] 
R. 
A. 
Howard 
and 
J. 
E. 
Matheson. 
Inuence 
diagrams. 
Decision 
Analysis, 
2(3), 
2005. 
Republished 
version 
of 
the 
original 
1981 
report. 
[138] 
A. 
Hyvarinen, 
J. 
Karhunen, 
and 
E. 
Oja. 
Independent 
Component 
Analysis. 
Wiley, 
2001. 
[139] 
Aapo 
Hyvarinen. 
Consistency 
of 
Pseudolikelihood 
Estimation 
of 
Fully 
Visible 
Boltzmann 
Machines. 
Neural 
Computation, 
18(10):2283{2292, 
2006. 
[140] 
M. 
Isard 
and 
A. 
Blake. 
CONDENSATION 
Conditional 
Density 
Propagation 
for 
Visual 
Tracking. 
International 
Journal 
of 
Computer 
Vision, 
29:5{28, 
1998. 
[141] 
T. 
S. 
Jaakkola 
and 
M. 
I. 
Jordan. 
Variational 
probabilistic 
inference 
and 
the 
qmr-dt 
network. 
Journal 
of 
Articial 
Intelligence 
Research, 
10:291{322, 
1999. 
[142] 
T. 
S. 
Jaakkola 
and 
M. 
I. 
Jordan. 
Bayesian 
parameter 
estimation 
via 
variational 
methods. 
Statistics 
and 
Computing, 
10(1):25{37, 
2000. 
[143] 
R. 
A. 
Jacobs, 
F. 
Peng, 
and 
M. 
A. 
Tanner. 
A 
Bayesian 
approach 
to 
model 
selection 
in 
hierarchical 
mixturesof-
experts 
architectures. 
Neural 
Networks, 
10(2):231{241, 
1997. 
[144] 
R. 
G. 
Jarrett. 
A 
note 
on 
the 
intervals 
between 
coal-mining 
disasters. 
Biometrika, 
(66):191{193, 
1979. 
[145] 
E. 
T. 
Jaynes. 
Probability 
Theory 
: 
The 
Logic 
of 
Science. 
Cambridge 
University 
Press, 
2003. 
[146] 
F. 
Jensen, 
F. 
V. 
Jensen, 
and 
D. 
Dittmer. 
From 
inuence 
diagrams 
to 
junction 
trees. 
In 
Proceedings 
of 
the 
10th 
Annual 
Conference 
on 
Uncertainty 
in 
Articial 
Intelligence 
(UAI-94), 
pages 
367{373, 
San 
Francisco, 
CA, 
1994. 
Morgan 
Kaufmann. 
[147] 
F. 
V. 
Jensen 
and 
F. 
Jensen. 
Optimal 
Junction 
Trees. 
In 
R. 
Lopez 
de 
Mantaras 
and 
D. 
Poole, 
editors, 
Uncertainty 
in 
Articial 
Intelligence, 
number 
10, 
pages 
360{366, 
San 
Francisco, 
CA, 
1994. 
Morgan 
Kaufmann. 
[148] 
F. 
V. 
Jensen 
and 
T. 
D. 
Nielson. 
Bayesian 
Networks 
and 
Decision 
Graphs. 
Springer 
Verlag, 
second 
edition, 
2007. 
[149] 
M. 
I. 
Jordan 
and 
R. 
A. 
Jacobs. 
Hierarchical 
mixtures 
of 
experts 
and 
the 
EM 
algorithm. 
Neural 
Computation, 
6:181{214, 
1994. 
[150] 
B. 
H. 
Juang, 
W. 
Chou, 
and 
C. 
H. 
Lee. 
Minimum 
classication 
error 
rate 
methods 
for 
speech 
recognition. 
IEEE 
Transactions 
on 
Speech 
and 
Audio 
Processing, 
5:257{265, 
1997. 
[151] 
L. 
P. 
Kaelbling, 
M. 
L. 
Littman, 
and 
A. 
R. 
Cassandra. 
Planning 
and 
acting 
in 
partially 
observable 
stochastic 
domains. 
Articial 
Intelligence, 
101(1-2):99{134, 
1998. 
[152] 
H. 
J. 
Kappen. 
An 
introduction 
to 
stochastic 
control 
theory, 
path 
integrals 
and 
reinforcement 
learning. 
In 
Proceedings 
9th 
Granada 
seminar 
on 
Computational 
Physics: 
Computational 
and 
Mathematical 
Modeling 
of 
Cooperative 
Behavior 
in 
Neural 
Systems, 
volume 
887, 
pages 
149{181. 
American 
Institute 
of 
Physics, 
2007. 


[153] 
H. 
J. 
Kappen 
and 
F. 
B. 
Rodrguez. 
Ecient 
learning 
in 
Boltzmann 
machines 
using 
linear 
response 
theory. 
Neural 
Compution, 
10(5):1137{1156, 
1998. 
[154] 
H. 
J. 
Kappen 
and 
W. 
Wiegerinck. 
Novel 
iteration 
schemes 
for 
the 
Cluster 
Variation 
Method. 
In 
T. 
G. 
Dietterich, 
S. 
Becker, 
and 
Z. 
Ghahramani, 
editors, 
Advances 
in 
Neural 
Information 
Processing 
Systems 
(NIPS), 
number 
14, 
pages 
415{422, 
Cambridge, 
MA, 
2002. 
MIT 
Press. 
DRAFT 
March 
9, 
2010 



BIBLIOGRAPHY 
BIBLIOGRAPHY 


[155] 
Y. 
Karklin 
and 
M. 
S. 
Lewicki. 
Emergence 
of 
complex 
cell 
properties 
by 
learning 
to 
generalize 
in 
natural 
scenes. 
Nature, 
(457):83{86, 
November 
2008. 
[156] 
G. 
Karypis 
and 
V. 
Kumar. 
A 
fast 
and 
high 
quality 
multilevel 
scheme 
for 
partitioning 
irregular 
graphs. 
Siam 
Journal 
on 
Scientic 
Computing, 
20(1):359{392, 
1998. 
[157] 
P. 
W. 
Kasteleyn. 
Dimer 
Statistics 
and 
Phase 
Transitions. 
Journal 
of 
Mathematical 
Physics, 
4(2):287{293, 
1963. 
[158] 
S. 
A. 
Kauman. 
At 
Home 
in 
the 
Universe: 
The 
Search 
for 
Laws 
of 
Self-Organization 
and 
Complexity. 
Oxford 
University 
Press, 
Oxford, 
UK, 
1995. 
[159] 
C-J. 
Kim. 
Dynamic 
linear 
models 
with 
Markov-switching. 
Journal 
of 
Econometrics, 
60:1{22, 
1994. 
[160] 
C-J. 
Kim 
and 
C. 
R. 
Nelson. 
State-Space 
models 
with 
regime 
switching. 
MIT 
Press, 
1999. 
[161] 
G. 
Kitagawa. 
The 
Two-Filter 
Formula 
for 
Smoothing 
and 
an 
implementation 
of 
the 
Gaussian-sum 
smoother. 
Annals 
of 
the 
Institute 
of 
Statistical 
Mathematics, 
46(4):605{623, 
1994. 
[162] 
U. 
B. 
Kjaerulff 
and 
A. 
L. 
Madsen. 
Bayesian 
Networks 
and 
Inuence 
Diagrams 
: 
A 
Guide 
to 
Construction 
and 
Analysis. 
Springer, 
2008. 
[163] 
A. 
Krogh, 
M. 
Brown, 
I. 
Mian, 
K. 
Sjolander, 
and 
D. 
Haussler. 
Hidden 
Markov 
models 
in 
computational 
biology: 
Applications 
to 
protein 
modeling. 
Journal 
of 
Molecular 
Biology, 
235:1501{1531, 
1994. 
[164] 
S. 
Kullback. 
Information 
Theory 
and 
Statistics. 
Dover, 
1968. 
[165] 
K. 
Kurihara, 
M. 
Welling, 
and 
Y. 
W. 
Teh. 
Collapsed 
Variational 
Dirichlet 
Process 
Mixture 
Models. 
In 
Proceedings 
of 
the 
International 
Joint 
Conference 
on 
Articial 
Intelligence, 
volume 
20, 
pages 
2796{2801, 
2007. 
[166] 
J. 
Laerty, 
A. 
McCallum, 
and 
F. 
Pereira. 
Conditional 
random 
elds: 
Probabilistic 
models 
for 
segmenting 
and 
labeling 
sequence 
data. 
In 
C. 
E. 
Brodley 
and 
A. 
P. 
Danyluk, 
editors, 
International 
Conference 
on 
Machine 
Learning, 
number 
18, 
pages 
282{289, 
San 
Francisco, 
CA, 
2001. 
Morgan 
Kaufmann. 
[167] 
H. 
Lass. 
Elements 
of 
Pure 
and 
Applied 
Mathematics. 
McGraw-Hill 
(reprinted 
by 
Dover), 
1957. 
[168] 
S. 
L. 
Lauritzen. 
Graphical 
Models. 
Oxford 
University 
Press, 
1996. 
[169] 
S. 
L. 
Lauritzen, 
A. 
P. 
Dawid, 
B. 
N. 
Larsen, 
and 
H-G. 
Leimer. 
Independence 
properties 
of 
directed 
Markov 
elds. 
Networks, 
20:491{505, 
1990. 
[170] 
S. 
L. 
Lauritzen 
and 
D. 
J. 
Spiegelhalter. 
Local 
computations 
with 
probabilities 
on 
graphical 
structures 
and 
their 
application 
to 
expert 
systems. 
Journal 
of 
Royal 
Statistical 
Society 
B, 
50(2):157 
– 
224, 
1988. 
[171] 
D. 
D. 
Lee 
and 
H. 
S. 
Seung. 
Algorithms 
for 
non-negative 
matrix 
factorization. 
In 
T. 
K. 
Leen, 
T. 
G. 
Dietterich, 
and 
V. 
Tresp, 
editors, 
Advances 
in 
Neural 
Information 
Processing 
Systems 
(NIPS), 
number 
13, 
pages 
556{562, 
Cambridge, 
MA, 
2001. 
MIT 
Press. 
[172] 
M. 
A. 
R. 
Leisink 
and 
H. 
J. 
Kappen. 
A 
Tighter 
Bound 
for 
Graphical 
Models. 
In 
Neural 
Computation, 
volume 
13, 
pages 
2149{2171. 
MIT 
Press, 
2001. 
[173] 
V. 
Lepar 
and 
P. 
P. 
Shenoy. 
A 
Comparison 
of 
Lauritzen-Spiegelhalter, 
Hugin, 
and 
Shenoy-Shafer 
Architectures 
for 
Computing 
Marginals 
of 
Probability 
Distributions. 
In 
G. 
Cooper 
and 
S. 
Moral, 
editors, 
Uncertainty 
in 
Articial 
Intelligence, 
number 
14, 
pages 
328{333, 
San 
Francisco, 
CA, 
1998. 
Morgan 
Kaufmann. 
[174] 
U. 
Lerner, 
R. 
Parr, 
D. 
Koller, 
and 
G. 
Biswas. 
Bayesian 
Fault 
Detection 
and 
Diagnosis 
in 
Dynamic 
Systems. 
In 
Proceedings 
of 
the 
Seventeenth 
National 
Conference 
on 
Articial 
Intelligence 
(AIII-00), 
pages 
531{537, 
2000. 
[175] 
U. 
N. 
Lerner. 
Hybrid 
Bayesian 
Networks 
for 
Reasoning 
about 
Complex 
Systems. 
Computer 
science 
department, 
Stanford 
University, 
2002. 
[176] 
R. 
Linsker. 
Improved 
local 
learning 
rule 
for 
information 
maximization 
and 
related 
applications. 
Neural 
Networks, 
18(3):261{265, 
2005. 
[177] 
Y. 
L. 
Loh, 
E. 
W. 
Carlson, 
and 
M. 
Y. 
J. 
Tan. 
Bond-propagation 
algorithm 
for 
thermodynamic 
functions 
in 
general 
two-dimensional 
Ising 
models. 
Physical 
Review 
B, 
76(1):014404, 
2007. 
[178] 
H. 
Lopes 
and 
M. 
West. 
Bayesian 
model 
assessment 
in 
factor 
analysis. 
Statistica 
Sinica, 
(14):41{67, 
2003. 
DRAFT 
March 
9, 
2010 



BIBLIOGRAPHY 
BIBLIOGRAPHY 


[179] 
T. 
J. 
Loredo. 
From 
Laplace 
To 
Supernova 
Sn 
1987A: 
Bayesian 
Inference 
In 
Astrophysics. 
In 
P.F. 
Fougere, 
editor, 
Maximum 
Entropy 
and 
Bayesian 
Methods, 
pages 
81{142. 
Kluwer, 
1990. 
[180] 
D. 
J. 
C. 
MacKay. 
Bayesian 
interpolation. 
Neural 
Computation, 
4(3):415{447, 
1992. 
[181] 
D. 
J. 
C. 
MacKay. 
Probable 
Networks 
and 
plausisble 
predictions 
– 
a 
review 
of 
practical 
Bayesian 
methods 
for 
supervised 
neural 
networks. 
Network: 
Computation 
in 
Neural 
Systems, 
6(3):469{505, 
1995. 
[182] 
D. 
J. 
C. 
MacKay. 
Introduction 
to 
Gaussian 
Processes. 
In 
Neural 
Networks 
and 
Machine 
Learning, 
volume 
168 
of 
NATO 
advanced 
study 
institute 
on 
generalization 
in 
neural 
networks 
and 
machine 
learning, 
pages 
133{165. 
Springer, 
August 
1998. 
[183] 
D. 
J. 
C. 
MacKay. 
Information 
Theory, 
Inference 
and 
Learning 
Algorithms. 
Cambridge 
University 
Press, 
2003. 
[184] 
U. 
Madhow. 
Fundamentals 
of 
Digital 
Communication. 
Cambridge 
University 
Press, 
2008. 
[185] 
K. 
V. 
Mardia, 
J. 
T. 
Kent, 
and 
J. 
M. 
Bibby. 
Multivariate 
Analysis. 
Academic 
Press, 
1997. 
[186] 
H. 
Markram, 
J. 
Lubke, 
M. 
Frotscher, 
and 
B. 
Sakmann. 
Regulation 
of 
synaptic 
ecacy 
by 
coincidence 
of 
postsynaptic 
APs 
and 
EPSPs. 
Science, 
275:213{215, 
1997. 
[187] 
G. 
McLachlan 
and 
T. 
Krishnan. 
The 
EM 
Algorithm 
and 
Extensions. 
John 
Wiley 
and 
Sons, 
1997. 
[188] 
G. 
McLachlan 
and 
D. 
Peel. 
Finite 
Mixture 
Models. 
Wiley 
Series 
in 
Probability 
and 
Statistics. 
Wiley-
Interscience, 
2000. 
[189] 
E. 
Meeds, 
Z. 
Ghahramani, 
R. 
M. 
Neal, 
and 
S. 
T. 
Roweis. 
Modeling 
Dyadic 
Data 
with 
Binary 
Latent 
Factors. 
In 
B. 
Scholkopf, 
J. 
Platt, 
and 
T. 
Homan, 
editors, 
Advances 
in 
Neural 
Information 
Processing 
Systems 
(NIPS), 
volume 
19, 
pages 
977{984, 
Cambridge, 
MA, 
2007. 
MIT 
Press. 
[190] 
M. 
Meila. 
An 
Accelerated 
Chow 
and 
Liu 
Algorithm: 
Fitting 
Tree 
Distributions 
to 
High-Dimensional 
Sparse 
Data. 
In 
I. 
Bratko, 
editor, 
International 
Conference 
on 
Machine 
Learning, 
pages 
249{257, 
San 
Francisco, 
CA, 
1999. 
Morgan 
Kaufmann. 
[191] 
M. 
Meila 
and 
M. 
I. 
Jordan. 
Triangulation 
by 
continuous 
embedding. 
In 
M. 
C. 
Mozer, 
M. 
I. 
Jordan, 
and 
T. 
Petsche, 
editors, 
Advances 
in 
Neural 
Information 
Processing 
Systems 
(NIPS), 
number 
9, 
pages 
557{563, 
Cambridge, 
MA, 
1997. 
MIT 
Press. 
[192] 
B. 
Mesot 
and 
D. 
Barber. 
Switching 
Linear 
Dynamical 
Systems 
for 
Noise 
Robust 
Speech 
Recognition. 
IEEE 
Transactions 
of 
Audio, 
Speech 
and 
Language 
Processing, 
15(6):1850{1858, 
2007. 
[193] 
N. 
Meuleau, 
M. 
Hauskrecht, 
K-E. 
Kim, 
L. 
Peshkin, 
Kaelbling. 
L. 
P., 
T. 
Dean, 
and 
C. 
Boutilier. 
Solving 
Very 
Large 
Weakly 
Coupled 
Markov 
Decision 
Processes. 
In 
Proceedings 
of 
the 
Fifteenth 
National 
Conference 
on 
Articial 
Intelligence, 
pages 
165{172, 
1998. 
[194] 
T. 
Mills. 
The 
Econometric 
Modelling 
of 
Financial 
Time 
Series. 
Cambridge 
University 
Press, 
2000. 
[195] 
T. 
Minka. 
Expectation 
Propagation 
for 
approximate 
Bayesian 
inference. 
In 
J. 
Breese 
and 
D. 
Koller, 
editors, 
Uncertainty 
in 
Articial 
Intelligence, 
number 
17, 
pages 
362{369, 
San 
Francisco, 
CA, 
2001. 
Morgan 
Kaufmann. 
[196] 
T. 
Minka. 
A 
comparison 
of 
numerical 
optimizers 
for 
logistic 
regression. 
Technical 
report, 
Microsoft 
Research, 
2003. 
research.microsoft.com/minka/papers/logreg. 
[197] 
T. 
Minka. 
Divergence 
measures 
and 
message 
passing. 
Technical 
Report 
MSR-TR-2005-173, 
Microsoft 
Research 
Ltd., 
Cambridge, 
UK, 
December 
2005. 
[198] 
A. 
Mira, 
J. 
Mller, 
and 
G. 
O. 
Roberts. 
Perfect 
slice 
samplers. 
Journal 
of 
the 
Royal 
Statistical 
Society, 
63(3):593{606, 
2001. 
Series 
B 
(Statistical 
Methodology). 
[199] 
C. 
Mitchell, 
M. 
Harper, 
and 
L. 
Jamieson. 
On 
the 
complexity 
of 
explicit 
duration 
HMM's. 
Speech 
and 
Audio 
Processing, 
IEEE 
Transactions 
on, 
3(3):213{217, 
May 
1995. 
[200] 
T. 
Mitchell. 
Machine 
Learning. 
McGraw-Hill, 
1997. 
[201] 
J. 
Mooij 
and 
H. 
J. 
Kappen. 
Sucient 
conditions 
for 
convergence 
of 
Loopy 
Belief 
Propagation. 
IEEE 
Inform
ation 
Theory, 
53:4422{4437, 
2007. 
[202] 
A. 
Moore. 
A 
tutorial 
on 
kd-trees. 
Technical 
report, 
1991. 
Available 
from 
http://www.cs.cmu.edu/awm/papers.html. 
DRAFT 
March 
9, 
2010 



BIBLIOGRAPHY 
BIBLIOGRAPHY 


[203] 
J. 
Moussouris. 
Gibbs 
and 
Markov 
Random 
Systems 
with 
Constraints. 
Journal 
of 
Statistical 
Physics, 
10:11{33, 
1974. 
[204] 
R. 
M. 
Neal. 
Connectionist 
Learning 
of 
Belief 
Networks. 
Articial 
Intelligence, 
56:71{113, 
1992. 
[205] 
R. 
M. 
Neal. 
Probabilistic 
inference 
using 
Markov 
Chain 
Monte 
Carlo 
methods. 
CRG-TR-93-1, 
Dept. 
of 
Computer 
Science, 
University 
of 
Toronto, 
1993. 
[206] 
R. 
M. 
Neal. 
Markov 
Chain 
Sampling 
Methods 
for 
Dirichlet 
Process 
Mixture 
Models. 
Journal 
of 
Computational 
and 
Graphical 
Statistics, 
9(2):249{265, 
2000. 
[207] 
R. 
M. 
Neal. 
Slice 
sampling. 
Annals 
of 
Statistics, 
31:705{767, 
2003. 
[208] 
R. 
E. 
Neapolitan. 
Learning 
Bayesian 
Networks. 
Prentice 
Hall, 
2003. 
[209] 
A. 
V. 
Nean, 
Luhong 
L., 
Xiaobo 
P., 
Liu 
X., 
C. 
Mao, 
and 
K. 
Murphy. 
A 
coupled 
HMM 
for 
audio-visual 
speech 
recognition. 
In 
IEEE 
International 
Conference 
on 
Acoustics, 
Speech, 
and 
Signal 
Processing, 
volume 
2, 
pages 
2013{2016, 
2002. 
[210] 
D. 
Nilsson. 
An 
ecient 
algorithm 
for 
nding 
the 
m 
most 
probable 
congurations 
in 
a 
probabilistic 
expert 
system. 
Statistics 
and 
Computing, 
8:159{173, 
1998. 
[211] 
D. 
Nilsson 
and 
J. 
Goldberger. 
Sequentially 
nnding 
the 
N-best 
list 
in 
Hidden 
Markov 
Models. 
Internation 
Joint 
Conference 
on 
Articial 
Intelligence 
(IJCAI), 
17, 
2001. 
[212] 
A. 
B. 
Noviko. 
On 
convergence 
proofs 
on 
perceptrons. 
In 
Symposium 
on 
the 
Mathematical 
Theory 
of 
Automata 
(New 
York, 
1962), 
volume 
12, 
pages 
615{622, 
Brooklyn, 
N.Y., 
1963. 
Polytechnic 
Press 
of 
Polytechnic 
Institute 
of 
Brooklyn. 
[213] 
F. 
J. 
Och 
and 
H. 
Ney. 
Discriminative 
training 
and 
maximum 
entropy 
models 
for 
statistical 
machine 
translation. 
In 
Proceedings 
of 
the 
Annual 
Meeting 
of 
the 
Association 
for 
Computational 
Linguistics, 
pages 
295{302, 
Philadelphia, 
July 
2002. 
[214] 
B. 
A. 
Olshausen 
and 
D. 
J. 
Field. 
Sparse 
coding 
with 
an 
overcomplete 
basis 
set: 
A 
strategy 
employed 
by 
V1? 
Vision 
Research, 
37:3311{3325, 
1998. 
[215] 
A. 
V. 
Oppenheim, 
R. 
W. 
Shafer, 
M. 
T. 
Yoder, 
and 
W. 
T. 
Padgett. 
Discrete-Time 
Signal 
Processing. 
Prentice 
Hall, 
third 
edition, 
2009. 
[216] 
M. 
Ostendorf, 
V. 
Digalakis, 
and 
O. 
A. 
Kimball. 
From 
HMMs 
to 
Segment 
Models: 
A 
Unied 
View 
of 
Stochastic 
Modeling 
for 
Speech 
Recognition. 
IEEE 
Transactions 
on 
Speech 
and 
Audio 
Processing, 
4:360{378, 
1995. 
[217] 
P. 
Paatero 
and 
U. 
Tapper. 
Positive 
matrix 
factorization: 
A 
non-negative 
factor 
model 
with 
optimal 
utilization 
of 
error 
estimates 
of 
data 
values. 
Environmetrics, 
5:111{126, 
1994. 
[218] 
V. 
Pavlovic, 
J. 
M. 
Rehg, 
and 
J. 
MacCormick. 
Learning 
switching 
linear 
models 
of 
human 
motion. 
In 
T. 
K. 
Leen, 
T. 
G. 
Dietterich, 
and 
V. 
Tresp, 
editors, 
Advances 
in 
Neural 
Information 
Processing 
Systems 
(NIPS), 
number 
13, 
pages 
981{987, 
Cambridge, 
MA, 
2001. 
MIT 
Press. 
[219] 
J. 
Pearl. 
Probabilistic 
Reasoning 
in 
Intelligent 
Systems 
: 
Networks 
of 
Plausible 
Inference. 
Morgan 
Kaufmann, 
1988. 
[220] 
J. 
Pearl. 
Causality: 
Models, 
Reasoning 
and 
Inference. 
Cambridge 
University 
Press, 
2000. 
[221] 
B. 
A. 
Pearlmutter 
and 
L. 
C. 
Parra. 
Maximum 
Likelihood 
Blind 
Source 
Separation: 
A 
Context-Sensitive 
Generalization 
of 
ICA. 
In 
M. 
C. 
Mozer, 
M. 
I. 
Jordan, 
and 
T. 
Petsche, 
editors, 
Advances 
in 
Neural 
Information 
Processing 
Systems 
(NIPS), 
number 
9, 
pages 
613{619, 
Cambridge, 
MA, 
1997. 
MIT 
Press. 
[222] 
K. 
B. 
Petersen 
and 
O. 
Winther. 
The 
EM 
algorithm 
in 
independent 
component 
analysis. 
In 
IEEE 
International 
Conference 
on 
Acoustics, 
Speech, 
and 
Signal 
Processing, 
volume 
5, 
pages 
169{172, 
2005. 
[223] 
J-P. 
Pster, 
T. 
Toyiozumi, 
D. 
Barber, 
and 
W. 
Gerstner. 
Optimal 
Spike-Timing 
Dependent 
Plasticity 
for 
Precise 
Action 
Potential 
Firing 
in 
Supervised 
learning. 
Neural 
Computation, 
18:1309{1339, 
2006. 
[224] 
J. 
Platt. 
Fast 
Training 
of 
Support 
Vector 
Machines 
Using 
Sequential 
Minimal 
Optimization. 
In 
B. 
Scholkopf, 
C. 
J. 
C. 
Burges, 
and 
A. 
J. 
Smola, 
editors, 
Advances 
in 
Kernel 
Methods 
-Support 
Vector 
Learning, 
pages 
185{208. 
MIT 
Press, 
1999. 
DRAFT 
March 
9, 
2010 



BIBLIOGRAPHY 
BIBLIOGRAPHY 


[225] 
I. 
Porteous, 
D. 
Newman, 
A. 
Ihler, 
A. 
Asuncion, 
P. 
Smyth, 
and 
M. 
Welling. 
Fast 
collapsed 
Gibbs 
sampling 
for 
Latent 
Dirichlet 
Allocation. 
In 
KDD 
'08: 
Proceeding 
of 
the 
14th 
ACM 
SIGKDD 
international 
conference 
on 
Knowledge 
discovery 
and 
data 
mining, 
pages 
569{577, 
New 
York, 
NY, 
USA, 
2008. 
ACM. 
[226] 
J. 
E. 
Potter 
and 
R. 
G. 
Stern. 
Statistical 
ltering 
of 
space 
navigation 
measurements. 
In 
American 
Institute 
of 
Aeronautics 
and 
Astronautics 
Guidance 
and 
Control 
Conference, 
volume 
13, 
pages 
775{801, 
Cambridge, 
Mass., 
August 
1963. 
[227] 
W. 
Press, 
W. 
Vettering, 
S. 
Teukolsky, 
and 
B. 
Flannery. 
Numerical 
Recipes 
in 
Fortran. 
Cambridge 
University 
Press, 
1992. 
[228] 
S. 
J. 
D. 
Prince 
and 
J. 
H. 
Elder. 
Probabilistic 
Linear 
Discriminant 
Analysis 
for 
Inferences 
About 
Identity. 
In 
IEEE 
11th 
International 
Conference 
on 
Computer 
Vision 
ICCV, 
pages 
1{8, 
2007. 
[229] 
L. 
R. 
Rabiner. 
A 
tutorial 
on 
hidden 
Markov 
models 
and 
selected 
applications 
in 
speech 
recognition. 
Proc. 
of 
the 
IEEE, 
77(2):257{286, 
1989. 
[230] 
C. 
E. 
Rasmussen 
and 
C. 
K. 
I. 
Williams. 
Gaussian 
Processes 
for 
Machine 
Learning. 
MIT 
Press, 
2006. 
[231] 
H. 
E. 
Rauch, 
G. 
Tung, 
and 
C. 
T. 
Striebel. 
Maximum 
Likelihood 
estimates 
of 
linear 
dynamic 
systems. 
American 
Institute 
of 
Aeronautics 
and 
Astronautics 
Journal 
(AIAAJ), 
3(8):1445{1450, 
1965. 
[232] 
T. 
Richardson 
and 
P. 
Spirtes. 
Ancestral 
Graph 
Markov 
Models. 
Annals 
of 
Statistics, 
30(4):962{1030, 
2002. 
[233] 
D. 
Rose, 
R. 
E. 
Tarjan, 
and 
E. 
S. 
Lueker. 
Algorithmic 
aspects 
of 
vertex 
elimination 
of 
graphs. 
SIAM 
Journal 
on 
Computing, 
(5):266{283, 
1976. 
[234] 
F. 
Rosenblatt. 
The 
Perceptron: 
A 
Probabilistic 
Model 
for 
Information 
Storage 
and 
Organization 
in 
the 
Brain. 
Psychological 
Review, 
65(6):386{408, 
1958. 
[235] 
D. 
B. 
Rubin. 
Using 
the 
SIR 
algorithm 
to 
simulate 
posterior 
distributions. 
In 
M. 
H. 
Bernardo, 
K. 
M. 
Degroot, 
D. 
V. 
Lindley, 
and 
A. 
F. 
M. 
Smith, 
editors, 
Bayesian 
Statistics 
3. 
Oxford 
University 
Press, 
1988. 
[236] 
D. 
Saad 
and 
M. 
Opper. 
Advanced 
Mean 
Field 
Methods 
Theory 
and 
Practice. 
MIT 
Press, 
2001. 
[237] 
R. 
Salakhutdinov, 
S. 
Roweis, 
and 
Z. 
Ghahramani. 
Optimization 
with 
EM 
and 
Expectation-Conjugate-
Gradient. 
In 
T. 
Fawcett 
and 
N. 
Mishra, 
editors, 
International 
Conference 
on 
Machine 
Learning, 
number 
20, 
pages 
672{679, 
Menlo 
Park, 
CA, 
2003. 
AAAI 
Press. 
[238] 
L. 
K. 
Saul, 
T. 
S. 
Jaakkola, 
and 
M. 
J. 
Jordan. 
Mean 
eld 
theory 
for 
sigmoid 
belief 
networks. 
Journal 
of 
Articial 
Intelligence 
Research, 
4:61{76, 
1996. 
[239] 
L. 
K. 
Saul 
and 
M. 
I. 
Jordan. 
Exploiting 
tractable 
substructures 
in 
intractable 
networks. 
In 
D. 
S. 
Touretzky, 
M. 
Mozer, 
and 
M. 
E. 
Hasselmo, 
editors, 
Advances 
in 
Neural 
Information 
Processing 
Systems 
(NIPS), 
number 
8, 
pages 
486{492, 
Cambridge, 
MA, 
1996. 
MIT 
Press. 
[240] 
L. 
Savage. 
The 
Foundations 
of 
Statistics. 
Wiley, 
1954. 
[241] 
R. 
D. 
Schachter. 
Bayes-ball: 
The 
rational 
pastime 
(for 
determining 
irrelevance 
and 
requisite 
information 
in 
belief 
networks 
and 
inuence 
diagrams). 
In 
G. 
Cooper 
and 
S. 
Moral, 
editors, 
Uncertainty 
in 
Articial 
Intelligence, 
number 
14, 
pages 
480{487, 
San 
Francisco, 
CA, 
1998. 
Morgan 
Kaufmann. 
[242] 
B. 
Scholkopf, 
A. 
Smola, 
and 
K. 
R. 
Muller. 
Nonlinear 
Component 
Analysis 
as 
a 
Kernel 
Eigenvalue 
Problem. 
Neural 
Computation, 
10:1299{1319, 
1998. 
[243] 
N. 
N. 
Schraudolph 
and 
D. 
Kamenetsky. 
Ecient 
Exact 
Inference 
in 
Planar 
Ising 
Models. 
In 
D. 
Koller, 
D. 
Schuurmans, 
Y. 
Bengio, 
and 
L. 
Bottou, 
editors, 
Advances 
in 
Neural 
Information 
Processing 
Systems 
(NIPS), 
number 
21, 
pages 
1417{1424, 
Cambridge, 
MA, 
2009. 
MIT 
Press. 
[244] 
E. 
Schwarz. 
Estimating 
the 
dimension 
of 
a 
model. 
Annals 
of 
Statistics, 
6(2):461{464, 
1978. 
[245] 
M. 
Seeger. 
Gaussian 
Processes 
for 
Machine 
Learning. 
International 
Journal 
of 
Neural 
Systems, 
14(2):69{106, 
2004. 
[246] 
M. 
Seeger. 
Expectation 
propagation 
for 
exponential 
families. 
Technical 
report, 
Department 
of 
EECS, 
Berkeley, 
2005. 
www.kyb.tuebingen.mpg.de/bs/people/seeger. 
DRAFT 
March 
9, 
2010 



BIBLIOGRAPHY 
BIBLIOGRAPHY 


[247] 
M. 
Seeger 
and 
H. 
Nickisch. 
Large 
scale 
variational 
inference 
and 
experimental 
design 
for 
sparse 
generalized 
linear 
models. 
Technical 
Report 
175, 
Max 
Planck 
Institute 
for 
Biological 
Cybernetics, 
September 
2008. 
[248] 
J. 
Shawe-Taylor 
and 
N. 
Cristianini. 
Kernel 
Methods 
for 
Pattern 
Analysis. 
Cambridge 
University 
Press, 
2004. 
[249] 
S. 
Siddiqi, 
B. 
Boots, 
and 
G. 
Gordon. 
A 
Constraint 
Generation 
Approach 
to 
Learning 
Stable 
Linear 
Dynamical 
Systems. 
In 
J. 
C. 
Platt, 
D. 
Koller, 
Y. 
Singer, 
and 
S. 
Roweis, 
editors, 
Advances 
in 
Neural 
Information 
Processing 
Systems 
(NIPS), 
number 
20, 
pages 
1329{1336, 
Cambridge, 
MA, 
2008. 
MIT 
Press. 
[250] 
T. 
Silander, 
P. 
Kontkanen, 
and 
P. 
Myllymaki. 
On 
Sensitivity 
of 
the 
MAP 
Bayesian 
Network 
Structure 
to 
the 
Equivalent 
Sample 
Size 
Parameter. 
In 
R. 
Parr 
and 
L. 
van 
der 
Gaag, 
editors, 
Uncertainty 
in 
Articial 
Intelligence, 
number 
23, 
pages 
360{367, 
Corvallis, 
Oregon, 
USA, 
2007. 
AUAI 
press. 
[251] 
S. 
S. 
Skiena. 
The 
algorithm 
design 
manual. 
Springer-Verlag, 
New 
York, 
USA, 
1998. 
[252] 
E. 
Smith 
and 
M. 
S. 
Lewicki. 
Ecient 
auditory 
coding. 
Nature, 
439(7079):978{982, 
2006. 
[253] 
P. 
Smolensky. 
Parallel 
Distributed 
Processing: 
Volume 
1: 
Foundations, 
chapter 
Information 
processing 
in 
dynamical 
systems: 
Foundations 
of 
harmony 
theory, 
pages 
194{281. 
MIT 
Press, 
Cambridge, 
MA, 
1986. 
[254] 
G. 
Sneddon. 
Studies 
in 
the 
atmospheric 
sciences, 
chapter 
A 
Statistical 
Perspective 
on 
Data 
Assimilation 
in 
Numerical 
Models. 
Number 
144 
in 
Lecture 
Notes 
in 
Statistics. 
Springer-Verlag, 
2000. 
[255] 
P. 
Sollich. 
Bayesian 
Methods 
for 
Support 
Vector 
Machines: 
Evidence 
and 
Predictive 
Class 
Probabilities. 
Machine 
Learning, 
46(1-3):21{52, 
2002. 
[256] 
D. 
X. 
Song, 
D. 
Wagner, 
and 
X. 
Tian. 
Timing 
Analysis 
of 
Keystrokes 
and 
Timing 
Attacks 
on 
SSH. 
In 
Proceedings 
of 
the 
10th 
conference 
on 
USENIX 
Security 
Symposium. 
USENIX 
Association, 
2001. 
[257] 
A. 
S. 
Spanias. 
Speech 
coding: 
a 
tutorial 
review. 
Proceedings 
of 
the 
IEEE, 
82(10):1541{1582, 
Oct 
1994. 
[258] 
D. 
J. 
Spiegelhalter, 
A. 
P. 
Dawid, 
S. 
L. 
Lauritzen, 
and 
R. 
G. 
Cowell. 
Bayesian 
analysis 
in 
expert 
systems. 
Statistical 
Science, 
8(3):219{247, 
1993. 
[259] 
P. 
Spirtes, 
C. 
Glymour, 
and 
R. 
Scheines. 
Causation, 
Prediction, 
and 
Search. 
MIT 
press, 
2 
edition, 
2000. 
[260] 
N. 
Srebro. 
Maximum 
Likelihood 
Bounded 
Tree-Width 
Markov 
Networks. 
In 
J. 
Breese 
and 
D. 
Koller, 
editors, 
Uncertainty 
in 
Articial 
Intelligence, 
number 
17, 
pages 
504{511, 
San 
Francisco, 
CA, 
2001. 
Morgan 
Kaufmann. 
[261] 
H. 
Steck. 
Constraint-Based 
Structural 
Learning 
in 
Bayesian 
Networks 
using 
Finite 
Data 
Sets. 
PhD 
thesis, 
Technical 
University 
Munich, 
2001. 
[262] 
H. 
Steck. 
Learning 
the 
Bayesian 
Network 
Structure: 
Dirichlet 
Prior 
vs 
Data. 
In 
D. 
A. 
McAllester 
and 
P. 
Myllymaki, 
editors, 
Uncertainty 
in 
Articial 
Intelligence, 
number 
24, 
pages 
511{518, 
Corvallis, 
Oregon, 
USA, 
2008. 
AUAI 
press. 
[263] 
H. 
Steck 
and 
T. 
Jaakkola. 
On 
the 
Dirichlet 
prior 
and 
Bayesian 
regularization. 
In 
S. 
Becker, 
S. 
Thrun, 
and 
K. 
Obermayer, 
editors, 
NIPS, 
pages 
697{704. 
MIT 
Press, 
2002. 
[264] 
M. 
Studeny. 
On 
mathematical 
description 
of 
probabilistic 
conditional 
independence 
structures. 
PhD 
thesis, 
Academy 
of 
Sciences 
of 
the 
Czech 
Republic, 
2001. 
[265] 
M. 
Studeny. 
On 
non-graphical 
description 
of 
models 
of 
conditional 
independence 
structure. 
In 
HSSS 
Workshop 
on 
Stochastic 
Systems 
for 
Individual 
Behaviours. 
Louvain 
la 
Neueve, 
Belgium, 
22-23 
January 
2001. 
[266] 
C. 
Sutton 
and 
A. 
McCallum. 
An 
introduction 
to 
conditional 
random 
elds 
for 
relational 
learning. 
In 
L. 
Getoor 
and 
B. 
Taskar, 
editors, 
Introduction 
to 
Statistical 
Relational 
Learning. 
MIT 
press, 
2006. 
[267] 
R. 
S. 
Sutton 
and 
A. 
G. 
Barto. 
Reinforcement 
Learning: 
An 
Introduction. 
MIT 
Press, 
1998. 
[268] 
R. 
J. 
Swendsen 
and 
J-S. 
Wang. 
Nonuniversal 
critical 
dynamics 
in 
Monte 
Carlo 
simulations. 
Physical 
Review 
Letters, 
58:86{88, 
1987. 
[269] 
B. 
K. 
Sy. 
A 
Recurrence 
Local 
Computation 
Approach 
Towards 
Ordering 
Composite 
Beliefs 
in 
Bayesian 
Belief 
Networks. 
International 
Journal 
of 
Approximate 
Reasoning, 
8:17{50, 
1993. 
[270] 
T. 
Sejnowski. 
The 
Book 
of 
Hebb. 
Neuron, 
24:773{776, 
1999. 
DRAFT 
March 
9, 
2010 



BIBLIOGRAPHY 
BIBLIOGRAPHY 


[271] 
R. 
E. 
Tarjan 
and 
M. 
Yannakakis. 
Simple 
linear-time 
algorithms 
to 
test 
chordality 
of 
graphs, 
test 
acyclicity 
of 
hypergraphs, 
and 
selectively 
reduce 
acyclic 
hypergraphs. 
SIAM 
Journal 
on 
Computing, 
13(3):566{579, 
1984. 
[272] 
S. 
J. 
Taylor. 
Modelling 
Financial 
Time 
Series. 
World 
Scientic, 
second 
edition, 
2008. 
[273] 
Y. 
W. 
Teh, 
D. 
Newman, 
and 
M. 
Welling. 
A 
Collapsed 
Variational 
Bayesian 
Inference 
Algorithm 
for 
Latent 
Dirichlet 
Allocation. 
In 
J. 
C. 
Platt, 
D. 
Koller, 
Y. 
Singer, 
and 
S. 
Roweis, 
editors, 
Advances 
in 
Neural 
Information 
Processing 
Systems 
(NIPS), 
number 
20, 
pages 
1481{1488, 
Cambridge, 
MA, 
2008. 
MIT 
Press. 
[274] 
Y. 
W. 
Teh 
and 
M. 
Welling. 
The 
unied 
propagation 
and 
scaling 
algorithm. 
In 
T. 
G. 
Dietterich, 
S. 
Becker, 
and 
Z. 
Ghahramani, 
editors, 
Advances 
in 
Neural 
Information 
Processing 
Systems 
(NIPS), 
number 
14, 
pages 
953{960, 
Cambridge, 
MA, 
2002. 
MIT 
Press. 
[275] 
M. 
Tipping 
and 
C. 
M. 
Bishop. 
Mixtures 
of 
probabilistic 
principal 
component 
analysers. 
Neural 
Computation, 
11(2):443{482, 
1999. 
[276] 
M. 
E. 
Tipping. 
Sparse 
Bayesian 
learning 
and 
the 
relevance 
vector 
machine. 
Journal 
of 
Machine 
Learning 
Research, 
1:211{244, 
2001. 
[277] 
D. 
M. 
Titterington, 
A. 
F. 
M. 
Smith, 
and 
U. 
E. 
Makov. 
Statistical 
analysis 
of 
nite 
mixture 
distributions. 
Wiley, 
1985. 
[278] 
E. 
Todorov. 
Ecient 
computation 
of 
optimal 
actions. 
Proceedings 
of 
the 
National 
Academy 
of 
Sciences 
of 
the 
United 
States 
of 
America 
(PNAS), 
106(28):11478{11483, 
2009. 
[279] 
M. 
Toussaint, 
S. 
Harmeling, 
and 
A. 
Storkey. 
Probabilistic 
inference 
for 
solving 
(PO)MDPs. 
Research 
Report 
EDI-INF-RR-0934, 
University 
of 
Edinburgh, 
School 
of 
Informatics, 
2006. 
[280] 
M. 
Tsodyks, 
K. 
Pawelzik, 
and 
H. 
Markram. 
Neural 
Networks 
with 
Dynamic 
Synapses. 
Neural 
Computation, 
10:821{835, 
1998. 
[281] 
P. 
Van 
Overschee 
and 
B. 
De 
Moor. 
Subspace 
Identication 
for 
Linear 
Systems; 
Theory, 
Implementations, 
Applications. 
Kluwer, 
1996. 
[282] 
V. 
Vapnik. 
The 
Nature 
of 
Statistical 
Learning 
Theory. 
Springer, 
New 
York, 
1995. 
[283] 
M. 
Verhaegen 
and 
P. 
Van 
Dooren. 
Numerical 
Aspects 
of 
Dierent 
Kalman 
Filter 
Implementations. 
IEEE 
Transactions 
of 
Automatic 
Control, 
31(10):907{917, 
1986. 
[284] 
T. 
Verma 
and 
J. 
Pearl. 
Causal 
networks 
: 
Semantics 
and 
expressiveness. 
In 
R. 
D. 
Schacter, 
T. 
S. 
Levitt, 
L. 
N. 
Kanal, 
and 
J.F. 
Lemmer, 
editors, 
Uncertainty 
in 
Articial 
Intelligence, 
volume 
4, 
pages 
69{76, 
Amsterdam, 
1990. 
North-Holland. 
[285] 
T. 
O. 
Virtanen, 
A. 
T. 
Cemgil, 
and 
S. 
J. 
Godsill. 
Bayesian 
extensions 
to 
nonnegative 
matrix 
factorisation 
for 
audio 
signal 
modelling. 
In 
IEEE 
International 
Conference 
on 
Acoustics, 
Speech, 
and 
Signal 
Processing, 
pages 
1825{1828, 
2008. 
[286] 
G. 
Wahba. 
Support 
Vector 
Machines, 
Repreducing 
Kernel 
Hilbert 
Spaces, 
and 
Randomized 
GACV, 
pages 
69{88. 
MIT 
Press, 
1999. 
[287] 
M. 
J. 
Wainwright 
and 
M. 
I. 
Jordan. 
Graphical 
models, 
exponential 
families, 
and 
variational 
inference. 
Found
ations 
and 
Trends 
in 
Machine 
Learning, 
1(1-2):1{305, 
2008. 
[288] 
H. 
Wallach. 
Ecient 
training 
of 
conditional 
random 
elds. 
Master's 
thesis, 
Division 
of 
Informatics, 
University 
of 
Edinburgh, 
2002. 
[289] 
Y. 
Wang, 
J. 
Hodges, 
and 
B. 
Tang. 
Classication 
of 
Web 
Documents 
Using 
a 
Naive 
Bayes 
Method. 
15th 
IEEE 
International 
Conference 
on 
Tools 
with 
Articial 
Intelligence, 
pages 
560{564, 
2003. 
[290] 
S. 
Waterhouse, 
D. 
Mackay, 
and 
T. 
Robinson. 
Bayesian 
methods 
for 
mixtures 
of 
experts. 
In 
D. 
S. 
Touretzky, 
M. 
Mozer, 
and 
M. 
E. 
Hasselmo, 
editors, 
Advances 
in 
Neural 
Information 
Processing 
Systems 
(NIPS), 
number 
8, 
pages 
351{357, 
Cambridge, 
MA, 
1996. 
MIT 
Press. 
[291] 
Y. 
Weiss 
and 
W. 
T. 
Freeman. 
Correctness 
of 
Belief 
Propagation 
in 
Gaussian 
Graphical 
Models 
of 
Arbitrary 
Topology. 
Neural 
Computation, 
13(10):2173{2200, 
2001. 
DRAFT 
March 
9, 
2010 



BIBLIOGRAPHY 
BIBLIOGRAPHY 


[292] 
M. 
Welling, 
T. 
P. 
Minka, 
and 
Y. 
W. 
Teh. 
Structured 
Region 
Graphs: 
Morphing 
EP 
into 
GBP. 
In 
F. 
Bacchus 
and 
T. 
Jaakkola, 
editors, 
Uncertainty 
in 
Articial 
Intelligence, 
number 
21, 
pages 
609{614, 
Corvallis, 
Oregon, 
USA, 
2005. 
AUAI 
press. 
[293] 
J. 
Whittaker. 
Graphical 
Models 
in 
Applied 
Multivariate 
Statistics. 
John 
Wiley 
& 
Sons, 
1990. 
[294] 
W. 
Wiegerinck. 
Variational 
approximations 
between 
mean 
eld 
theory 
and 
the 
Junction 
Tree 
algorithm. 
In 
C. 
Boutilier 
and 
M. 
Goldszmidt, 
editors, 
Uncertainty 
in 
Articial 
Intelligence, 
number 
16, 
pages 
626{633, 
San 
Francisco, 
CA, 
2000. 
Morgan 
Kaufmann. 
[295] 
W. 
Wiegerinck 
and 
T. 
Heskes. 
Fractional 
Belief 
Propagation. 
In 
S. 
Becker, 
S. 
Thrun, 
and 
K. 
Obermayer, 
editors, 
Advances 
in 
Neural 
Information 
Processing 
Systems 
(NIPS), 
number 
15, 
pages 
438{445, 
Cambridge, 
MA, 
2003. 
MIT 
Press. 
[296] 
C. 
K. 
I. 
Williams. 
Computing 
with 
innite 
networks. 
In 
M. 
C. 
Mozer, 
M. 
I. 
Jordan, 
and 
T. 
Petsche, 
editors, 
Advances 
in 
Neural 
Information 
Processing 
Systems 
NIPS 
9, 
pages 
295{301, 
Cambridge, 
MA, 
1997. 
MIT 
Press. 
[297] 
C. 
K. 
I. 
Williams 
and 
D. 
Barber. 
Bayesian 
classication 
with 
Gaussian 
processes. 
IEEE 
Trans 
Pattern 
Analysis 
and 
Machine 
Intelligence, 
20:1342{1351, 
1998. 
[298] 
C. 
Yanover 
and 
Y. 
Weiss. 
Finding 
the 
M 
Most 
Probable 
Congurations 
Using 
Loopy 
Belief 
Propagation. 
In 
S. 
Thrun, 
L. 
Saul, 
and 
B. 
Scholkopf, 
editors, 
Advances 
in 
Neural 
Information 
Processing 
Systems 
(NIPS), 
number 
16, 
pages 
1457{1464, 
Cambridge, 
MA, 
2004. 
MIT 
Press. 
[299] 
J. 
S. 
Yedidia, 
W. 
T. 
Freeman, 
and 
Y. 
Weiss. 
Constructing 
free-energy 
approximations 
and 
generalized 
belief 
propagation 
algorithms. 
Information 
Theory, 
IEEE 
Transactions 
on, 
51(7):2282{2312, 
July 
2005. 
[300] 
S. 
Young, 
D. 
Kershaw, 
J. 
Odell, 
D. 
Ollason, 
V. 
Valtchev, 
and 
P. 
Woodland. 
The 
HTK 
Book 
Version 
3.0. 
Cambridge 
University 
Press, 
2000. 
[301] 
A. 
L. 
Yuille 
and 
A. 
Rangarajan. 
The 
concave-convex 
procedure. 
Neural 
Computation, 
15(4):915{936, 
2003. 
[302] 
J.-H. 
Zhao, 
P. 
L. 
H. 
Yu, 
and 
Q. 
Jiang. 
ML 
estimation 
for 
factor 
analysis: 
EM 
or 
non-EM? 
Statistics 
and 
Computing, 
18(2):109{123, 
2008. 
[303] 
O. 
Zoeter. 
Monitoring 
non-linear 
and 
switching 
dynamical 
systems. 
PhD 
thesis, 
Radboud 
University 
Nijmegen, 
2005. 
DRAFT 
March 
9, 
2010 



Index 


1 
- 
of 
- 
M 
coding, 
207 
N-max-product, 
74 
-expansion, 
536 
-recursion, 
417 
-recursion, 
418 
-recursion, 
419 


absorbing 
state, 
75 
absorption, 
87 


inuence 
diagram, 
117 
acceptance 
function, 
500 
active 
learning, 
253 
adjacency 
matrix, 
21, 
385 
algebraic 
Riccati 
equation, 
448 
ancestor, 
19 
ancestral 
ordering, 
494 
ancestral 
sampling, 
494 
antifreeze, 
231 
approximate 
inference, 
103, 
379, 
515 


belief 
propagation, 
529, 
530 
Bethe 
free 
energy, 
528 
double 
integration 
bound, 
539 
expectation 
propagation, 
530 
graph 
cut, 
535 
Laplace 
approximation, 
515 
switching 
linear 
dynamical 
system, 
458 
variational 
approach, 
516 
variational 
inference, 
519 


AR 
model, 
see 
auto-regressive 
model 
Articial 
Life, 
482 
asychronous 
updating, 
521 
asymmetry, 
113 
auto-regressive 
model, 
438 


switching, 
452 


time-varying, 
440 
automatic 
relevance 
determination, 
351 
auxiliary 
variable 
sampling, 
501 
average, 
139 


backtracking, 
71 
bag 
of 
words, 
208, 
287 
batch 
update, 
322 
Baum-Welch, 
423 


Bayes 
Information 
Criterion, 
247 


Bayes’ 
factor, 
184, 
241 
model 
selection, 
241 
theorem, 
8 


Bayes’ 
rule, 
see 
Bayes’ 
theorem 


Bayesian 
decision 
theory, 
259 
hypothesis 
testing, 
241, 
263 
image 
denoising, 
519 
linear 
model, 
333 
mixture 
model, 
373 
model 
selection, 
241 


Occam's 
razor, 
244 


outcome 
analysis, 
263 
Bayesian 
Dirichlet 
score, 
184 
Bayesian 
linear 
model, 
348 
BD 
score, 
184 


BDeu 
score, 
185 
BDeu 
score, 
185 
belief 
network 


asbestos-smoking-cancer, 
177 
cascade, 
32 
chest 
clinic, 
45 
divorcing 
parents, 
43 
dynamic, 
430 
noisy 
AND 
gate, 
44 
noisy 
logic 
gate, 
44 
noisy 
OR 
gate, 
44 
sigmoid, 
239 
structure 
learning, 
180 
training 


Bayesian, 
174 
belief 
propagation, 
66, 
529 


loopy, 
526 
belief 
revision, 
73, 
see 
max-product 
Bellman's 
equation, 
121 
Bessel 
function, 
354 
beta 


distribution, 
145 
function, 
145, 
168 
Bethe 
free 
energy, 
528 


579 



INDEX 
INDEX 


bias, 
142 


unbiased 
estimator, 
142 
bigram, 
422 
binary 
entropy, 
521 
bioinformatics, 
431 
black 
and 
white 
sampling, 
512 
black-box, 
261 
Blahut-Arimoto 
algorithm, 
526 
Boltzmann 
machine, 
52, 
60, 
189, 
539 


restricted, 
60 
bond 
propagation, 
81 
Bonferroni 
inequality, 
17 
Boolean 
network, 
482 
Bradly-Terry-Luce 
model, 
405 
bucket 
elimination, 
78 
burn 
in, 
498 


calculus, 
551 
canonical 
correlation 
analysis, 
300 


constrained 
factor 
analysis, 
398 
canonical 
variates, 
305 
causal 
consistency, 
113 
causality, 
39, 
113 


do 
calculus, 
42 
inuence 
diagrams, 
42 
post 
intervention 
distribution, 
42 


CCA 


see 
canonical 
correlation 
analysis, 
300 
centering, 
152 
chain 
graph, 
55 


chain 
component, 
55 
chain 
rule, 
553 
chain 
structure, 
71 
changepoint 
model, 
468 
checkerboard, 
512 
chest 
clinic, 
45 


missing 
data, 
237 


with 
decisions, 
131 
children, 
see 
directed 
acyclic 
graph 
Cholesky, 
306 
chord, 
95 
chordal, 
95 
Chow-Liu 
tree, 
210 
classication, 
252, 
324, 
358 


Bayesian, 
340 
boundary, 
205 
error 
analysis, 
263 
linear 
parameter 
model, 
319 
multiple 
classes, 
324 
performance, 
263 


random 
guessing, 
270 
softmax, 
324 


clique, 
20 
decomposition, 
383 
graph, 
86 


matrix, 
22, 
383 
cliquo, 
22 
Cluster 
Variation 
method, 
529 
clustering, 
253 
collaborative 
ltering, 
291 
collider, 
see 
directed 
acyclic 
graph 
commute, 
546 
compatibility 
function, 
511 
competition 
model 


Bradly-Terry-Luce, 
405 
Elo, 
406 
TrueSkill, 
406 


competition 
models, 
405 
concave 
function, 
554 
condindep.m, 
59 
conditional 
entropy, 
524 
conditional 
likelihood, 
200 
conditional 
mutual 
information, 
212 
conditional 
probability, 
4 
conditional 
random 
eld, 
193, 
428 
conditioning, 
151 


loop 
cut 
set, 
80 


conjugate 
distribution, 
168 
exponential 
family, 
156 
Gaussian, 
154, 
155 
prior, 
156 


conjugate 
gradient, 
324 
conjugate 
gradients 
algorithm, 
560 
conjugate 
vector, 
558 
conjugate 
vectors 
algorithm, 
559 
connected 
components, 
20 
connected 
graph, 
20 
consistent, 
91 
consistent 
estimator, 
197 
convex 
function, 
554 
correction 
smoother, 
419, 
446 
correlation 


matrix, 
140 
cosine 
similarity, 
289 
coupled 
HMM, 
430 
covariance, 
140 


matrix, 
140 


covariance 
function, 
318, 
349, 
351 
-exponential, 
354 
construction, 
352 
Gibbs, 
355 
isotropic, 
354 
Matern, 
354 
Mercer 
kernel, 
356 
neural 
network, 
355 
non-stationary, 
355 
Ornstein-Uhlenbeck, 
354, 
356 
periodic, 
354 
rational 
quadratic, 
354 
smoothness, 
356 


DRAFT 
March 
9, 
2010 



INDEX 
INDEX 


squared 
exponential, 
354, 
357 
stationary, 
353 
CPT, 
see 
conditional 
probability 
table 
CRF, 
see 
conditional 
random 
eld 
critical 
point, 
555 
cross-validation, 
256 
cumulant, 
492 
curse 
of 
dimensionality, 
124, 
316 
cut 
set 
conditioning, 
79 


D-map, 
see 
dependence 
map 
DAG, 
see 
directed 
acyclic 
graph 
data 
anomaly 
detection, 
253 
catagorical, 
263 
dyadic, 
382 
handwritten 
digits, 
323 
labelled, 
251 
monadic, 
383 
numerical, 
263 
ordinal, 
263 
unlabelled, 
252 
data 
compression, 
376 
vector 
quantisation, 
376 
decision 
boundary, 
319 
decision 
function, 
254 
decision 
theory, 
107, 
168, 
259 
decision 
tree, 
108 
decomposable, 
95 
degree, 
22 
degree 
of 
belief, 
8 
delta 
function, 
see 
Dirac 
delta 
function 
Kronecker, 
142 
density 
estimation, 
365 
Parzen 
estimator, 
375 
dependence 
map, 
57 
descendant, 
19 
design 
matrix, 
339, 
348 
detailed 
balance, 
499 
determinant, 
547 
deterministic 
latent 
variable 
model, 
482 
dierentiation, 
552 
digamma 
function, 
160 
digit 
data, 
283 
Dijkstra's 
algorithm, 
76 
dimension 
reduction 
linear, 
279, 
282 
supervised, 
303 
dimensionality 
reduction 
linear, 
285 
non-linear, 
285 
Dirac 
delta 
function, 
141, 
142, 
357 
directed 
acyclic 
graph, 
32 
ancestor, 
19 


ancestral 
order, 
21 
cascade, 
32 
children, 
19 
collider, 
34 
descendant, 
19 
family, 
19 
immorality, 
38 
Markov 
Blanket, 
19 
moralisation, 
54 
parents, 
19 
direction 
bias, 
428 
directional 
derivative, 
553 
Dirichlet 
distribution, 
147 
Dirichlet 
process 
mixture 
models, 
379 
discount 
factor, 
122 
discriminative 
approach, 
260 
training, 
260 
discriminative 
approach, 
259 
discriminative 
training, 
425 
dissimilarity 
function, 
273 
distributed 
computation, 
475 
distribution 
Bernoulli, 
143, 
368 
beta, 
145, 
148, 
160, 
176, 
243 
binomial, 
143 
Categorical, 
143 
change 
of 
variables, 
159 
conjugate, 
154 
continuous, 
4 
density, 
4 
Dirichlet, 
147, 
161, 
178, 
183, 
379, 
380 
discrete, 
3 
divergence, 
157 
double 
exponential, 
146 
empirical, 
141, 
255 
average, 
139 
expectation, 
139 
exponential, 
144 
exponential 
family, 
155 
canonical, 
156 
gamma, 
373 
mode, 
161 
Gauss-gamma, 
155, 
162 
Gauss-inverse-gamma, 
154 
Gaussian 
canonical 
exponential 
form, 
156 
conditioning, 
151 
conjugate, 
154, 
155 
entropy, 
150 
isotropic, 
149 
mixture, 
163 
multivariate, 
148 
normalisation, 
158, 
159 


DRAFT 
March 
9, 
2010 



INDEX 
INDEX 


partitioned, 
150 
propagation, 
152 
system 
reversal, 
151 
univariate, 
146 


inverse 
gamma, 
145 
inverse 
Wishart, 
155 
joint, 
4 
kurtosis, 
141 
Laplace, 
146 
marginal, 
4 
mode, 
139 
multinomial, 
144 
normal, 
146 
Poisson, 
144, 
163 
Polya, 
378 
scaled 
mixture, 
147 
skewness, 
141 
Student's 
t, 
146 
uniform, 
144, 
157 
Wishart, 
373 


domain, 
3 
double 
integration 
bound, 
539 
dual 
parameters, 
317 
dual 
representation, 
316 
dyadic 
data, 
382 
dynamic 
Bayesian 
network, 
430 
dynamic 
synapses, 
486 
dynamical 
system 


linear, 
437 
non-linear, 
482 
dynamics 
reversal, 
446 


edge 
list, 
21 
ecient 
IPF, 
191 
eigen 


decomposition, 
149, 
282, 
550 
equation, 
281 
function, 
551 
problem, 
300 
spectrum, 
356 
value, 
281, 
548 


Elo 
model, 
405 
emission 
distribution, 
416 
emission 
matrix, 
442 
empirical 


independence, 
182 
empirical 
distribution, 
141, 
170, 
255 
empirical 
risk, 
255 


penalised, 
256 
empirical 
risk 
minimisation, 
256 
energy, 
221 
entropy, 
99, 
157, 
163, 
221, 
524 


dierential, 
157, 
164 
Gaussian, 
150 
EP, 
see 
expectation 
propagation 


error 
function, 
319 
estimator 
consistent, 
197 


evidence, 
see 
marginal 
likelihood 
hard, 
29 
likelihood, 
30 
soft, 
29 
uncertain, 
29 
virtual, 
30 


Evidence 
Procedure, 
335 
exact 
sampling, 
494 
expectation, 
see 
average 
expectation 
correction, 
462, 
463 
expectation 
maximisation, 
126, 
135, 
220, 
293, 
336, 


423, 
450 
algorithm, 
220, 
222 
antifreeze, 
231 
belief 
networks, 
224 
E-step, 
221 
energy, 
221 
entropy, 
221 
failure 
case, 
230 
generalised, 
126 
intractable 
energy, 
229 
M-step, 
221 
Markov 
decision 
process, 
126 
mixture 
model, 
366 
partial 
E-step, 
229 
partial 
M-step, 
229 
variational 
Bayes, 
233 
Viterbi 
training, 
229 


expectation 
propagation, 
530 


exponential 
family, 
155 
canonical 
form, 
156 
conjugate, 
156 


extended 
observability 
matrix, 
451 


face 
model, 
395 


factor 
analysis, 
391 
factor 
rotation, 
390 
probabilistic 
PCA, 
397 
training 


EM, 
394 


SVD, 
392 
factor 
graph, 
58 
factor 
loading, 
389 
family, 
see 
directed 
acyclic 
graph 
feature 
map, 
298 
ltering, 
417 
nite 
dimensional 
Gaussian 
Process, 
349 
Fisher 
information, 
161 
Fisher's 
linear 
discriminant, 
303 
Floyd-Warshall-Roy 
algorithm, 
77 
forward 
sampling, 
494 
Forward-Backward, 
418 


DRAFT 
March 
9, 
2010 



INDEX 
INDEX 


Forward-Sampling-Resampling, 
510 


gamma 
digamma, 
160 
distribution, 
145 
function, 
146, 
160 


Gaussian 
canonical 
representation, 
149 
distribution, 
146 
moment 
representation, 
149, 
444 
sub, 
141 
super, 
141 


Gaussian 
mixture 
model, 
370 
Bayesian, 
373 
collapsing 
the 
mixture, 
461 
EM 
algorithm, 
370 
innite 
problems, 
373 
k-means, 
375 
Parzen 
estimator, 
375 
symmetry 
breaking, 
373 


Gaussian 
process, 
347 
classication, 
358 
Laplace 
approximation, 
359 
multiple 
classes, 
362 
regression, 
350 
smoothness, 
356 
weight 
space 
view, 
348 


Gaussian 
sum 
ltering, 
458 
Gaussian 
sum 
smoothing, 
462 
generalisation, 
251, 
255 
generalised 
pseudo 
Bayes, 
466 
generative 


approach, 
259 
model, 
259 
training, 
259 


generative 
approach, 
259 
Gibbs 
sampling, 
495 
Glicko, 
406 
GMM, 
see 
Gaussian 
mixture 
model 
Google, 
413 
gradient, 
552 


descent, 
555 


natural, 
556 
Gram 
matrix, 
317 
Gram-Schmidt 
procedure, 
558 
Gramm 
matrix, 
299 
graph, 
19 


adjacency 
matrix, 
21 
chain, 
55 
chain 
structured, 
71 
chordal, 
95 
clique, 
20, 
22, 
86 
clique 
matrix, 
22 
cliquo, 
22 
connected, 
20 


cut, 
535 
decomposable, 
95 
descendant, 
22 
directed, 
19 
disconnected, 
99 
edge 
list, 
21 
factor, 
58 
loopy, 
20 
multiply-connected, 
20, 
93 
neighbour, 
20 
path, 
19 
separation, 
54 
set 
chain, 
102 
singly-connected, 
20 
skeleton, 
38 
spanning 
tree, 
21 
tree, 
20 
triangulated, 
95 
undirected, 
19, 
20 
vertex 


degree, 
22 
graph 
cut 
algorithm, 
535 
graph 
partitioning, 
381 
Gull-MacKay 
iteration, 
338 


Hamilton-Jacobi 
equation, 
122 
Hamiltonian 
dynamics, 
503 
Hammersley 
Cliord 
theorem, 
53 
handwritten 
digits, 
323 
Hankel 
matrix, 
451 
harmonium, 
see 
restricted 
Boltzmann 
machine 
Heaviside 
step 
function, 
384 
Hebb, 
476 
Hebb 
rule, 
476 
hedge 
fund, 
247 
Hermitian, 
545 
Hessian, 
324, 
553 
hidden 
Markov 
model, 
99, 
230, 
416 


a 
recursion, 
417 
ß 
recursion, 
418 
coupled, 
430 
direction 
bias, 
428 
discriminative 
training, 
425 
duration 
model, 
427 
entropy, 
99 
ltering, 
417 
input-output, 
427 
likelihood, 
419 
most 
likely 
state 
(MAP), 
420 
pairwise 
marginal, 
419 
Rauch 
Tung 
Striebel 
smoother, 
419 
smoothing 


parallel, 
418 
sequential, 
418 
viterbi, 
74 


DRAFT 
March 
9, 
2010 



INDEX 
INDEX 


Viterbi 
algorithm, 
420 
hidden 
variables, 
217 
HMM, 
see 
hidden 
Markov 
model 
Hopeld 
network, 
475 
augmented, 
483 
capacity, 
479 
Hebb 
rule, 
477 
heteroassociative, 
484 
maximum 
likelihood, 
478 
perceptron, 
479 
pseudo 
inverse 
rule, 
477 
sequence 
learning, 
476 
hybrid 
Monte 
Carlo, 
502 
hyper 
Markov, 
196 
hyper 
tree, 
98 
hyperparameter, 
156, 
176, 
335 
hyperplane, 
545 
hypothesis 
testing, 
241 
Bayesian 
error 
analysis, 
263 


I-map, 
see 
independence 
map 
ICA, 
399 
identically 
and 
independently 
distributed, 
165 
identiability, 
449 
identity 
matrix, 
546 
IID, 
see 
identically 
and 
independently 
distributed 
IM 
algorithm, 
see 
information-maximisation 
algorithm 
immorality, 
38 
importance 
distribution, 
506 
sampling, 
506 
particle 
lter, 
509 
resampling, 
508 
sequential, 
508 
weight, 
507 
incidence 
matrix, 
22 
independence 
Bayesian, 
183 
conditional, 
26, 
33 
empirical, 
182 
map, 
57 
Markov 
equivalent, 
38 
mutual 
information, 
182 
naive 
Bayes, 
203 
parameter, 
174 
perfect 
map, 
57 
independent 
components 
analysis, 
364, 
400, 
402 
indicator 
function, 
11 
indicator 
model, 
378 
induced 
representation, 
94 
inference 
bond 
propagation, 
81 
bucket 
elimination, 
78 
causal, 
39 


cut 
set 
conditioning, 
79 
HMM, 
417 
linear 
dynamical 
system, 
443 
MAP, 
81 
marginal, 
63 
Markov 
decision 
process, 
124, 
126 
max-product, 
71 
message 
passing, 
63 
mixed, 
77 
MPM, 
81 
sum-product 
algorithm, 
68 
transfer 
matrix, 
65 
variable 
elimination, 
63 
inuence 
diagram, 
111 
absorption, 
117 
asymmetry, 
113 
causal 
consistency, 
113 
chest 
clinic, 
131 
decision 
potential, 
116 
fundamental 
link, 
112 
information 
link, 
111 
junction 
tree, 
116 
no 
forgetting 
principle, 
112 
partial 
order, 
112 
probability 
potential, 
116 
solving, 
115 
utility, 
111 
utility 
potential, 
116 
information 
link, 
111 
information 
maximisation, 
526 
information 
retrieval, 
288, 
413 
information-maximisation 
algorithm, 
525 
innovation 
noise, 
438 
input-output 
HMM, 
427 
inverse 
modus 
ponens, 
12 
IPF, 
188, 
see 
iterative 
proportional 
tting 
ecient, 
191 
Ising 
model, 
54, 
see 
Markov 
network, 
81 
approximate 
inference, 
519 
isotropic, 
149, 
372 
isotropic 
covariance 
functions, 
353 
item 
response 
theory, 
404 
Iterated 
Conditional 
Modes, 
534 
iterative 
proportional 
tting, 
188 
iterative 
scaling, 
192 


Jerey's 
rule, 
29 
Jensen's 
inequality, 
540, 
554 
Joseph's 
symmetrized 
update, 
446 
jump 
Markov 
model, 
see 
switching 
linear 
dynamical 
system 
junction 
tree, 
88, 
92 
absorption, 
87 
algorithm, 
85, 
97 
clique 
graph, 
86 


DRAFT 
March 
9, 
2010 



INDEX 
INDEX 


computational 
complexity, 
98 
conditional 
marginal, 
100 
consistent, 
91 
hyper 
tree, 
98 
inuence 
diagram, 
116 
marginal 
likelihood, 
99 
most 
likely 
state, 
101 
normalisation 
constant, 
99 
potential, 
86 
running 
intersection 
property, 
89 
separator, 
86 
strong, 
117 
strong 
triangulation, 
117 
tree 
width, 
98 
triangulation, 
94 


k-means, 
375 
Kalman 
lter, 
442 
Kalman 
gain, 
445 
KD-tree, 
274 
kernel, 
316, 
317, 
see 
covariance 
function 
classier, 
324 
kidnapped 
robot, 
421 
Kikuchi, 
529 
KL 
divergence, 
see 
Kullback-Leibler 
divergence 
KNN, 
see 
nearest 
neighbour 
Kronecker 
delta, 
142, 
546 
Kullback-Leibler 
divergence, 
157, 
220, 
517 
kurtosis, 
141 


labelled 
data, 
251 
Lagrange 
multiplier, 
562 
Lagrangian, 
563 
Laplace 
approximation, 
341, 
359, 
515 
latent 
ability 
model, 
403 
latent 
Dirichlet 
allocation, 
380 
latent 
linear 
model, 
389 
latent 
semantic 
analysis, 
287 
latent 
topic, 
287 
latent 
variable, 
217 
deterministic, 
482 
model, 
217 
lattice 
model, 
54 
LDA 
regularised, 
309 
LDS, 
see 
linear 
dynamical 
system 
leaky 
integrate 
and 
re 
model, 
486 
Leapfrog 
discretisation, 
503 
learning 
active, 
253 
anomaly 
detection, 
253 
Bayesian, 
166 
belief 
network, 
171 
belief 
networks 
EM, 
224 


Dirichlet 
prior, 
178 
inference, 
165 
nearest 
neighbour, 
273 
online, 
253 
query, 
253 
reinforcement, 
254 
semi-supervised, 
254, 
377 
sequences, 
423, 
449, 
476 
sequential, 
253 
structure, 
180 
supervised, 
251 
unsupervised, 
252, 
389 
learning 
rate, 
322 
likelihood, 
10, 
99, 
419, 
447, 
461 
bound, 
220 
marginal, 
10, 
69 
model, 
10, 
180 
approximate, 
246 
pseudo, 
196 
likelihood 
decomposable, 
180 
line 
search, 
557 
linear 
algebra, 
543 
linear 
dimension 
reduction, 
279, 
282 
canonical 
correlation 
analysis, 
300 
latent 
semantic 
analysis, 
287 
non-negative 
matrix 
factorisation, 
295 
probabilistic 
latent 
semantic 
analysis, 
292 
supervised, 
303 
unsupervised, 
279 
Linear 
Discriminant 
Analysis, 
303 
linear 
discriminant 
analysis, 
303 
as 
regression, 
308 
penalised, 
308 
regularised, 
308 
linear 
dynamical 
system, 
151, 
442 
cross 
moment, 
447 
dynamics 
reversal, 
446 
ltering, 
444 
identiability, 
449 
inference, 
443 
learning, 
449 
likelihood, 
447 
most 
likely 
state, 
448 
numerical 
stability, 
443 
Riccati 
equations, 
448 
smoothing, 
446 
subspace 
method, 
451 
switching, 
457 
symmetrising 
updates, 
446 
linear 
Gaussian 
state 
space 
model, 
442 
linear 
model, 
311 
Bayesian, 
333 
classication, 
319 
factor 
analysis, 
391 
latent, 
389 


DRAFT 
March 
9, 
2010 



INDEX 
INDEX 


regression, 
312 
linear 
parameter 
model, 
312 


Bayesian, 
334 
linear 
perceptron, 
321 
linear 
separability, 
320 
linear 
transformation, 
546 
linearly 
independent, 
544 
linearly 
separable, 
320 
Linsker's 
as-if-Gaussian 
approximation, 
526 
localisation, 
420 
logic 


Aristotle, 
12 
logistic 
regression, 
319 
logistic 
sigmoid, 
319 
logit, 
319 
loop 
cut 
set, 
80 
loopy, 
20 
loss 
function, 
254, 
255 


zero-one, 
254 
loss 
matrix, 
255 
Luenberger 
expanding 
subspace 
theorem, 
559 


Mahalanobis 
distance, 
273, 
302 


manifold 
linear, 
279 
low 
dimensional, 
279 


MAP, 
see 
most 
probable 
a 
posteriori 
MAR, 
see 
missing 
at 
random 
margin, 
325, 
326 


soft, 
327 
marginal, 
4 
generalised, 
117 
marginal 
likelihood, 
10, 
69, 
99, 
219, 
335, 
351 


approximate, 
337, 
340, 
342, 
361 
marginalisation, 
4 
Markov 


chain, 
238, 
411 
rst 
order, 
438 
stationary 
distribution, 
412 


equivalent, 
38 
global, 
52 
hyper, 
196 
local, 
51 
model, 
411 
pairwise, 
51 
random 
eld, 
192 


approximation, 
519 
Markov 
blanket, 
see 
directed 
acyclic 
graph, 
496 
Markov 
chain, 
63, 
129, 
499 


absorbing 
state, 
75 
detailed 
balance, 
499 
PageRank, 
412 


Markov 
chain 
Monte 
Carlo, 
499 
auxiliary 
variable, 
501 
hybrid 
Monte 
Carlo, 
502 


slice 
sampling, 
505 


Swendson-Wang, 
504 
Gibbs 
sampling, 
496 
Metropolis-Hastings, 
499 
proposal 
distribution, 
500 
structured 
Gibbs 
sampling, 
497 


Markov 
decision 
process, 
120, 
133 
Bellman's 
equation, 
121 
discount 
factor, 
122 
non-stationary 
policy, 
127 
partially 
observable, 
129 
planning, 
124 
policy 
iteration, 
123 
reinforcement 
learning, 
130 
stationary, 
125 
stationary 
deterministic 
policy, 
128 
temporally 
unbounded, 
122 
value 
iteration, 
122 
variational 
inference, 
126 


Markov 
equivalence, 
38 


Markov 
network, 
50 
Boltzmann 
machine, 
52 
continuous-state 
temporal, 
437 
discrete-state 
temporal, 
411 
Gibbs 
distribution, 
50 
Gibbs 
network, 
52 
Hammersley 
Cliord 
theorem, 
53 
pairwise, 
50 
potential, 
50 


Markov 
random 
eld, 
53, 
540, 
542 
alpha-expansion, 
536 
attractive 
binary, 
534 
graph 
cut, 
535 
map, 
533 
Potts 
model, 
536 


matrix, 
545 
adjacency, 
22, 
385 
Cholesky, 
306 
clique, 
22 
Gramm, 
299 
Hankel, 
451 
incidence, 
22 
inversion, 
548 
inversion 
lemma, 
551 
orthogonal, 
547 
positive 
denite, 
550 
pseudo 
inverse, 
548 
rank, 
547 


matrix 
factorisation, 
295 
max-product, 
71 


N 
most 
probable 
states, 
74 
max-sum, 
75 
maximum 
cardinality 
checking, 
96 
maximum 
likelihood, 
152, 
169, 
170, 
321 


belief 
network, 
171 


DRAFT 
March 
9, 
2010 



INDEX 
INDEX 


Chow-Liu 
tree, 
212 
counting, 
171 
empirical 
distribution, 
170 
factor 
analysis, 
391 
Gaussian, 
153 
gradient 
optimisation, 
236 
Markov 
network, 
185 
ML-II, 
236 
naive 
Bayes, 
204 
properties, 
196 


Maximum 
Likelihood 


Hopeld 
network, 
478 
MCMC, 
see 
Markov 
chain 
Monte 
Carlo 
MDP, 
see 
Markov 
decision 
process 
mean 
eld 
theory, 
522 


asynchronous 
updating, 
522 
Mercer 
kernel, 
356 
message 


passing, 
88 


schedule, 
68, 
88 
message 
passing, 
63 
Metropolis-Hastings 
acceptance 
function, 
500 
Metropolis-Hastings 
sampling, 
499 
minimum 
clique 
cover, 
384 
missing 
at 
random, 
218 


completely, 
219 
missing 
data, 
217 
mixed 
inference, 
77 
mixed 
membership 
model, 
380 
mixing 
matrix, 
400 
mixture 


Gaussian, 
163 


mixture 
model, 
365 
Bernoulli 
product, 
368 
Dirichlet 
process 
mixture, 
379 
expectation 
maximisation, 
366 
factor 
analysis, 
394 
Gaussian, 
370 
indicator 
approach, 
378 
Markov 
chain, 
414 
PCA, 
394 


mixture 
of 
experts, 
377 
MN, 
see 
Markov 
network 
mode, 
139 
model 


auto-regressive, 
438 
changepoint, 
468 
deterministic 
latent 
variable, 
482 
faces, 
395 
leaky 
integrate 
and 
re, 
486 
linear, 
311 
mixed 
membership, 
380 
mixture, 
365 
Rasch, 
403 


model 
selection, 
241 


approximate, 
246 
moment 
generating 
function, 
159 
moment 
representation, 
444 
momentum, 
556 
monadic 
data, 
383 
money 


nancial 
prediction, 
247 


loadsa, 
247 
moralisation, 
54, 
92 
most 
probable 
a 
posteriori, 
10 
most 
probable 
path 


multiple-source 
multiple-sink, 
76 
most 
probable 
state 


N 
most 
probable, 
73 
MRF, 
see 
Markov 
random 
eld 
multiply-connected, 
20 
multiply-connected-distributions, 
93 
multpots.m, 
16 
mutual 
information, 
182, 
211, 
524 


approximation, 
524 
conditional, 
182 
maximisation, 
525 


naive 
Bayes, 
203, 
368 
Bayesian, 
208 
tree 
augmented, 
210 


Naive 
Mean 
Field, 
522 
naive 
mean 
eld 
theory, 
520 
natural 
gradient, 
556 
nearest 
neighbour, 
273 


probabilistic, 
275 
network 
ow, 
540 
network 
modelling, 
297 
neural 
computation, 
475 
neural 
network, 
332, 
484 


depression, 
486 
dynamic 
synapses, 
486 
leaky 
integrate 
and 
re, 
486 


Newton 
update, 
324 
Newton's 
method, 
561 
no 
forgetting 
principle, 
112 
node 


extremal, 
73 


simplical, 
73 
non-negative 
matrix 
factorisation, 
295 
normal 
distribution, 
146 
normal 
equations, 
313 
normalised 
importance 
weights, 
507 


observed 
linear 
dynamical 
system, 
437 
Occam's 
razor, 
244 
One 
of 
m 
encoding, 
263 
online 
learning, 
253 
optimisation, 
174, 
555 


Broyden-Fletcher-Goldfarb-Shanno, 
562 


DRAFT 
March 
9, 
2010 



INDEX 
INDEX 


conjugate 
gradients 
algorithm, 
560 
conjugate 
vectors 
algorithm, 
559 
constrained 
optimisation, 
562 
critical 
point, 
555 
gradient 
descent, 
555 
Luenberger 
expanding 
subspace 
theorem, 
559 
Newton's 
method, 
561 
quasi 
Newton 
method, 
561 
ordinary 
least 
squares, 
311 
Ornstein-Uhlenbeck, 
318 
orthogonal, 
543 
orthogonal 
least 
squares, 
311 
orthonormal, 
544 
outcome 
analysis, 
264 
outlier, 
329 
over-complete 
representation, 
292 
over-complete 
representations, 
292 
overcounting, 
85 
overtting, 
194, 
245, 
350 


PageRank, 
412 
pairwise 
comparison 
models, 
405 
pairwise 
Markov 
network, 
50 
parents, 
see 
directed 
acyclic 
graph 
part-of-speech 
tagging, 
431 
Partial 
Least 
Squares, 
399 
partial 
order, 
112 
partially 
observable 
MDP, 
129 
particle 
lter, 
509 
partition 
function, 
50, 
539 
partitioned 
matrix 
inversion, 
159 
Parzen 
estimator, 
275, 
375 
path, 
508 
blocked, 
35 
PC 
algorithm, 
181 
PCA, 
see 
Principal 
Components 
Analysis 
perceptron, 
321 
logistic 
regression, 
321 
perfect 
elimination 
order, 
97 
perfect 
map, 
see 
independence 
perfect 
sampling, 
494 
planning, 
124 
plant 
monitoring, 
253 
plate, 
166 
Poisson 
distribution, 
144, 
163 
policy, 
122 
iteration, 
123 
non-stationary, 
127 
stationary 
deterministic, 
128 
Polya 
distribution, 
378 
POMDP, 
see 
partially 
observable 
MDP 
positive 
denite 
kernel, 
318 
matrix, 
349 


parameterisation, 
386 
posterior, 
10, 
165 
Dirichlet, 
178 
potential, 
50 
Potts 
model, 
536 
precision, 
148, 
155, 
334 
prediction 
auto-regression, 
438 
nancial, 
247 
non-parametric, 
347 
parameteric, 
347 
predictive 
variance, 
334 
predictor-corrector, 
417 
Principal 
Components 
Analysis, 
279, 
285 
algorithm, 
282 
high 
dimensional 
data, 
285 
kernel, 
298 
latent 
semantic 
analysis, 
287 
missing 
data, 
289 
probabilistic, 
397 
principal 
directions, 
282 
printer 
nightmare, 
198 
missing 
data, 
237 
prior, 
10, 
165 
probabilistic 
latent 
semantic 
analysis, 
292 
conditional, 
295 
EM 
algorithm, 
293 
probabilistic 
PCA, 
397 
probability 
conditional, 
4 
function, 
173 
density, 
4 
frequentist, 
8 
posterior, 
10 
potential, 
116 
prior, 
10 
subjective, 
8 
probit, 
319 
probit 
regression, 
319 
projection, 
544 
proposal 
distribution, 
500 
Pseudo 
Inverse, 
476 
pseudo 
inverse 
Hopeld 
network, 
477 
pseudo 
likelihood, 
196 


quadratic 
form, 
550 
quadratic 
programming, 
326 
query 
learning, 
253 
questionnaire 
analysis, 
403 


radial 
basis 
functions, 
315 
Raleigh 
quotient, 
306 
Random 
Boolean 
networks, 
482 
Rasch 
model, 
403 


DRAFT 
March 
9, 
2010 



INDEX 
INDEX 


Bayesian, 
404 
Rauch-Tung-Striebel, 
419 
reabsorption, 
102 
region 
graphs, 
529 
regresion 
linear 
parameter 
model, 
312 
regression, 
252, 
348 
logisitic, 
319 
regularisation, 
245, 
256, 
314, 
324 
reinforcement 
learning, 
130, 
254 
Relevance 
Vector 
Machine, 
339 
relevance 
vector 
machine, 
339, 
344 
reparameterisation, 
85 
representation 
dual, 
316 
over-complete, 
292 
sparse, 
292 
under-complete, 
292 
resampling, 
507 
reset 
model, 
468 
residuals, 
313 
resolution, 
12 
responsibility, 
372 
restricted 
Boltzmann 
machine, 
60 
Riccati 
equation, 
448 
risk, 
255 
robust 
classication, 
329 
Rose-Tarjan-Lueker 
elimination, 
96 
running 
intersection 
property, 
89, 
91 


sample 
mean, 
141 
variance, 
141 
sampling, 
379 
ancestral, 
494 
Gibbs, 
495 
importance, 
506 
multi-variate, 
493 
particle 
lter, 
509 
univariate, 
492 
Sampling 
Importance 
Resampling, 
507 
scalar 
product, 
543 
scaled 
mixture, 
147 
search 
engine, 
413 
self 
localisation 
and 
mapping, 
422 
semi-supervised 
learning, 
254 
lower 
dimensional 
representations, 
261 
separator, 
86 
sequential 
importance 
sampling, 
508 
sequential 
minimal 
optimisation, 
329 
set 
chain, 
102 
shortest 
path, 
75 
shortest 
weighted 
path, 
76 
sigmoid 
logistic, 
319 


sigmoid 
belief 
network, 
239 
sigmoid 
function 
approximate 
average, 
342 
simple 
path, 
75 
simplical 
nodes, 
96 
Simpson's 
Paradox, 
40 
singly-connected, 
20 
singular, 
548 
Singular 
Value 
Decomposition, 
286, 
550 
thin, 
307 
skeleton, 
38, 
181 
skewness, 
141 
slice 
sampling, 
505 
smoothing, 
418 
softmax 
function, 
324, 
377 
spam 
ltering, 
208 
spanning 
tree, 
21 
sparse 
representation, 
292 
spectrogram, 
441 
speech 
recognition, 
430 
spike 
response 
model, 
484 
squared 
Euclidean 
distance, 
273 
squared 
exponential, 
318 
standard 
deviation, 
140 
standard 
normal 
distribution, 
146 
stationary, 
499 
distribution, 
66 
stationary 
Markov 
chain, 
412 
stationary 
planner, 
125 
stop 
words, 
381 
strong 
Junction 
Tree, 
117 
strong 
triangulation, 
117 
structure 
learning, 
180 
Bayesian, 
184 
network 
scoring, 
184 
PC 
algorithm, 
181 
undirected, 
196 
structured 
Expectation 
Propagation, 
532 
subsampling, 
498 
subspace 
method, 
451 
sum-product, 
526 
sum-product 
algorithm, 
68 
supervised 
learning, 
251 
-semi, 
254 
classication, 
252 
regression, 
252 
support 
vector 
machine, 
325 
chunking, 
329 
training, 
329 
support 
vectors, 
327 
SVD, 
see 
Singular 
Value 
Decomposition 
SVM, 
see 
support 
vector 
machine 
Swendson-Wang 
sampling, 
504 
switching 
AR 
model, 
452 


DRAFT 
March 
9, 
2010 



INDEX 
INDEX 


switching 
Kalman 
lter, 
see 
switching 
linear 
dynamical 
system 


switching 
linear 
dynamical 
system, 
457 
changepoint 
model, 
468 
expectation 
correction, 
462 
ltering, 
458 
Gaussian 
sum 
smoothing, 
462 
generalised 
Pseudo 
Bayes, 
466 
inference 


computational 
complexity, 
458 
likelihood, 
461 
smoothing, 
464 


switching 
linear 
dynamical 
systemcollapsing 
Gaus


sians, 
461 
symmetry 
breaking, 
373 
system 
reversal, 
151 


tagging, 
431 
tall 
matrix, 
292 
Taylor 
expansion, 
555 
term-document 
matrix, 
287 
test 
set, 
251 
text 
analysis, 
296, 
380 


latent 
semantic 
analysis, 
287 
latent 
topic, 
287 
probabilistic 
latent 
semantic 
analysis, 
292 


time-invariant 
LDS, 
448 
Tower 
of 
Hanoi, 
124 
trace-log 
formula, 
551 
train 
set, 
251 
training 


batch, 
322 
discriminative, 
260 
generative, 
259 
generative-discriminative, 
261 
HMM, 
423 
linear 
dynamical 
system, 
449 
online, 
323 


transfer 
matrix, 
65 
transition 
distribution, 
416 
transition 
matrix, 
437, 
442 
tree, 
20, 
64 


Chow-Liu, 
210 
tree 
augmented 
network, 
212 
tree 
width, 
98 
triangulation, 
94, 
96 


check, 
97 
greedy 
elimination, 
96 
maximum 
cardinality, 
96 
strong, 
117 
variable 
elimination, 
96 


TrueSkill, 
406 
two-lter 
smoother, 
418 


uncertainty, 
157 


under-complete 
representation, 
292 
undirected 
graph, 
20 
undirected 
model 


learning 
hidden 
variable, 
237 
latent 
variable, 
237 


uniform 
distribution, 
144 
unit 
vector, 
543 
unlabelled 
data, 
253 
unsupervised 
learning, 
252, 
389 
utility, 
107, 
254 


matrix, 
255 
message, 
116 
money, 
107 
potential, 
116 
zero-one 
loss, 
254 


validation, 
256 


cross, 
256 
value, 
121 
value 
iteration, 
122 
variable 


hidden, 
220 
missing, 
220 
visible, 
220 


variable 
elimination, 
63 
variance, 
140 
variational 
approximation 


factorised, 
520 
structured, 
522, 
524 
variational 
Bayes, 
231 


expectation 
maximisation, 
233 
variational 
inference, 
126 
varimax, 
390 
vector 
algebra, 
543 
vector 
quantisation, 
376 
Viterbi, 
74, 
229 
Viterbi 
algorithm, 
420 
Viterbi 
alignment, 
416 
Voronoi 
tessellation, 
273 


web 
modelling, 
297 
website, 
297 


analysis, 
413 
whitening, 
152, 
163 
Woodbury 
formula, 
551 


XOR 
function, 
321 


zero-one 
loss, 
254, 
329 


DRAFT 
March 
9, 
2010 



